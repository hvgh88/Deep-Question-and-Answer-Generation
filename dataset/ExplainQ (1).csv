question,answer,context
Explain team building,"Team building is a collective term for various types of activities used to enhance social relations and define roles within teams, often involving collaborative tasks.It is distinct from team training, which is designed by a combine of business managers, learning and developmentOD (Internal or external) and an HR Business Partner (if the role exists) to improve the efficiency, rather than interpersonal relations.Over time, these activities are intended to improve performance in a team-based environment.A 2008 meta-analysis found that team-development activities, including team building and team training, improve both a teams objective performance and that teams subjective supervisory ratings.","Team building is a collective term for various types of activities used to enhance social relations and define roles within teams, often involving collaborative tasks. It is distinct from team training, which is designed by a combine of business managers, learning and development/OD (Internal or external) and an HR Business Partner (if the role exists) to improve the efficiency, rather than interpersonal relations.

Many team-building exercises aim to expose and address interpersonal problems within the group.Over time, these activities are intended to improve performance in a team-based environment. Team building is one of the foundations of organizational development that can be applied to groups such as sports teams, school classes, military units or flight crews. The formal definition of team-building includes:

aligning around goals
building effective working relationships
reducing team members' role ambiguity
finding solutions to team problemsTeam building is one of the most widely used group-development activities in organizations. A common strategy is to have a ""team-building retreat"" or ""corporate love-in,"" where team members try to address underlying concerns and build trust by engaging in activities that are not part of what they ordinarily do as a team.Of all organizational activities, one study found team-development to have the strongest effect (versus financial measures) for improving organizational performance. A 2008 meta-analysis found that team-development activities, including team building and team training, improve both a team's objective performance and that team's subjective supervisory ratings. Team building can also be achieved by targeted personal self-disclosure activities."
"Explain why, the arithmetic mean should not be used to average normalized execution times","In contrast to arithmetic means, geometric means of normalized execution times are consistent no matter which machine is the reference. Hence,  the arithmetic mean should not be used to average normalized execution times."," i Time i - -------------Time j Normalized Execution Time and the Pros and Cons of Geometric Means A second approach to unequal mixture of programs in the workload is to normalize execution times to a reference machine and then take the average of the normalized execution times. This is the approach used by the SPEC benchmarks, where a base time on a SPARCstation is used for reference. This measurement gives a warm fuzzy feeling, because it suggests that performance of new programs can be predicted by simply multiplying this number times its performance on the reference machine. Average normalized execution time can be expressed as either an arithmetic or geometric mean. The formula for the geometric mean is n Execution time ratioi where Execution time ratioi is the execution time, normalized to the reference machine, for the ith program of a total of n in the workload. Geometric means also have a nice property for two samples Xi and Yi: Xi Geometric mean ( X i ) -------------------------------------------------= Geometric mean ----- Geometric mean ( Y i ) Y i As a result, taking either the ratio of the means or the mean of the ratios yields the same result. In contrast to arithmetic means, geometric means of normalized execution times are consistent no matter which machine is the reference. Hence, the arithmetic mean should not be used to average normalized execution times. Figure 1.17 shows some variations using both arithmetic and geometric means of normalized times. Because the weightings in weighted arithmetic means are set proportionate to execution times on a given machine, as in Figure 1.16, they are influenced not only by frequency of use in the workload, but also by the peculiarities of a particular machine and the size of program input. The geometric mean of normalized execution times, on the other hand, is independent of the running times of the individual programs, and it doesnt matter which machine is used to normalize. If a situation arose in comparative performance evaluation where the programs were fixed but the inputs were not, then competitors could rig the results of weighted arithmetic means by making their best performing benchmark have the largest input and therefore dominate execution time. In such a situation the geometric mean would be less misleading than the arithmetic mean"
"Explain why, floating-point performance is much less important for performance than integers and character strings, yet virtually every server processor still includes floating-point instructions","Before starting with the general principles, lets review the three application areas from the last chapter. Desktop computing emphasizes performance of programs with integer and floating-point data types, with little regard for program size or processor power consumption. For example, code size has never been reported in the four generations of SPEC benchmarks. Servers today are used primarily for database, file server, and web applications, plus some timesharing applications for many users. Hence,  floating-point performance is much less important for performance than integers and character strings, yet virtually every server processor still includes floating-point instructions."," Throughout this chapter, we examine a wide variety of architectural measurements. Clearly, these measurements depend on the programs measured and on the compilers used in making the measurements. The results should not be interpreted as absolute, and you might see different data if you did the measurement with a different compiler or a different set of programs. The authors believe that the measurements in this chapter are reasonably indicative of a class of typical applications. Many of the measurements are presented using a small set of benchmarks, so that the data can be reasonably displayed and the differences among programs can be seen. An architect for a new computer would want to analyze a much larger collection of programs before making architectural decisions. The measurements shown are usually dynamicthat is, the frequency of a measured event is weighed by the number of times that event occurs during execution of the measured program. Before starting with the general principles, lets review the three application areas from the last chapter. Desktop computing emphasizes performance of programs with integer and floating-point data types, with little regard for program size or processor power consumption. For example, code size has never been reported in the four generations of SPEC benchmarks. Servers today are used primarily for database, file server, and web applications, plus some timesharing applications for many users. Hence, floating-point performance is much less important for performance than integers and character strings, yet virtually every server processor still includes floating-point instructions. Embedded applications value cost and power, so code size is important because less memory is both cheaper and lower power, and some classes of instructions (such as floating point) may be optional to reduce chip costs. Thus, instruction sets for all three applications are very similar; Appendix B <RISC> takes advantage of the similarities to describe eight instruction sets in just 43 pages. In point of fact, the MIPS architecture that drives this chapter has been used successfully in desktops, servers, and embedded applications"
"Explain why, many DSPs have this second novel addressing mode bit reverse addressingwhereby the hardware reverses the lower bits of the address, with the number of bits reversed depending on the step of the FFT algorithm","The DSP solution is based on the observation that the resulting binary address is simply the reverse of the initial address! For example, address 1002 (4) becomes 0012(1). Hence,  many DSPs have this second novel addressing mode bit reverse addressingwhereby the hardware reverses the lower bits of the address, with the number of bits reversed depending on the step of the FFT algorithm."," One variation makes assumptions about the buffer size starting at an address that ends in xxx00.00 and so uses just a single buffer length register per address register, Even though DSPs are tightly targeted to a small number of algorithms, its surprising this next addressing mode is included for just one application: Fast Fourier Transform (FFT). FFTs start or end their processing with data shuffled in a particular order. For eight data items in a radix-2 FFT, the transformation is listed below, with addresses in parentheses shown in binary: 0 (0002) 0 (0002) => 2 (0102) 2 (0102) => 4 (1002) 1 (0012) => 6 (1102) 3 (0112) => Without special support such address transformation would take an extra memory access to get the new address, or involve a fair amount of logical instructions to transform the address. The DSP solution is based on the observation that the resulting binary address is simply the reverse of the initial address! For example, address 1002 (4) becomes 0012(1). Hence, many DSPs have this second novel addressing mode bit reverse addressingwhereby the hardware reverses the lower bits of the address, with the number of bits reversed depending on the step of the FFT algorithm. As DSP programmers migrate towards larger programs and hence become more attracted to compilers, they have been trying to use the compiler technology developed for the desktop and embedded computers. Such compilers have no hope of taking high-level language code and producing these two addressing modes, so they are limited to assembly language programmer. As stated before, the DSP community routinely uses library routines, and hence programmers may benefit even if they write at a higher level"
"Explain why, fixed-point data are fractions between -1 and +1","DSPs add fixed point to the data types discussed so far. If you think of integers as having a binary point to the right of the least significant bit, fixed point has a binary point just to the right of the sign bit. Hence,  fixed-point data are fractions between -1 and +1."," Assuming a triangle is visible, when it is rendered it is filled with pixels. Pixels are usually 32 bits, usually consisting of four 8-bit channels: R (red), G (green), B (blue) and A (which denotes the transparency of the surface or transparency of the pixel when the pixel is rendered). DSPs add fixed point to the data types discussed so far. If you think of integers as having a binary point to the right of the least significant bit, fixed point has a binary point just to the right of the sign bit. Hence, fixed-point data are fractions between -1 and +1. Here are three simple16-bit patterns: 0000 1000 0000 0000 What values do they represent if they are twos complement integers? Fixedpoint numbers? Number representation tells us that the i-th digit to the left of the binary point represents 2i-1 and the i-th digit to the right of the binary point represents 2-i. First assume these three patterns are integers. Then the binary point is to the far right, so they represent 214, 211, and (214+ 211+ 23), or 16384, 2048, and 18440"
"Explain why, values are almost never pushed or popped on the stack","The stack is used to allocate local variables. The stack is grown and shrunk on procedure call or return, respectively. Objects on the stack are addressed relative to the stack pointer and are primarily scalars (single variables) rather than arrays. The stack is used for activation records, not as a stack for evaluating expressions. Hence,  values are almost never pushed or popped on the stack."," Crosscutting Issues: The Role of Compilers 11% 12% 21% Program, lucas, level 1 Compiler lucas, level 0 optimimcf, level 3 zation level mcf, level 2 100% 76% 76% 84% 100% mcf, level 0 0% 40% 80% % of unoptimized instructions executed Branches/Calls Loads/Stores FIGURE 2.26 Change in instruction count for the programs lucas and mcf from the SPEC2000 as compiler optimization levels vary. Level 0 is the same as unoptimized code. Level 1 includes local optimizations, code scheduling, and local register allocation. Level 2 includes global optimizations, loop transformations (software pipelining), and global register allocation. Level 3 adds procedure integration. These experiments were performed on the Alpha compilers. The Impact of Compiler Technology on the Architects Decisions The interaction of compilers and high-level languages significantly affects how programs use an instruction set architecture. There are two important questions: How are variables allocated and addressed? How many registers are needed to allocate variables appropriately? To address these questions, we must look at the three separate areas in which current high-level languages allocate their data: n The stack is used to allocate local variables. The stack is grown and shrunk on procedure call or return, respectively. Objects on the stack are addressed relative to the stack pointer and are primarily scalars (single variables) rather than arrays. The stack is used for activation records, not as a stack for evaluating expressions. Hence, values are almost never pushed or popped on the stack. The global data area is used to allocate statically declared objects, such as global variables and constants. A large percentage of these objects are arrays or other aggregate data structures"
"Explain why, we can think of Intels MMX or PowerPCs AltiVec as simply short vector computers: MMX with vectors of eight 8-bit elements, four 16-bit elements, or two 32-bit elements, and AltiVec with vectors twice that length","The SIMD instructions are really an abbreviated version of an elegant architecture style that has its own compiler technology. As explained in Appendix F, vector architectures operate on vectors of data. Invented originally for scientific codes, multimedia kernels are often vectorizable as well. Hence,  we can think of Intels MMX or PowerPCs AltiVec as simply short vector computers: MMX with vectors of eight 8-bit elements, four 16-bit elements, or two 32-bit elements, and AltiVec with vectors twice that length."," Compiler Support (or lack thereof) for Multimedia Instructions Alas, the designers of the SIMD instructions that operate on several narrow data times in a single clock cycle consciously ignored the prior subsection. These instructions tend to be solutions, not primitives, they are short of registers, and the data types do not match existing programming languages. Architects hoped to find an inexpensive solution that would help some users, but in reality, only a few low-level graphics library routines use them. The SIMD instructions are really an abbreviated version of an elegant architecture style that has its own compiler technology. As explained in Appendix F, vector architectures operate on vectors of data. Invented originally for scientific codes, multimedia kernels are often vectorizable as well. Hence, we can think of Intels MMX or PowerPCs AltiVec as simply short vector computers: MMX with vectors of eight 8-bit elements, four 16-bit elements, or two 32-bit elements, and AltiVec with vectors twice that length. They are implemented as simply adjacent, narrow elements in wide registers These abbreviated architectures build the vector register size into the architecture: the sum of the sizes of the elements is limited to 64 bits for MMX and 128 bits for AltiVec. When Intel decided to expand to 128 bit vectors, it added a whole new set of instructions, called SSE. The missing elegance from these architectures involves the specification of the vector length and the memory addressing modes. By making the vector width variable, these vectors seemlessly switch between different data widths simply by increasing the number of elements per vector. For example, vectors could have, say, 32 64-bit elements, 64 32-bit elements, 128 16-bit elements, and 256 8-bit elements. Another advantage is that the number of elements per vector register can vary between generations while remaining binary compatible. One generation might have 32 64-bit elements per vector register, and the next have 64 64-bit elements. (The number of elements per register is located in a status register.) The number of elements executed per clock cycle is also implementation dependent, and all run the same binary code. Thus, one generation might operate 64bits per clock cycle, and another at 256-bits per clock cycle"
"Explain why, these SIMD instructions are commonly found only in hand coded libraries","Having short, architecture limited vectors with few registers and simple memory addressing modes makes it more difficult to use vectorizing compiler technology. Another challenge is that no programming language (yet) has support for operations on these narrow data. Hence,  these SIMD instructions are commonly found only in hand coded libraries."," The total is 20 instructions to perform the 20 operations in the C code above to convert 8 pixels [Kozyrakis 2000]. (Since a vector might have 32 64-bit elements, this code actually converts up to 32 x 8 or 256 pixels.) In contrast, Intels web site shows a library routine to perform the same calculation on eight pixels takes 116 MMX instructions plus 6 80x86 instructions [Intel 2001]. This sixfold increase in instructions is due to the large number of instructions to load and unpack RBG pixels and to pack and store YUV pixels, since there are no strided memory accesses. n Having short, architecture limited vectors with few registers and simple memory addressing modes makes it more difficult to use vectorizing compiler technology. Another challenge is that no programming language (yet) has support for operations on these narrow data. Hence, these SIMD instructions are commonly found only in hand coded libraries. Summary: The Role of Compilers This section leads to several recommendations. First, we expect a new instruction set architecture to have at least 16 general-purpose registersnot counting separate registers for floating-point numbersto simplify allocation of registers using graph coloring. The advice on orthogonality suggests that all supported addressing modes apply to all instructions that transfer data. Finally, the last three pieces of adviceprovide primitives instead of solutions, simplify trade-offs between alternatives, dont bind constants at runtimeall suggest that it is better to err on the side of simplicity. In other words, understand that less is more in the design of an instruction set. Alas, SIMD extensions are more an example of good marketing than outstanding achievement of hardware/software co-design"
"Explain why, the quality of development tools and the availability of off-the-shelf application software components became, for many users, more important than performance in selecting a processor","From the mid 1980s to the mid 1990s, many new commercial DSP architectures were introduced. For the most part, these architectures followed a gradual, evolutionary path, adopting incremental improvements rather than fundamental innovations when compared with the earliest DSPs like the TMS32010. DSP application programs expanded from a few hundred lines of source code to tens of thousands of lines. Hence,  the quality of development tools and the availability of off-the-shelf application software components became, for many users, more important than performance in selecting a processor."," Like the first microprocessors, these early DSPs had simple architectures. In contrast with their general-purpose cousins, though, DSPs adopted a range of specialized features to boost performance and efficiency in signal processing tasks. For example, a single-cycle multiplier aided arithmetic performance. Specialized datapaths streamlined multiply-accumulate operations and provided features to minimize numeric errors, such as saturation arithmetic. Separate program and data memories provided the memory bandwidth required to keep the relatively powerful datapaths fed. Dedicated, specialized addressing hardware sped simple addressing operations, such autoincrement addressing. Complex, specialized instruction sets allowed these processors to combine many operations in a single instruction, but only certain limited combinations of operations were supported. From the mid 1980s to the mid 1990s, many new commercial DSP architectures were introduced. For the most part, these architectures followed a gradual, evolutionary path, adopting incremental improvements rather than fundamental innovations when compared with the earliest DSPs like the TMS32010. DSP application programs expanded from a few hundred lines of source code to tens of thousands of lines. Hence, the quality of development tools and the availability of off-the-shelf application software components became, for many users, more important than performance in selecting a processor. Today, chips based on these conventional DSP architectures still dominate DSP applications, and are used in products such as cellular telephones, disk drives (for servo control), and consumer audio devices. Early DSP architectures had proven effective, but the highly specialized and constrained instruction sets that gave them their performance and efficiency also created processors that were difficult targets for compiler writers. The performance and efficiency demands of most DSP applications could not be met by the resulting weak compilers, so much softwareall software for some processor was written in assembly language. As applications became larger and more complex, assembly language programming became less practical. Users also suffered from the incompatibility of many new DSP architectures with their predecessors, which forced them to periodically rewrite large amounts of existing application software"
"Explain why, the ROB is a source of operands for instructions, just as the reservation stations provide operands in Tomasulos algorithm"," The ROB holds the result of an instruction between the time the operation associated with the instruction completes and the time the instruction commits. Hence,  the ROB is a source of operands for instructions, just as the reservation stations provide operands in Tomasulos algorithm."," The reorder buffer (ROB, for short) provides additional registers in the same way as the reservation stations in Tomasulos algorithm extend the register set. The ROB holds the result of an instruction between the time the operation associated with the instruction completes and the time the instruction commits. Hence, the ROB is a source of operands for instructions, just as the reservation stations provide operands in Tomasulos algorithm. The key difference is that in Tomasulos algorithm, once an instruction writes its result, any subsequently issued instructions will find the result in the register file. With speculation, the register file is not updated until the instruction commits (and we know definitively that the instruction should execute); thus, the ROB supplies operands in the interval between completion of instruction execution and instruction commit. The ROB is similar the store buffer in Tomasulos algorithm, and we integrate the function of the store buffer into the ROB for simplicity"
"Explain why, predicting all branches as taken is the better approach","In the SPEC programs, however, more than half of the forward-going branches are taken. Hence,  predicting all branches as taken is the better approach."," To perform these optimizations, we need to predict the branch statically when we compile the program. There are several different methods to statically predict branch behavior. The simplest scheme is to predict a branch as taken. This scheme has an average misprediction rate that is equal to the untaken branch frequency, which for the SPEC programs is 34%. Unfortunately, the misprediction rate ranges from not very accurate (59%) to highly accurate (9%). A better alternative is to predict on the basis of branch direction, choosing backward-going branches to be taken and forward-going branches to be not taken. For some programs and compilation systems, the frequency of forward taken branches may be significantly less than 50%, and this scheme will do better than just predicting all branches as taken. In the SPEC programs, however, more than half of the forward-going branches are taken. Hence, predicting all branches as taken is the better approach. Even for other benchmarks or compilers, directionbased prediction is unlikely to generate an overall misprediction rate of less than 30% to 40%. An enhancement of this technique was explored by Ball and Larus; their approach uses program context information and generates more accurate predictions than a simple scheme based solely on branch direction. A still more accurate technique is to predict branches on the basis of profile information collected from earlier runs. The key observation that makes this worthwhile is that the behavior of branches is often bimodally distributed; that is, an individual branch is often highly biased toward taken or untaken. Figure 4.3 shows the success of branch prediction using this strategy. The same input data were used for runs and for collecting the profile; other studies have shown that Static Branch Prediction changing the input so that the profile is for a different run leads to only a small change in the accuracy of profile-based prediction"
"Explain why, caches needed to be blocking and to cause all the functional units to stall","Early VLIWs operated in lock-step; there was no hazard detection hardware at all. This structure dictated that a stall in any functional unit pipeline must cause the entire processor to stall, since all the functional units must be kept synchronized. Although a compiler may be able to schedule the deterministic functional units to prevent stalls, predicting which data accesses will encounter a cache stall and scheduling them is very difficult. Hence,  caches needed to be blocking and to cause all the functional units to stall."," pand them when they are read into the cache or are decoded. We will see techniques to reduce code size increases in both Sections 4.7 and 4.8. Early VLIWs operated in lock-step; there was no hazard detection hardware at all. This structure dictated that a stall in any functional unit pipeline must cause the entire processor to stall, since all the functional units must be kept synchronized. Although a compiler may be able to schedule the deterministic functional units to prevent stalls, predicting which data accesses will encounter a cache stall and scheduling them is very difficult. Hence, caches needed to be blocking and to cause all the functional units to stall. As the issue rate and number of memory references becomes large, this synchronization restriction becomes unacceptable. In more recent processors, the functional units operate more independently, and the compiler is used to avoid hazards at issue time, while hardware checks allow for unsynchronized execution once instructions are issued"
"Explain why, instructions that were data-dependent on the assignment to C, and which execute after this code fragment, will be affected","First, the assignment is moved over the join point of the else part into the portion corresponding to the then part. This movement makes the instructions for C control-dependent on the branch and means that they will not execute if the else path, which is the infrequent path, is chosen. Hence,  instructions that were data-dependent on the assignment to C, and which execute after this code fragment, will be affected."," Moving the assignment to C up to before the first branch requires two steps. First, the assignment is moved over the join point of the else part into the portion corresponding to the then part. This movement makes the instructions for C control-dependent on the branch and means that they will not execute if the else path, which is the infrequent path, is chosen. Hence, instructions that were data-dependent on the assignment to C, and which execute after this code fragment, will be affected. To ensure the correct value is computed for such instructions, a copy is made of the instructions that compute and assign to C on the else path. Second, we can move C from the then part of the branch across the branch condition, if it does not affect any data flow into the branch condition. If C is moved to before the if-test, the copy of C in the else branch can usually be eliminated, since it will be redundant. We can see from this example that global code scheduling is subject to many constraints. This observation is what led designers to provide hardware support to make such code motion easier, and section 4.5 explores such support in detail"
"Explain why, just as other VLIW processors have found it useful to have support for speculative reordering, such support is important in Crusoe","One of the major challenges of a software-based implementation of an instruction set is maintaining the exception behavior of the original ISA while achieving good performance. In particular, achieving good performance often means reordering operations that correspond to instructions in the original program, which means that operations will be executed in a different order than in a strict sequential interpretation. This reordering is crucial to obtaining good performance when the target is a VLIW. Hence,  just as other VLIW processors have found it useful to have support for speculative reordering, such support is important in Crusoe."," The Crusoe processor: software translation and hardware support The software responsible for implementing the x86 instruction set uses a variety of techniques to establish a balance between execution speed and translation time. Initially, and for lowest overhead execution, the x86 code can be interpreted on an instruction by instruction basis. If a code segment is executed several times, it can be translated into an equivalent Crusoe code sequence, and the translation can be cached. The unit of translation is at least a basic block, since we know that if any instruction is executed in the block, they will all be executed. Translating an entire block both improves the translated code quality and reduces the translation overhead, since the translator need only be called once per basic block. Even a quick translation of a basic block can produce acceptable results, since simple code scheduling can be done during the translation. One of the major challenges of a software-based implementation of an instruction set is maintaining the exception behavior of the original ISA while achieving good performance. In particular, achieving good performance often means reordering operations that correspond to instructions in the original program, which means that operations will be executed in a different order than in a strict sequential interpretation. This reordering is crucial to obtaining good performance when the target is a VLIW. Hence, just as other VLIW processors have found it useful to have support for speculative reordering, such support is important in Crusoe. The Crusoe support for speculative reordering consists of four major parts: a shadowed register file, a program-controlled store buffer, memory alias detection hardware with speculative loads, and a conditional move instruction (called select) that is used to do if-conversion on x86 code sequences"
"Explain why, embedded computers may not chose hardware-intensive optimizations in the quest of better memory hierarchy performance, as would most desktop and server computers","The memory hierarchy of the embedded computers is often quite different from that of the desktop and server. First, embedded computers are often used in real-time applications, and hence programmers must worry about worst case per- formance. This concern is problematic for caches that improve average case performance, but can degrade worst case performance; well mention some techniques to address this later in the chapter. Second, embedded applications are often concerned about power and battery life. The best way to save power is to have less hardware. Hence,  embedded computers may not chose hardware-intensive optimizations in the quest of better memory hierarchy performance, as would most desktop and server computers."," Desktop computers are primarily running one application at a time on top of an operating system for a single user, whereas server computers may typically have hundreds of users potentially running potentially dozens of applications simultaneously. These characteristics result in more context switches, which effectively increases compulsory miss rates. Thus, desktop computers are concerned more with average latency from the memory hierarchy whereas server computers are also concerned about memory bandwidth. Although protection is important on desktop computers to deter programs from clobbering each other, server computers must prevent one user from accessing anothers data, and hence the importance of protection escalates. Server computers also tend to be much larger, with more memory and disk storage, and hence often run much larger applications. In 2001 virtually all servers can be purchased as multiprocessors with hundreds of disks, which places even greater bandwidth demands on the memory hierarchy. The memory hierarchy of the embedded computers is often quite different from that of the desktop and server. First, embedded computers are often used in real-time applications, and hence programmers must worry about worst case per- formance. This concern is problematic for caches that improve average case performance, but can degrade worst case performance; well mention some techniques to address this later in the chapter. Second, embedded applications are often concerned about power and battery life. The best way to save power is to have less hardware. Hence, embedded computers may not chose hardware-intensive optimizations in the quest of better memory hierarchy performance, as would most desktop and server computers. Third, embedded applications are typically only running one application and use a very simple operating system, if they one at all. Hence, the protection role of the memory hierarchy is often diminished. Finally, the main memory itself may be quite smallless than one megabyteand there is often no disk storage. This chapter is a tour of the general principles of memory hierarchy using the desktop as the generic example, but we will take detours to point out where the memory hierarchy of servers and embedded computers diverge from the desktop"
"Explain how, the protection role of the memory hierarchy is often diminished","The memory hierarchy of the embedded computers is often quite different from that of the desktop and server. First, embedded computers are often used in real-time applications, and hence programmers must worry about worst case per- formance. This concern is problematic for caches that improve average case performance, but can degrade worst case performance; well mention some techniques to address this later in the chapter. Second, embedded applications are often concerned about power and battery life. The best way to save power is to have less hardware. Hence, embedded computers may not chose hardware-intensive optimizations in the quest of better memory hierarchy performance, as would most desktop and server computers. Third, embedded applications are typically only running one application and use a very simple operating system, if they one at all. Hence,  the protection role of the memory hierarchy is often diminished."," Desktop computers are primarily running one application at a time on top of an operating system for a single user, whereas server computers may typically have hundreds of users potentially running potentially dozens of applications simultaneously. These characteristics result in more context switches, which effectively increases compulsory miss rates. Thus, desktop computers are concerned more with average latency from the memory hierarchy whereas server computers are also concerned about memory bandwidth. Although protection is important on desktop computers to deter programs from clobbering each other, server computers must prevent one user from accessing anothers data, and hence the importance of protection escalates. Server computers also tend to be much larger, with more memory and disk storage, and hence often run much larger applications. In 2001 virtually all servers can be purchased as multiprocessors with hundreds of disks, which places even greater bandwidth demands on the memory hierarchy. The memory hierarchy of the embedded computers is often quite different from that of the desktop and server. First, embedded computers are often used in real-time applications, and hence programmers must worry about worst case per- formance. This concern is problematic for caches that improve average case performance, but can degrade worst case performance; well mention some techniques to address this later in the chapter. Second, embedded applications are often concerned about power and battery life. The best way to save power is to have less hardware. Hence, embedded computers may not chose hardware-intensive optimizations in the quest of better memory hierarchy performance, as would most desktop and server computers. Third, embedded applications are typically only running one application and use a very simple operating system, if they one at all. Hence, the protection role of the memory hierarchy is often diminished. Finally, the main memory itself may be quite smallless than one megabyteand there is often no disk storage. This chapter is a tour of the general principles of memory hierarchy using the desktop as the generic example, but we will take detours to point out where the memory hierarchy of servers and embedded computers diverge from the desktop"
"Explain why, a single cache would present a structural hazard for loads and stores, leading to stalls","the data cache cannot supply all the memory needs of the processor: the processor also needs instructions. Although a single cache could try to supply both, it can be a bottleneck. For example, when a load or store instruction is executed, the pipelined processor will simultaneously request both a data word and an instruction word. Hence,  a single cache would present a structural hazard for loads and stores, leading to stalls."," A write miss is very similar to a read miss, since the 21264 allocates a block on a read or a write miss. We have seen how it works, but the data cache cannot supply all the memory needs of the processor: the processor also needs instructions. Although a single cache could try to supply both, it can be a bottleneck. For example, when a load or store instruction is executed, the pipelined processor will simultaneously request both a data word and an instruction word. Hence, a single cache would present a structural hazard for loads and stores, leading to stalls. One simple way to conquer this problem is to divide it: one cache is dedicated to instructions and another to data. Separate caches are found in most recent processors, including the Alpha 21264. Hence, it has a 64-KB instruction cache as well as the 64-KB data cache. The CPU knows whether it is issuing an instruction address or a data address, so there can be separate ports for both, thereby doubling the bandwidth between the memory hierarchy and the CPU. Separate caches also offer the opportunity of optimizing each cache separately: different capacities, block sizes, and associativities may lead to better performance. (In contrast to the instruction caches and data caches of the 21264, the terms unified or mixed are applied to caches that can contain either instructions or data.) Instruction cache Unified cache 8.16 3.82 1.36 0.61 0.30 0.02 caches. Separating instructions and data removes misses due to conflicts between instruction blocks and data blocks, but the split also fixes the cache space devoted to each type. Which is more important to miss rates? A fair comparison of separate instruction and data caches to unified caches requires the total cache size to be the same. For example, a separate 16-KB instruction cache and 16-KB data cache should be compared to a 32-KB unified cache. Calculating the average miss rate with separate instruction and data caches necessitates knowing the percentage of memory references to each cache. Figure 2.32 on page 149 suggests the split is 100%/(100% + 37% + 10%) or about 78% instruction references to (37% + 10%)/(100% + 37% + 10%) or about 22% data references. Splitting affects performance beyond what is indicated by the change in miss rates, as we shall see shortly"
"Explain why, the pressure of a fast processor clock cycle encourages simple cache designs, but the increasing miss penalty rewards associativity, as the following example suggests","Like many of these examples, improving one aspect of the average memory access time comes at the expense of another. Increasing block size reduced miss rate while increasing miss penalty, and greater associativity can come at the cost of increased hit time (see Figure 5.24 on page 431 in section 5.7.)Hence,  the pressure of a fast processor clock cycle encourages simple cache designs, but the increasing miss penalty rewards associativity, as the following example suggests."," Second Miss Rate Reduction Technique: Larger caches The obvious way to reduce capacity misses in Figures 5.14 and 5.15 above is to increases capacity of the cache. The obvious drawback is longer hit time and higher cost. This technique has been especially popular in off-chip caches: The size of second or third level caches in 2001 equals the size of main memory in desktop computers from the first edition of this book, only a decade before! Third Miss Rate Reduction Technique: Higher Associativity The first is that eight-way set associative is for practical purposes as effective in reducing misses for these sized caches as fully associative. You can see the difference by comparing the 8-way entries to the capacity miss column in Figure 5.14, since capacity misses are calculated using fully associative cache. The second observation, called the 2:1 cache rule of thumb and found on the front inside cover, is that Reducing Miss Rate a direct-mapped cache of size N has about the same miss rate as a 2-way setassociative cache of size N/2. This held for cache sizes less than 128 KB. Like many of these examples, improving one aspect of the average memory access time comes at the expense of another. Increasing block size reduced miss rate while increasing miss penalty, and greater associativity can come at the cost of increased hit time (see Figure 5.24 on page 431 in section 5.7.)Hence, the pressure of a fast processor clock cycle encourages simple cache designs, but the increasing miss penalty rewards associativity, as the following example suggests. EXAMPLE Assume higher associativity would increase the clock cycle time as listed below: Clock cycle time2-way = 1.36 Clock cycle time1-way Clock cycle time4-way = 1.44 Clock cycle time1-way Clock cycle time8-way = 1.52 Clock cycle time1-way Assume that the hit time is 1 clock cycle, that the miss penalty for the direct-mapped case is 25 clock cycles to an L2 cache that never misses, and that the miss penalty need not be rounded to an integral number of clock cycles. Using Figure 5.14 for miss rates, for which cache sizes are each of these three statements true? Average memory access time8-way < Average memory access time4-way < Average memory access time2-way < Average memory access time4-way Average memory access time2-way Average memory access time1-way Average memory access time8-way Average memory access time4-way Average memory access time2-way Average memory access time1-way = Hit time8-way + Miss rate8-way Miss penalty8-way = 1.52 + Miss rate8-way 25 = 1.44 + Miss rate4-way 25 = 1.36 + Miss rate2-way 25 = 1.00 + Miss rate1-way 25 The miss penalty is the same time in each case, so we leave it as 25 clock cycles. For example, the average memory access time for a 4-KB directmapped cache is Average memory access time1-way = 1.00 + (0.133 25) = 3.44 and the time for a 512-KB, eight-way set-associative cache is Average memory access time8-way = 1.52 + (0.006 25) = 1.66 Using these formulas and the miss rates from Figure 5.14, Figure 5.19 shows the average memory access time for each cache and associativity"
"Explain why, it is important to indicate for each set which block should be the fast hit and which should be the slow one","Pseudo-associative caches then have one fast and one slow hit timecorresponding to a regular hit and a pseudo hitin addition to the miss penalty. of the direct-mapped cache became slow hit times in the pseudo-associative cache. The performance would then be degraded by this optimization. Hence,  it is important to indicate for each set which block should be the fast hit and which should be the slow one."," Pseudo-associative caches then have one fast and one slow hit timecorresponding to a regular hit and a pseudo hitin addition to the miss penalty. of the direct-mapped cache became slow hit times in the pseudo-associative cache. The performance would then be degraded by this optimization. Hence, it is important to indicate for each set which block should be the fast hit and which should be the slow one. One way is simply to make the upper one fast and swap the contents of the blocks. Another danger is that the miss penalty may become slightly longer, adding the time to check another cache entry. Hit time Pseudo hit time Time Fifth Miss Rate Reduction Technique: Compiler Optimizations Thus far our techniques to reduce misses have required changes to or additions to the hardware: larger blocks, larger caches, higher associativity, or pseudo-associativity. This final technique reduces miss rates without any hardware changes"
"Explain why, a fast hit time is multiplied in importance beyond the average memory access time formula because it helps everything"," Reducing Hit Time Hit time is critical because it affects the clock rate of the processor; in many processors today the cache access time limits the clock cycle rate, even for processors that take multiple clock cycles to access the cache. Hence,  a fast hit time is multiplied in importance beyond the average memory access time formula because it helps everything."," Now that we have spent nearly 30 pages on techniques that reduce cache misses or miss penalty in sections 5.4 to 5.6, it is time to look at reducing the final component of average memory access time. Reducing Hit Time Hit time is critical because it affects the clock rate of the processor; in many processors today the cache access time limits the clock cycle rate, even for processors that take multiple clock cycles to access the cache. Hence, a fast hit time is multiplied in importance beyond the average memory access time formula because it helps everything. This section gives four general techniques. First Hit Time Reduction Technique: Small and Simple Caches A time-consuming portion of a cache hit is using the index portion of the address to read the tag memory and then compare it to the address. Our guideline from helps the hit time. It is also critical to keep the cache small enough to fit on the same chip as the processor to avoid the time penalty of going off-chip. The second suggestion is to keep the cache simple, such as using direct mapping (see page 414). A main benefit of direct-mapped caches is that the designer can overlap the tag check with the transmission of the data. This effectively reduces hit time. Hence, the pressure of a fast clock cycle encourages small and simple cache designs for first-level caches. For second level caches, some designs strike a compromise by keeping the tags on-chip and the data off-chip, promising a fast tag check, yet providing the greater capacity of separate memory chips"
"Explain how, the pressure of a fast clock cycle encourages small and simple cache designs for first-level caches","A time-consuming portion of a cache hit is using the index portion of the address to read the tag memory and then compare it to the address. Our guideline from helps the hit time. It is also critical to keep the cache small enough to fit on the same chip as the processor to avoid the time penalty of going off-chip. The second suggestion is to keep the cache simple, such as using direct mapping (see page 414). A main benefit of direct-mapped caches is that the designer can overlap the tag check with the transmission of the data. This effectively reduces hit time. Hence,  the pressure of a fast clock cycle encourages small and simple cache designs for first-level caches."," Reducing Hit Time Hit time is critical because it affects the clock rate of the processor; in many processors today the cache access time limits the clock cycle rate, even for processors that take multiple clock cycles to access the cache. Hence, a fast hit time is multiplied in importance beyond the average memory access time formula because it helps everything. This section gives four general techniques. First Hit Time Reduction Technique: Small and Simple Caches A time-consuming portion of a cache hit is using the index portion of the address to read the tag memory and then compare it to the address. Our guideline from helps the hit time. It is also critical to keep the cache small enough to fit on the same chip as the processor to avoid the time penalty of going off-chip. The second suggestion is to keep the cache simple, such as using direct mapping (see page 414). A main benefit of direct-mapped caches is that the designer can overlap the tag check with the transmission of the data. This effectively reduces hit time. Hence, the pressure of a fast clock cycle encourages small and simple cache designs for first-level caches. For second level caches, some designs strike a compromise by keeping the tags on-chip and the data off-chip, promising a fast tag check, yet providing the greater capacity of separate memory chips. One approach to determining the impact on hit time in advance of building a chip is to use CAD tools. CACTI is a program to estimate the access time of alternative cache structures on CMOS microprocessors within 10% of more detailed CAD tools. For a given minimum feature size, it estimates the hit time of caches as you vary cache size, associativity, and number of read/write ports. Figure 5.24 shows the estimated impact on hit time as cache size and associativity are varied. Depending on cache size, for these parameters the model suggests that hit times for direct mapped is 1.2 to 1.5 times faster than 2-way set associative; 2way is l.02 to 1.11 times faster than 4-way; and 4-way is 1.0 to 1.08 times faster than fully associative (except for a 256 KB cache, which is 1.19 times faster)"
"Explain how, ROM also provides a level of protection to the code of embedded computers"," In addition to being non-volatile, ROM is also non-destructible; nothing the computer can do can modify the contents of this memory. Hence,  ROM also provides a level of protection to the code of embedded computers."," The first is Read-Only Memory (ROM). ROM is programmed at time of manufacture, needing only a single transistor per bit to represent 1 or 0. ROM is used for the embedded program and for constants, often included as part of a larger chip. In addition to being non-volatile, ROM is also non-destructible; nothing the computer can do can modify the contents of this memory. Hence, ROM also provides a level of protection to the code of embedded computers. Since addressbased protection is often not enabled in embedded processors, ROM can fulfill an important role. The second memory technology offers non-volatility but allows the memory to be modified. Flash memory allows the embedded device to alter nonvolatile memory after the system is manufactured, which can shorten product development. Flash memory, described in on page 498 in Chapter 7, allows reading at almost DRAM speeds but writing flash is 10 to 100 times slower. In 2001, the DRAM capacity per chip and the megabytes per dollar is about four to eight times greater than flash memory"
"Explain why, there must be a means of sharing a smaller amount of physical memory among many processes","At any instant in time computers are running multiple processes, each with its own address space. (Processes are described in the next section.) It would be too expensive to dedicate a full-address-space worth of memory for each process, especially since many processes use only a small part of their address space. Hence,  there must be a means of sharing a smaller amount of physical memory among many processes."," Virtual Memory a system has been devised to make the core drum combination appear to the programmer as a single level store, the requisite transfers taking place automatically. Kilburn et al. [1962] At any instant in time computers are running multiple processes, each with its own address space. (Processes are described in the next section.) It would be too expensive to dedicate a full-address-space worth of memory for each process, especially since many processes use only a small part of their address space. Hence, there must be a means of sharing a smaller amount of physical memory among many processes. One way to do this, virtual memory, divides physical memory into blocks and allocates them to different processes. Inherent in such an approach must be a protection scheme that restricts a process to the blocks belonging only to that process. Most forms of virtual memory also reduce the time to start a program, since not all code and data need be in physical memory before a program can begin. Although protection provided by virtual memory is essential for current computers, sharing is not the reason that virtual memory was invented. If a program became too large for physical memory, it was the programmers job to make it fit"
"Explain why, a page replacement algorithm such as LRU must rely on disabling reads and writes occasionally to record reads and writes to pages to measure usage and whether or not pages are dirty","The Address Space Number (ASN) is used like a Process ID for virtual caches, in that the TLB is not flushed on a context switch, only when ASNs are recycled. The next fields of an entry are protection permissions (Prot) and the valid bit (V). Note that there is no specific reference, use bit, or dirty bit. Hence,  a page replacement algorithm such as LRU must rely on disabling reads and writes occasionally to record reads and writes to pages to measure usage and whether or not pages are dirty."," translation labeled. The TLB uses fully associative placement; thus, the translation begins (steps 1 and 2) by sending the virtual address to all tags. Of course, the tag must be marked valid to allow a match. At the same time, the type of memory access is checked for a violation (also in step 2) against protection information in the TLB. Address Virtual page Space Number number <8> <35> Page offset <13> <8> ASN <4> <1> Prot V <35> Tag <31> Physical address (Low-order 13 bits <13> of address) 44- or 41-bit physical (High-order 31/28 bits of address) address <31 or 28 > FIGURE 5.36 Operation of the Alpha 21264 data TLB during address translation. The four steps of a TLB hit are shown as circled numbers. The Address Space Number (ASN) is used like a Process ID for virtual caches, in that the TLB is not flushed on a context switch, only when ASNs are recycled. The next fields of an entry are protection permissions (Prot) and the valid bit (V). Note that there is no specific reference, use bit, or dirty bit. Hence, a page replacement algorithm such as LRU must rely on disabling reads and writes occasionally to record reads and writes to pages to measure usage and whether or not pages are dirty. The advantage of these omissions is that the TLB need not be written during normal memory accesses nor during a TLB miss. Alpha 21264 has an option of either 44-bit or 41-bit physical addresses. This TLB has 128 entries"
"Explain why, the memory system must identify speculatively executed instructions and conditionally executed instructions and suppress the corresponding exception","Inherent in CPUs that support speculative execution or conditional instructions is the possibility of generating invalid addresses that would not occur without speculative execution. Not only would this be incorrect behavior if exceptions were taken, the benefits of speculative execution would be swamped by false exception overhead. Hence,  the memory system must identify speculatively executed instructions and conditionally executed instructions and suppress the corresponding exception."," For example, the UltraSPARC III fetches up to 4 instructions per clock cycle, and executes up to 4, with up to 2 being loads or stores. Hence, the instruction cache must deliver 128 bits per clock cycle and the data cache must support two 64-bit accesses per clock cycle. Speculative Execution and the Memory System Inherent in CPUs that support speculative execution or conditional instructions is the possibility of generating invalid addresses that would not occur without speculative execution. Not only would this be incorrect behavior if exceptions were taken, the benefits of speculative execution would be swamped by false exception overhead. Hence, the memory system must identify speculatively executed instructions and conditionally executed instructions and suppress the corresponding exception. By similar reasoning, we cannot allow such instructions to cause the cache to stall on a miss, for again unnecessary stalls could overwhelm the benefits of speculation. Hence, these CPUs must be matched with nonblocking caches (see page 421)"
"Explain why, every data address must be sent to the data TLB at the same time as it is sent to the data cache","The instruction cache is virtually indexed and tagged, but the data cache has virtual index but physical tags. Hence,  every data address must be sent to the data TLB at the same time as it is sent to the data cache."," Let's really start at the beginning, when the Alpha is turned on. Hardware on the chip loads the instruction cache serially from an external PROM. This initialization fills up to 64-KB worth of instructions (16K instructions) into the cache. The same serial interface (and PROM) also loads configuration information that specifies L2 cache speed/timing, system port speed/timing, and much other infor- Page offset<13> Virtual page number <35> ASN <8> Store Queue/ ASN Data Out <8> <64> Instruction <128> PC <9> Index I C A C H E Block offset (1024 ASN Prot V Tag blocks) <8> <4> <1><33> 14 Predict =? Way Line Data <1> <11> <512> D T L B Data virtual page Page offset<13> number <35> Data in <64> <8><4><1> 23 <35> ASNProt V Tag (High-order 28 or 31 bits of physical address) <31> I T L B <31> Physical address 128:1 Mux <9> 4 <8><4><1> ASNProt V D C A C H E <31> Physical address <35> Tag Block offset Valid <1> Tag <29> Data <512> 26 =? (High-order 28 or 31 bits of physical address) 5 Instruction prefetcher =? Tag <38> <31> <44> 21 Address 31 2:1 Mux =? Victim buffer Alpha 21264 <44> C A C H E <17> Index V D <1> <1> Tag <21> <128> Data <512> M A I N (131,072 blocks) =? M E M O R Y <64> System Chip Memory Crossbar <256> <256> DIMM DIMM FIGURE 5.43 The overall picture of the Alpha 21264 memory hierarchy. Individual components can be seen in greater detail in Figures 5.7 (page 388) and 5.36 (page 454). The instruction cache is virtually indexed and tagged, but the data cache has virtual index but physical tags. Hence, every data address must be sent to the data TLB at the same time as it is sent to the data cache. Both the instruction and data TLB's have 128 entries. Each TLB entry can map a page of size 8KB, 64KB, 512KB, or 4MB. The 21264 supports a 48-bit or 43-bit virtual address and a 44-bit or 41-bit physical address. Putting It All Together: Alpha 21264 Memory Hierarchy mation necessary for the 21264 to communicate with the external logic. This code completes the remainder of the processor and system initialization"
"Explain why, the data cache tags are examined in parallel with the L2 cache tags during an L2 lookup","the data cache can be virtually addressed and physically tagged. On a miss, the cache controller must check for a synonym (two different virtual addresses that reference the same physical address). Hence,  the data cache tags are examined in parallel with the L2 cache tags during an L2 lookup."," Assuming that we have a valid PTE in the data TLB (step 27), the two tags and valid bits are compared to the physical page frame (step 28), with a match sending the desired 8 bytes to the CPU (step 29). A miss goes to the second-level cache, which proceeds similar to an instruction miss (Step 30), except that it must check the victim buffer first to be sure the block is not there (Step 31). As mentioned in section 5.7, the data cache can be virtually addressed and physically tagged. On a miss, the cache controller must check for a synonym (two different virtual addresses that reference the same physical address). Hence, the data cache tags are examined in parallel with the L2 cache tags during an L2 lookup. As the minimum page size is 8 KB or 13 bits and the cache index plus block offset is 15 bits, the cache must check 22 or 4 blocks per set for synonyms. If it finds a synonym, the offending block is invalidated. This guarantees that a cache block can reside in one of the 8 possible data cache locations at any given time"
"Explain why, the chip includes a DRAM controller on the chip, which they claim saved 80 ns of latency"," In addition to several wide busses for memory bandwidth, the designers of UltraSPARC were concerned about latency. Hence,  the chip includes a DRAM controller on the chip, which they claim saved 80 ns of latency."," Error correction codes enable buses and memories to both detect and correct errors. The idea is to calculate and storage parity over different subsets of the bits in the protected word. When parity does not match it indicates an error. By looking at which of the overlapping subsets have a parity error and which dont, its possible to determine the bit that failed. The Sun Fire ECC was also designed to detect any pair of bit errors, and also to detect if a whole DRAM chip failed turning all the bits of an 8-bit wide chip to zero. Such codes are generally classified as Single Error Correcting/Double Error Detecting (SEC/DED). The UltraSPARC sends these ECC bits between the memory chips and the microprocessor, so that errors that occur on the high-speed buses are also detected and corrected. In addition to several wide busses for memory bandwidth, the designers of UltraSPARC were concerned about latency. Hence, the chip includes a DRAM controller on the chip, which they claim saved 80 ns of latency. The result is 220 ns to the local memory and 280 ns to the non-local memory. (This server supports non-uniform memory access shared memory, described in Chapter 6.) Since Another View: The Sun Fire 6800 Server Multiprocessor Coherence Instruction Clock cycles Per Instruction for Memory Accesses 0.5 1 MB 2 MB 4 MB Cache Size FIGURE 5.49 Clock cycles per instruction for memory accesses versus off-chip cache size for a four-processor server. Note how much higher the performance impact is for large caches than for the SPEC2000 programs in Figure 5.15 on page 410. The workload includes the Oracle commercial database engine for the online transaction processing (OLTP) and decision support systems and the AltaVista search engine for the Web index search. This data was collected by Barroso et al [1998] using the Alpha 21164 microprocessor. memory is connected directly to the processor to lower latency, its size is a function of the number of processors. The limit in 2001 is 8 GB per processor"
"Explain why, the architect's decision to add only 4 more address bits than found in the predecessor of the PDP-11"," Bell and Strecker [1976] reflected on the PDP-11 and identified a small address space as the only architectural mistake that is difficult to recover from. At the time of the creation of PDP-11, core memories were increasing at a very slow rate. In addition, competition from 100 other minicomputer companies meant that DEC might not have a cost-competitive product if every address had to go Historical Perspective and References through the 16-bit datapath twice. Hence,  the architect's decision to add only 4 more address bits than found in the predecessor of the PDP-11."," Today there is little interest in capabilities either from the operating systems or the computer architecture communities, despite growing interest in protection and security. Bell and Strecker [1976] reflected on the PDP-11 and identified a small address space as the only architectural mistake that is difficult to recover from. At the time of the creation of PDP-11, core memories were increasing at a very slow rate. In addition, competition from 100 other minicomputer companies meant that DEC might not have a cost-competitive product if every address had to go Historical Perspective and References through the 16-bit datapath twice. Hence, the architect's decision to add only 4 more address bits than found in the predecessor of the PDP-11. The architects of the IBM 360 were aware of the importance of address size and planned for the architecture to extend to 32 bits of address. Only 24 bits were used in the IBM 360, however, because the low-end 360 models would have been even slower with the larger addresses in 1964. Unfortunately, the architects didnt reveal their plans to the software people, and programmers who stored extra information in the upper 8 unused address bit foiled the expansion effort. (Apple made a similar mistake 20 years later with the 24-bit address in the Motorola 68000, which required a procedure to later determine 32-bit clean programs for the Macintosh when later 68000s used the full 32-bit virtual address.) Virtually every computer since then, will check to make sure the unused bits stay unused, and trap if the bits have the wrong value"
"Explain why, a read miss to a block in the shared state is a miss for that cache block but for a different address","The cache states are shown in circles with any access permitted by the CPU without a state transition shown in parenthesis under the name of the state. The stimulus causing a state change is shown on the transition arcs in regular type, and any bus actions generated as part of the state transition are shown on the transition arc in bold. The stimulus actions apply to a block in the cache, not to a specific address in the cache. Hence,  a read miss to a block in the shared state is a miss for that cache block but for a different address."," states of the protocol are duplicated to represent transitions based on CPU requests (on the left, which corresponds to the top half of the table in Figure 6.11), as opposed to transitions based on bus requests (on the right, which corresponds Symmetric Shared-Memory Architectures to the bottom half of the table in Figure 6.11). Boldface type is used to distinguish the bus actions, as opposed to the conditions on which a state transition depends. The state in each node represents the state of the selected cache block specified by the processor or bus request. re Write miss for this block ac ac C Place write miss on bus Exclusive (read/write) Place read miss on bus Cache state transitions based on requests from CPU Exclusive (read/write) Shared (read only) rit e m -ba em ck or bl y oc ac k ce ; a ss bo CPU read miss Write miss for this block W re ad ad mis w m s rit is W e s m on rite is -b s ac on bu s k bu bl s oc k C PU w rit e Shared (read only) Write-back block; abort memory access Invalid Read miss for this block Cache state transitions based on requests from the bus CPU write miss Write-back cache block Place write miss on bus CPU write hit CPU read hit FIGURE 6.11 A write-invalidate, cache-coherence protocol for a write-back cache showing the states and state transitions for each block in the cache. The cache states are shown in circles with any access permitted by the CPU without a state transition shown in parenthesis under the name of the state. The stimulus causing a state change is shown on the transition arcs in regular type, and any bus actions generated as part of the state transition are shown on the transition arc in bold. The stimulus actions apply to a block in the cache, not to a specific address in the cache. Hence, a read miss to a block in the shared state is a miss for that cache block but for a different address. The left side of the diagram shows state transitions based on actions of the CPU associated with this cache; the right side shows transitions based on operations on the bus. A read miss in the exclusive or shared state and a write miss in the exclusive state occur when the address requested by the CPU does not match the address in the cache block. Such a miss is a standard cache replacement miss. An attempt to write a block in the shared state always generates a miss, even if the block is present in the cache, since the block must be made exclusive. Whenever a bus transaction occurs, all caches that contain the cache block specified in the bus transaction take the action dictated by the right half of the diagram. The protocol assumes that memory provides data on a read miss for a block that is clean in all caches. In actual implementations, these two sets of state diagrams are combined. This protocol is somewhat simpler than those in use in existing multiprocessors"
Explain why the total time for the write is 40 + 40 + 50 = 130 cycles,"When we wait for invalidates, each write takes the sum of the ownership time plus the time to complete the invalidates. Since the invalidates can overlap, we need only worry about the last one, which starts 10 + 10 + 10 + 10 = 40 cycles after ownership is established. Hence, the total time for the write is 40 + 40 + 50 = 130 cycles."," The simplest way to implement sequential consistency is to require a processor to delay the completion of any memory access until all the invalidations caused by that access are completed. Of course, it is equally effective to delay the next memory access until the previous one is completed. Remember that memory consistency involves operations among different variables: the two accesses that must be ordered are actually to different memory locations. In our example, we must delay the read of A or B (A==0 or B==0) until the previous write has completed (B=1 or A=1). Under sequential consistency, we cannot, for example, simply place the write in a write buffer and continue with the read. Although sequential consistency presents a simple programming paradigm, it reduces potential Models of Memory Consistency: An Introduction performance, especially in a multiprocessor with a large number of processors or long interconnect delays, as we can see in the following Example. EXAMPLE Suppose we have a processor where a write miss takes 40 cycles to establish ownership, 10 cycles to issue each invalidate after ownership is established, and 50 cycles for an invalidate to complete and be acknowledged once it is issued. Assuming that four other processors share a cache block, how long does a write miss stall the writing processor if the processor is sequentially consistent? Assume that the invalidates must be explicitly acknowledged before the directory controller knows they are completed. Suppose we could continue executing after obtaining ownership for the write miss without waiting for the invalidates; how long would the write take? When we wait for invalidates, each write takes the sum of the ownership time plus the time to complete the invalidates. Since the invalidates can overlap, we need only worry about the last one, which starts 10 + 10 + 10 + 10 = 40 cycles after ownership is established. Hence the total time for the write is 40 + 40 + 50 = 130 cycles. In comparison, the ownership time is only 40 cycles. With appropriate write-buffer implementations it is even possible to continue before ownership is established. n To provide better performance, researchers and architects have explored two different routes. First, they developed ambitious implementations that preserve sequential consistency but use latency hiding techniques to reduce the penalty; we discuss these in the section on cross-cutting issues (see page 731). Second, they developed less restrictive memory consistency models that allow for faster hardware. Such models can affect how the programmer sees the multiprocessor, so before we discuss these less restrictive models, lets look at what the programmer expects"
"Explain how, Wildfire is a cache-coherent nonuniform memory access architecture (cc-NUMA) with large nodes","Wildfire can connect 2 to 4 E6000 multiprocessors by replacing one dual processor (or I/O) board with a Wildfire Interface board (WFI), yielding up to 112 processors (4 x 28), as shown in Figure 6.45. The WFI board supports one coherent address space across all four multiprocessor nodes with the two high-order address bits used to designate which node contains a memory address. Hence,  Wildfire is a cache-coherent nonuniform memory access architecture (cc-NUMA) with large nodes."," Each processor board contains 2 UltraSPARC III processors. Wildfire can connect 2 to 4 E6000 multiprocessors by replacing one dual processor (or I/O) board with a Wildfire Interface board (WFI), yielding up to 112 processors (4 x 28), as shown in Figure 6.45. The WFI board supports one coherent address space across all four multiprocessor nodes with the two high-order address bits used to designate which node contains a memory address. Hence, Wildfire is a cache-coherent nonuniform memory access architecture (cc-NUMA) with large nodes. Within each node of 28 processors, memory is uniformly accessible, only processes that span nodes need to worry about the non uniformity in memory access times. The WFI plugs into the bus and sees all memory requests; it implements the global coherence across up to four nodes. Each WFI has three ports that connect to up to three additional Wildfire nodes, each with a dual directional 800 MB/sec connection. WFI uses a simple directory scheme, similar to that discussed in Section 6.7. To keep the amount of directory state small, the directory is actually a cache, which is backed by the main memory in the node. When an miss occurs, the request is routed to the home node for the requested address. When the re- Putting It All Together: Suns Wildfire Prototype Memory I/O CPU WFI WFI WFI FIGURE 6.45 The Wildfire Architecture uses a bus-based SUN Enterprise server as its building blocks. The WFI replaces a processor or I/O board and provides internode coherency and communication, as well as hardware support for page replication"
"Explain why, storage systems are typically held to a higher standard of dependability than the rest of the computer","After 15 years of doubling processor performance every 18 months, processor performance is not the problem it once was. Many would find highly dependable systems much more attractive than faster versions of todays systems with todays level of unreliability. Although it is frustrating when a program crashes, people become hysterical if they lose their data. Hence,  storage systems are typically held to a higher standard of dependability than the rest of the computer."," This shift in focus from computation to communication and storage of information emphasizes reliability and scalability as well as cost-performance. To reflect the increasing importance of I/O, the third edition of this book has twice as many I/O chapters as the first edition and half as many on instruction set architecture. This chapter covers storage I/O and the next covers communication I/O. Although two chapters cannot fully vindicate I/O, they may at least atone for some of the sins of the past and restore some balance. Types of Storage Devices Does Performance Matter? After 15 years of doubling processor performance every 18 months, processor performance is not the problem it once was. Many would find highly dependable systems much more attractive than faster versions of todays systems with todays level of unreliability. Although it is frustrating when a program crashes, people become hysterical if they lose their data. Hence, storage systems are typically held to a higher standard of dependability than the rest of the computer. Because of traditional demands placed on storageand because a new century needs new challengesthis chapter defines reliability, availability, and dependability and shows how to improve them. Dependability is the bedrock of storage, yet it also has its own rich performance theoryqueueing theorythat balances throughput versus response time"
"Explain why, I/O buses are more likely to be asynchronous than are memory buses"," The choice of synchronous versus asynchronous bus has implications not only for data bandwidth, but also for an I/O systems physical distance and the number of devices that can be connected to the bus. Hence,  I/O buses are more likely to be asynchronous than are memory buses."," Asynchrony makes it much easier to accommodate a variety of devices and to lengthen the bus without worrying about clock skew or synchronization problems. If a synchronous bus can be used, it is usually faster than an asynchronous bus because it avoids the overhead of synchronizing the bus for each transaction. The choice of synchronous versus asynchronous bus has implications not only for data bandwidth, but also for an I/O systems physical distance and the number of devices that can be connected to the bus. Hence, I/O buses are more likely to be asynchronous than are memory buses. Figure 7.11 suggests when to use one over the other. Bus Standards The number and variety of I/O devices is flexible on many computers, permitting customers to tailor computers to their needs. The I/O bus is the interface to which devices are connected. Standards that let the computer designer and I/O-device designer work independently play a large role in buses. As long as both designers meet the requirements, any I/O device can connect to any computer. The I/O bus standard is the document that defines how to connect devices to computers"
"Explain why, a system with hot spares and hot swapping need never go off-line","A related issue is hot swapping. Systems with hot swapping allow components to be replaced shutting down the computer. Hence,  a system with hot spares and hot swapping need never go off-line; the missing data is constructed immediately onto spares and the broken component is replaced to replenish the spare pool."," Another issue in the design of RAID systems is decreasing the mean time to repair. This reduction is typically done by adding hot spares to the system: extra disks that are not used in normal operation. When a failure occurs on an active disk in a RAID, an idle hot spare is first pressed into service. The data missing from the failed disk is then reconstructed onto the hot spare using the redundant data from the other RAID disks. If this process is performed automatically, MTTR is significantly reduced because waiting for the operator in the repair process is no longer the pacing item (see section 7.9). A related issue is hot swapping. Systems with hot swapping allow components to be replaced shutting down the computer. Hence, a system with hot spares and hot swapping need never go off-line; the missing data is constructed immediately onto spares and the broken component is replaced to replenish the spare pool. We cover here the most popular of these RAID levels; readers interested in more detail should see the paper by Chen et al. [1994]"
"Explain why, the term RAID 0 has become widely used","RAID 0 something of a misnomer as there is no redundancy, it is not in the original RAID taxonomy, and striping predates RAID. However, RAID levels are often left to the operator to set when creating a storage system, and RAID 0 is often listed as one of the options. Hence,  the term RAID 0 has become widely used."," No Redundancy (RAID 0) This notation is refers to a disk array in which data is striped but there is no redundancy to tolerate disk failure. Striping across a set of disks makes the collection appear to software as a single large disk, which simplifies storage management. It also improves performance for large accesses, since many disks can operate at once. Video editing systems, for example, often stripe their data. RAID 0 something of a misnomer as there is no redundancy, it is not in the original RAID taxonomy, and striping predates RAID. However, RAID levels are often left to the operator to set when creating a storage system, and RAID 0 is often listed as one of the options. Hence, the term RAID 0 has become widely used. Mirroring (RAID 1) This traditional scheme for tolerating disk failure, called mirroring or shadowing, uses twice as many disks as does RAID 0. Whenever data is written to one disk, that data is also written to a redundant disk, so that there are always two copies of the information. If a disk fails, the system just goes to the mirror to get the desired information. Mirroring is the most expensive RAID solution, since it requires the most disks"
"Explain why, we can look at the size and scope of failures, rather than assuming that all are equally important","Commission (FCC) requires that all telephone companies submit explanations when they experience an outage that affects at least 30,000 people or lasts thirty minutes. These detailed disruption reports do not suffer from the self-reporting problem of earlier figures, as investigators determine the cause of the outage rather than operators of the equipment. Kuhn [1997] studied the causes of outages between 1992 and 1994 and Enriquez [2001] did a follow-up study for the first half of 2001. In addition to reporting number of outages, the FCC data includes the number of customers affected and how long they were affected. Hence,  we can look at the size and scope of failures, rather than assuming that all are equally important."," 100% 15% Other: app, power, network System management: actions + N/problem Operating System 15% % Failures per Category 50% 18% 20% 0% Hardware 10% FIGURE 7.22 Causes of system failures on Digital VAX systems between 1985 and 1993 collected by Murphy and Gent [1995]. System management crashes include having several crashes for the same problem, suggesting that the problem was difficult for the operator to diagnose. It also included operator actions that directly resulted in crashes, such as giving parameters bad values, bad configurations, and bad application installation. FCC The final set of data comes from the government. The Federal Communications Commission (FCC) requires that all telephone companies submit explanations when they experience an outage that affects at least 30,000 people or lasts thirty minutes. These detailed disruption reports do not suffer from the self-reporting problem of earlier figures, as investigators determine the cause of the outage rather than operators of the equipment. Kuhn [1997] studied the causes of outages between 1992 and 1994 and Enriquez [2001] did a follow-up study for the first half of 2001. In addition to reporting number of outages, the FCC data includes the number of customers affected and how long they were affected. Hence, we can look at the size and scope of failures, rather than assuming that all are equally important. Figure 7.23 plots the absolute and relative number of customer-outage minutes for those years, broken into four categories: n n n n Although there was a significant improvement in failures due to overloading of the network over the years, failures due to humans increased, from about one third to two thirds of the customer-outage minutes. These four examples and others suggest that the primary cause of failures in large systems today is faults by human operators. Hardware faults have declined due to a decreasing number of chips in systems, reduced power, and fewer connectors. Hardware dependability has improved through fault tolerance techniques such as RAID. At least some operating systems are considering reliability implications before new adding features, so in 2001 the failures largely occur elsewhere"
"Explain why, the distinction between measurements and predicted distributions is often blurred","Queuing theory makes a sharp distinction between past events, which can be characterized by measurements using simple arithmetic, and future events, which are predictions requiring more sophisticated mathematics. In computer systems, we commonly predict the future from the past; one example is least recently used block replacement (see Chapter 5). Hence,  the distinction between measurements and predicted distributions is often blurred;"," As mentioned above, these equations and this section are based on an area of applied mathematics called queuing theory, which offers equations to predict behavior of such random variables. Real systems are too complex for queuing theory to provide exact analysis, and hence queuing theory works best when only approximate answers are needed. Queuing theory makes a sharp distinction between past events, which can be characterized by measurements using simple arithmetic, and future events, which are predictions requiring more sophisticated mathematics. In computer systems, we commonly predict the future from the past; one example is least recently used block replacement (see Chapter 5). Hence, the distinction between measurements and predicted distributions is often blurred; we use measurements to verify the type of distribution and then rely on the distribution thereafter. Lets review the assumptions about the queuing model: n n n The system is in equilibrium"
"Explain how, a market for large, slow, quiet disks may develop","The personal computer created a market for small form-factor disk drives, since the 14-inch disk drives used in mainframes were bigger than the PC. In 2001, the 3.5-inch drive is the market leader, although the smaller 2.5-inch drive needed for laptop computers is significant in sales volume. Personal video recorderswhich record television on disk instead of tapemay become a significant consumer of disk drives. Existing form factors and speed are sufficient, with the focus on low noise and high capacity for PVRs. Hence,  a market for large, slow, quiet disks may develop."," Because of this competition, the gap in time between when a density record is achieved in the lab and when a disk is shipped with that density has closed considerably. In 2001, the lab record is 60 Gbits/square inch, but drives are shipping with a third of that density. It is also unclear to disk engineers whether evolutionary change will achieve 1000 Gbits/square inch. The personal computer created a market for small form-factor disk drives, since the 14-inch disk drives used in mainframes were bigger than the PC. In 2001, the 3.5-inch drive is the market leader, although the smaller 2.5-inch drive needed for laptop computers is significant in sales volume. Personal video recorderswhich record television on disk instead of tapemay become a significant consumer of disk drives. Existing form factors and speed are sufficient, with the focus on low noise and high capacity for PVRs. Hence, a market for large, slow, quiet disks may develop. It remains to be seen whether hand-held devices or video cameras, requiring even smaller disks, will become as significant in sales volume as PCs or laptops. For example, 1.8-inch drives were developed in the early 1990s for palmtop computers, but that market chose Flash instead, and hence 1.8-inch drives disappeared. RAID The small form factor hard disks for PCs in the 1980s led a group at Berkeley to propose Redundant Arrays of Inexpensive Disks, or RAID. This group had worked on the Reduced Instruction Set Computers effort, and so expected much faster CPUs to become available. Their questions were what could be done with the small disks that accompanied their PCs, and what could be done in the area of I/O to keep up with much faster processors. They argued to replace one main- Historical Perspective and References frame drive with 50 small drives, as you could get much greater performance with that many independent arms. The many small drives even offered savings in power consumption and floor space"
"Explain why, each message must have at least 1 bit in addition to the data ","For one machine to get data from the other, it must first send a request containing the address of the data it desires from the other node. When a request arrives, the machine must send a reply with the data. Hence,  each message must have at least 1 bit in addition to the data to determine whether the message is a new request or a reply to an earlier request."," FIGURE 8.4 A simple network connecting two machines. For one machine to get data from the other, it must first send a request containing the address of the data it desires from the other node. When a request arrives, the machine must send a reply with the data. Hence, each message must have at least 1 bit in addition to the data to determine whether the message is a new request or a reply to an earlier request. The network must distinguish between information needed to deliver the message, typically called the header or the trailer depending on where it is relative to the data, and the payload, which contains the data. Figure 8.5 shows the format of messages in our simple network. This example shows a single-word payload, but messages in some interconnection networks can include hundreds of words. Interconnection networks involve normally software. Even this simple example invokes software to translate requests and replies into messages with the appropriate headers. An application program must usually cooperate with the operating system to send a message to another machine, since the network will be shared with all the processes running on the two machines, and the operating system cannot allow messages for one process to be received by another. Thus, the messaging software must have some way to distinguish between processes; this distinction may be included in an expanded header. Although hardware support can reduce the amount of work, most is done by software"
"Explain why, message size is important in getting full benefit of fast networks"," Thus, we must lower overhead as well as increase network bandwidth unless messages are very large.n Hence,  message size is important in getting full benefit of fast networks."," EXAMPLE Plot the effective bandwidth versus message size for overheads of 25 and 250 microseconds and for network bandwidths of 100, 1000, and 10000 Mbits/second. Vary message size from 16 bytes to 4 megabytes. For what message sizes is the effective bandwidth virtually the same as the raw network bandwidth? If overhead is 250 microseconds, for what message sizes is the effective bandwidth always less than 100 Mbits/second? microseconds and a network bandwidth of Y Mbits/second. To amortize the cost of high overhead, message sizes must be four megabytes for effective bandwidth to be about the same as network bandwidth. Assuming the high overhead, message sizes about 3K bytes or less will not break the 100 Mbits/second barrier no matter what the actual network bandwidth. Thus, we must lower overhead as well as increase network bandwidth unless messages are very large.n Hence, message size is important in getting full benefit of fast networks. What is the natural size of messages? Figure 8.9 above shows the size of Network File System (NFS) messages for 239 machines at Berkeley collected over a period of one week. One plot is cumulative in messages sent, and the other is cumulative in data bytes sent. The maximum NFS message size is just over 8 KB, yet 95% of the messages are less than 192 bytes long..Figure 8.10 below shows the similar results for Internet traffic, where the maximum transfer unit was 1500 bytes. Effective bandwidth (Mbits/sec) o25,bw10000 o25,bw1000 o25,bw100 o250,bw10000 o250,bw1000 o250,bw100 4K 64K 256K 1 M Message size (bytes) FIGURE 8.8 Bandwidth delivered versus message size for overheads of 25 and 250 microseconds and for network bandwidths of 100, 1000, and 10000 Mbits/second. Note that with 250 microseconds of overhead and a network bandwidth of 1000 Mbits/second, only the 4-MB message size gets an effective bandwidth of 1000 Mbits/second. In fact, message sizes must be greater than 256 B for the effective bandwidth to exceed 10 Mbits/second. The notation oX,bwY means an overhead of X microseconds and a network bandwidth of Y Mbits/second. <<Artist: label lines, drop legend.>> Again, 60% of the messages are less than 192 bytes long, and 1500-byte messages represented 50% of the bytes transferred. Many applications send far more small messages than large messages, since requests and acknowledgements are more frequent than data Summarizing this section, even this simple network has brought up the issues of protection, reliability, heterogeneity, software protocols, and a more sophisticated performance model. The next four sections address other key questions: n n n 8.2 100% 90% Messages 80% 70% 60% Cumulative percentage 50% 40% 30% Data bytes 20% 10% 19 25 10 15 20 25 30 35 40 51 61 71 81 12 Message size (bytes) FIGURE 8.9 Cumulative percentage of messages and data transferred as message size varies for NFS traffic. Each x-axis entry includes all bytes up to the next one; e.g., 32 represents 32 bytes to 63 bytes. More than half the bytes are sent in 8-KB messages, but 95% of the messages are less than 192 bytes. Figure 8.50 (page 651) shows details of this measurement. Collected at the University of California at Berkeley"
"Explain why, the network interface cards for fiber optics are considerably more expensive than for Cat5 copper wire"," In both cases, fiber optics has the additional cost of optical-to-electrical and electrical-to-optical conversion as part of the computer interface. Hence,  the network interface cards for fiber optics are considerably more expensive than for Cat5 copper wire."," These taps and repeaters also reduce optical signal strength, reducing the useful distance of a single piece of fiber. In both cases, fiber optics has the additional cost of optical-to-electrical and electrical-to-optical conversion as part of the computer interface. Hence, the network interface cards for fiber optics are considerably more expensive than for Cat5 copper wire. In 2001, most switches for fiber involve such a conversion to allow switching, although expensive all optical switches are beginning to be available. To achieve even more bandwidth from a fiber, wavelength division multiplexing (WDM) sends different streams simultaneously on the same fiber using different wavelengths of light, and then demultiplexes the different wavelengths at the receiver. In 2001, WDM can deliver a combined 40 Gbits/second using about 8 wavelengths, with plans to go to 80 wavelengths and deliver 400 Gbits/second"
"Explain why, the long distance lines are busy based on the number of conversations, and not on the amount of information being sent at a particular time","Although a good match for voice, frequency-division multiplexing is inefficient for sending data. The problem is that the frequency channel is dedicated to the conversation whether or not there is anything being said. Hence,  the long distance lines are busy based on the number of conversations, and not on the amount of information being sent at a particular time."," Connection-Oriented versus Connectionless Communication Before computers arrived on the scene, the telecommunications industry allowed communication around the world. An operator set up a connection between a caller and a callee, and once the connection is established, a conversation can continue for hours. To share transmission lines over long distances, the telecommunications industry used switches to multiplex several conversations on the same lines. Since audio transmissions have relatively low bandwidth, the solution was to divide the bandwidth of the transmission line into a fixed number of frequencies, with each frequency assigned to a conversation. This technique is called frequency-division multiplexing. Although a good match for voice, frequency-division multiplexing is inefficient for sending data. The problem is that the frequency channel is dedicated to the conversation whether or not there is anything being said. Hence, the long distance lines are busy based on the number of conversations, and not on the amount of information being sent at a particular time. An alternative style of communication is called connectionless, where each package is routed to the destination by looking at its address. The postal system is a good example of connectionless communication. Closely related to the idea of connection versus connectionless communication are the terms circuit switching and packet switching. Circuit switching is the traditional way to offer a connection-based service. A circuit is established from source to destination to carry the conversation, reserving bandwidth until the circuit is broken. The alternative to circuit-switched transmission is to divide the information into packets, or frames, with each packet including the destination of the packet plus a portion of the information. Queuing theory in section 6.4 tells us that packets cannot use all of the bandwidth, but in general, this packetswitched approach allows more use of the bandwidth of the medium and is the traditional way to support connectionless communication"
"Explain why, ATM switches are simpler conceptually","Switched media use three solutions for routing. In source-based routing, the message specifies the path to the destination. Since the network merely follows directions, it can be simpler. A second alternative is the virtual circuit, whereby a circuit is established between source and destination, and the message simply names the circuit to follow. ATM uses virtual circuits. The third approach is a destination-based routing, where the message merely contains a destination address, and the switch must pick a path to deliver the message. IP uses destination routing. Hence,  ATM switches are simpler conceptually; once a virtual circuit is established, packet switching is very fast."," Shared media has a simple solution: The message is broadcast to all nodes that share the media, and each node looks at an address within the message to see whether the message is for that node. This routing also made it easy to broadcast one message to all nodes by reserving one address for everyone; broadcast is much harder to support in switch-based networks. Switched media use three solutions for routing. In source-based routing, the message specifies the path to the destination. Since the network merely follows directions, it can be simpler. A second alternative is the virtual circuit, whereby a circuit is established between source and destination, and the message simply names the circuit to follow. ATM uses virtual circuits. The third approach is a destination-based routing, where the message merely contains a destination address, and the switch must pick a path to deliver the message. IP uses destination routing. Hence, ATM switches are simpler conceptually; once a virtual circuit is established, packet switching is very fast. On the other hand, IP routers must decide how to route every packet it receives by doing a routing table lookup on every packet. Destination-based routing may be deterministic and always follow the same path, or it may be adaptive, allowing the network to pick different routes to avoid failures or congestion. Closely related to adaptive routing is randomized routing, whereby the network will randomly pick between several equally good paths to spread the traffic throughout the network, thereby avoiding hot spots"
"Explain how, flow control helps congestion control, but it is not a universal solution"," Note that flow control describes just two nodes of the interconnection and not the total interconnection network between all end systems. Congestion control refers to schemes that reduce traffic when the collective traffic of all nodes is too large for the network to handle. Hence,  flow control helps congestion control, but it is not a universal solution."," A more sophisticated variation of feedback is for the ultimate destination to give the original sender a credit to send n packets before getting permission to send more. These are generically called credit-based flow control. A window is one version of credit-based flow control. The windows size determines the minimum frequency of communication from receiver to sender. The goal of the window is to send enough packets to overlap the latency of the interconnection with the overhead to send and receive a packet. The TCP protocol uses a window. This brings us to a point of confusion on terminology in many papers and textbooks. Note that flow control describes just two nodes of the interconnection and not the total interconnection network between all end systems. Congestion control refers to schemes that reduce traffic when the collective traffic of all nodes is too large for the network to handle. Hence, flow control helps congestion control, but it is not a universal solution. Choke packets are basis of the third scheme. The observation is that you only want to limit traffic when the network is congested. The idea is for each switch to see how busy it is, entering a warning state when it passes a threshold. Each packet received by the switch in a warning state are sent back to the source via a choke packet that includes the intended destination. The source is expected to reduce traffic to that destination by a fixed percentage. Since it likely will have already sent many packets along that path, it waits for all the packets in transit to be returned before taking choke packets seriously"
"Explain why, companies like Amazon, AOL, Google, Hotmail, Inktomi, WebTV, and Yahoo rely on clusters of PCs or workstations to provide services used by millions of people every day","Low cost, scaling and fault isolation proved a perfect match to the companies providing services over the Internet since the mid 1990s. Internet applications such as search engines and email servers are amenable to more loosely coupled computers, as the parallelism consists of millions of independent tasks. Hence,  companies like Amazon, AOL, Google, Hotmail, Inktomi, WebTV, and Yahoo rely on clusters of PCs or workstations to provide services used by millions of people every day."," A more radical approach is to keep storage outside of the cluster, possibly over a SAN, so that all computers inside can be treated as clones of one another. As the nodes may cost on the order of a few thousand dollars, it can be cheaper to simply discard a flaky node than spend the labor costs to try hard to repair it. The tasks of the failed node are then handed off to another clone. Clusters are also benefiting from faster SANs and from network interface cards that offer loweroverhead communication. Popularity of Clusters Low cost, scaling and fault isolation proved a perfect match to the companies providing services over the Internet since the mid 1990s. Internet applications such as search engines and email servers are amenable to more loosely coupled computers, as the parallelism consists of millions of independent tasks. Hence, companies like Amazon, AOL, Google, Hotmail, Inktomi, WebTV, and Yahoo rely on clusters of PCs or workstations to provide services used by millions of people every day. We delve into Google in section 8.11. Clusters are growing in popularity in the scientific computing market as well"
"Explain why, as more users share a channel there is more noise, and the signal to noise ratio gradually degrades","To enhance privacy, CDMA uses pseudo-random sequences from a set of 64 predefined codes. To synchronize the handset and base station so as to pick a common pseudo-random seed, CDMA relies on a clock from the Global Positioning System, which continuously transmits an accurate time signal. By carefully selecting the codes, the shared traffic sounds like random noise to the listener. Hence,  as more users share a channel there is more noise, and the signal to noise ratio gradually degrades."," Rather than send each signal five times as in AMPS, each bit is stretched so that it takes eleven times the minimum frequency, thereby accommodating interference and yet successful transmission. The base station receives the messages its separates them into the separate 9600 bits/second streams for each call. To enhance privacy, CDMA uses pseudo-random sequences from a set of 64 predefined codes. To synchronize the handset and base station so as to pick a common pseudo-random seed, CDMA relies on a clock from the Global Positioning System, which continuously transmits an accurate time signal. By carefully selecting the codes, the shared traffic sounds like random noise to the listener. Hence, as more users share a channel there is more noise, and the signal to noise ratio gradually degrades. Thus, the capacity of the CDMA system is a matter of taste, depending upon sensitivity of the listener to background noise. In addition, CDMA uses speech compression and varies the rate of data transferred depending how much activity is going on in the call. Both these techniques preserve bandwidth, which allows for more calls per cell. CDMA must regulate power carefully so that signals near the cell tower do not overwhelm those from far away, with the goal of all signals reach the tower at about the same level. The side benefit is that CDMA handsets emit less power, which both helps battery life and increases capacity when users are close to the tower"
Explain why conditional branches are properly considered as conditionally executing the unconditional branch instruction,"Its hard to pick the most unusual feature of ARM, but perhaps it is conditional execution of instructions. Every instruction starts with a 4-bit field that determines whether it will act as a nop or as a real instruction, depending on the condition codes. Hence, conditional branches are properly considered as conditionally executing the unconditional branch instruction."," Multiply/add and multiply/subtract are floating-point operations that can launch two independent floating-point operations in a single instruction in addition to the fused multiply/add and fused multiply/negate/add introduced in version 2.0 of PA-RISC. C.12 Instructions Unique to ARM Its hard to pick the most unusual feature of ARM, but perhaps it is conditional execution of instructions. Every instruction starts with a 4-bit field that determines whether it will act as a nop or as a real instruction, depending on the condition codes. Hence conditional branches are properly considered as conditionally executing the unconditional branch instruction. Conditional execution allows avoiding a branch to jump over a single instruction. It takes less code space and time to simply conditionally execute one instruction. The 12-bit immediate field has a novel interpretation. The 8 least-significant bits are zero-extended to a 32-bit value, then rotated right the number of bits specified in the first 4 bits of the field multiplied by 2. Whether this split actually catches more immediates than a simple 12-bit field would be an interesting study"
Explain the decision for one operand per instruction using a stack,"Kahans history [1990] of the stack architecture selection for the 8086 is entertaining. The floating-point architecture of the companion 8087 had to be retrofitted into the 8086 opcode space, making it inconvenient to offer two operands per instruction as found in the rest of the 8086. Hence, the decision for one operand per instruction using a stack: The designers task was to make a Virtue of this Necessity."," Although the 68000 was chosen for the popular Macintosh, the Macintosh was never as pervasive as the PC, partly because Apple did not allow clones until recently, and the 68000 did not acquire the same software leverage that the 8086 enjoys. The Motorola 68000 may have been more significant technically than the 8086, but the impact of the selection by IBM and IBMs open architecture strategy dominated the technical advantages of the 68000 in the market. Kahans history [1990] of the stack architecture selection for the 8086 is entertaining. The floating-point architecture of the companion 8087 had to be retrofitted into the 8086 opcode space, making it inconvenient to offer two operands per instruction as found in the rest of the 8086. Hence the decision for one operand per instruction using a stack: The designers task was to make a Virtue of this Necessity. Rather than the classical stack architecture, which has no provision for avoiding common subexpressions from being pushed and popped from memory into the top of the stack found in registers, Intel tried to combine a flat register file with a stack. The reasoning was the restriction of the top of stack as one operand was not so bad since it only required the execution of an FXCH instruction (which swapped registers) to get the same result as a two-operand instruction, and FXCH was much faster than the floating-point operations of the 8087. Since floating-point expressions are not that complex, Kahan reasoned that eight registers meant that the stack would rarely overflow. Hence he urged that the 8087 use this hybrid scheme with the provision that stack overflow or stack underflow would interrupt the 8086 so that interrupt software could give the illusion to the compiler writer of an unlimited stack for floating-point data. The Intel 8087 was implemented in Israel, and 7500 miles and 10 time zones made communication difficult from California. According to Palmer and Morse [1984]: Historical Perspective and References D-23 Unfortunately, nobody tried to write a software stack manager until after the 8087 was built, and by then it was too late; what was too complicated to perform in hardware turned out to be even worse in software. One thing found lacking is the ability to conveniently determine if an invalid-operation exception is indeed due to a stack overflow. . . . Also lacking is the ability to restart the instruction that caused the stack overflow . . . [p. 93] The result is that the stack exceptions are too slow to handle in software. As Kahan [1990] says: Consequently, almost all higher-level languages compilers emit inefficient code for the 80x87 family, degrading the chips performance by typically 50% with spurious stores and loads necessary simply to preclude stack over/underflow. . . "
"Explain why, measuring time in chimes is a better approximation for long vectors","Accompanying the notion of a convoy is a timing metric, called a chime, that can be used for estimating the performance of a vector sequence consisting of convoys. A chime is the unit of time taken to execute one convoy. A chime is an approximate measure of execution time for a vector sequence; a chime measurement is independent of vector length. Thus, a vector sequence that consists of m convoys executes in m chimes, and for a vector length of n, this is approximately m n clock cycles. A chime approximation ignores some processor-specific overheads, many of which are dependent on vector length. Hence,  measuring time in chimes is a better approximation for long vectors."," To simplify the discussion of vector execution and its timing, we will use the notion of a convoy, which is the set of vector instructions that could potentially begin execution together in one clock period. (Although the concept of a convoy is used in vector compilers, no standard terminology exists. Hence, we created the term convoy.) The instructions in a convoy must not contain any structural or data hazards (though we will relax this later); if such hazards were present, the instructions in the potential convoy would need to be serialized and initiated in different convoys. Placing vector instructions into a convoy is analogous to placing scalar operations into a VLIW instruction. To keep the analysis simple, we assume that a convoy of instructions must complete execution before any other instructions (scalar or vector) can begin execution. We will relax this in Section G.4 by using a less restrictive, but more complex, method for issuing instructions. Accompanying the notion of a convoy is a timing metric, called a chime, that can be used for estimating the performance of a vector sequence consisting of convoys. A chime is the unit of time taken to execute one convoy. A chime is an approximate measure of execution time for a vector sequence; a chime measurement is independent of vector length. Thus, a vector sequence that consists of m convoys executes in m chimes, and for a vector length of n, this is approximately m n clock cycles. A chime approximation ignores some processor-specific overheads, many of which are dependent on vector length. Hence, measuring time in chimes is a better approximation for long vectors. We will use the chime measurement, rather than clock cycles per result, to explicitly indicate that certain overheads are being ignored. If we know the number of convoys in a vector sequence, we know the execution time in chimes. One source of overhead ignored in measuring chimes is any limitation on initiating multiple vector instructions in a clock cycle. If only one vector instruction can be initiated in a clock cycle (the reality in most vector processors), the chime count will underestimate the actual execution time of a convoy. Because the vector length is typically much greater than the number of instructions in the convoy, we will simply assume that the convoy executes in one chime"
Explain why the overall product cost is lower because of the competition among the suppliers of the components and the volume efficiencies the suppliers can achieve,"Commodities are products that are sold by multiple vendors in large volumes and are essentially identical. Virtually all the products sold on the shelves of grocery stores are commodities, as are standard DRAMs, disks, monitors, and keyboards. In the past 10 years, much of the low end of the computer business has become a commodity business focused on building IBM-compatible PCs. There are a variety of vendors that ship virtually identical products and are highly competitive. Of course, this competition decreases the gap between cost and selling price, but it also decreases cost. Reductions occur because a commodity market has both volume and a clear product definition, which allows multiple suppliers to compete in building components for the commodity product. Hence, the overall product cost is lower because of the competition among the suppliers of the components and the volume efficiencies the suppliers can achieve."," Volume is a second key factor in determining cost. Increasing volumes affect cost in several ways. First, they decrease the time needed to get down the learning curve, which is partly proportional to the number of systems (or chips) manufactured. Second, volume decreases cost, since it increases purchasing and manufacturing efficiency. As a rule of thumb, some designers have estimated that cost decreases about 10% for each doubling of volume. Also, volume decreases the amount of development cost that must be amortized by each machine, thus allowing cost and selling price to be closer. We will return to the other factors influencing selling price shortly. Commodities are products that are sold by multiple vendors in large volumes and are essentially identical. Virtually all the products sold on the shelves of grocery stores are commodities, as are standard DRAMs, disks, monitors, and keyboards. In the past 10 years, much of the low end of the computer business has become a commodity business focused on building IBM-compatible PCs. There are a variety of vendors that ship virtually identical products and are highly competitive. Of course, this competition decreases the gap between cost and selling price, but it also decreases cost. Reductions occur because a commodity market has both volume and a clear product definition, which allows multiple suppliers to compete in building components for the commodity product. As a result, the overall product cost is lower because of the competition among the suppliers of the components and the volume efficiencies the suppliers can achieve. This has led to the low-end of the computer business being able to achieve better priceperformance than other sectors, and yielded greater growth at the low-end, albeit with very limited profits (as is typical in any commodity business). Cost of an Integrated Circuit Why would a computer architecture book have a section on integrated circuit costs? In an increasingly competitive computer marketplace where standard 16 MB 50 Dollars per DRAM chip 1 MB 256 KB Final chip cost 64 KB 16 KB 19 91 89 87 85 19 19 79 78 Year FIGURE 1.5 Prices of six generations of DRAMs (from 16Kb to 64 Mb) over time in 1977 dollars, showing the learning curve at work. A 1977 dollar is worth about $2.95 in 2001; more than half of this inflation occurred in the five-year period of 197782, during which the value changed to $1.59. The cost of a megabyte of memory has dropped incredibly during this period, from over $5000 in 1977 to about $0.35 in 2000, and an amazing $0.08 in 2001 (in 1977 dollars)! Each generation drops in constant dollar price by a factor of 10 to 30 over its lifetime. Starting in about 1996, an explosion of manufacturers has dramatically reduced margins and increased the rate at which prices fall, as well as the eventual final price for a DRAM"
Explain why the high-end of the embedded space has recently moved to multiple-issue processors!,"All the leading edge desktop and server processors are large, complex chips with more than 15 million transistors per processor. Notwithstanding, a simple two-way superscalar that issues FP instructions in parallel with integer instructions, or dual issues integer instructions (but not memory references) can probably be built with little impact on clock rate and with a tiny die size (in comparison to todays processes). Such a processor should perform well with a higher sustained to peak ratio than the high-end wide-issue processors and can be amazingly cost-effective. Hence, the high-end of the embedded space has recently moved to multiple-issue processors! Whether approaches based primarily on faster clock rates, simpler hardware, and more static scheduling or approaches using more sophisticated hardware to achieve lower CPI will win out is difficult to say and may depend on the benchmarks."," Rather than embracing dramatic new approaches in microarchitecture, the last five years have focused on raising the clock rates of multiple issue machines and narrowing the gap between peak and sustained performance. The dynamicallyscheduled, multiple-issue processors announced in the last two years (the Alpha 21264, the Pentium III and 4, and the AMD Athlon) have same basic structure and similar sustained issue rates (three to four instructions per clock) as the first dynamically-scheduled, multiple-issue processors announced in 1995! But, the clock rates are 4 to 8 times higher, the caches are 2 to 4 times bigger, there are 2 to 4 times as many renaming registers, and twice as many load/store units! The result is performance that is 6 to 10 times higher. All the leading edge desktop and server processors are large, complex chips with more than 15 million transistors per processor. Notwithstanding, a simple two-way superscalar that issues FP instructions in parallel with integer instructions, or dual issues integer instructions (but not memory references) can probably be built with little impact on clock rate and with a tiny die size (in comparison to todays processes). Such a processor should perform well with a higher sustained to peak ratio than the high-end wide-issue processors and can be amazingly cost-effective. As a result, the high-end of the embedded space has recently moved to multiple-issue processors! Whether approaches based primarily on faster clock rates, simpler hardware, and more static scheduling or approaches using more sophisticated hardware to achieve lower CPI will win out is difficult to say and may depend on the benchmarks. Practical Limitations on Exploiting More ILP Independent of the method used to exploit ILP, there are potential limitations that arise from employing more transistors. When the number of transistors employed is increased, the clock period is often determined by wire delays encountered both in distributing the clock and in the communication path of critical signals, Concluding Remarks such as those that signal exceptions. These delays make it more difficult to employ increased numbers of transistors to exploit more ILP, while also increasing the clock rate. These problems are sometimes overcome by adding additional stages, which are reserved just for communicating signals across longer wires"
Explain why both algorithms and compilers are changing from the traditional focus of reducing operations to reducing cache misses,"The increasing processor-memory gap has meant that cache misses are a primary cause of lower than expected performance. Hence, both algorithms and compilers are changing from the traditional focus of reducing operations to reducing cache misses."," Summary of Reducing Cache Miss Rate This section first presented the three Cs model of cache misses: compulsory, capacity, and conflict. This intuitive model led to three obvious optimizations: larger block size to reduce compulsory misses, larger cache size to reduce capacity misses, and higher associativity to reduce conflict misses. Since higher associativity may affect cache hit time or cache power consumption, way prediction checks only a piece of the cache for hits and then on a miss checks the rest. The final technique is the favorite of the hardware designer, leaving cache optimizations to the compiler. Reducing Cache Miss Penalty or Miss Rate via Parallelism j x k y 2 j z 0 1 i 3 k 4 5 FIGURE 5.22 The age of accesses to the arrays x, y, and z. Note in contrast to The increasing processor-memory gap has meant that cache misses are a primary cause of lower than expected performance. As a result, both algorithms and compilers are changing from the traditional focus of reducing operations to reducing cache misses. The next section increases performance by having the processor and memory hierarchy operate in parallel, with compilers again playing a significant role in orchestrating this parallelism"
Explain why raw bit error rates are typically a thousand to a million times higher than copper wire,"Typically, wireless communication is selected because the communicating devices are mobile or because wiring is inconvenient, which means the wireless network must rearrange itself dynamically. Such rearrangement makes routing more challenging. A second challenge is that wireless signals are not protected and hence are subject to mutual interference, especially as devices move. Power is the another challenge for wireless communication, both because the devices tend to be battery powered and because antennas radiate power to communicate and little of it reaches the receiver. Hence, raw bit error rates are typically a thousand to a million times higher than copper wire."," Description Path loss Received power divided by transmitted power; the radio must overcome signal-to-noise ratio (SNR) of noise from interference. Path loss is exponential in distance, and depends on interference if its above 100 meters 1 Watt transmit power, 1 GHz transmit frequency, 1 Mbits/sec data rate at 10-7 BER, distance between radios can be 728 meters in free space vs. 4 meters in a dense jungle Shadow fading Received signal blocked by objects, buildings outdoors or walls indoors; increase power to improve received SNR. It depends on the number of objects and their dielectric properties If transmitter is moving, need to change transmit power to ensure received SNR in region Multipath fading Interference between multiple versions of signal that arrive at different times, determined by time between fastest signal and slowest signal relative to signal bandwidth. 900 MHz transmit frequency signal power changes every 30 cm Frequency reuse, adjacent channel, narrow band interference FIGURE 8.46 Typically, wireless communication is selected because the communicating devices are mobile or because wiring is inconvenient, which means the wireless network must rearrange itself dynamically. Such rearrangement makes routing more challenging. A second challenge is that wireless signals are not protected and hence are subject to mutual interference, especially as devices move. Power is the another challenge for wireless communication, both because the devices tend to be battery powered and because antennas radiate power to communicate and little of it reaches the receiver. As a result, raw bit error rates are typically a thousand to a million times higher than copper wire. There are two primary architectures for wireless networks: base-station architectures and peer-to-peer architectures. Base stations are connected by land lines for longer distance communication, and the mobile units communicate only with a single local base station. Peer-to-peer architectures allow mobile units to communicate with each other, and messages hop from one unit to the next until delivered to the desired unit. Although peer-to-peer is more reconfigurable, base stations tend to be more reliable since there is only one hop between the device and the station. Cellular telephony, the most popular example of wireless networks, relies on radio with base stations"
Explain why all leading vector supercomputers are now built with the same CMOS technology as superscalar microprocessors,"Because of the huge investment in CMOS technology made possible by the success of the desktop computer, CMOS now offers competitive transistor performance with much greater transistor density and much reduced power dissipation compared with these more exotic technologies. Hence, all leading vector supercomputers are now built with the same CMOS technology as superscalar microprocessors."," For example, the cache on the Cray SV1 can support 384 outstanding cache misses per CPU, while for microprocessors 816 outstanding misses is a more typical maximum number. Historical Perspective and References G-43 Another example is the demise of bipolar ECL or gallium arsenide as technologies of choice for supercomputer CPU logic. Because of the huge investment in CMOS technology made possible by the success of the desktop computer, CMOS now offers competitive transistor performance with much greater transistor density and much reduced power dissipation compared with these more exotic technologies. As a result, all leading vector supercomputers are now built with the same CMOS technology as superscalar microprocessors. The primary reason that vector supercomputers now have lower clock rates than commodity microprocessors is that they are developed using standard cell ASIC techniques rather than full custom circuit design to reduce the engineering design cost. While a microprocessor design may sell tens of millions of copies and can amortize the design cost over this large number of units, a vector supercomputer is considered a success if over a hundred units are sold! Conversely, superscalar microprocessor designs have begun to absorb some of the techniques made popular in earlier vector computer systems. Many multimedia applications contain code that can be vectorized, and as discussed in Chapter 2, most commercial microprocessor ISAs have added multimedia extensions that resemble short vector instructions. A common technique is to allow a wide 64-bit register to be split into smaller subwords that are operated on in parallel. This idea was used in the early TI ASC and CDC STAR-100 vector machines, where a 64-bit lane could be split into two 32-bit lanes to give higher performance on lower-precision data. Although the initial microprocessor multimedia extensions were very limited in scope, newer extensions such as AltiVec for the IBM/Motorola PowerPC and SSE2 for the Intel x86 processors have both increased the vector length to 128 bits (still small compared with the 4096 bits in a VMIPS vector register) and added better support for vector compilers. Vector instructions are particularly appealing for embedded processors because they support high degrees of parallelism at low cost and with low power dissipation, and have been used in several game machines such as the Nintendo-64 and the Sony Playstation 2 to boost graphics performance. We expect that microprocessors will continue to extend their support for vector operations, as this represents a much simpler approach to boosting performance for an important class of applications compared with the hardware complexity of increasing scalar instruction issue width, or the software complexity of managing multiple parallel processors"
Explain why moving an instruction across a branch and making it conditional will slow the program down whenever the moved instruction would not have been normally executed,"Predicated instructions that are annulled (i.e., whose conditions are false) still take some processor resources. An annulled predicated instruction requires fetch resources at a minimum, and in most processors functional unit execution time. Hence, moving an instruction across a branch and making it conditional will slow the program down whenever the moved instruction would not have been normally executed."," The major complication in implementing predicated instructions is deciding when to annul an instruction. Predicated instructions may either be annulled during instruction issue or later in the pipeline before they commit any results or raise an exception. Each choice has a disadvantage. If predicated instructions are annulled early in the pipeline, the value of the controlling condition must be known early to prevent a stall for a data hazard. Since data dependent branch conditions, which tend to be less predictable, are candidates for conversion to predicated execution, this choice can lead to more pipeline stalls. Because of this potential for data hazard stalls, no design with predicated execution (or conditional move) annuls instructions early. Instead, all existing processors annul instructions later in the pipeline, which means that annulled instructions will consume functional unit resources and potentially have a negative impact on performance. A variety of other pipeline implementation techniques, such as forwarding, interact with predicated instructions further complicating the implementation. Predicated or conditional instructions are extremely useful for implementing short alternative control flows, for eliminating some unpredictable branches, and for reducing the overhead of global code scheduling. Nonetheless, the usefulness of conditional instructions is limited by several factors: n Predicated instructions that are annulled (i.e., whose conditions are false) still take some processor resources. An annulled predicated instruction requires fetch resources at a minimum, and in most processors functional unit execution time. Therefore, moving an instruction across a branch and making it conditional will slow the program down whenever the moved instruction would not have been normally executed. Likewise, predicating a control dependent portion of code and eliminating a branch may slow down the processor if that code would not have been executed. An important exception to these situations occurs when the cycles used by the moved instruction when it is not performed would have been idle anyway (as in the superscalar example above). Moving an instruction across a branch or converting a code segment to predicated execution is essentially speculating on the outcome of the branch. Conditional instructions make this easier but do not eliminate the execution time taken by an incorrect guess. In simple cases, where we trade a conditional move for a branch and a move, using conditional moves or predication is almost always better. When longer code sequences are made conditional, the benefits are more limited. n n Predicated instructions are most useful when the predicate can be evaluated early. If the condition evaluation and predicated instructions cannot be separated (because of data dependences in determining the condition), then a conditional instruction may result in a stall for a data hazard. With branch prediction and speculation, such stalls can be avoided, at least when the branches are predicted accurately"
"Explain why even if memory hierarchies for two computers are identical, the CPU with the higher clock rate has a larger number of clock cycles per miss and hence a higher memory portion of CPI","As this example illustrates, cache behavior can have enormous impact on performance. Furthermore, cache misses have a double-barreled impact on a CPU with a low CPI and a fast clock: 1. The lower the CPIexecution, the higher the relative impact of a fixed number of cache miss clock cycles. 2. When calculating CPI, the cache miss penalty is measured in CPU clock cycles for a miss. Hence, even if memory hierarchies for two computers are identical, the CPU with the higher clock rate has a larger number of clock cycles per miss and hence a higher memory portion of CPI."," Memory stall clock cycles CPU time = IC CPI execution + --------------------------------------------------------------- Clock cycle time Instruction The performance, including cache misses, is CPU timewith cache = IC (1.0 + (30 / 1000 100)) Clock cycle time = IC 4.00 Clock cycle time Now calculating performance using miss rate: Memory accesses CPU time = IC ( CPI execution + Miss rate ------------------------------------------ Miss penalty Clock cycle time Instruction CPU timewith cache = IC (1.0 + (1.5 2% 100)) Clock cycle time = IC 4.00 Clock cycle time The clock cycle time and instruction count are the same, with or without a cache. Thus, CPU time increases fourfold, with CPI from 1.00 for a perfect cache to 4.00 with a cache that can miss. Without any memory hierarchy at all the CPI would increase again to 1.0 + 100 1.5 or 151 a factor of almost 40 times longer than a system with a cache! n As this example illustrates, cache behavior can have enormous impact on performance. Furthermore, cache misses have a double-barreled impact on a CPU with a low CPI and a fast clock: 1. The lower the CPIexecution, the higher the relative impact of a fixed number of cache miss clock cycles. 2. When calculating CPI, the cache miss penalty is measured in CPU clock cycles for a miss. Therefore, even if memory hierarchies for two computers are identical, the CPU with the higher clock rate has a larger number of clock cycles per miss and hence a higher memory portion of CPI. The importance of the cache for CPUs with low CPI and high clock rates is thus greater, and, consequently, greater is the danger of neglecting cache behavior in assessing performance of such computers. Amdahls Law strikes again! Although minimizing average memory access time is a reasonable goaland we will use it in much of this chapterkeep in mind that the final goal is to reduce CPU execution time. The next example shows how these two can differ"
Explain why these multiprocessors are often called message passing multiprocessors,"With each of these organizations for the address space, there is an associated communication mechanism. For a multiprocessor with a shared address space, that address space can be used to communicate data implicitly via load and store operations; hence the name shared memory for such multiprocessors. For a multiprocessor with multiple address spaces, communication of data is done by explicitly passing messages among the processors. Hence, these multiprocessors are often called message passing multiprocessors."," Alternatively, the address space can consist of multiple private address spaces that are logically disjoint and cannot be addressed by a remote processor. In such multiprocessors, the same physical address on two different processors refers to two different locations in two different memories. Each processor-memory module is essentially a separate computer; therefore these parallel processors have been called multicomputers. As pointed out in the previous chapter, a multicomputer can even consist of completely separate computers connected on a local area network, which, today, are popularly called clusters. For applications that require little or no communication and can make use of separate memories, such clusters of processors, whether using a standardized or customized interconnect, can form a very cost-effective approach (see Section 7.x). With each of these organizations for the address space, there is an associated communication mechanism. For a multiprocessor with a shared address space, that address space can be used to communicate data implicitly via load and store operations; hence the name shared memory for such multiprocessors. For a multiprocessor with multiple address spaces, communication of data is done by explicitly passing messages among the processors. Therefore, these multiprocessors are often called message passing multiprocessors. In message passing multiprocessors, communication occurs by sending messages that request action or deliver data just as with the network protocols discussed in section 7.2. For example, if one processor wants to access or operate on data in a remote memory, it can send a message to request the data or to perform some operation on the data. In such cases, the message can be thought of as a remote procedure call (RPC). When the destination processor receives the message, either by polling for it or via an interrupt, it performs the operation or access on behalf of the remote processor and returns the result with a reply message. This type of message passing is also called synchronous, since the initiating processor sends a request and waits until the reply is returned before continuing. Software systems have been constructed to encapsulate the details of sending and receiving messages, including passing complex arguments or return values, presenting a clean RPC facility to the programmer"
Explain why we focus on implementation with write-back caches,"For a write-back cache, however, the problem of finding the most recent data value is harder, since the most recent value of a data item can be in a cache rather than in memory. Happily, write-back caches can use the same snooping scheme both for caches misses and for writes: Each processor snoops every address placed on the bus. If a processor finds that it has a dirty copy of the requested cache block, it provides that cache block in response to the read request and causes the memory access to be aborted. Since write-back caches generate lower requirements for memory bandwidth, they are greatly preferable in a multiprocessor, despite the slight increase in complexity. Hence, we focus on implementation with write-back caches."," The serialization of access enforced by the bus also forces serialization of writes, since when two processors compete to write to the same location, one must obtain bus access before the other. The first processor to obtain bus access will cause the other processors copy to be invalidated, causing writes to be strictly serialized. One implication of this scheme is that a write to a shared data item cannot complete until it obtains bus access. In addition to invalidating outstanding copies of a cache block that is being written into, we also need to locate a data item when a cache miss occurs. In a write-through cache, it is easy to find the recent value of a data item, since all written data are always sent to the memory, from which the most recent value of a data item can always be fetched. (Write buffers can lead to some additional complexities, which are discussed in section 6.8.) For a write-back cache, however, the problem of finding the most recent data value is harder, since the most recent value of a data item can be in a cache rather than in memory. Happily, write-back caches can use the same snooping scheme both for caches misses and for writes: Each processor snoops every address placed on the bus. If a processor finds that it has a dirty copy of the requested cache block, it provides that cache block in response to the read request and causes the memory access to be aborted. Since write-back caches generate lower requirements for memory bandwidth, they are greatly preferable in a multiprocessor, despite the slight increase in complexity. Therefore, we focus on implementation with write-back caches. The normal cache tags can be used to implement the process of snooping, and the valid bit for each block makes invalidation easy to implement. Read misses, whether generated by an invalidation or by some other event, are also straightforward since they simply rely on the snooping capability. For writes wed like to know whether any other copies of the block are cached, because, if there are no other cached copies, then the write need not be placed on the bus in a write-back cache. Not sending the write reduces both the time taken by the write and the required bandwidth"
"Explain why increasing the total amount of cache while keeping the total problem size fixed will have a more significant effect on the capacity miss rate, at least until each subgrid fits within an individual processors cache","In FFT, the capacity miss rate drops (from nearly 7% to just over 5%) but the coherence miss rate increases (from about 1% to about 2.7%), leading to a constant overall miss rate. Ocean shows a combination of effects, including some that relate to the partitioning of the grid and how grid boundaries map to cache blocks. For a typical 2D grid code the communication-generated misses are proportional to the boundary of each partition of the grid, while the capacity misses are proportional to the area of the grid. Hence, increasing the total amount of cache while keeping the total problem size fixed will have a more significant effect on the capacity miss rate, at least until each subgrid fits within an individual processors cache."," the number of processors from one to sixteen, while keeping the problem size constant. As we increase the number of processors, the total amount of cache increases, usually causing the capacity misses to drop. In contrast, increasing the processor count usually causes the amount of communication to increase, in turn causing the coherence misses to rise. The magnitude of these two effects differs by application. In FFT, the capacity miss rate drops (from nearly 7% to just over 5%) but the coherence miss rate increases (from about 1% to about 2.7%), leading to a constant overall miss rate. Ocean shows a combination of effects, including some that relate to the partitioning of the grid and how grid boundaries map to cache blocks. For a typical 2D grid code the communication-generated misses are proportional to the boundary of each partition of the grid, while the capacity misses are proportional to the area of the grid. Therefore, increasing the total amount of cache while keeping the total problem size fixed will have a more significant effect on the capacity miss rate, at least until each subgrid fits within an individual processors cache. The significant jump in miss rate between one and two proces- FFT 2% 8% 7% 1% 0% 2 Processor count Miss rate 4% 3% 2% Ocean 1% 0% 4 Processor count Barnes 1% Miss rate 0% 4 20% 18% 16% 14% 12% 10% 8% 6% 4% 2% 0% 4 Processor count Coherence miss rate FIGURE 6.23 Data miss rates can vary in nonobvious ways as the processor count is increased from one to sixteen. The miss rates include both coherence and capacity miss rates. The compulsory misses in these benchmarks are all very small and are included in the capacity misses. Most of the misses in these applications are generated by accesses to data that is potentially shared, although in the applications with larger miss rates (FFT and Ocean), it is the capacity misses rather than the coherence misses that comprise the majority of the miss rate. Data is potentially shared if it is allocated in a portion of the address space used for shared data. In all except Ocean, the potentially shared data is heavily shared, while in Ocean only the boundaries of the subgrids are actually shared, although the entire grid is treated as a potentially shared data object. Of course, since the boundaries change as we increase the processor count (for a fixed-size problem), different amounts of the grid become shared. The anomalous increase in capacity miss rate for Ocean in moving from one to two processors arises because of conflict misses in accessing the subgrids. In all cases except Ocean, the fraction of the cache misses caused by coherence transactions rises when a fixed-size problem is run on an increasing number of processors. In Ocean, the coherence misses initially fall as we add processors due to a large number of misses that are write ownership misses to data that is potentially, but not actually, shared. As the subgrids begin to fit in the aggregate cache (around 16 processors), this effect lessens. The single processor numbers include write upgrade misses, which occur in this protocol even if the data is not actually shared, since it is in the shared state. For all these runs, the cache size is 64 KB, two-way set associative, with 32-byte blocks. Notice that the scale on the y-axis for each benchmark is different, so that the behavior of the individual benchmarks can be seen clearly. Performance of Symmetric Shared-Memory Multiprocessors sors occurs because of conflicts that arise from the way in which the multiple grids are mapped to the caches. This conflict is present for direct-mapped and two-way set associative caches, but fades at higher associativities. Such conflicts are not unusual in array-based applications, especially when there are multiple grids in use at once. In Barnes and LU the increase in processor count has little effect on the miss rate, sometimes causing a slight increase and sometimes causing a slight decrease"
Explain why when comparing adders we will simply compare the number of logic levels in each one,"In general, the time a circuit takes to produce an output is proportional to the maximum number of logic levels through which a signal travels. However, determining the exact relationship between logic levels and timings is highly technology dependent. Hence, when comparing adders we will simply compare the number of logic levels in each one."," Later, however, we will see that setting the low-order carry-in bit to 1 is useful for performing subtraction. In general, the time a circuit takes to produce an output is proportional to the maximum number of logic levels through which a signal travels. However, determining the exact relationship between logic levels and timings is highly technology dependent. Therefore, when comparing adders we will simply compare the number of logic levels in each one. How many levels are there for a ripple-carry adder? It takes two levels to compute c1 from a0 and b0. Then it takes two more levels to compute c2 from c1, a1, b1, and so on, up to cn. So there are a total of 2n levels. Typical values of n are 32 for integer arithmetic and 53 for doubleprecision floating point. The ripple-carry adder is the slowest adder, but also the cheapest. It can be built with only n simple cells, connected in a simple, regular way. Because the ripple-carry adder is relatively slow compared with the designs discussed in Section H.8, you might wonder why it is used at all. In technologies like CMOS, even though ripple adders take time O(n), the constant factor is very small. In such cases short ripple adders are often used as building blocks in larger adders"
Explain why compilers wont generate code that utilizes a double-precision result,"A second issue concerns multiplication. Should the result of multiplying two n-bit numbers be a 2n-bit result, or should multiplication just return the low-order n bits, signaling overflow if the result doesnt fit in n bits? An argument in favor of an n-bit result is that in virtually all high-level languages, multiplication is an operation in which arguments are integer variables and the result is an integer variable of the same type. Hence, compilers wont generate code that utilizes a double-precision result."," What about unsigned addition? Notice that none of the architectures in Figure H.5 traps on unsigned overflow. The reason for this is that the primary use of unsigned arithmetic is in manipulating addresses. It is convenient to be able to subtract from an unsigned address by adding. For example, when n = 4, we can subtract 2 from the unsigned address 10 = 10102 by adding 14 = 11102. This generates an overflow, but we would not want a trap to be generated. A second issue concerns multiplication. Should the result of multiplying two n-bit numbers be a 2n-bit result, or should multiplication just return the low-order n bits, signaling overflow if the result doesnt fit in n bits? An argument in favor of an n-bit result is that in virtually all high-level languages, multiplication is an operation in which arguments are integer variables and the result is an integer variable of the same type. Therefore, compilers wont generate code that utilizes a double-precision result. An argument in favor of a 2n-bit result is that it can be used by an assembly language routine to substantially speed up multiplication of multiple-precision integers (by about a factor of 3). A third issue concerns machines that want to execute one instruction every cycle. It is rarely practical to perform a multiplication or division in the same amount of time that an addition or register-register move takes. There are three possible approaches to this problem. The first is to have a single-cycle multiplystep instruction. This might do one step of the Booth algorithm. The second Trap on signed overflow? Trap on unsigned overflow? Set bit on signed overflow? Set bit on unsigned overflow? If enable is on Yes. Add sets V bit"
"Explain why the ability to scale up the computing capacity, the memory, the storage, and the I/O bandwidth of a server are crucial","A second key feature of server systems is an emphasis on scalability. Server systems often grow over their lifetime in response to a growing demand for the services they support or an increase in functional requirements. Hence, the ability to scale up the computing capacity, the memory, the storage, and the I/O bandwidth of a server are crucial."," Why is availability crucial? Consider the servers running Yahoo!, taking orders for Cisco, or running auctions on EBay. Obviously such systems must be operating seven days a week, 24 hours a day. Failure of such a server system is far more catastrophic than failure of a single desktop. Although it is hard to estimate the cost of downtime, Figure 1.2 shows one analysis, assuming that downtime is distributed uniformly and does not occur solely during idle times. As we can see, the estimated costs of an unavailable system are high, and the estimated costs in Application Cost of downtime per hour (thousands of $) Annual losses (millions of $) with downtime of 1% (87.6 hrs/yr) 0.5% (43.8 hrs/yr) 0.1% (8.8 hrs/yr) $6,450 Credit card authorization $22.8 $6.6 Home shopping channel $9.9 $1.0 $7.9 $0.8 Catalog sales center Airline reservation center $7.9 $0.8 $1.8 On-line network fees $2.2 $0.2 $0.6 FIGURE 1.2 The cost of an unavailable system is shown by analyzing the cost of downtime (in terms of immediately lost revenue), assuming three different levels of availability. This assumes downtime is distributed uniformly. This data is from Kembel [2000] and was collected an analyzed by Contingency Planning Research. A second key feature of server systems is an emphasis on scalability. Server systems often grow over their lifetime in response to a growing demand for the services they support or an increase in functional requirements. Thus, the ability to scale up the computing capacity, the memory, the storage, and the I/O bandwidth of a server are crucial. Lastly, servers are designed for efficient throughput. That is, the overall performance of the serverin terms of transactions per minute or web pages served per secondis what is crucial. Responsiveness to an individual request remains important, but overall efficiency and cost-effectiveness, as determined by how many requests can be handled in a unit time, are the key metrics for most servers"
Explain why small changes in cost can have a larger than obvious impact,"The relationship between price and volume can increase the impact of changes in cost, especially at the low end of the market. Typically, fewer computers are sold as the price increases. Furthermore, as volume decreases, costs rise, leading to further increases in price. Hence, small changes in cost can have a larger than obvious impact."," Notice that the largest single item is the CPU, closely followed by the monitor. (Interestingly, in 1995, the DRAM memory at about 1/3 of the total cost was the most expensive component! Since then, cost per MB has dropped by about a factor of 15!) Touma [1993] discusses computer system costs and pricing in more detail. These numbers are based on estimates of volume pricing for the various components. The relationship between price and volume can increase the impact of changes in cost, especially at the low end of the market. Typically, fewer computers are sold as the price increases. Furthermore, as volume decreases, costs rise, leading to further increases in price. Thus, small changes in cost can have a larger than obvious impact. The relationship between cost and price is a complex one with entire books written on the subject. The purpose of this section is to give you a simple introduction to what factors determine price and typical ranges for these factors. The categories that make up price can be shown either as a tax on cost or as a percentage of the price. We will look at the information both ways. These differences between price and cost also depend on where in the computer marketplace a company is selling. To show these differences, Figure 1.10 shows how the dif- Cost, Price and their Trends ference between cost of materials and list price is decomposed, with the price increasing from left to right as we add each type of overhead"
Explain why the native MFLOPS rating is not the same as the normalized MFLOPS rating reported in the supercomputer literature,"FLOPS are created equal. To overcome this problem, normalized or weighted MFLOPS measures were developed. Figure 1.37 shows how the authors of the Livermore Loops benchmark calculate the number of normalized floating-point operations per program according to the operations actually found in the source code. Hence, the native MFLOPS rating is not the same as the normalized MFLOPS rating reported in the supercomputer literature, which has come as a surprise to a few computer designers."," [15] <1.5,1.9> Calculate the arithmetic, geometric, and harmonic means of MFLOPS for each machine. [10] <1.5,1.9> Which of the three means matches the relative performance of total execution time? OK exercise, but needs updating 1.16 [10/12] <1.9,1.6> One problem cited with MFLOPS as a measure is that not all FLOPS are created equal. To overcome this problem, normalized or weighted MFLOPS measures were developed. Figure 1.37 shows how the authors of the Livermore Loops benchmark calculate the number of normalized floating-point operations per program according to the operations actually found in the source code. Thus, the native MFLOPS rating is not the same as the normalized MFLOPS rating reported in the supercomputer literature, which has come as a surprise to a few computer designers. Real FP operations Add, Subtract, Compare, Multiply Divide, Square root Functions (Expo, Sin,...) FIGURE 1.37 Real versus normalized floating-point operations. The number of normalized floating-point operations per real operation in a program used by the authors of the Livermore FORTRAN Kernels, or Livermore Loops, to calculate MFLOPS. A kernel with one Add, one Divide, and one Sin would be credited with 13 normalized floating-point operations"
"Explain why even in computers that allow misaligned access, programs with aligned accesses run faster","Misalignment causes hardware complications, since the memory is typically aligned on a multiple of a word or double-word boundary. A misaligned memory access may, therefore, take multiple aligned memory references. Hence, even in computers that allow misaligned access, programs with aligned accesses run faster."," FIGURE 2.5 Aligned and misaligned addresses of byte, half word, word, and double word objects for byte addressed computers. For each misaligned example some objects require two memory accesses to complete. Every aligned object can always complete in one memory access, as long as the memory is as wide as the object. The figure shows the memory organized as 8 bytes wide. The byte offsets that label the columns specify the low-order three bits of the address. Why would someone design a computer with alignment restrictions? Misalignment causes hardware complications, since the memory is typically aligned on a multiple of a word or double-word boundary. A misaligned memory access may, therefore, take multiple aligned memory references. Thus, even in computers that allow misaligned access, programs with aligned accesses run faster. Memory Addressing Even if data are aligned, supporting byte, half-word, and word accesses requires an alignment network to align bytes, half words, and words in 64-bit registers. For example, in Figure 2.5 above, suppose we read a byte from an address with its three low order bits having the value 4. We will need shift right 3 bytes to align the byte to the proper place in a 64-bit register. Depending on the instruction, the computer may also need to sign-extend the quantity. Stores are easy: only the addressed bytes in memory may be altered. On some computers a byte, half word, and word operation does not affect the upper portion of a register. Although all the computers discussed in this book permit byte, half-word, and word accesses to memory, only the IBM 360/370, Intel 80x86, and VAX supports ALU operations on register operands narrower than the full width"
Explain why the usage of various addressing modes is quite important in helping the architect choose what to include," Addressing modes have the ability to significantly reduce instruction counts; they also add to the complexity of building a computer and may increase the average CPI (clock cycles per instruction) of computers that implement those modes. Hence, the usage of various addressing modes is quite important in helping the architect choose what to include."," the names differ among architectures. In this figure and throughout the book, we will use an extension of the C programming language as a hardware description notation. In this figure, only one non-C feature is used: The left arrow () is used for assignment. We also use the array Mem as the name for main memory and the array Regs for registers. Thus, Mem[Regs[R1]] refers to the contents of the memory location whose address is given by the contents of register 1 (R1). Later, we will introduce extensions for accessing and transferring data smaller than a word. Addressing modes have the ability to significantly reduce instruction counts; they also add to the complexity of building a computer and may increase the average CPI (clock cycles per instruction) of computers that implement those modes. Thus, the usage of various addressing modes is quite important in helping the architect choose what to include. Addressing mode Meaning Register Regs[R4]Regs[R4] + Regs[R3] Immediate Regs[R4]Regs[R4]+3 Displacement Regs[R4]Regs[R4] + Mem[100+Regs[R1]] Accessing local variables (+ simulates register indirect, direct addressing modes) Add R4,(R1) Regs[R4]Regs[R4] + Mem[Regs[R1]] Accessing using a pointer or a computed address"
"Explain why calculations that are exact in decimal can be close but inexact in binary, which can be a problem for financial transactions","One reason to use decimal operands is to get results that exactly match decimal numbers, as some decimal fractions do not have an exact representation in binary. For example, 0.1010 is a simple fraction in decimal but in binary it requires an infinite set of repeating digits: 0.0001100110011...2. Hence, calculations that are exact in decimal can be close but inexact in binary, which can be a problem for financial transactions."," For business applications, some architectures support a decimal format, usually called packed decimal or binary-coded decimal4 bits are used to encode the values 09, and 2 decimal digits are packed into each byte. Numeric character strings are sometimes called unpacked decimal, and operationscalled packing and unpackingare usually provided for converting back and forth between them. ; Type and Size of Operands One reason to use decimal operands is to get results that exactly match decimal numbers, as some decimal fractions do not have an exact representation in binary. For example, 0.1010 is a simple fraction in decimal but in binary it requires an infinite set of repeating digits: 0.0001100110011...2. Thus, calculations that are exact in decimal can be close but inexact in binary, which can be a problem for financial transactions. (See Appendix G <Float> to learn more about precise arithmetic.) Our SPEC benchmarks use byte or character, half word (short integer), word (integer), double word (long integer) and floating-point data types. Figure 2.12 shows the dynamic distribution of the sizes of objects referenced from memory for these programs. The frequency of access to different data types helps in deciding what types are most important to support efficiently. Should the computer have a 64-bit access path, or would taking two cycles to access a double word be satisfactory? As we saw earlier, byte accesses require an alignment network: How important is it to support bytes as primitives? Figure 2.12 uses memory references to examine the types of data being accessed. In some architectures, objects in registers may be accessed as bytes or half words. However, such access is very infrequenton the VAX, it accounts for no more than 12% of register references, or roughly 6% of all operand accesses in these programs"
Explain why 8B means an operation on 8 bytes in a single instruction,"B stands for byte (8 bits), H for halfword (16 bits), and W for word (32 bits). Hence, 8B means an operation on 8 bytes in a single instruction."," This doubling in performance is typically accomplished by doubling the number of floating-point units, making it more expensive than just suppressing carries in integer adders. DSP operations DSPs also provide operations found in the first three rows of Figure 2.15, but they change the semantics a bit. First, because they are often used in real time ap- Instruction category Alpha MAX HP PA-RISC MAX2 Intel Pentium MMX 4H 4H Multiply Compare Shift right/left Shift right arithmetic Power PC AltiVec 8B,4H,2W 4H,2W 16B, 8H, 4W 16B, 8H 8B,4H,2W (=,>) 16B, 8H, 4W (=,>,>=,<,<=) 16B, 8H, 4W 16B, 8H, 4W 8H Shift and add (saturating) And/or/xor 4H 8B,4H,2W 8B,4H,2W 8B,4H,2W 8B 8B Maximum/minimum 16B, 8H, 4W 2W->2B, 4H->4B 2B->2W, 4B->4H 4H,2W (=,not=,>,<=) 4H 4H->4B, 2W->2H 4W->4B, 8H->8B 2W->2H, 2W->2B, 4H>4B 2B->2W, 4B->4H 4B->4W, 8B->8H 4B->4H, 2*4B->8B FIGURE 2.17 Summary of multimedia support for desktop RISCs. Note the diversity of support, with little in common across the five architectures. All are fixed width operations, performing multiple narrow operations on either a 64-bit or 128bit ALU. B stands for byte (8 bits), H for halfword (16 bits), and W for word (32 bits). Thus, 8B means an operation on 8 bytes in a single instruction. Note that AltiVec assume a128-bit ALU, and the rest assume 64 bits. Pack and unpack use the notation 2*2W to mean 2 operands each with 2 words. This table is a simplification of the full multimedia architectures, leaving out many details. For example, HP MAX2 includes an instruction to calculate averages, and SPARC VIS includes instructions to set registers to constants. Also, this table does not include the memory alignment operation of AltiVec, MAX and VIS plications, there is not an option of causing an exception on arithmetic overflow (otherwise it could miss an event); thus, the result will be used no matter what the inputs. To support such an unyielding environment, DSP architectures use saturating arithmetic: if the result is too large to be represented, it is set to the largest representable number, depending on the sign of the result. In contrast, twos complement arithmetic can add a small positive number to a large positive number and end up with a negative result. DSP algorithms rely on saturating arithmetic, and would be incorrect if run on a computer without it. A second issue for DSPs is that there are several modes to round the wider accumulators into the narrower data words, just as the IEEE 754 has several rounding modes to chose from"
Explain why compilers make assumptions about the ability of later steps to deal with certain problems,"The complexity of writing a correct compiler is a major limitation on the amount of optimization that can be done. Although the multiple-pass structure helps reduce compiler complexity, it also means that the compiler must order and perform some transformations before others. In the diagram of the optimizing compiler in Figure 2.24, we can see that certain high-level optimizations are performed long before it is known what the resulting code will look like. Once such a transformation is made, the compiler cant afford to go back and revisit all steps, possibly undoing transformations. Such iteration would be prohibitive, both in compilation time and in complexity. Hence, compilers make assumptions about the ability of later steps to deal with certain problems."," The Structure of Recent Compilers To begin, lets look at what optimizing compilers are like today. Figure 2.24 shows the structure of recent compilers A compiler writers first goal is correctnessall valid programs must be compiled correctly. The second goal is usually speed of the compiled code. Typically, a whole set of other goals follows these two, including fast compilation, debugging support, and interoperability among languages. Normally, the passes in the compiler transform higher-level, more abstract representations into progressively lower-level representations. Eventually it reaches the instruction set. This structure helps manage the complexity of the transformations and makes writing a bug-free compiler easier. The complexity of writing a correct compiler is a major limitation on the amount of optimization that can be done. Although the multiple-pass structure helps reduce compiler complexity, it also means that the compiler must order and perform some transformations before others. In the diagram of the optimizing compiler in Figure 2.24, we can see that certain high-level optimizations are performed long before it is known what the resulting code will look like. Once such a transformation is made, the compiler cant afford to go back and revisit all steps, possibly undoing transformations. Such iteration would be prohibitive, both in compilation time and in complexity. Thus, compilers make assumptions about the ability of later steps to deal with certain problems. For example, com- Dependencies Language dependent; machine independent Front-end per language Function Transform language to common intermediate form Intermediate representation Somewhat language dependent, largely machine independent Small language dependencies; machine dependencies slight (e.g., register counts/types) Highly machine dependent; language independent High-level optimizations Global optimizer For example, loop transformations and procedure inlining (also called procedure integration) Including global and local optimizations + register allocation Detailed instruction selection and machine-dependent optimizations; may include or be followed by assembler FIGURE 2.24 Compilers typically consist of two to four passes, with more highly optimizing compilers having more passes. This structure maximizes the probability that a program compiled at various levels of optimization will produce the same output when given the same input. The optimizing passes are designed to be optional and may be skipped when faster compilation is the goal and lower quality code is acceptable. A pass is simply one phase in which the compiler reads and transforms the entire program. (The term phase is often used interchangeably with pass.) Because the optimizing passes are separated, multiple languages can use the same optimizing and code-generation passes. Only a new front end is required for a new language. pilers usually have to choose which procedure calls to expand in-line before they know the exact size of the procedure being called. Compiler writers call this problem the phase-ordering problem"
Explain why the notion of a company standardizing on a single architecture was a radical one,"They believed that a family of computers of the same architecture should be able to run the same software. Although this idea may seem obvious to us today, it was quite novel at that time. IBM, although it was the leading company in the industry, had five different architectures before the 360. Hence, the notion of a company standardizing on a single architecture was a radical one."," Although a few have proposed hardware to directly execute the JVM instructions (see [McGhan 1998]), thus far none of these proposals have been significant commercially. The hope instead is that Just In Time (JIT) Java compilerswhich compile during run time to the native instruction set of the computer running the Java programwill overcome the performance penalty of interpretation. The popularity of Java has also lead to compilers that compile directly into the native hardware instruction sets, bypassing the illusion of the Java bytecodes. Computer Architecture Defined IBM coined the term computer architecture in the early 1960s. Amdahl, Blaauw, and Brooks [1964] used the term to refer to the programmer-visible portion of the IBM 360 instruction set. They believed that a family of computers of the same architecture should be able to run the same software. Although this idea may seem obvious to us today, it was quite novel at that time. IBM, although it was the leading company in the industry, had five different architectures before the 360. Thus, the notion of a company standardizing on a single architecture was a radical one. The 360 designers hoped that defining a common architecture would bring six different divisions of IBM together. Their definition of architecture was ... the structure of a computer that a machine language programmer must understand to write a correct (timing independent) program for that machine. The term machine language programmer meant that compatibility would hold, even in machine language, while timing independent allowed different implementations. This architecture blazed the path for binary compatibility, which others have followed"
Explain why engineers were motivated to adapt microprocessor technology to the needs of DSP applications,"Predictable program flow (typically characterized by nested loops) hardware proved effective in some applications, but lacked the flexibility and reusability of a programmable processor. Hence, engineers were motivated to adapt microprocessor technology to the needs of DSP applications."," Predictable program flow (typically characterized by nested loops). 2.16 hardware proved effective in some applications, but lacked the flexibility and reusability of a programmable processor. Thus, engineers were motivated to adapt microprocessor technology to the needs of DSP applications. The first commercial DSPs emerged in the early 1980s, about 10 years after Intels introduction of the 4004. A number of companies, including Intel, developed early DSPs, but most of these early devices were not commercially successful. NECs PD7710, introduced in 1980, became the first merchant-market DSP to ship in volume quantities, but was hampered by weak development tools"
"Explain why if there is a dependence between two closely spaced instructions in the pipeline, this will lead to a hazard and a stall will result"," Instructions are issued in program order and if an instruction is stalled in the pipeline, no later instructions can proceed. Hence, if there is a dependence between two closely spaced instructions in the pipeline, this will lead to a hazard and a stall will result."," Although a dynamically scheduled processor cannot change the data flow, it tries to avoid stalling when dependences, which could generate hazards, are present. In contrast, static pipeline scheduling by the compiler (covered in the next chapter) tries to minimize stalls by separating dependent instructions so that they will not lead to hazards. Of course, compiler pipeline scheduling can also be used on code destined to run on a processor with a dynamically scheduled pipeline. Dynamic Scheduling: The Idea A major limitation of the simple pipelining techniques we discuss in Appendix A is that they all use in-order instruction issue and execution: Instructions are issued in program order and if an instruction is stalled in the pipeline, no later instructions can proceed. Thus, if there is a dependence between two closely spaced instructions in the pipeline, this will lead to a hazard and a stall will result. If there are multiple functional units, these units could lie idle. If instruction j depends on a long-running instruction i, currently in execution in the pipeline, then all instruc- tions after j must be stalled until i is finished and j can execute. For example, consider this code: DIV.D ADD.D SUB.D F0,F2,F4 F10,F0,F8 F12,F8,F14 The SUB.D instruction cannot execute because the dependence of ADD.D on DIV.D causes the pipeline to stall; yet SUB.D is not data dependent on anything in the pipeline. This hazard creates a performance limitation that can be eliminated by not requiring instructions to execute in program order. In the classic five-stage pipeline developed in the first chapter, both structural and data hazards could be checked during instruction decode (ID): When an instruction could execute without hazards, it was issued from ID knowing that all data hazards had been resolved. To allow us to begin executing the SUB.D in the above example, we must separate the issue process into two parts: checking for any structural hazards and waiting for the absence of a data hazard. We can still check for structural hazards when we issue the instruction; thus, we still use inorder instruction issue (i.e., instructions issue in program order), but we want an instruction to begin execution as soon as its data operand is available. Thus, this pipeline does out-of-order execution, which implies out-of-order completion"
"Explain why this pipeline does out-of-order execution, which implies out-of-order completion","In the classic five-stage pipeline developed in the first chapter, both structural and data hazards could be checked during instruction decode (ID): When an instruction could execute without hazards, it was issued from ID knowing that all data hazards had been resolved. To allow us to begin executing the SUB.D in the above example, we must separate the issue process into two parts: checking for any structural hazards and waiting for the absence of a data hazard. We can still check for structural hazards when we issue the instruction; thus, we still use inorder instruction issue (i.e., instructions issue in program order), but we want an instruction to begin execution as soon as its data operand is available. Hence, this pipeline does out-of-order execution, which implies out-of-order completion."," Dynamic Scheduling: The Idea A major limitation of the simple pipelining techniques we discuss in Appendix A is that they all use in-order instruction issue and execution: Instructions are issued in program order and if an instruction is stalled in the pipeline, no later instructions can proceed. Thus, if there is a dependence between two closely spaced instructions in the pipeline, this will lead to a hazard and a stall will result. If there are multiple functional units, these units could lie idle. If instruction j depends on a long-running instruction i, currently in execution in the pipeline, then all instruc- tions after j must be stalled until i is finished and j can execute. For example, consider this code: DIV.D ADD.D SUB.D F0,F2,F4 F10,F0,F8 F12,F8,F14 The SUB.D instruction cannot execute because the dependence of ADD.D on DIV.D causes the pipeline to stall; yet SUB.D is not data dependent on anything in the pipeline. This hazard creates a performance limitation that can be eliminated by not requiring instructions to execute in program order. In the classic five-stage pipeline developed in the first chapter, both structural and data hazards could be checked during instruction decode (ID): When an instruction could execute without hazards, it was issued from ID knowing that all data hazards had been resolved. To allow us to begin executing the SUB.D in the above example, we must separate the issue process into two parts: checking for any structural hazards and waiting for the absence of a data hazard. We can still check for structural hazards when we issue the instruction; thus, we still use inorder instruction issue (i.e., instructions issue in program order), but we want an instruction to begin execution as soon as its data operand is available. Thus, this pipeline does out-of-order execution, which implies out-of-order completion. Out-of-order execution introduces the possibility of WAR and WAW hazards, which do not exist in the five-stage integer pipeline and its logical extension to an in-order floating-point pipeline. Consider the following MIPS floating-point code sequence: DIV.D ADD.D SUB.D MULT.D F0,F2,F4 F6,F0,F8 F8,F10,F14 F6,F10,F8 There is an antidependence between the ADD.D and the SUB.D, and if the pipeline executes the SUB.D before the ADD.D (which is waiting for the DIV.D), it will violate the antidependence, yielding a WAR hazard. Likewise, to avoid violating output dependences, such as the write of F6 by MULT.D, WAW hazards must be handled. As we will see, both these hazards are avoided by the use of register renaming"
Explain why the prediction accuracy for this branch that is taken 90% of the time is only 80% (two incorrect predictions and eight correct ones),"The steady-state prediction behavior will mispredict on the first and last loop iterations. Mispredicting the last iteration is inevitable since the prediction bit will say taken (the branch has been taken nine times in a row at that point). The misprediction on the first iteration happens because the bit is flipped on prior execution of the last iteration of the loop, since the branch was not taken on that iteration. Hence, the prediction accuracy for this branch that is taken 90% of the time is only 80% (two incorrect predictions and eight correct ones)."," This simple one-bit prediction scheme has a performance shortcoming: Even if a branch is almost always taken, we will likely predict incorrectly twice, rather than once, when it is not taken. The following example shows this. EXAMPLE Consider a loop branch whose behavior is taken nine times in a row, then not taken once. What is the prediction accuracy for this branch, assuming the prediction bit for this branch remains in the prediction buffer? The steady-state prediction behavior will mispredict on the first and last loop iterations. Mispredicting the last iteration is inevitable since the prediction bit will say taken (the branch has been taken nine times in a row at that point). The misprediction on the first iteration happens because the bit is flipped on prior execution of the last iteration of the loop, since the branch was not taken on that iteration. Thus, the prediction accuracy for this branch that is taken 90% of the time is only 80% (two incorrect predictions and eight correct ones). In general, for branches used to form loopsa branch is taken many times in a row and then not taken once a one-bit predictor will mispredict at twice the rate that the branch is not taken. It seems that we should expect that the accuracy of the predictor would at least match the taken branch frequency for these highly regular branches. n To remedy this, two-bit prediction schemes are often used. In a two-bit scheme, a prediction must miss twice before it is changed. Figure 3.7 shows the finite-state processor for a two-bit prediction scheme"
Explain why by the end of ID we know enough to fetch the next predicted instruction," For the classic, five-stage pipeline, a branch-prediction buffer is accessed during the ID cycle, so that at the end of ID we know the branch-target address (since it is computed during ID), the fall-through address (computed during IF), and the prediction. Hence, by the end of ID we know enough to fetch the next predicted instruction."," Branch Target Buffers To reduce the branch penalty for our five-stage pipeline, we need to know from what address to fetch by the end of IF. This requirement means we must know whether the as-yet-undecoded instruction is a branch and, if so, what the next PC should be. If the instruction is a branch and we know what the next PC should be, we can have a branch penalty of zero. A branch-prediction cache that stores the predicted address for the next instruction after a branch is called a branch-target buffer or branch-target cache. For the classic, five-stage pipeline, a branch-prediction buffer is accessed during the ID cycle, so that at the end of ID we know the branch-target address (since it is computed during ID), the fall-through address (computed during IF), and the prediction. Thus, by the end of ID we know enough to fetch the next predicted instruction. For a branch-target buffer, we access the buffer during the IF stage using the instruction address of the fetched instruction, a possible branch, to index the buffer. If we get a hit, then we know the predicted instruction address at the end of the IF cycle, which is one cycle earlier than for a branch-prediction buffer. Because we are predicting the next instruction address and will send it out before decoding the instruction, we must know whether the fetched instruction is predicted as a taken branch. Figure 3.19 shows what the branch-target buffer looks like. If the PC of the fetched instruction matches a PC in the buffer, then the corresponding predicted PC is used as the next PC. In Chapter 5 we will discuss caches in much more detail; we will see that the hardware for this branch-target buffer is essentially identical to the hardware for a cache"
Explain why focusing on procedure returns seems appropriate,"Another method that designers have studied and included in many recent processors is a technique for predicting indirect jumps, that is, jumps whose destination address varies at runtime. Although high-level language programs will generate such jumps for indirect procedure calls, select or case statements, and FORTRAN-computed gotos, the vast majority of the indirect jumps come from procedure returns. For example, for the SPEC89 benchmarks procedure returns account for 85% of the indirect jumps on average. For languages like C++ and Java, procedure returns are even more frequent. Hence, focusing on procedure returns seems appropriate."," As designers try to increase the number of instructions executed per clock, instruction fetch will become an ever more significant bottleneck and clever new ideas will be needed to deliver instructions at the necessary rate. One of the emerging ideas, called trace caches, is discussed in Chapter 5. Return Address Predictors Another method that designers have studied and included in many recent processors is a technique for predicting indirect jumps, that is, jumps whose destination address varies at runtime. Although high-level language programs will generate such jumps for indirect procedure calls, select or case statements, and FORTRAN-computed gotos, the vast majority of the indirect jumps come from procedure returns. For example, for the SPEC89 benchmarks procedure returns account for 85% of the indirect jumps on average. For languages like C++ and Java, procedure returns are even more frequent. Thus, focusing on procedure returns seems appropriate. Though procedure returns can be predicted with a branch-target buffer, the accuracy of such a prediction technique can be low if the procedure is called from High Performance Instruction Delivery multiple sites and the calls from one site are not clustered in time. To overcome this problem, the concept of a small buffer of return addresses operating as a stack has been proposed. This structure caches the most recent return addresses: pushing a return address on the stack at a call and popping one off at a return. If the cache is sufficiently large (i.e., as large as the maximum call depth), it will predict the returns perfectly. Figure 3.22 shows the performance of such a return buffer with 116 elements for a number of the SPEC benchmarks. We will use this type of return predictor when we examine the studies of ILP in section 3.8"
Explain why instruction issue is likely to be one limitation on the clock rate of superscalar processors,"As we increase the processors issue rate, further pipelining of the issue stage could become necessary. Although breaking the issue stage into two stages is reasonably straightforward, it is less obvious how to pipeline it further. Hence, instruction issue is likely to be one limitation on the clock rate of superscalar processors."," This division is not, however, totally straightforward because the processor must also detect any hazards between the two packets of instructions while they are still in the issue pipeline. One approach is to use the first stage of the issue pipeline to decide how many instructions from the packet can issue simultaneously, ignoring instructions already issued, and use the second stage to examine hazards among the selected instructions and those that have already been issued. By splitting the issue pipestage and pipelining it, the performance cost of superscalar instruction issue tends to be higher branch penalties, further increasing the importance of branch prediction. As we increase the processors issue rate, further pipelining of the issue stage could become necessary. Although breaking the issue stage into two stages is reasonably straightforward, it is less obvious how to pipeline it further. Thus, instruction issue is likely to be one limitation on the clock rate of superscalar processors. A Statically Scheduled Superscalar MIPS Processor What would the MIPS processor look like as a superscalar? For simplicity, lets assume two instructions can be issued per clock cycle and that one of the instructions can be a load, store, branch, or integer ALU operation, and the other can be any floating-point operation. Note that we consider loads and stores, including those to floating-point registers, as integer operations. As we will see, issue of an integer operation in parallel with a floating-point operation is much simpler and less demanding than arbitrary dual issue. This configuration is, in fact, very close to the organization used in the HP 7100 processor. Although high-end desktop processors now do four or more issues per clock, dual issue superscalar pipelines are becoming common at the high-end of the embedded processor market"
"Explain why the number of cycles of latency between a source instruction and an instruction consuming the result is one cycle for integer ALU operations, two cycles for loads, and three cycles for FP add","Assume that both a floating-point and an integer operation can be issued on every clock cycle, even if they are dependent. Assume one integer functional unit used for both ALU operations and effective address calculations and a separate pipelined FP functional unit for each operation type. Assume that issue and write results take one cycle each and that there is dynamic branch-prediction hardware and a separate functional unit to evaluate branch conditions. As in most dynamically scheduled processors, the presence of the write results stage means that the effective instruction latencies will be one cycle longer than in a simple in-order pipeline. Hence, the number of cycles of latency between a source instruction and an instruction consuming the result is one cycle for integer ALU operations, two cycles for loads, and three cycles for FP add."," Assume that we have the most general implementation of a two issue dynamically scheduled processor, meaning that it can issue any pair of instructions if there are reservation stations of the right type available. Because the interaction of the integer and floating point instructions is crucial, we also extend Tomasulos scheme to deal with both the integer and floating point functional units and registers. Lets see how a simple loop executes on this processor. EXAMPLE Consider the execution of the following simple loop, which adds a scalar in F2 to each element of a vector in memory. Use a MIPS pipeline extended with Tomasulos algorithm and with multiple issue: Loop: ; F0=array element ; add scalar in F2 ; store result ; decrement pointer ; 8 bytes (per DW) BNE R1,R2,LOOP ; branch R1!=zero Assume that both a floating-point and an integer operation can be issued on every clock cycle, even if they are dependent. Assume one integer functional unit used for both ALU operations and effective address calculations and a separate pipelined FP functional unit for each operation type. Assume that issue and write results take one cycle each and that there is dynamic branch-prediction hardware and a separate functional unit to evaluate branch conditions. As in most dynamically scheduled processors, the presence of the write results stage means that the effective instruction latencies will be one cycle longer than in a simple in-order pipeline. Thus, the number of cycles of latency between a source instruction and an instruction consuming the result is one cycle for integer ALU operations, two cycles for loads, and three cycles for FP add. Create a table showing when each instruction issues, begins execution, and writes its result to the CDB for the first three iterations of the loop. Assume two CDBs and assume that branches single issue (no delayed branches) but that branch prediction is perfect. Also show the resource usage for the integer unit, the floating point unit, the data cache, and the two CDBs. L.D ADD.D S.D DADDIU F0,0(R1) F4,F0,F2 F4,0(R1) R1,R1,#-8 The loop will be dynamically unwound and, whenever possible, instructions will be issued in pairs. The execution timing is shown in continue to fetch and issue a new loop iteration every three clock cycles and sustaining one iteration every three cycles would lead to an IPC of 5/ 3 = 1.67. The instruction execution rate, however, is lower: by looking at the execute stage we can see that the sustained instruction completion rate is 15/16 = 0.94. Assuming the branches are perfectly predicted, the issue unit will eventually fill all the reservation stations and will stall"
"Explain why all the aspects of handling branchesprediction accuracy, misprediction detection, and misprediction recoveryincrease in importance","In practice, machines that speculate try to recover as early as possible after a branch is mispredicted. This recovery can be done by clearing the ROB for all entries that appear after the mispredicted branch, allowing those that are before the branch in the ROB to continue, and restarting the fetch at the correct branch successor. In speculative processors, however, performance is more sensitive to the branch prediction mechanisms, since the impact of a misprediction will be higher. Hence, all the aspects of handling branchesprediction accuracy, misprediction detection, and misprediction recoveryincrease in importance."," Reorder buffer Entry Instruction Destination L.D Commit Mem[0+Regs[R1]] no F4,F0,F2 S.D Write result yes Write result Regs[R1]8 yes R1,R2,Loop L.D Write result Mem[#4] yes F4,F0,F2 S.D Write result yes Write result #4 8 yes Write result FP register status Field Reorder # Busy F3 no no yes F8 no no FIGURE 3.31 Only the L.D and MUL.D instructions have committed, though all the others have completed execution. Hence, no reservation stations are busy and none are shown. The remaining instructions will be committed as fast as possible. The first two reorder buffers are empty, but are shown for completeness. In practice, machines that speculate try to recover as early as possible after a branch is mispredicted. This recovery can be done by clearing the ROB for all entries that appear after the mispredicted branch, allowing those that are before the branch in the ROB to continue, and restarting the fetch at the correct branch successor. In speculative processors, however, performance is more sensitive to the branch prediction mechanisms, since the impact of a misprediction will be higher. Thus, all the aspects of handling branchesprediction accuracy, misprediction detection, and misprediction recoveryincrease in importance. Exceptions are handled by not recognizing the exception until it is ready to commit. If a speculated instruction raises an exception, the exception is recorded in the ROB. If a branch misprediction arises and the instruction should not have been executed, the exception is flushed along with the instruction when the ROB is cleared. If the instruction reaches the head of the ROB, then we know it is no longer speculative and the exception should really be taken. We can also try to handle exceptions as soon as they arise and all earlier branches are resolved, but this is more challenging in the case of exceptions than for branch mispredict and, because it occurs less frequently, not as critical"
Explain why a dynamic processor might be able to more closely match the amount of parallelism uncovered by our ideal processor,"To build a processor that even comes close to perfect branch prediction and perfect alias analysis requires extensive dynamic analysis, since static compile-time schemes cannot be perfect. Of course, most realistic dynamic schemes will not be perfect, but the use of dynamic schemes will provide the ability to uncover parallelism that cannot be analyzed by static compile-time analysis. Hence, a dynamic processor might be able to more closely match the amount of parallelism uncovered by our ideal processor."," The floating-point programs are loop-intensive and have large amounts of loop-level parallelism. Artist, Please round the value labels to integers on this graph. Limitations on the Window Size and Maximum Issue Count To build a processor that even comes close to perfect branch prediction and perfect alias analysis requires extensive dynamic analysis, since static compile-time schemes cannot be perfect. Of course, most realistic dynamic schemes will not be perfect, but the use of dynamic schemes will provide the ability to uncover parallelism that cannot be analyzed by static compile-time analysis. Thus, a dynamic processor might be able to more closely match the amount of parallelism uncovered by our ideal processor. How close could a real dynamically scheduled, speculative processor come to the ideal processor? To gain insight into this question, consider what the perfect processor must do: 1. Look arbitrarily far ahead to find a set of instructions to issue, predicting all branches perfectly"
"Explain how the total window size is limited by the required storage, the comparisons, and a limited issue rate, which makes larger window less helpful","The set of instructions that are examined for simultaneous execution is called the window. Each instruction in the window must be kept in the processor and the number of comparisons required every clock is equal to the maximum completion rate times the window size times the number of operands per instruction (today typically 6 x 80 x 2= 960), since every pending instruction must look at every completing instruction for either of its operands. Hence, the total window size is limited by the required storage, the comparisons, and a limited issue rate, which makes larger window less helpful."," In existing and near-term processors, the costs are not quite so high, since we need only detect dependence pairs and the limited number of registers allows different solutions. Furthermore, in a real processor, issue occurs in-order and dependent instructions are handled by a renaming process that accommodates dependent renaming in one clock. Once instructions are issued, the detection of dependences is handled in a distributed fashion by the reservation stations or scoreboard. The set of instructions that are examined for simultaneous execution is called the window. Each instruction in the window must be kept in the processor and the number of comparisons required every clock is equal to the maximum completion rate times the window size times the number of operands per instruction (today typically 6 x 80 x 2= 960), since every pending instruction must look at every completing instruction for either of its operands. Thus, the total window size is limited by the required storage, the comparisons, and a limited issue rate, which makes larger window less helpful. To date, the window size has been in the range of 32 to 126, which can require over 2,000 comparisons. The HP PA 8600 reportedly has over 7,000 comparators! The window size directly limits the number of instructions that begin execution in a given cycle. In practice, real processors will have a more limited number of functional units (e.g., no processor has handled more than two memory references per clock or more than two FP operations), as well as limited numbers of buses and register access ports, which serve as limits on the number of instructions initiated in the same clock. Thus, the maximum number of instructions that may issue, begin execution, or commit in the same clock cycle is usually much smaller than the window size. Obviously, the number of possible implementation constraints in a multiple issue processor is large, including: issues per clock, functional units and unit latency, register file ports, functional unit queues (which may be fewer than units), issue limits for branches, and limitations on instruction commit. Each of these acts as constraint on the ILP. Rather than try to understand each of these effects, however, we will focus on limiting the size of the window, with the understanding that all other restrictions would further reduce the amount of parallelism that can be exploited"
"Explain why the maximum number of instructions that may issue, begin execution, or commit in the same clock cycle is usually much smaller than the window size","The window size directly limits the number of instructions that begin execution in a given cycle. In practice, real processors will have a more limited number of functional units (e.g., no processor has handled more than two memory references per clock or more than two FP operations), as well as limited numbers of buses and register access ports, which serve as limits on the number of instructions initiated in the same clock. Hence, the maximum number of instructions that may issue, begin execution, or commit in the same clock cycle is usually much smaller than the window size."," In existing and near-term processors, the costs are not quite so high, since we need only detect dependence pairs and the limited number of registers allows different solutions. Furthermore, in a real processor, issue occurs in-order and dependent instructions are handled by a renaming process that accommodates dependent renaming in one clock. Once instructions are issued, the detection of dependences is handled in a distributed fashion by the reservation stations or scoreboard. The set of instructions that are examined for simultaneous execution is called the window. Each instruction in the window must be kept in the processor and the number of comparisons required every clock is equal to the maximum completion rate times the window size times the number of operands per instruction (today typically 6 x 80 x 2= 960), since every pending instruction must look at every completing instruction for either of its operands. Thus, the total window size is limited by the required storage, the comparisons, and a limited issue rate, which makes larger window less helpful. To date, the window size has been in the range of 32 to 126, which can require over 2,000 comparisons. The HP PA 8600 reportedly has over 7,000 comparators! The window size directly limits the number of instructions that begin execution in a given cycle. In practice, real processors will have a more limited number of functional units (e.g., no processor has handled more than two memory references per clock or more than two FP operations), as well as limited numbers of buses and register access ports, which serve as limits on the number of instructions initiated in the same clock. Thus, the maximum number of instructions that may issue, begin execution, or commit in the same clock cycle is usually much smaller than the window size. Obviously, the number of possible implementation constraints in a multiple issue processor is large, including: issues per clock, functional units and unit latency, register file ports, functional unit queues (which may be fewer than units), issue limits for branches, and limitations on instruction commit. Each of these acts as constraint on the ILP. Rather than try to understand each of these effects, however, we will focus on limiting the size of the window, with the understanding that all other restrictions would further reduce the amount of parallelism that can be exploited"
"Explain why while we would expect more performance to be lost to cache misses than on the Pentium II, the relatively faster and higher bandwidth secondary caches should reduce this effect somewhat"," The Pentium Pro has the smallest set of primary caches among the P6 based microprocessors; it has, however, a high bandwidth interface to the secondary caches. Hence, while we would expect more performance to be lost to cache misses than on the Pentium II, the relatively faster and higher bandwidth secondary caches should reduce this effect somewhat."," stage, and the capacity of buffers between stages. A stage will not achieve its throughput if either the input buffer cannot supply enough operands or the output buffer lacks capacity. In addition, internal restrictions or dynamic events (such as a cache miss) can cause a stall within all the units. For example, an instruction cache miss will prevent the instruction fetch stage from generating 16 bytes of instructions; similarly, three instructions can be decoded only under certain restrictions in how they map to uops. Performance of the Pentium Pro Implementation This section looks at some performance measurements for the Pentium Pro implementation. The Pentium Pro has the smallest set of primary caches among the P6 based microprocessors; it has, however, a high bandwidth interface to the secondary caches. Thus, while we would expect more performance to be lost to cache misses than on the Pentium II, the relatively faster and higher bandwidth secondary caches should reduce this effect somewhat. The measurements in this section use a 200 MHz Pentium Pro with a 256KB secondary cache and a 66 MHz main memory bus. The data for this section comes from a study by Bhandarkar and Ding [1997] that uses SPEC CPU95 as the benchmark set. Instruction fetch 16 bytes per cycle Instruction decode 3 instructions per cycle Renaming 3 uops per cycle Reservation stations (20) Execution units (5 total) Reorder buffer (40 entries) Graduation unit (3 uops per cycle) FIGURE 3.49 The P6 processor pipeline showing the throughput of each stage and the total buffering provided between stages. The buffering provided is either as bytes (before instruction decoding), as uops (after decoding and translation), as reservation station entries (after issue), or as reorder buffer entries (after execution). There are five execution units, each of which can potentially initiate a new uop every cycle (though some are not fully pipelined as shown in Figure 3.48). Recall that during renaming an instruction reserves a reorder buffer entry, so that stalls can occur during renaming/ issue when the reorder buffer is full. Notice that the instruction fetch unit can fill the entire prefetch buffer in one cycle; if the buffer is partially full, fewer bytes will be fetched"
Explain why it is likely that the deeper pipeline and larger pipeline stall penalties on the Pentium 4 lead to a higher CPI for these two programs and reduce some of the gain from the high clock rate,"For the two integer benchmarks, the situation is somewhat different. In both cases the Pentium 4 delivers less than linear scaling with the increase in clock rate. If we assume the instruction counts are identical for integer codes on the two processors, then the CPI for the two integer benchmarks is higher on the Pentium 4 (by a factor of 1.1 for gcc and a factor of 1.5 for vortex). Looking at the data for the Pentium Pro, we can see that the these benchmarks have relatively low level-2 miss rates and that they hide much of their level-1 miss penalty through dynamic scheduling and speculation. Hence, it is likely that the deeper pipeline and larger pipeline stall penalties on the Pentium 4 lead to a higher CPI for these two programs and reduce some of the gain from the high clock rate."," 400 0 vortex applu Pentium III Pentium 4 FIGURE 3.58 The performance of the Pentium 4 for four SPEC2000 benchmarks (two integer: gcc and vortex, and two floating point: apllu and mgrid) exceeds the Pentium III by a factor of between 1.2 and 2.9. This exceeds the purely clock speed advantage for the floating point benchmarks and is less than the clock speed advantage for the integer programs. For the two integer benchmarks, the situation is somewhat different. In both cases the Pentium 4 delivers less than linear scaling with the increase in clock rate. If we assume the instruction counts are identical for integer codes on the two processors, then the CPI for the two integer benchmarks is higher on the Pentium 4 (by a factor of 1.1 for gcc and a factor of 1.5 for vortex). Looking at the data for the Pentium Pro, we can see that the these benchmarks have relatively low level-2 miss rates and that they hide much of their level-1 miss penalty through dynamic scheduling and speculation. Thus, it is likely that the deeper pipeline and larger pipeline stall penalties on the Pentium 4 lead to a higher CPI for these two programs and reduce some of the gain from the high clock rate. One interesting question is: why did the designers at Intel decide on the approach they took for the Pentium 4? On the surface, the alternative of doubling the issue rate of the Pentium III, as opposed to doubling the pipeline depth and the clock rate, looks at least as attractive. Of course, there are numerous changes between the two architectures, making an exact analysis of the tradeoffs difficult"
"Explain why despite the potentially greater efficiency of exploiting thread-level parallelism, it is likely that ILP-based approaches will continue to be the primary focus for desktop-oriented processors","The investment required to program applications to expose thread-level parallelism, makes it costly to switch the large established base of software to multiprocessors. This is especially true for desktop applications, where the natural paralelism that is present in many server environments, is harder to find. Hence, despite the potentially greater efficiency of exploiting thread-level parallelism, it is likely that ILP-based approaches will continue to be the primary focus for desktop-oriented processors."," Thread level parallelism is important alternative to instruction level parallelism primarily because it could be more cost-effective to exploit than instruction level parallelism. There are many important applications where thread level parallelism occurs naturally, as it does in many server applications, In other cases, the software is being written from scratch and expressing the inherent parallelism is easy, as is true in some embedded applications. Chapter 6 explores multiprocessors and the support they provide for thread level parallelism. The investment required to program applications to expose thread-level parallelism, makes it costly to switch the large established base of software to multiprocessors. This is especially true for desktop applications, where the natural paralelism that is present in many server environments, is harder to find. Thus, despite the potentially greater efficiency of exploiting thread-level parallelism, it is likely that ILP-based approaches will continue to be the primary focus for desktop-oriented processors. Crosscutting Issues: Using an ILP Datapath to Exploit TLP Thread-level and instruction-level parallelism exploit two different kinds of parallel structure in a program. One natural question to ask is whether it is possible for a processor oriented at instruction level parallelism to exploit thread level parallelism"
"Explain why increasing the clock rate by X is almost always a better choice than increasing the issue width by X, though often the clock rate increase may rely largely on deeper pipelining, substantially narrowing the advantage"," One insight that was clear in 1995 and remains clear in 2000 is that the peak to sustained performance ratios for multiple-issue processors are often quite large and typically grow as the issue rate grows. Hence, increasing the clock rate by X is almost always a better choice than increasing the issue width by X, though often the clock rate increase may rely largely on deeper pipelining, substantially narrowing the advantage."," Given the availability of the Alpha 21264 at 800 MHz, the Pentium III at 1.1 GHz, the AMD Athlon at 1.3 GHz, and the Pentium 4 at 2 GHz, it is clear that the limitation was primarily our understanding of how to build such processors. It is also likely that this the first generation of CAD tools used for more than two million logic transistors was a limitation. One insight that was clear in 1995 and remains clear in 2000 is that the peak to sustained performance ratios for multiple-issue processors are often quite large and typically grow as the issue rate grows. Thus, increasing the clock rate by X is almost always a better choice than increasing the issue width by X, though often the clock rate increase may rely largely on deeper pipelining, substantially narrowing the advantage. This insight probably played a role in motivating Intel to pursue a deeper pipeline for the Pentium 4, rather than trying to increase the issue width. Recall, however, the fundamental observation we made in Chapter 1 about the improvement in semiconductor technologies: the number of transistors available grows faster than the speed of the transistors. Thus, a strategy that focuses only on deeper pipelining may not be the best use of the technology in the long run.. Rather than embracing dramatic new approaches in microarchitecture, the last five years have focused on raising the clock rates of multiple issue machines and narrowing the gap between peak and sustained performance. The dynamicallyscheduled, multiple-issue processors announced in the last two years (the Alpha 21264, the Pentium III and 4, and the AMD Athlon) have same basic structure and similar sustained issue rates (three to four instructions per clock) as the first dynamically-scheduled, multiple-issue processors announced in 1995! But, the clock rates are 4 to 8 times higher, the caches are 2 to 4 times bigger, there are 2 to 4 times as many renaming registers, and twice as many load/store units! The result is performance that is 6 to 10 times higher"
Explain why a strategy that focuses only on deeper pipelining may not be the best use of the technology in the long run,"One insight that was clear in 1995 and remains clear in 2000 is that the peak to sustained performance ratios for multiple-issue processors are often quite large and typically grow as the issue rate grows. Thus, increasing the clock rate by X is almost always a better choice than increasing the issue width by X, though often the clock rate increase may rely largely on deeper pipelining, substantially narrowing the advantage. This insight probably played a role in motivating Intel to pursue a deeper pipeline for the Pentium 4, rather than trying to increase the issue width. Recall, however, the fundamental observation we made in Chapter 1 about the improvement in semiconductor technologies: the number of transistors available grows faster than the speed of the transistors. Hence, a strategy that focuses only on deeper pipelining may not be the best use of the technology in the long run."," Given the availability of the Alpha 21264 at 800 MHz, the Pentium III at 1.1 GHz, the AMD Athlon at 1.3 GHz, and the Pentium 4 at 2 GHz, it is clear that the limitation was primarily our understanding of how to build such processors. It is also likely that this the first generation of CAD tools used for more than two million logic transistors was a limitation. One insight that was clear in 1995 and remains clear in 2000 is that the peak to sustained performance ratios for multiple-issue processors are often quite large and typically grow as the issue rate grows. Thus, increasing the clock rate by X is almost always a better choice than increasing the issue width by X, though often the clock rate increase may rely largely on deeper pipelining, substantially narrowing the advantage. This insight probably played a role in motivating Intel to pursue a deeper pipeline for the Pentium 4, rather than trying to increase the issue width. Recall, however, the fundamental observation we made in Chapter 1 about the improvement in semiconductor technologies: the number of transistors available grows faster than the speed of the transistors. Thus, a strategy that focuses only on deeper pipelining may not be the best use of the technology in the long run.. Rather than embracing dramatic new approaches in microarchitecture, the last five years have focused on raising the clock rates of multiple issue machines and narrowing the gap between peak and sustained performance. The dynamicallyscheduled, multiple-issue processors announced in the last two years (the Alpha 21264, the Pentium III and 4, and the AMD Athlon) have same basic structure and similar sustained issue rates (three to four instructions per clock) as the first dynamically-scheduled, multiple-issue processors announced in 1995! But, the clock rates are 4 to 8 times higher, the caches are 2 to 4 times bigger, there are 2 to 4 times as many renaming registers, and twice as many load/store units! The result is performance that is 6 to 10 times higher"
"Explain why we will want to use different registers for each iteration, increasing the required register count","In this case, we can eliminate the data use stall by creating additional independent instructions within the loop body. If we simply replicated the instructions when we unrolled the loop, the resulting use of the same registers could prevent us from effectively scheduling the loop. Hence, we will want to use different registers for each iteration, increasing the required register count."," Loop unrolling can also be used to improve scheduling. Because it eliminates the branch, it allows instructions from different iterations to be scheduled together. In this case, we can eliminate the data use stall by creating additional independent instructions within the loop body. If we simply replicated the instructions when we unrolled the loop, the resulting use of the same registers could prevent us from effectively scheduling the loop. Thus, we will want to use different registers for each iteration, increasing the required register count. EXAMPLE Show our loop unrolled so that there are four copies of the loop body, assuming R1 is initially a multiple of 32, which means that the number of loop iterations is a multiple of 4. Eliminate any obviously redundant computations and do not reuse any of the registers"
Explain why different numbers of functional units and unit latencies require different versions of the code,"Binary code compatibility has also been a major logistical problem for VLIWs. In a strict VLIW approach, the code sequence makes use of both the instruction set definition and the detailed pipeline structure, including both functional units and their latencies. Hence, different numbers of functional units and unit latencies require different versions of the code."," In more recent processors, the functional units operate more independently, and the compiler is used to avoid hazards at issue time, while hardware checks allow for unsynchronized execution once instructions are issued. Binary code compatibility has also been a major logistical problem for VLIWs. In a strict VLIW approach, the code sequence makes use of both the instruction set definition and the detailed pipeline structure, including both functional units and their latencies. Thus, different numbers of functional units and unit latencies require different versions of the code. This requirement makes migrating between successive implementations, or between implementations with different issue widths, more difficult than it is for a superscalar design. Of course, obtaining improved performance from a new superscalar design may require recompilation. Nonetheless, the ability to run old binary files is a practical advantage for the superscalar approach. One possible solution to this migration problem, and the problem of binary code compatibility in general, is object-code translation or emulation. This technology is developing quickly and could play a significant role in future migration schemes. Another approach is to temper the strictness of the approach so that binary compatibility is still feasible. This later approach is used in the IA-64 architecture, as we will see in Section 4.7"
Explain why desktop computers are concerned more with average latency from the memory hierarchy whereas server computers are also concerned about memory bandwidth,"Desktop computers are primarily running one application at a time on top of an operating system for a single user, whereas server computers may typically have hundreds of users potentially running potentially dozens of applications simultaneously. These characteristics result in more context switches, which effectively increases compulsory miss rates. Hence, desktop computers are concerned more with average latency from the memory hierarchy whereas server computers are also concerned about memory bandwidth."," Introduction Performance 20 19 19 19 19 19 19 19 19 19 Year Memory FIGURE 5.2 Starting with 1980 performance as a baseline, the gap in performance between memory and CPUs are plotted over time. Note that the vertical axis must be on a logarithmic scale to record the size of the CPU-DRAM performance gap. The memory baseline is 64-KB DRAM in 1980, with three years to the next generation and a 7% per year performance improvement in latency (see Figure 5.30 on page 444). The CPU line assumes a 1.35 improvement per year until 1986, and a 1.55 improvement thereafter. <<NOTE: Artist continue the X-axis to 2005: DRAM should be 5.4 in 2005, and CPU should be 25,000 in 2005. This means the Y-axis needs to be extended to 100,000 to accommodate.>> The 21264 is a microprocessor designed for desktop and servers. Even these two related classes of computers have different concerns in a memory hierarchy. Desktop computers are primarily running one application at a time on top of an operating system for a single user, whereas server computers may typically have hundreds of users potentially running potentially dozens of applications simultaneously. These characteristics result in more context switches, which effectively increases compulsory miss rates. Thus, desktop computers are concerned more with average latency from the memory hierarchy whereas server computers are also concerned about memory bandwidth. Although protection is important on desktop computers to deter programs from clobbering each other, server computers must prevent one user from accessing anothers data, and hence the importance of protection escalates. Server computers also tend to be much larger, with more memory and disk storage, and hence often run much larger applications. In 2001 virtually all servers can be purchased as multiprocessors with hundreds of disks, which places even greater bandwidth demands on the memory hierarchy. The memory hierarchy of the embedded computers is often quite different from that of the desktop and server. First, embedded computers are often used in real-time applications, and hence programmers must worry about worst case per- formance. This concern is problematic for caches that improve average case performance, but can degrade worst case performance; well mention some techniques to address this later in the chapter. Second, embedded applications are often concerned about power and battery life. The best way to save power is to have less hardware. Hence, embedded computers may not chose hardware-intensive optimizations in the quest of better memory hierarchy performance, as would most desktop and server computers. Third, embedded applications are typically only running one application and use a very simple operating system, if they one at all. Hence, the protection role of the memory hierarchy is often diminished. Finally, the main memory itself may be quite smallless than one megabyteand there is often no disk storage"
"Explain why misses per instruction are most popular with architects working with a single computer family, although the similarity of RISC architectures allows one to give insights into others","The advantage of misses per instruction is that it is independent of the hardware implementation. For example, the 21264 fetches about twice as many instructions as are actually committed, which can artificially reduce the miss rate if measured as misses per memory reference rather than per instruction. The drawback is that misses per instruction is architecture dependent; for example, the average number of memory accesses per instruction may be very different for an 80x86 versus MIPS. Hence, misses per instruction are most popular with architects working with a single computer family, although the similarity of RISC architectures allows one to give insights into others."," EXAMPLE Assume we have a computer where the clocks per instruction (CPI) is 1.0 when all memory accesses hit in the cache. The only data accesses are loads and stores, and these total 50% of the instructions. If the miss penalty is 25 clock cycles and the miss rate is 2%, how much faster would the computer be if all instructions were cache hits? First compute the performance for the computer that always hits: CPU execution time = ( CPU clock cycles + Memory stall cycles ) Clock cycle = ( IC CPI + 0 ) Clock cycle = IC 1.0 Clock cycle Now for the computer with the real cache, first we compute memory stall cycles: Memory accesses Memory stall cycles = IC ------------------------------------------ Miss rate Miss penalty Instruction = IC ( 1 + 0.5 ) 0.02 25 = IC 0.75 data accesses per instruction. The total performance is thus CPU execution time cache = ( IC 1.0 + IC 0.75 ) Clock cycle = 1.75 IC Clock cycle The performance ratio is the inverse of the execution times: CPU execution time cache 1.75 IC Clock cycle = -----------------------------------------------------------------------------------------------------------------CPU execution time 1.0 IC Clock cycle = 1.75 n Some designers prefer measuring miss rate as misses per instruction rather than misses per memory reference. These two are related: Misses Memory accesses Miss rate Memory accesses = Miss rate ------------------------------------------------------------------- = ---------------------------------------------------------------------Instruction Instruction Instruction Count The latter formula is useful when you know the average number of memory accesses per instruction as it allows you to convert miss rate into misses per instruction, and vice versa. For example, we can turn the miss rate per memory reference in the example above into misses per instruction: Memory accesses Misses = 0.02 1.5 = 0.030 -------------------------- = Miss rate ----------------------------------------Instruction Instruction By the way, misses per instruction are often reported as misses per 1000 instructions to show integers instead of fractions. Thus, the answer above could also be expressed as 30 misses per 1000 instructions. The advantage of misses per instruction is that it is independent of the hardware implementation. For example, the 21264 fetches about twice as many instructions as are actually committed, which can artificially reduce the miss rate if measured as misses per memory reference rather than per instruction. The drawback is that misses per instruction is architecture dependent; for example, the average number of memory accesses per instruction may be very different for an 80x86 versus MIPS. Thus, misses per instruction are most popular with architects working with a single computer family, although the similarity of RISC architectures allows one to give insights into others. EXAMPLE To show equivalency between the two miss rate equations, lets redo the example above, this time assuming a miss rate per 1000 instructions of 30. What is memory stall time in terms of instruction count? Review of the ABCs of Caches Recomputing the memory stall cycles: Memory stall cycles = Number of misses Miss penalty Misses = IC -------------------------- Miss penalty Instruction Misses = IC 1000 -------------------------------------------- Miss penalty Instruction 1000 = IC 1000 30 25 = IC 1000 750 = IC 0.75 n Four Memory Hierarchy Questions We continue our introduction to caches by answering the four common questions for the first level of the memory hierarchy: Q1: Where can a block be placed in the upper level? (Block placement) Q2: How is a block found if it is in the upper level? (Block identification) Q3: Which block should be replaced on a miss? (Block replacement) Q4: What happens on a write? (Write strategy) The answers to these questions help us understand the different trade-offs of memories at different levels of a hierarchy; hence we ask these four questions on every example"
Explain why we can consider many alternatives in the second-level cache that would be ill chosen for the first-level cache,"With these definitions in place, we can consider the parameters of secondlevel caches. The foremost difference between the two levels is that the speed of the first-level cache affects the clock rate of the CPU, while the speed of the second-level cache only affects the miss penalty of the first-level cache. Hence, we can consider many alternatives in the second-level cache that would be ill chosen for the first-level cache."," with the size of a second-level cache for one design. From these figures we can gain two insights. The first is that the global cache miss rate is very similar to the single cache miss rate of the second-level cache, provided that the second-level cache is much larger than the first-level cache. Hence, our intuition and knowledge about the first-level caches apply. The second insight is that the local cache rate is not a good measure of secondary caches; it is a function of the miss rate of the first-level cache, and hence can vary by changing the first-level cache. Thus, the global cache miss rate should be used when evaluating second-level caches. With these definitions in place, we can consider the parameters of secondlevel caches. The foremost difference between the two levels is that the speed of the first-level cache affects the clock rate of the CPU, while the speed of the second-level cache only affects the miss penalty of the first-level cache. Thus, we can consider many alternatives in the second-level cache that would be ill chosen for the first-level cache. There are two major questions for the design of the second-level cache: Will it lower the average memory access time portion of the CPI, and how much does it cost? The initial decision is the size of a second-level cache. Since everything in the first-level cache is likely to be in the second-level cache, the second-level cache should be much bigger than the first. If second-level caches are just a little bigger, the local miss rate will be high. This observation inspires design of huge secondlevel cachesthe size of main memory in older computers! One question is whether set associativity makes more sense for second-level caches. EXAMPLE Given the data below, what is the impact of second-level cache associativity on its miss penalty? n Hit timeL2 for direct mapped = 10 clock cycles Two-way set associativity increases hit time by 0.1 clock cycles to 10.1 clock cycles 100% Reducing Cache Miss Penalty 98% 96% Local miss rate Global miss rate Single cache miss rate 80% 70% 60% 55% 51% 46% 40% 39% 34% 30% 20% 10% 4% % 4% % 4% % 0% 4% % 3% 3% 2% 1% Cache size (KB) FIGURE 5.10 Miss rates versus cache size for multilevel caches. Second-level caches smaller than the sum of the two 64-KB first level make little sense, as reflected in the high miss rates. After 256 KB the single cache is within 10% of the global miss rates. The miss rate of a single-level cache versus size is plotted against the local miss rate and global miss rate of a second-level cache using a 32-KB first-level cache. The L2 Caches (unified) were 2-way set-associative with LRU replacement. Each had split L1instruction and data caches that were 64KB 2-way set-associative with LRU replacement. The block size for both L1 and L2 caches was 64 bytes. Data was collected for as in Figure 5.6.<<Artist please separate numbers for graphs at bottom>> n n n For a direct-mapped second-level cache, the first-level cache miss penalty is Miss penalty1- way L2 = 10 + 25% 100 = 35.0 clock cycles 1.02 1.06 L2 hit = 8 clock cycles L2 hit = 16 clock cycles 1.10 1.14 Level two cache size (KB) 1.60 1.65 1.76 1.82 1.94 1.99 2.34 2.39 1.00 1.50 2.00 2.50 Relative execution time FIGURE 5.11 Relative execution time by second-level cache size. The two bars are for different clock cycles for a L2 cache hit. The reference execution time of 1.00 is for an 8192KB second-level cache with a one-clock-cycle latency on a second-level hit. These data were collected the same way as in Figure 5.10, using a simulator to imitate the Alpha 21264"
Explain why a miss might move from a capacity miss to a conflict miss as cache size changes,"changing cache size changes conflict misses as well as capacity misses, since a larger cache spreads out references to more blocks. Hence, a miss might move from a capacity miss to a conflict miss as cache size changes."," Another approach to improving the three Cs is to make blocks larger to reduce the number of compulsory misses, but, as we shall see, large blocks can increase other kinds of misses. The three Cs give insight into the cause of misses, but this simple model has its limits; it gives you insight into average behavior but may not explain an individual miss. For example, changing cache size changes conflict misses as well as capacity misses, since a larger cache spreads out references to more blocks. Thus, a miss might move from a capacity miss to a conflict miss as cache size changes. Note that the three Cs also ignore replacement policy, since it is difficult to model and since, in general, it is less significant. In specific circumstances the replacement policy can actually lead to anomalous behavior, such as poorer miss rates for larger associativity, which contradicts the three Cs model. (Some have proposed using an address trace to determine optimal placement to avoid placement misses from the 3 Cs model; weve not followed that advice here.) Alas, many of the techniques that reduce miss rates also increase hit time or miss penalty. The desirability of reducing miss rates using the five techniques presented in the rest of this section must be balanced against the goal of making the whole system fast. This first example shows the importance of a balanced perspective"
Explain why the hardware must check on misses to be sure it is not to block already being requested to avoid possible incoherency problems and to save time,"An added complexity of multiple outstanding misses is that it is now possible for there to be more than one miss request to the same block. For example, with 64 byte blocks there could be a miss to address 1000 and then later a miss to address 1032. Hence, the hardware must check on misses to be sure it is not to block already being requested to avoid possible incoherency problems and to save time."," trace-driven simulation. Cache studies involving an out-of-order CPUs use execution-driven simulation to evaluate innovations, as avoiding a cache miss that is completely hidden by dynamic issue does not help performance. An added complexity of multiple outstanding misses is that it is now possible for there to be more than one miss request to the same block. For example, with 64 byte blocks there could be a miss to address 1000 and then later a miss to address 1032. Thus, the hardware must check on misses to be sure it is not to block already being requested to avoid possible incoherency problems and to save time. Second Miss Penalty/Rate Reduction Technique: Hardware Prefetching of Instructions and Data Nonblocking caches effectively reduce the miss penalty by overlapping execution with memory access. To have value, we need a processor that can allow instructions to execute out-of-order. Another approach is to prefetch items before they are requested by the processor. Both instructions and data can be prefetched, either directly into the caches or into an external buffer that can be more quickly accessed than main memory"
Explain why refresh is another reason for variability of memory latency and hence cache miss penalty,"Earlier sections presented main memory as if operated like a Swiss train, consistently delivering the goods exactly according to schedule. Refresh belies that myth, for some accesses take much longer than others do. Hence, refresh is another reason for variability of memory latency and hence cache miss penalty."," Since the memory matrix in a DRAM is conceptually square, the number of steps in a refresh is usually the square root of the DRAM capacity. DRAM designers try to keep time spent refreshing to be less than 5% of the total time. Earlier sections presented main memory as if operated like a Swiss train, consistently delivering the goods exactly according to schedule. Refresh belies that myth, for some accesses take much longer than others do. Thus, refresh is another reason for variability of memory latency and hence cache miss penalty. Amdahl suggested a rule of thumb that memory capacity should grow linearly with CPU speed to keep a balanced system, so that a 1000 MIPS processor should have 1000 megabytes of memory. CPU designers rely on DRAMs to supply that demand: in the past they expected a four-fold improvement in capacity every three years, or 55% per year. Unfortunately, the performance of DRAMs is growing at a much slower rate. Figure 5.30 shows a performance improvement in row access time, which is related to latency, of about 5% per year. The CAS or Data Transfer Time, which is related to bandwidth, is growing at more than twice that rate"
"Explain why unlike DRAMs, there is no difference between access time and cycle time in SRAM","In DRAM designs the emphasis is on cost per bit and capacity, while SRAM designs are concerned with speed and capacity. (Because of this concern, SRAM address lines are not multiplexed.). Hence, unlike DRAMs, there is no difference between access time and cycle time."," Memory Technology This difference in refresh alone can make a difference for embedded applications. Devices often go into low power or standby mode for long periods. SRAM needs only minimal power to retain the charge in standby mode, but DRAMs must continue to be refreshed occasionally so as to not lose information. In DRAM designs the emphasis is on cost per bit and capacity, while SRAM designs are concerned with speed and capacity. (Because of this concern, SRAM address lines are not multiplexed.). Thus, unlike DRAMs, there is no difference between access time and cycle time. For memories designed in comparable technologies, the capacity of DRAMs is roughly 4 to 8 times that of SRAMs. The cycle time of SRAMs is 8 to 16 times faster than DRAMs, but they are also 8 to 16 times as expensive. Embedded Processor Memory Technology: ROM and Flash Embedded computers usually have small memories, and most do not have a disk to act as non-volatile storage. Two memory technologies are found in embedded computers to address this problem"
Explain why operating systems allow blocks to be placed anywhere in main memory,"The miss penalty for virtual memory involves access to a rotating magnetic storage device and is therefore quite high. Given the choice of lower miss rates or a simpler placement algorithm, operating systems designers normally pick lower miss rates because of the exorbitant miss penalty. Hence, operating systems allow blocks to be placed anywhere in main memory."," We are now ready to answer the four memory-hierarchy questions for virtual memory. Q1: Where can a block be placed in main memory? The miss penalty for virtual memory involves access to a rotating magnetic storage device and is therefore quite high. Given the choice of lower miss rates or a simpler placement algorithm, operating systems designers normally pick lower miss rates because of the exorbitant miss penalty. Thus, operating systems allow blocks to be placed anywhere in main memory. According to the terminology in Both paging and segmentation rely on a data structure that is indexed by the page or segment number. This data structure contains the physical address of the block. For segmentation, the offset is added to the segments physical address to obtain the final physical address. For paging, the offset is simply concatenated to this physical page address (see Figure 5.35). Virtual address Virtual page number Page table Page offset Main memory This data structure, containing the physical page addresses, usually takes the form of a page table. Indexed by the virtual page number, the size of the table is the number of pages in the virtual address space. Given a 32-bit virtual address, 4-KB pages, and 4 bytes per page table entry, the size of the page table would be (232/212) 22 = 222 or 4 MB"
Explain why the write strategy is always write back,"The level below main memory contains rotating magnetic disks that take millions of clock cycles to access. Because of the great discrepancy in access time, no one has yet built a virtual memory operating system that writes through main memory to disk on every store by the CPU. (This remark should not be interpreted as an opportunity to become famous by being the first to build one!) Hence, the write strategy is always write back."," To help the operating system estimate LRU, many processors provide a use bit or reference bit, which is logically set whenever a page is accessed. (To reduce work, it is actually set only on a translation buffer miss, which is described shortly.) The operating system periodically clears the use bits and later records them so it can determine which pages were touched during a particular time period. By keeping track in this way, the operating system can select a page that is among the least-recently referenced. Q4: What happens on a write? The level below main memory contains rotating magnetic disks that take millions of clock cycles to access. Because of the great discrepancy in access time, no one has yet built a virtual memory operating system that writes through main memory to disk on every store by the CPU. (This remark should not be interpreted as an opportunity to become famous by being the first to build one!) Thus, the write strategy is always write back. Since the cost of an unnecessary access to the next-lower level is so high, virtual memory systems usually include a dirty bit. It allows blocks to be written to disk only if they have been altered since being read from the disk"
Explain why at any instant it must be possible to switch from one process to another,"Multiprogramming leads to the concept of a process. Metaphorically, a process is a programs breathing air and living spacethat is, a running program plus any state needed to continue running it. Time-sharing is a variation of multiprogramming that shares the CPU and memory with several interactive users at the same time, giving the illusion that all users have their own computers. Hence, at any instant it must be possible to switch from one process to another."," Although this is a simple example, the major difference between this drawing and a real cache is replication. First, there is only one L1 cache. When there are two L1 caches, the top half of the diagram is duplicated. Note this would lead to two TLBs, which is typical. Hence, one cache and TLB is for instructions, driven from the PC, and one cache and TLB is for data, driven from the effective address. The second simplification is that all the caches and TLBs are direct mapped. If any were N-way set associative, then we would replicate each set of tag memory, comparators, and data memory N times and connect data memories with a N:1 multiplexor to select a hit. Of course, if the total cache size remained the same, the cache index would also shrink by N bits according to the formula in Protection and Examples of Virtual Memory The invention of multiprogramming, where a computer would be shared by several programs running concurrently, led to new demands for protection and sharing among programs. These demands are closely tied to virtual memory in computers today, and so we cover the topic here along with two examples of virtual memory. Multiprogramming leads to the concept of a process. Metaphorically, a process is a programs breathing air and living spacethat is, a running program plus any state needed to continue running it. Time-sharing is a variation of multiprogramming that shares the CPU and memory with several interactive users at the same time, giving the illusion that all users have their own computers. Thus, at any instant it must be possible to switch from one process to another. This exchange is called a process switch or context switch. A process must operate correctly whether it executes continuously from start to finish, or is interrupted repeatedly and switched with other processes. The responsibility for maintaining correct process behavior is shared by designers of the computer and the operating system. The computer designer must ensure that the CPU portion of the process state can be saved and restored. The operating system designer must guarantee that processes do not interfere with each others computations"
"Explain why the user can try any virtual address, but by controlling the page table entries the operating system controls what physical memory is accessed","Although we have explained translation of legal addresses, what prevents the user from creating illegal address translations and getting into mischief? The page tables themselves are protected from being written by user programs. Hence, the user can try any virtual address, but by controlling the page table entries the operating system controls what physical memory is accessed."," The maximum virtual address and physical address is then tied to the page size. The original architecture document allows for the Alpha to expand the minimum page size from 8 KB up to 64 KB, thereby increasing the virtual address to 3 13 + 16 or 55 bits and the maximum physical address to 32 + 16 or 48 bits. In fact, the upcoming 21364 supports both. It will be interesting to see whether or not operating systems accommodate such expansion plans. Although we have explained translation of legal addresses, what prevents the user from creating illegal address translations and getting into mischief? The page tables themselves are protected from being written by user programs. Thus, the user can try any virtual address, but by controlling the page table entries the operating system controls what physical memory is accessed. Sharing of memory between processes is accomplished by having a page table entry in each address space point to the same physical memory page. The Alpha 21264 employs two TLBs to reduce address translation time, one for instruction accesses and another for data accesses. Figure 5.40 shows the important parameters. The Alpha allows the operating system to tell the TLB that contiguous sequences of pages can act as one: the options are 8, 64, and 512 times the minimum page size. Thus, the variable page size of a PTE mapping makes the match more challenging, as the size of the space being mapped in the PTE also must be checked to determine the match. Figure 5.36 above describes the data TLB"
Explain cyber threat hunting,Cyber threat hunting is a proactive cyber defence activity.It is the process of proactively and iteratively searching through networks to detect and isolate advanced threats that evade existing security solutions.,"Cyber threat hunting is a proactive cyber defence activity. It is ""the process of proactively and iteratively searching through networks to detect and isolate advanced threats that evade existing security solutions."" This is in contrast to traditional threat management measures, such as firewalls, intrusion detection systems (IDS), malware sandbox (computer security) and SIEM systems, which typically involve an investigation of evidence-based data after there has been a warning of a potential threat."
Explain why the variable page size of a PTE mapping makes the match more challenging,"The Alpha 21264 employs two TLBs to reduce address translation time, one for instruction accesses and another for data accesses. Figure 5.40 shows the important parameters. The Alpha allows the operating system to tell the TLB that contiguous sequences of pages can act as one: the options are 8, 64, and 512 times the minimum page size. Hence, the variable page size of a PTE mapping makes the match more challenging, as the size of the space being mapped in the PTE also must be checked to determine the match."," Although we have explained translation of legal addresses, what prevents the user from creating illegal address translations and getting into mischief? The page tables themselves are protected from being written by user programs. Thus, the user can try any virtual address, but by controlling the page table entries the operating system controls what physical memory is accessed. Sharing of memory between processes is accomplished by having a page table entry in each address space point to the same physical memory page. The Alpha 21264 employs two TLBs to reduce address translation time, one for instruction accesses and another for data accesses. Figure 5.40 shows the important parameters. The Alpha allows the operating system to tell the TLB that contiguous sequences of pages can act as one: the options are 8, 64, and 512 times the minimum page size. Thus, the variable page size of a PTE mapping makes the match more challenging, as the size of the space being mapped in the PTE also must be checked to determine the match. Figure 5.36 above describes the data TLB. Protection and Examples of Virtual Memory Description 1 PTE (8 bytes) 1 clock cycle Miss penalty (average) TLB size Same for Instruction and Data TLBs: 128 PTEs per TLB, each of which can map 1, 8, 64, or 512 pages Round robin (Not applicable) Fully associative Memory management in the Alpha 21264 is typical of most desktop or server computers today, relying on page-level address translation and correct operation of the operating system to provide safety to multiple processes sharing the computer. In the next section we see a protection scheme for individuals who want to trust the operating system as little as possible"
"Explain why when these address parameters are loaded into the segment registers, they will set the requested protection level to the proper value"," When an operating system routine is invoked, it can execute an instruction that sets this 2-bit field in all address parameters with the protection level of the user that called the routine. Hence, when these address parameters are loaded into the segment registers, they will set the requested protection level to the proper value."," This scheme still leaves open the potential loophole of having the operating system use the users address, passed as parameters, with the operating systems security level, instead of with the users level. The IA-32 solves this problem by dedicating 2 bits in every CPU segment register to the requested protection level. When an operating system routine is invoked, it can execute an instruction that sets this 2-bit field in all address parameters with the protection level of the user that called the routine. Thus, when these address parameters are loaded into the segment registers, they will set the requested protection level to the proper value. The IA-32 hardware then uses the requested protection level to prevent any foolishness: No segment can be accessed from the system routine using those parameters if it has a more-privileged protection level than requested"
Explain why the page frame of the instructions data address is sent to the data TLB (step 23) at the same time as the (9+3)-bit index from the virtual address is sent to the data cache (step 24),"If this initial instruction is a load, the data addresses is also sent to the data cache. It is 64 KB, 2-way set-associative, and write-back with a 64-byte block size. Unlike the instruction cache, the data cache is virtually indexed and physically tagged. Hence, the page frame of the instructions data address is sent to the data TLB (step 23) at the same time as the (9+3)-bit index from the virtual address is sent to the data cache (step 24)."," The new data are loaded into the instruction cache as soon as they arrive (step 20). It also goes into a (L1) victim buffer (step 21), and is later written to the L2 cache (step 22). The victim buffer is of size 8, so many victims can be queued before being written back either to the L2 or to memory. The 21264 can also manage up to 8 simultaneous cache block misses, allowing it to hit under 8 misses as described in section 5.4. If this initial instruction is a load, the data addresses is also sent to the data cache. It is 64 KB, 2-way set-associative, and write-back with a 64-byte block size. Unlike the instruction cache, the data cache is virtually indexed and physically tagged. Thus, the page frame of the instructions data address is sent to the data TLB (step 23) at the same time as the (9+3)-bit index from the virtual address is sent to the data cache (step 24). The data TLB is a fully associative cache containing 128 PTEs (step 25), each of which represents page sizes from 8 KB to 4 MB. A TLB miss will trap to PAL code to load the valid PTE for this address. In the worst case, the page is not in memory, and the operating system gets the page from disk, just as before. Since millions of instructions could execute during a page fault, the operating system will swap in another process if one is waiting to run"
"Explain why rather than trying to solve the problem by avoiding it in software, small-scale multiprocessors adopt a hardware solution by introducing a protocol to maintain coherent caches","Coherent caches provide migration, since a data item can be moved to a local cache and used there in a transparent fashion. This migration reduces both the latency to access a shared data item that is allocated remotely and the bandwidth demand on the shared memory.Coherent caches also provide replication for shared data that is being simultaneously read, since the caches make a copy of the data item in the local cache. Replication reduces both latency of access and contention for a read shared data item. Supporting this migration and replication is critical to performance in accessing shared data. Hence, rather than trying to solve the problem by avoiding it in software, small-scale multiprocessors adopt a hardware solution by introducing a protocol to maintain coherent caches."," Coherent caches provide migration, since a data item can be moved to a local cache and used there in a transparent fashion. This migration reduces both the latency to access a shared data item that is allocated remotely and the bandwidth demand on the shared memory. Coherent caches also provide replication for shared data that is being simultaneously read, since the caches make a copy of the data item in the local cache. Replication reduces both latency of access and contention for a read shared data item. Supporting this migration and replication is critical to performance in accessing shared data. Thus, rather than trying to solve the problem by avoiding it in software, small-scale multiprocessors adopt a hardware solution by introducing a protocol to maintain coherent caches. The protocols to maintain coherence for multiple processors are called cachecoherence protocols. Key to implementing a cache-coherence protocol is tracking the state of any sharing of a data block. There are two classes of protocols, which use different techniques to track the sharing status, in use: n Directory basedThe sharing status of a block of physical memory is kept in just one location, called the directory; we focus on this approach in section 6.5, when we discuss scalable shared-memory architecture"
"Explain why when the read occurs, it misses in the cache and is forced to fetch a new copy of the data","There are two ways to maintain the coherence requirement described in the previous subsection. One method is to ensure that a processor has exclusive access to a data item before it writes that item. This style of protocol is called a write invalidate protocol because it invalidates other copies on a write. It is by far the most common protocol, both for snooping and for directory schemes. Exclusive access ensures that no other readable or writable copies of an item exist when the write occurs: all other cached copies of the item are invalidated. with write-back caches in action To see how this protocol ensures coherence, consider a write followed by a read by another processor: Since the write requires exclusive access, any copy held by the reading processor must be invalidated (hence the protocol name). Hence, when the read occurs, it misses in the cache and is forced to fetch a new copy of the data."," Snooping Protocols There are two ways to maintain the coherence requirement described in the previous subsection. One method is to ensure that a processor has exclusive access to a data item before it writes that item. This style of protocol is called a write invalidate protocol because it invalidates other copies on a write. It is by far the most common protocol, both for snooping and for directory schemes. Exclusive access ensures that no other readable or writable copies of an item exist when the write occurs: all other cached copies of the item are invalidated. with write-back caches in action To see how this protocol ensures coherence, consider a write followed by a read by another processor: Since the write requires exclusive access, any copy held by the reading processor must be invalidated (hence the protocol name). Thus, when the read occurs, it misses in the cache and is forced to fetch a new copy of the data. For a write, we require that the writing processor have exclusive access, preventing any other processor from being able to write simultaneously. If two processors do attempt to write the same data simultaneously, one of them wins the race (well see how we decide who wins shortly), causing the other processors copy to be invalidated. For the other processor to complete its write, it must obtain a new copy of the data, which must now contain the updated value. Therefore, this protocol enforces write serialization. Bus activity Contents of CPU As cache Contents of CPU Bs cache Contents of memory location X Cache miss for X CPU B reads X Invalidation for X CPU B reads X 0 0 FIGURE 6.8 An example of an invalidation protocol working on a snooping bus for a single cache block (X) with write-back caches. We assume that neither cache initially holds X and that the value of X in memory is 0. The CPU and memory contents show the value after the processor and bus activity have both completed. A blank indicates no activity or no copy cached. When the second miss by B occurs, CPU A responds with the value canceling the response from memory"
"Explain why we will find it useful to decompose not only the uniprocessor and multiprocessor miss rates, but also the true-sharing and false-sharing miss rates"," True sharing and false sharing miss rates can be affected by a variety of changes in the cache architecture. Hence, we will find it useful to decompose not only the uniprocessor and multiprocessor miss rates, but also the true-sharing and false-sharing miss rates."," This event is a true sharing miss, since the value being read was written by P2. True sharing and false sharing miss rates can be affected by a variety of changes in the cache architecture. Thus, we will find it useful to decompose not only the uniprocessor and multiprocessor miss rates, but also the true-sharing and false-sharing miss rates. Performance Measurements of the Commercial Workload The performance measurements of the commercial workload, which we examine in this section, were taken either on a Alphaserver 4100, or using a configurable simulator modeled after the Alphaserver 4100. The Alphaserver 4100 used for these measurements has four processors, each of which is an Alpha 21164 running at 300 MHz. Each processor has a three-level cache hierarchy: n n L1 consist of a pair of 8 KB direct-mapped on-chip caches, one for instruction and one for data. The block size is 32-bytes, and the data cache is write-through to L2, using a write buffer"
"Explain why at 4 and 8 MB, the true sharing misses generate the dominant fraction of the misses","The relative performance of the OLTP workload as the size of the L3 cache, which is set as 2-way set associative, is grown from 1 MB to 8MB. Interestingly, the performance of the 1 MB, 2-way set associative cache is very similar to the direct-mapped 2 MB cache that is used in the Alphaserver 4100. fortunately, the cold, false sharing, and true sharing misses are unaffected by a larger L3. Hence, at 4 and 8 MB, the true sharing misses generate the dominant fraction of the misses."," To better understand how the L3 miss rate responds, we ask: What factors contribute to the L3 miss rate and how do they change as the L3 cache grows? Figure 6.15 shows this data, displaying the number of memory access cycles contributed per instruction from five sources. The two largest sources of memory access cycles (due to L3 misses) with a 1 MB L3 are instruction and capacity/conflict misses. With a larger L3 these two sources shrink to be minor contributors. Un- 70 Idle PAL Code Memory Access L2/L3 Cache Access Instruction Execution 20 1 MB 4 MB L3 Cache Size FIGURE 6.14 The relative performance of the OLTP workload as the size of the L3 cache, which is set as 2-way set associative, is grown from 1 MB to 8MB. Interestingly, the performance of the 1 MB, 2-way set associative cache is very similar to the direct-mapped 2 MB cache that is used in the Alphaserver 4100. fortunately, the cold, false sharing, and true sharing misses are unaffected by a larger L3. Thus, at 4 and 8 MB, the true sharing misses generate the dominant fraction of the misses. Clearly, increasing the cache size eliminates most of the uniprocessor misses, while leaving the multiprocessor misses untouched. How does increasing the processor count affect different types of misses? Figure 6.16 shows this data assuming a base configuration with a 2 MB, 2-way set associative L3 cache. As we might expect, the increase in the true sharing miss rate, which is not compensated for by any decrease in the uniprocessor misses, leads to an overall increase in the memory access cycles per instruction"
"Explain why when an instruction pair is effectively atomic, no other processor can change the value between the instruction pair","The pair of instructions is effectively atomic if it appears as if all other operations executed by any processor occurred before or after the pair. Hence, when an instruction pair is effectively atomic, no other processor can change the value between the instruction pair."," A slightly different approach to providing this atomic read-and-update operation has been used in some recent multiprocessors. Implementing a single atomic memory operation introduces some challenges, since it requires both a memory read and a write in a single, uninterruptible instruction. This requirement complicates the implementation of coherence, since the hardware cannot allow any other operations between the read and the write, and yet must not deadlock. An alternative is to have a pair of instructions where the second instruction returns a value from which it can be deduced whether the pair of instructions was executed as if the instructions were atomic. The pair of instructions is effectively atomic if it appears as if all other operations executed by any processor occurred before or after the pair. Thus, when an instruction pair is effectively atomic, no other processor can change the value between the instruction pair. The pair of instructions includes a special load called a load linked or load locked and a special store called a store conditional. These instructions are used in sequence: If the contents of the memory location specified by the load linked are changed before the store conditional to the same address occurs, then the store conditional fails. If the processor does a context switch between the two instructions, then the store conditional also fails. The store conditional is defined to return a value indicating whether or not the store was successful. Since the load linked returns the initial value and the store conditional returns 1 if it succeeds and 0 otherwise, the following sequence implements an atomic exchange on the memory location specified by the contents of R1: Synchronization MOV LL SC BEQZ MOV R3,R4,R0 R2,0(R1) R3,0(R1) R3,try R4,R2 ;mov exchange value ;load linked ;store conditional ;branch store fails ;put load value in R4 At the end of this sequence the contents of R4 and the memory location specified by R1 have been atomically exchanged (ignoring any effect from delayed branches). Any time a processor intervenes and modifies the value in memory between the LL and SC instructions, the SC returns 0 in R3, causing the code sequence to try again"
"Explain why it is often useful to measure the speedup as processors are added both for a fixed-size problem and for a scaled version of the problem, providing an unscaled and a scaled version of the speedup curves","Users and designers are often interested in knowing not just how well a multiprocessor performs with a certain fixed number of processors, but also how the performance scales as more processors are added. In many cases, it makes sense to scale the application or benchmark, since if the benchmark is unscaled, effects arising from limited parallelism and increases in communication can lead to results that are pessimistic when the expectation is that more processors will be used to solve larger problems. Hence, it is often useful to measure the speedup as processors are added both for a fixed-size problem and for a scaled version of the problem, providing an unscaled and a scaled version of the speedup curves."," Measuring wall-clock time obviously makes sense; in a parallel processor, measuring CPU time can be misleading because the processors may be idle but unavailable for other uses. Users and designers are often interested in knowing not just how well a multiprocessor performs with a certain fixed number of processors, but also how the performance scales as more processors are added. In many cases, it makes sense to scale the application or benchmark, since if the benchmark is unscaled, effects arising from limited parallelism and increases in communication can lead to results that are pessimistic when the expectation is that more processors will be used to solve larger problems. Thus, it is often useful to measure the speedup as processors are added both for a fixed-size problem and for a scaled version of the problem, providing an unscaled and a scaled version of the speedup curves. The choice of how to measure the uniprocessor algorithm is also important to avoid anomalous results, since using the parallel version of the benchmark may understate the uniprocessor performance and thus overstate the speedup, as discussed with an example in section 6.14. Once we have decided to measure scaled speedup, the question is how to scale the application. Lets assume that we have determined that running a benchmark of size n on p processors makes sense. The question is how to scale the benchmark to run on m p processors. There are two obvious ways to scale the problem: keeping the amount of memory used per processor constant; and keeping the total execution time, assuming perfect speedup, constant. The first method, called memory-constrained scaling, specifies running a problem of size m n on m p processors. The second method, called time-constrained scaling, requires that we know the relationship between the running time and the problem size, since the former is kept constant. For example, suppose the running time of the application with data size n on p processors is proportional to n2/p. Then with timeconstrained scaling, the problem to run is the problem whose ideal running time on m p processors is still n2/p. The problem with this ideal running time has size m n "
Explain why the WFI maintains a structure that maps the address of a replicated page (the local physical address) to its original physical address (called the global address),"In addition to the page counters that the operating system uses to decide when to migrate or replicate a page, CMR requires special support to map between physical and virtual addresses of replicated pages. First, when a page is replicated the page tables are changed to refer to the local physical memory address of the duplicated page. To maintain coherence, however, a miss to this page must be sent to the home node to check the directory entry in that node. Hence, the WFI maintains a structure that maps the address of a replicated page (the local physical address) to its original physical address (called the global address) and generates the appropriate remote memory request, just as if the page were never replicated."," CMR, like S-COMA, maintains coherence at the unit of a cache-block, rather than at the page level. This choice is important for two reasons. First, maintaining coherence at the page level is likely to lead to a significant numbers of false sharing misses; we saw this increase in false sharing misses with increases in block size in Section 6.3. Second, the large size of a page means that even true sharing misses are likely to end up moving many bytes of data that are never used. These two drawbacks have limited the usefulness of the Shared Virtual Memory approach, which we discussed on page 733. CMR avoids these problems by making the unit of coherence a cache block and by selectively migrating and replicating some pages, while leaving others as standard NUMA pages that are accessed remotely when a cache miss occurs. Putting It All Together: Suns Wildfire Prototype In addition to the page counters that the operating system uses to decide when to migrate or replicate a page, CMR requires special support to map between physical and virtual addresses of replicated pages. First, when a page is replicated the page tables are changed to refer to the local physical memory address of the duplicated page. To maintain coherence, however, a miss to this page must be sent to the home node to check the directory entry in that node. Thus, the WFI maintains a structure that maps the address of a replicated page (the local physical address) to its original physical address (called the global address) and generates the appropriate remote memory request, just as if the page were never replicated. When a write-back request or invalidation request is received, the global address must be translated to the local address, and the WFI maintains such a mapping for all pages that have been replicated. By maintaining these two maps, pages can be replicated while maintaining coherence at the unit of a cache block, which increases the usefulness of page replication. Performance of Wildfire In this section we look at the performance of the Wildfire prototype starting first with basic performance measures such as latency for memory accesses and bandwidth and then turning to application performance. Since Wildfire is a research prototype, rather than a product, its performance evaluation for applications is limited, but some interesting experiments that evaluate the use of page migration and replication are available"
"Explain why despite the fact that the E10000 interconnect has performance equal to that of Wildfire, the performance of Wildfire is about 1"," The global broadcast of the E10000 has nontrivial overhead. Hence, despite the fact that the E10000 interconnect has performance equal to that of Wildfire, the performance of Wildfire is about 1."," Putting It All Together: Suns Wildfire Prototype The 24-processor runs include a 3-node Wildfire configuration (with an 8-processor E6000 in each Wildfire node), a 6-node E10000 and a 24-processor E6000. The performance differences among the 24-processor runs on Wildfire, the E1000, and the E6000 arise primarily from bus and interconnect differences. The global broadcast of the E10000 has nontrivial overhead. Thus, despite the fact that the E10000 interconnect has performance equal to that of Wildfire, the performance of Wildfire is about 1.17 times better. For the E6000, the measured bus usage for the 24-processor runs is between 90% and 100%, leading to a significant bottleneck and lengthened memory access time. Overall, Wildfire has a performance advantage of about 1.19 versus the E6000. Equally importantly, these measurements tell us that configurations of Wildfire with larger processor counts per node will not have good performance, at least for applications with behavior similar to this solver. The 36 processor runs confirm this view. The 36-processor runs compare three alternatives: a 9-node E10000, a 2x18 configure of Wildfire (each Wildfire node is an 18-processor E6000) and a 4x9 configuration of Wildfire (each Wildfire node is an 9-processor E6000). The most interesting comparison here involve the 36-processor versus 24-processor results"
Explain how I/O performance increasingly limits system performance and effectiveness,"Suppose response time is just 10% longer than CPU time. First we speed up the CPU by a factor of 10, while neglecting I/O. Amdahls Law tells us the speedup is only 5 times, half of what we would have achieved if both were sped up tenfold. Similarly, making the CPU 100 times faster without improving the I/O would obtain a speedup of only 10 times, squandering 90% of the potential. If, as predicted in Chapter 1, performance of CPUs improves at 55% per year and I/O did not improve, every task would become I/Obound. There would be no reason to buy faster CPUsand no jobs for CPU designers. Hence, I/O performance increasingly limits system performance and effectiveness."," There are several points to make in reply. First, this is an argument that performance is measured as throughputnumber of tasks completed per hourversus response time. Plainly, if users didnt care about response time, interactive software never would have been invented, and there would be no workstations or personal computers today; section 7.7 gives experimental evidence of the importance of response time. It may also be expensive to rely on running other processes, since paging traffic from process switching might actually increase I/ O. Furthermore, with mobile devices and desktop computing, there is only one person per computer and thus fewer processes than in timesharing. Many times the only waiting process is the human being! Moreover, applications such as transaction processing (section 7.7) place strict limits on response time as part of the performance analysis. I/Os revenge is at hand. Suppose response time is just 10% longer than CPU time. First we speed up the CPU by a factor of 10, while neglecting I/O. Amdahls Law tells us the speedup is only 5 times, half of what we would have achieved if both were sped up tenfold. Similarly, making the CPU 100 times faster without improving the I/O would obtain a speedup of only 10 times, squandering 90% of the potential. If, as predicted in Chapter 1, performance of CPUs improves at 55% per year and I/O did not improve, every task would become I/Obound. There would be no reason to buy faster CPUsand no jobs for CPU designers. Thus, I/O performance increasingly limits system performance and effectiveness. Does CPU Performance Matter? Moores Law leads to both large, fast CPUs but also to very small, cheap CPUs"
Explain why the final component of disk-access time is controller time," To handle the complexities of disconnect/connect and read ahead, there is usually, in addition to the disk drive, a device called a disk controller. Hence, the final component of disk-access time is controller time, which is the overhead the controller imposes in performing an I/O access."," There is also a desire to amortize this long access by reading more than simply what is requested; this is called read ahead. Read ahead is another case of computer designs trying to leverage spatial locality to enhance performance (see will already be available. These sectors go into buffers on the disk that act as a cache. As Figure 7.2 shows, the size of this buffer varies from 0.125 to 4 MB. The hit rate presumably comes solely from spatial locality, but disk-caching algorithms are proprietary and so their techniques and hit rates are unknown. Transfers to and from the buffer operate at the speed of the I/O bus versus the speed of the disk media. In 2001, the I/O bus speeds vary from 80 to 320 MB per second. To handle the complexities of disconnect/connect and read ahead, there is usually, in addition to the disk drive, a device called a disk controller. Thus, the final component of disk-access time is controller time, which is the overhead the controller imposes in performing an I/O access. When referring to the performance of a disk in a computer system, the time spent waiting for a disk to become free (queuing delay) is added to this time. What is the average time to read or write a 512-byte sector for a disk? The advertised average seek time is 5 ms, the transfer rate is 40 MB/second, it rotates at 10000 RPM, and the controller overhead is 0.1 ms. Assume the disk is idle so that there is no queuing delay. In addition, calculate the time assuming the advertised seek time is three times longer than the measured seek time"
"Explain why as disks continue to grow, its is not only more expensive to use tapes for backups, it will also take much longer to recover if a disaster occurs","In addition to the issue of capacity, another challenge is recovery time. Tapes are also not keeping up in bandwidth of disks. Hence, as disks continue to grow, its is not only more expensive to use tapes for backups, it will also take much longer to recover if a disaster occurs."," As disks are a closed system, the disk heads need only read the platters that are enclosed with them, and this advantage explains why disks are improving at rates that much more rapid. In addition to the issue of capacity, another challenge is recovery time. Tapes are also not keeping up in bandwidth of disks. Thus, as disks continue to grow, its is not only more expensive to use tapes for backups, it will also take much longer to recover if a disaster occurs. This growing gap between rate of improvement in disks and tapes calls into question the sensibility of tape backup for disk storage"
Explain why information is organized in Flash as linked lists of blocks," If the logical data is smaller than the Flash block size, the good data that should survive must be copied to another block before the old block can be erased. Hence, information is organized in Flash as linked lists of blocks."," Compared to disks, Flash memories offer low power consumption (less than 50 milliwatts), can be sold in small sizes, and offer read access times comparable to DRAMs. In 2001, a 16 Mbit Flash memory has a 65 ns access time, and a 128 Mbit Flash memory has a 150 ns access time. Some memories even borrow the page mode assesses acceleration from DRAM to bring the time per word down in block transfers to 25 to 40 ns. Unlike DRAMs, writing is much slower and more complicated, sharing characteristics with the older electrically programmable read-only memories (EPROM) and electrically erasable and programmable readonly memories (EEPROM). A block of Flash memory are first electrically erased, and then written with 0s and 1s. If the logical data is smaller than the Flash block size, the good data that should survive must be copied to another block before the old block can be erased. Thus, information is organized in Flash as linked lists of blocks. Such concerns lead to software that collects good data into fewer blocks so that the rest Types of Storage Devices can be erased. The linked list structure is also used by some companies to map out bad blocks and offer reduced memory parts at half price rather than discard flawed chips. The electrical properties of Flash memory are not as well understood as DRAM. Each companys experience, including whether it manufactured EPROM or EEPROM before Flash, affects the organization that it selects. The two basic types of Flash are based on the whether the building blocks for the bits are NOR or NAND gates. NOR Flash devices in 2000 typically take one to two seconds to erase 64 KB to 128 KB blocks, while NAND Flash devices take 5 to 6 millisecond to erase smaller blocks of 4 KB to 8 KB. Programming takes 10 microseconds per byte for NOR devices and 1.5 microseconds per byte for NAND devices. The number of times bits can be erased and still retain information is also often limited, typically about 100,000 cycles for NOR devices and 1,000,000 for some NAND devices"
"Explain why a split-transaction bus has higher bandwidth, but it usually has higher latency than a bus that is held during the complete transaction","The read transaction is broken into a read-request transaction that contains the address and a memory-reply transaction that contains the data. Each transaction must now be tagged so that the CPU and memory can tell which reply is for which request. Split transactions make the bus available for other masters while the memory reads the words from the requested address. It also normally means that the CPU must arbitrate for the bus to send the data and the memory must arbitrate for the bus to return the data. Hence, a split-transaction bus has higher bandwidth, but it usually has higher latency than a bus that is held during the complete transaction."," With multiple masters, a bus can offer higher bandwidth by using packets, as opposed to holding the bus for the full transaction. This technique is called split transactions. (Some systems call this ability connect/disconnect, a pipelined bus, a pended bus, or a packet-switched bus; the next chapter goes into more detail on packet switching.) Figure 7.9 shows the split-transaction bus. The idea is to divide bus events into requests and replies, so that bus can be used in the time between the request and the reply. The read transaction is broken into a read-request transaction that contains the address and a memory-reply transaction that contains the data. Each transaction must now be tagged so that the CPU and memory can tell which reply is for which request. Split transactions make the bus available for other masters while the memory reads the words from the requested address. It also normally means that the CPU must arbitrate for the bus to send the data and the memory must arbitrate for the bus to return the data. Thus, a split-transaction bus has higher bandwidth, but it usually has higher latency than a bus that is held during the complete transaction. The final item in Figure 7.8, clocking, concerns whether a bus is synchronous or asynchronous. If a bus is synchronous, it includes a clock in the control lines and a fixed protocol for sending address and data relative to the clock. Since little or no logic is needed to decide what to do next, these buses can be both fast and inexpensive. They have two major disadvantages, however. Because of clockskew problems, synchronous buses cannot be long, and everything on the bus must run at the same clock rate. Some buses allow multiple speed devices on a BusesConnecting I/O Devices to CPU/Memory Data addr1 addr2 addr3 Wait 1 OK 1 FIGURE 7.9 A split-transaction bus. Here the address on the bus corresponds to a later memory access"
"Explain why an error is the manifestation in the system of a fault, and a failure is the manifestation on the service of an error","When a fault occurs it creates a latent error, which becomes effective when it is activated; when the error actually affects the delivered service, a failure occurs. The time between the occurrence of an error and the resulting failure is the error latency. Hence, an error is the manifestation in the system of a fault, and a failure is the manifestation on the service of an error."," A system failure occurs when the actual behavior deviates from the specified behavior. The failure occurred because an error, a defect in that module. The cause of an error is a fault. When a fault occurs it creates a latent error, which becomes effective when it is activated; when the error actually affects the delivered service, a failure occurs. The time between the occurrence of an error and the resulting failure is the error latency. Thus, an error is the manifestation in the system of a fault, and a failure is the manifestation on the service of an error. Lets go back to our motivating examples above. A programming mistake is a fault; the consequence is an error (or latent error) in the software; upon activation, the error becomes effective; when this effective error produces erroneous data which affect the delivered service, a failure occurs. An alpha particle hitting a DRAM can be considered a fault; if it changes the memory, it creates an error; the error will remain latent until the effected memory word is read; if the effective word error affects the delivered service, a failure occurs. (If ECC corrected the error, a failure would not occur.) A mistake by a human operator is a fault; the resulting altered data is an error; it is latent until activated; and so on as before"
Explain why an effective error is either a formerly latent error in that component or it has propagated from another error in that component or from elsewhere," To clarify, the relation between faults, errors, and failures is: n n b) an error may cycle between its latent and effective states; c) an effective error often propagates from one component to another, thereby creating new errors. Hence, an effective error is either a formerly latent error in that component or it has propagated from another error in that component or from elsewhere."," Lets go back to our motivating examples above. A programming mistake is a fault; the consequence is an error (or latent error) in the software; upon activation, the error becomes effective; when this effective error produces erroneous data which affect the delivered service, a failure occurs. An alpha particle hitting a DRAM can be considered a fault; if it changes the memory, it creates an error; the error will remain latent until the effected memory word is read; if the effective word error affects the delivered service, a failure occurs. (If ECC corrected the error, a failure would not occur.) A mistake by a human operator is a fault; the resulting altered data is an error; it is latent until activated; and so on as before. To clarify, the relation between faults, errors, and failures is: n n b) an error may cycle between its latent and effective states; c) an effective error often propagates from one component to another, thereby creating new errors. Thus, an effective error is either a formerly latent error in that component or it has propagated from another error in that component or from elsewhere. n n We can now return to see how Laprie defines reliability and availability. Users perceive a system alternating between two states of delivered service with respect to the service specification: 1. Service accomplishment, where the service is delivered as specified, 2. Service interruption, where the delivered service is different from the specified service"
Explain why the small write involves four disk accesses to two disks instead of accessing all disks,"We must read the old data from the disk being written, compare old data to the new data to see which bits change, read the old parity, change the corresponding bits, then write the new data and new parity. Hence, the small write involves four disk accesses to two disks instead of accessing all disks."," Writes are another matter. It would seem that each small write would demand that all other disks be accessed to read the rest of the information needed to recalculate the new parity, as in Figure 7.18. A small write would require reading the old data and old parity, adding the new information, and then writing the new parity to the parity disk and the new data to the data disk. The key insight to reduce this overhead is that parity is simply a sum of information; by watching which bits change when we write the new information, we need only change the corresponding bits on the parity disk. Figure 7.18 shows the shortcut. We must read the old data from the disk being written, compare old data to the new data to see which bits change, read the old parity, change the corresponding bits, then write the new data and new parity. Thus, the small write involves four disk accesses to two disks instead of accessing all disks. This organization is RAID 4. RAID 4 efficiently supports a mixture of large reads, large writes, small reads, and small writes. One drawback to the system is that the parity disk must be up- D0 D0 XOR 4. Write New data P 5. Write + 2. Read P + 3. Write D3 P 4. Write FIGURE 7.18 Small write update on RAID 3 vs. RAID 4/ RAID5. This optimization for small writes reduces the number of disk accesses as well as the number of disks occupied. This figure assumes we have four blocks of data and one block of parity. The straightforward RAID 3 parity calculation at the top of the figure reads blocks D1, D2, and D3 before adding block D0 to calculate the new parity P. (In case you were wondering, the new data D0 comes directly from the CPU, so disks are not involved in reading it.) The RAID 4/ RAID 5shortcut at the bottom reads the old value D0 and compares it to the new value D0 to see which bits will change. You then read to old parity P and then change the corresponding bits to form P. The logical function exclusive or does exactly what we want. This example replaces 3 disks reads (D1, D2, D3) and 2 disk writes (D0,P) involving all the disks for 2 disk reads (D0,P) and 2 disk writes (D0,P) which involve just 2 disks. Increasing the size of the parity group increases the savings of the shortcut"
Explain why the storage overhead is twice that of RAID 5,"Parity based schemes protect against a single, self-identifying failures. When a single failure is not sufficient, parity can be generalized to have a second calculation over the data and another check disk of information. Yet another parity block is added to allow recovery from a second failure. Hence, the storage overhead is twice that of RAID 5."," RAID 5 FIGURE 7.19 Block-interleaved parity (RAID 4) versus distributed block-interleaved parity (RAID 5). By distributing parity blocks to all disks, some small writes can be performed in parallel. P+Q redundancy (RAID 6) Parity based schemes protect against a single, self-identifying failures. When a single failure is not sufficient, parity can be generalized to have a second calculation over the data and another check disk of information. Yet another parity block is added to allow recovery from a second failure. Thus, the storage overhead is twice that of RAID 5. The small write shortcut of Figure 7.18 works as well, except now there are six disk accesses instead of four to update both P and Q information. RAID Summary The higher throughput, measured either as megabytes per second or as I/Os per second, as well the ability to recover from failures make RAID attractive"
Explain why the challenge for dependable systems of the future is either to tolerate faults by operators or to avoid faults by simplifying the tasks of system administration," Although failures may be initiated due to faults by operators, it is a poor reflection on the state of the art of systems that the process of maintenance and upgrading are so error prone. Hence, the challenge for dependable systems of the future is either to tolerate faults by operators or to avoid faults by simplifying the tasks of system administration."," These four examples and others suggest that the primary cause of failures in large systems today is faults by human operators. Hardware faults have declined due to a decreasing number of chips in systems, reduced power, and fewer connectors. Hardware dependability has improved through fault tolerance techniques such as RAID. At least some operating systems are considering reliability implications before new adding features, so in 2001 the failures largely occur elsewhere. Although failures may be initiated due to faults by operators, it is a poor reflection on the state of the art of systems that the process of maintenance and upgrading are so error prone. Thus, the challenge for dependable systems of the future is either to tolerate faults by operators or to avoid faults by simplifying the tasks of system administration. We have now covered the bedrock issue of dependability, giving definitions, case studies, and techniques to improve it. The next step in the storage tour is performance. Well cover performance metrics, queuing theory, and benchmarks"
"Explain why there is a policy choice between taking a performance hit during reconstruction, or lengthening the window of vulnerability and thus lowering the predicted MTTF","As RAID systems can lose data if a second disk fails before completing reconstruction, the longer the reconstruction (MTTR), the lower the availability (see section 6.7 below). Increased reconstruction speed implies decreased application performance, however, as reconstruction steals I/O resources from running applications. Hence, there is a policy choice between taking a performance hit during reconstruction, or lengthening the window of vulnerability and thus lowering the predicted MTTF."," top graphs show Linux (on the left) and Solaris (on the right). Both systems automatically reconstruct data onto a hot spare disk immediately when a disk failure is detected. As can be seen in the figure, Linux reconstructs slowly and Solaris reconstructs quickly. Windows is shown in the bottom; a single disk failed so the data is still available, but this system does not begin reconstructing on the hot spare until the operator gives permission. Linux and Solaris, in contrast, start reconstruction upon the fault injection. Reconstruction Hits per second Reconstruction 110 80 30 70 10 40 80 Linux 150 20 40 Time (minutes) Windows FIGURE 7.34 Availability benchmark for software RAID systems on the same computer running Redhat 6.0 Linux, Solaris 7, and Windows 2000 operating systems. Note the difference in philosophy on speed of reconstruction of Linux vs. Windows and Solaris. The Y-axis is behavior in hits per second running SPEC WEB99. The arrow indicates time of fault insertion. The lines at the top give the 99% confidence interval of performance before the fault is inserted. A 99% confidence interval means the if the variable is outside of this range, the probability is only 1% that this value would appear. <<Artist: please add reconstruction arrows like in upper right graph to dip in lower graph; there are no Excel sheet for these graphs>> As RAID systems can lose data if a second disk fails before completing reconstruction, the longer the reconstruction (MTTR), the lower the availability (see section 6.7 below). Increased reconstruction speed implies decreased application performance, however, as reconstruction steals I/O resources from running applications. Thus, there is a policy choice between taking a performance hit during reconstruction, or lengthening the window of vulnerability and thus lowering the predicted MTTF. Crosscutting Issues Although none of the tested system documented their reconstruction policies outside of the source code, even a single fault injection was able to give insight into those policies. The experiments revealed that both Linux and Solaris initiate automatic reconstruction of the RAID volume onto a hot spare when an active disk is taken out of service due to a failure. Although Windows supports RAID reconstruction, the reconstruction must be initiated manually. Thus, without human intervention, a Windows system will not rebuild redundancy after a first failure, and will remain susceptible to a second failure indefinitely, which increases the window of vulnerability increases the window of vulnerability"
"Explain why without human intervention, a Windows system will not rebuild redundancy after a first failure, and will remain susceptible to a second failure indefinitely","Although Windows supports RAID reconstruction, the reconstruction must be initiated manually. Hence, without human intervention, a Windows system will not rebuild redundancy after a first failure, and will remain susceptible to a second failure indefinitely, which increases the window of vulnerability increases the window of vulnerability."," Reconstruction Hits per second Reconstruction 110 80 30 70 10 40 80 Linux 150 20 40 Time (minutes) Windows FIGURE 7.34 Availability benchmark for software RAID systems on the same computer running Redhat 6.0 Linux, Solaris 7, and Windows 2000 operating systems. Note the difference in philosophy on speed of reconstruction of Linux vs. Windows and Solaris. The Y-axis is behavior in hits per second running SPEC WEB99. The arrow indicates time of fault insertion. The lines at the top give the 99% confidence interval of performance before the fault is inserted. A 99% confidence interval means the if the variable is outside of this range, the probability is only 1% that this value would appear. <<Artist: please add reconstruction arrows like in upper right graph to dip in lower graph; there are no Excel sheet for these graphs>> As RAID systems can lose data if a second disk fails before completing reconstruction, the longer the reconstruction (MTTR), the lower the availability (see section 6.7 below). Increased reconstruction speed implies decreased application performance, however, as reconstruction steals I/O resources from running applications. Thus, there is a policy choice between taking a performance hit during reconstruction, or lengthening the window of vulnerability and thus lowering the predicted MTTF. Crosscutting Issues Although none of the tested system documented their reconstruction policies outside of the source code, even a single fault injection was able to give insight into those policies. The experiments revealed that both Linux and Solaris initiate automatic reconstruction of the RAID volume onto a hot spare when an active disk is taken out of service due to a failure. Although Windows supports RAID reconstruction, the reconstruction must be initiated manually. Thus, without human intervention, a Windows system will not rebuild redundancy after a first failure, and will remain susceptible to a second failure indefinitely, which increases the window of vulnerability increases the window of vulnerability. The fault-injection experiments also provided insight into other availability policies of Linux, Solaris, and Windows 2000 concerning automatic spare utilization, reconstruction rates, transient errors, and so on. Again, no system documented their policies"
"Explain why a new I/O controller designed to efficiently transfer 1-MB files would never see more than 63 KB at a time under early UNIX, no matter how large the files","For example, many I/O controllers used in early UNIX systems were 16-bit microprocessors. To avoid problems with 16-bit addresses in controllers, UNIX was changed to limit the maximum I/O transfer to 63 KB or less. Hence, a new I/O controller designed to efficiently transfer 1-MB files would never see more than 63 KB at a time under early UNIX, no matter how large the files."," Crosscutting Issues Thus far, we have ignored the role of the operating system in storage. In a manner analogous to the way compilers use an instruction set, operating systems determine what I/O techniques implemented by the hardware will actually be used. For example, many I/O controllers used in early UNIX systems were 16-bit microprocessors. To avoid problems with 16-bit addresses in controllers, UNIX was changed to limit the maximum I/O transfer to 63 KB or less. Thus, a new I/O controller designed to efficiently transfer 1-MB files would never see more than 63 KB at a time under early UNIX, no matter how large the files. The operating system enforces the protection between processes, which must include I/O activity as well as memory accesses. Since I/O is typically between a device and memory, the operating system must endure safety"
"Explain why if the chance of a second failure before repair is large, then MTDL is small, and vice versa","For RAID, data is lost only if a second disk failure occurs in the group protected by parity before the first failed disk is repaired. Mean time until data loss is the mean time until a disk will fail divided by the chance that one of the remaining disks in the parity group will fail before the first failure is repaired. Hence, if the chance of a second failure before repair is large, then MTDL is small, and vice versa."," dundant hardware: extra disks, controllers, power supplies, fans, and controllers in a RAID-5 configuration. To calculate reliability now, we need a formula to show what to expect when we can tolerate a failure and still provide service. To simplify the calculations we assume that the lifetimes of the components are exponentially distributed and the there is no dependency between the component failures. Instead of mean time to failure, we calculate mean time until data loss (MTDL), for a single failure will not, in general, result in lost service. For RAID, data is lost only if a second disk failure occurs in the group protected by parity before the first failed disk is repaired. Mean time until data loss is the mean time until a disk will fail divided by the chance that one of the remaining disks in the parity group will fail before the first failure is repaired. Thus, if the chance of a second failure before repair is large, then MTDL is small, and vice versa. Assuming independent failures, since we have N disks, the mean time until one disk fails is MTTF disk N . The good approximation of the probability of the second failure is MTTR over the mean time until one of the remaining G -1 disks in the parity group will fail. Similar to before, the means time for G -1 disks is ( MTTF disk ( G 1 ) ). Hence, a reasonable approximation for MTDL for a RAID is [Chen 1994]: MTTF disk MTTF disk N MTTF disk N MTDL = ---------------------------------------------------- = -------------------------------------------------- = -----------------------------------------------------------MTTR disk ( G 1 ) MTTR disk N ( G 1 ) MTTR disk (--------------------------------------------------MTTF ( G 1 ) )disk where N is the number of disks in the system and G is the number of disks in a group protected by parity. Thus, MTDL increases with increased disk reliability, reduced parity group size, and reduced mean time to repair (MTTR)"
Explain why some CPUs assumed that the cache was bigger than what other CPUs assumed,"the data structure representing which portions of the cache were available or fenced is not identical in each CPU. Hence, some CPUs assumed that the cache was bigger than what other CPUs assumed."," As mentioned above, the EMC array has the ability to shrink the size of the cache in response to faults by fencing off a portion of the cache. It also has error correction that can prevent a fault from causing a failure. The system under test had 8 GB of cache and 96 disks each with 18 GB of capacity, and it was connected to an IBM mainframe over 12 channels. The workload was random I/O with 75% reads and 25% writes. Performance was evaluated using EMC benchmarks. The first fault tests the behavior of the system when the CPUs in the front and back end get confused: the data structure representing which portions of the cache were available or fenced is not identical in each CPU. Thus, some CPUs assumed that the cache was bigger than what other CPUs assumed. Figure 7.42 shows the behavior when half of the CPUs are out-of-sync. A fault was injected at the 5-th minute and corrected at the 10-th minute. The I/O rate increases in the 12-th minute as the system catches up with delayed requests. Performance dropped because some CPUs would try to access disabled memory, generating an error. As each error happened there was a short delay to report it; as the number of CPUs reporting errors increased, so did the delay"
Explain why on a SCSI string some disks can be seeking and others loading their track buffer while one is transferring data from its buffer over the SCSI bus,"many SCSI-compatible disk drives include a track buffer on the disk itself, supporting read ahead and connect/disconnect. Hence, on a SCSI string some disks can be seeking and others loading their track buffer while one is transferring data from its buffer over the SCSI bus."," In recognition of their role, in 1999 Garth Gibson, Randy Katz, and David Patterson received the IEEE Reynold B. Johnson Information Storage Award for the development of Redundant Arrays of Inexpensive Disks (RAID). I/O Buses and Controllers The ubiquitous microprocessor has inspired not only the personal computers of the 1970s, but also the trend in the late 1980s and 1990s of moving controller functions into I/O devices. I/O devices continued this trend by moving controllers into the devices themselves. These devices are called intelligent devices, and some bus standards (e.g., SCSI) have been created specifically for them. Intelligent devices can relax the timing constraints by handling many low-level tasks themselves and queuing the results. For example, many SCSI-compatible disk drives include a track buffer on the disk itself, supporting read ahead and connect/disconnect. Thus, on a SCSI string some disks can be seeking and others loading their track buffer while one is transferring data from its buffer over the SCSI bus. The controller in the original RAMAC, built from vacuum tubes, only needed to move the head over the desired track, wait for the data to pass under the head, and transfer data with calculated parity. SCSI, which stands for small computer systems interface, is an example of one company inventing a bus and generously encouraging other companies to build devices that would plug into it. Shugart created this bus, originally called SASI. It was later standardized by the IEEE"
Explain why the messaging software must have some way to distinguish between processes;,"Interconnection networks involve normally software. Even this simple example invokes software to translate requests and replies into messages with the appropriate headers. An application program must usually cooperate with the operating system to send a message to another machine, since the network will be shared with all the processes running on the two machines, and the operating system cannot allow messages for one process to be received by another. Hence, the messaging software must have some way to distinguish between processes; this distinction may be included in an expanded header."," For one machine to get data from the other, it must first send a request containing the address of the data it desires from the other node. When a request arrives, the machine must send a reply with the data. Hence, each message must have at least 1 bit in addition to the data to determine whether the message is a new request or a reply to an earlier request. The network must distinguish between information needed to deliver the message, typically called the header or the trailer depending on where it is relative to the data, and the payload, which contains the data. Figure 8.5 shows the format of messages in our simple network. This example shows a single-word payload, but messages in some interconnection networks can include hundreds of words. Interconnection networks involve normally software. Even this simple example invokes software to translate requests and replies into messages with the appropriate headers. An application program must usually cooperate with the operating system to send a message to another machine, since the network will be shared with all the processes running on the two machines, and the operating system cannot allow messages for one process to be received by another. Thus, the messaging software must have some way to distinguish between processes; this distinction may be included in an expanded header. Although hardware support can reduce the amount of work, most is done by software. In addition to protection, network software is often responsible for ensuring reliable delivery of messages. The twin responsibilities are ensuring that the message is neither garbled nor lost in transit"
"Explain why interconnection bandwidth is reserved as circuits are established rather than consumed as data are sent, and if the network is full, no more circuits can be established","One advantage of a circuit-switched network is that once a circuit is established, it ensures there is sufficient bandwidth to deliver all the information sent along that circuit. Moreover, switches along a path can be requested to give specific quality of service guarantees. Hence, interconnection bandwidth is reserved as circuits are established rather than consumed as data are sent, and if the network is full, no more circuits can be established."," A final routing issue is the order in which packets arrive. Some networks require that packets arrive in the order sent. The alternative removes this restriction, requiring software to reassemble the packets in proper order. Congestion Control One advantage of a circuit-switched network is that once a circuit is established, it ensures there is sufficient bandwidth to deliver all the information sent along that circuit. Moreover, switches along a path can be requested to give specific quality of service guarantees. Thus, interconnection bandwidth is reserved as circuits are established rather than consumed as data are sent, and if the network is full, no more circuits can be established. You may have encountered this blockage when trying to place a long distance phone call on a popular holiday or to a television show, as the telephone system tells you that all circuits are busy and asks you to please call back at a later time. Packet-switched networks generally do not reserve interconnect bandwidth in advance, so the interconnection network can become clogged with too many packets. Just as with rush hour traffic, a traffic jam of packets increases packet latency. Packets take longer to arrive, and in extreme cases fewer packets per second are delivered by the interconnect, just as is the case for the poor rush-hour commuters. There is even the computer equivalent of gridlock: deadlock is Connecting More Than Two Computers achieved when packets in the interconnect can make no forward progress no matter what sequence of events happens. Chapter 6 addresses how to avoid this ultimate congestion in the context of a multiprocessor"
Explain why a sequential program in a cluster has 1/Nth the memory available compared to a sequential program in a shared memory multiprocessor," A second weakness is the division of memory: a cluster of N machines has N independent memories and N copies of the operating system, but a shared address multiprocessor allows a single program to use almost all the memory in the computer. Hence, a sequential program in a cluster has 1/Nth the memory available compared to a sequential program in a shared memory multiprocessor."," Performance Challenges of Clusters One drawback is that clusters are usually connected using the I/O bus of the computer, whereas multiprocessors are usually connected on the memory bus of the computer. The memory bus has higher bandwidth and much lower latency, allowing multiprocessors to drive the network link at higher speed and to have fewer conflicts with I/O traffic on I/O-intensive applications. This connection point also means that clusters generally use software-based communication while multiprocessors use hardware for communication. However, it makes connections nonstandard and hence more expensive. A second weakness is the division of memory: a cluster of N machines has N independent memories and N copies of the operating system, but a shared address multiprocessor allows a single program to use almost all the memory in the computer. Thus, a sequential program in a cluster has 1/Nth the memory available compared to a sequential program in a shared memory multiprocessor. Interest- ingly, the drop in DRAM prices has made memory costs so low that this multiprocessor advantage is much less important in 2001 than it was in 1995. The primary issue in 2001 is whether the maximum memory per cluster node is sufficient for the application. Dependability and Scalability Advantage of Clusters The weakness of separate memories for program size turns out to be a strength in system availability and expansibility. Since a cluster consists of independent computers are connected through a local area network, it is much easier to replace a machine without bringing down the system in a cluster than in an shared memory multiprocessor. Fundamentally, the shared address means that it is difficult to isolate a processor and replace a processor without significant work by the operating system and hardware designer. Since the cluster software is a layer that runs on top of local operating systems running on each computer, it is much easier to disconnect and replace a broken machine"
Explain why a guideline is that collocation sites are designed assuming no more than 100 watts per square foot," Pacific Gas and Electric in Silicon Valley limits a single building to have no more than 12 megawatts of power and the typical size of a building is no more than 100,000 square feet. Hence, a guideline is that collocation sites are designed assuming no more than 100 watts per square foot."," Designing a Cluster Collocation rates are negotiated and much cheaper per unit as space requirements increase. A rough guideline in 2001 is that rack space, which includes one 20-amp circuit, costs $800 to $1200 per month. It drops by 20% if you use more than 75 to 100 racks. Each additional 20 amp circuit per rack costs another $200 to $400 per month. Although we are not calculating these costs in this case, they also charge for network bandwidth: $1500 to $2000 per Mbits/sec per month, if your continuous use is just 1-10 Mbits/second, drops to $500 to $750 per Mbits/ sec per month, if your continuous use measures 1 - 2 Gbits/second. Pacific Gas and Electric in Silicon Valley limits a single building to have no more than 12 megawatts of power and the typical size of a building is no more than 100,000 square feet. Thus, a guideline is that collocation sites are designed assuming no more than 100 watts per square foot. If you include the space for people to get access to a rack to repair and replace components, a rack needs about 10 square feet. Thus, collocation sites expect at most 1000 watts per rack. EXAMPLE Using the information in Figure 8.38, calculate the total cost of ownership for three years: purchase prices, operator costs, and maintenance costs"
Explain why collocation sites expect at most 1000 watts per rack,"Pacific Gas and Electric in Silicon Valley limits a single building to have no more than 12 megawatts of power and the typical size of a building is no more than 100,000 square feet. Thus, a guideline is that collocation sites are designed assuming no more than 100 watts per square foot. If you include the space for people to get access to a rack to repair and replace components, a rack needs about 10 square feet. Hence, collocation sites expect at most 1000 watts per rack."," Designing a Cluster Collocation rates are negotiated and much cheaper per unit as space requirements increase. A rough guideline in 2001 is that rack space, which includes one 20-amp circuit, costs $800 to $1200 per month. It drops by 20% if you use more than 75 to 100 racks. Each additional 20 amp circuit per rack costs another $200 to $400 per month. Although we are not calculating these costs in this case, they also charge for network bandwidth: $1500 to $2000 per Mbits/sec per month, if your continuous use is just 1-10 Mbits/second, drops to $500 to $750 per Mbits/ sec per month, if your continuous use measures 1 - 2 Gbits/second. Pacific Gas and Electric in Silicon Valley limits a single building to have no more than 12 megawatts of power and the typical size of a building is no more than 100,000 square feet. Thus, a guideline is that collocation sites are designed assuming no more than 100 watts per square foot. If you include the space for people to get access to a rack to repair and replace components, a rack needs about 10 square feet. Thus, collocation sites expect at most 1000 watts per rack. EXAMPLE Using the information in Figure 8.38, calculate the total cost of ownership for three years: purchase prices, operator costs, and maintenance costs"
"Explain why operator cost is 3 x $100,000 = $300,000 or 3 x $50,000 = $150,000"," To keep things simple, we assume each system with local disks needs a full -time operator, but the clusters that access their disks over an SAN with RAID need only a half-time operator. Hence, operator cost is 3 x $100,000 = $300,000 or 3 x $50,000 = $150,000."," EXAMPLE Using the information in Figure 8.38, calculate the total cost of ownership for three years: purchase prices, operator costs, and maintenance costs. To keep things simple, we assume each system with local disks needs a full -time operator, but the clusters that access their disks over an SAN with RAID need only a half-time operator. Thus, operator cost is 3 x $100,000 = $300,000 or 3 x $50,000 = $150,000. For backup, lets assume we need enough tapes to store 2 TB for a full dump. We need four sets for the weekly dumps plus six more sets so that we can have a six-month archive. Tape units normally compress their data to get a factor of two in density, so well assume compression successfully turns 40 GB drives into 80 GB drives. The cost of these tapes is: 2000GB 10 -------------------------- $ 70 = 10 25 $ 70 = $17, 500 80GB/tape The daily backups depend on the amount of data changed. If 2 tapes per day are sufficient (up to 8% changes per day), we need to spend another 7 2 $ 70 = 14 $ 70 = $ 980 The figure lists maintenance costs for the computers and the LAN"
Explain why there are 52 x 2 or 104 SCSI cables attached to the 28 RAID controllers which support up to 28 x 4 or 106 strings,"IBM used a RAID product that plugs into a PCI card and provides four SCSI strings. To get higher availability and performance, each enclosure attaches to two SCSI buses. Hence, there are 52 x 2 or 104 SCSI cables attached to the 28 RAID controllers which support up to 28 x 4 or 106 strings."," The totals are 560 9.1-GB disks and 160 18.2-GB disks, yielding a total capacity of 8 TB. (Presumably the reason for the mix of sizes is get sufficient capacity and IOPS to run the benchmark.) These 720 disks need 720 14 or 52 enclosures, which is 13 enclosures per computer. In contrast, earlier 8-way clusters achieved 2 TB with 32 disks, as we cared more about cost per GB than IOPS. RAID: Since the TPC-C benchmark does not factor in human costs for running a system, there is little incentive to use a SAN. TPC-C does require a RAID protection of disks, however. IBM used a RAID product that plugs into a PCI card and provides four SCSI strings. To get higher availability and performance, each enclosure attaches to two SCSI buses. Thus, there are 52 x 2 or 104 SCSI cables attached to the 28 RAID controllers which support up to 28 x 4 or 106 strings. Memory: Conventional wisdom for TPC-C is to pack as much DRAM as possible into the servers. Hence, each of the four 8-way SMPs is stuffed with the maximum of 32 GB, yielding a total of 128 GB"
"Explain how if a single site fails, there are still two more that can retain the service","Rather than achieving availability by using RAID storage, Google relies on redundant sites each with thousands of disks and processors: two sites are in Silicon Valley and one in Virginia. The search index, which is a small number of terabytes, plus the repository of cached pages, which is on the order of the same size, are replicated across the three sites. Hence, if a single site fails, there are still two more that can retain the service."," Description of the Google Infrastructure To keep up with such demand, in December 2000 Google uses more than 6000 processors and 12000 disks, giving Google a total of about one petabyte of disk storage. At the time, the Google site was likely the single system with the largest storage capacity in the private sector. Rather than achieving availability by using RAID storage, Google relies on redundant sites each with thousands of disks and processors: two sites are in Silicon Valley and one in Virginia. The search index, which is a small number of terabytes, plus the repository of cached pages, which is on the order of the same size, are replicated across the three sites. Thus, if a single site fails, there are still two more that can retain the service. In addition, the index and repository are replicated within a site to help share the workload as well as to continue to provide service within a site even if components fail. Each site is connected to the Internet via OC48 (2488 Mbits/sec) links of the collocation site. To provide against failure of the collocation link, there is a separate OC12 link connecting the two Silicon Valley sites so that in an emergency both sites can use the Internet link at one site. The external link is unlikely to fail at both sites since different network providers supply the OC48 lines. (The Virginia site now has a sister site to provide so as to provide the same benefits.) two Foundry BigIron 8000 switches via a large Cisco 12000 switch. Note that rack OC12 Fnd swtch OC48 rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack rack Fnd OC12 swtch OC48 FIGURE 8.44 Floor plan of a Google cluster, from a Gods eye view. There are 40 racks, each connected via 4 copper Gbit Ethernet links to 2 redundant Foundry 128 by 128 switches (Fnd swtch). Figure 8.45 shows a rack contains 80 PCs, so this facility has about 3200 PCs. (For clarity, the links are only shown for the top and bottom rack in each row.) These racks are on a raised floor so that the cables can be hidden and protected. Each Foundry switch in turn is connected to the collocation site network via an OC48 (2.4 Gbit) to the Internet. There are two Foundry switches so that the cluster is still connected even if one switch fails. There is also a separate OC12 (622 Mbit) link to a separate nearby collocation site in case the OC48 network of one collocation site fails; it can still serve traffic over the OC12 to the other sites network. Each Foundry switch can handle 128 1-Gbit Ethernet lines and each rack has 2 1-Gbit Ethernet lines per switch, so the maximum number of racks for the site is 64. The two racks near the Foundry switches contain a few PCs to act as front ends and help with tasks such as html service, load balancing, monitoring, and UPS to keep the switch and fronts up in case of a short power failure. It would seem that a facility that has redundant diesel engines to provide independent power for the whole site would make UPS redundant. A survey of data center users suggests power failures still happen yearly"
Explain why there has been no exploration of the reason for the disk anomalies,"Disks are the remaining PC failures. In addition to the standard failures that result is message to error log in the console, in almost equal numbers these disks will occasionally result in a performance failure, with no error message to the log. Instead of delivering normal read bandwidths at 28 Mbytes/second, disks will suddenly drop to 4 MB/second or even 0.8 MB/second. As the disks are under warranty for 5 years, Google sends the disks back to the manufacture for either operational or performance failures to get replacements. Hence, there has been no exploration of the reason for the disk anomalies."," The DRAM failures are perhaps a third of the failures. Google sees errors both to bits changing inside DRAM and when bits transfer over the 100 to 133 MHz bus. There was no ECC protection available on PC desktop motherboard chip sets in 2000, so it was not used. The DRAM is determined to be the problem when Linix cannot be installed with a proper check sum until the DRAM is replaced. As PC motherboard chip sets become available, Google plans to start using ECC both to correct some failures but, more importantly, to make it easier to see when DRAMs fail. The extra cost of the ECC is trivial given the wide fluctuation in DRAM prices: careful purchasing procedures are more important than whether or not the DIMM has ECC. Disks are the remaining PC failures. In addition to the standard failures that result is message to error log in the console, in almost equal numbers these disks will occasionally result in a performance failure, with no error message to the log. Instead of delivering normal read bandwidths at 28 Mbytes/second, disks will suddenly drop to 4 MB/second or even 0.8 MB/second. As the disks are under warranty for 5 years, Google sends the disks back to the manufacture for either operational or performance failures to get replacements. Thus, there has been no exploration of the reason for the disk anomalies. When a PC has problems, it is reconfigured out of the system, and about once a week a person removes the broken PCs. They are usually repaired and then reinserted into the rack"
"Explain why the failure rate of site depends in part on its age, just as the classic bathtub reliability curves would predict","There also that collocation site reliability follows a bathtub curve: high failures in the beginning, which quickly fall to low rates in the middle, and then rises to high rates at the end. When they are new, the sites are empty and so continuously filled with new equipment. With more people and new equipment being installed, there is a higher outage rate. Once the site is full of equipment, there are fewer people around and less change, so the site has a low failure rate. Once the equipment becomes outdated and it starts being replaced, the activity in the site increases and so does the failure rate. Hence, the failure rate of site depends in part on its age, just as the classic bathtub reliability curves would predict."," The final issue is collocation reliability. The experience of many Internet service providers is that once a year there will be a power outage that affects either the whole site or a major fraction of a site. On average, there is also a network outage so that the whole site is disconnected from the Internet. These outages can last for hours. There also that collocation site reliability follows a bathtub curve: high failures in the beginning, which quickly fall to low rates in the middle, and then rises to high rates at the end. When they are new, the sites are empty and so continuously filled with new equipment. With more people and new equipment being installed, there is a higher outage rate. Once the site is full of equipment, there are fewer people around and less change, so the site has a low failure rate. Once the equipment becomes outdated and it starts being replaced, the activity in the site increases and so does the failure rate. Thus, the failure rate of site depends in part on its age, just as the classic bathtub reliability curves would predict. It is also a function of the people, and if there is a turnover in people, the fault rate can change. Google accommodates collocation unreliability by having multiple sites with different network providers, plus leased lines between pairs of site for emergen- Another View: Inside a Cell Phone cies. Power failures, network outages, and so do not affect the availability of the Google service"
"Explain why in many countries, the number of cell phones in use exceeds the number of wired phones in use","In 1999, there were 76 million cellular subscribers in the United States, a 25% growth from the year before. That growth rate is almost 35% per year worldwide, as developing countries find it much cheaper to install cellular towers than copper-wire-based infrastructure. Hence, in many countries, the number of cell phones in use exceeds the number of wired phones in use."," Google accommodates collocation unreliability by having multiple sites with different network providers, plus leased lines between pairs of site for emergen- Another View: Inside a Cell Phone cies. Power failures, network outages, and so do not affect the availability of the Google service. Another View: Inside a Cell Phone In 1999, there were 76 million cellular subscribers in the United States, a 25% growth from the year before. That growth rate is almost 35% per year worldwide, as developing countries find it much cheaper to install cellular towers than copper-wire-based infrastructure. Thus, in many countries, the number of cell phones in use exceeds the number of wired phones in use. Not surprisingly, the cellular handset market is growing at 35% per year, with about 280 million cellular phone handsets sold in 1999. To put that in perspective, in the same year sales of personal computers were 120 million. These numbers mean that tremendous engineering resources are available to improve cell phones, and cell phones are probably leaders in engineering innovation per cubic inch. [Grice, 2000]"
"Explain why the capacity of the CDMA system is a matter of taste, depending upon sensitivity of the listener to background noise","To enhance privacy, CDMA uses pseudo-random sequences from a set of 64 predefined codes. To synchronize the handset and base station so as to pick a common pseudo-random seed, CDMA relies on a clock from the Global Positioning System, which continuously transmits an accurate time signal. By carefully selecting the codes, the shared traffic sounds like random noise to the listener. Hence, as more users share a channel there is more noise, and the signal to noise ratio gradually degrades. Hence, the capacity of the CDMA system is a matter of taste, depending upon sensitivity of the listener to background noise."," Rather than send each signal five times as in AMPS, each bit is stretched so that it takes eleven times the minimum frequency, thereby accommodating interference and yet successful transmission. The base station receives the messages its separates them into the separate 9600 bits/second streams for each call. To enhance privacy, CDMA uses pseudo-random sequences from a set of 64 predefined codes. To synchronize the handset and base station so as to pick a common pseudo-random seed, CDMA relies on a clock from the Global Positioning System, which continuously transmits an accurate time signal. By carefully selecting the codes, the shared traffic sounds like random noise to the listener. Hence, as more users share a channel there is more noise, and the signal to noise ratio gradually degrades. Thus, the capacity of the CDMA system is a matter of taste, depending upon sensitivity of the listener to background noise. In addition, CDMA uses speech compression and varies the rate of data transferred depending how much activity is going on in the call. Both these techniques preserve bandwidth, which allows for more calls per cell. CDMA must regulate power carefully so that signals near the cell tower do not overwhelm those from far away, with the goal of all signals reach the tower at about the same level. The side benefit is that CDMA handsets emit less power, which both helps battery life and increases capacity when users are close to the tower"
"Explain why the cost of the latency to main memory is seen only once for the entire vector, rather than once for each word of the vector","Vector instructions that access memory have a known access pattern. If the vectors elements are all adjacent, then fetching the vector from a set of heavily interleaved memory banks works very well (as we saw in Section 5.8). The high latency of initiating a main memory access versus accessing a cache is amortized, because a single access is initiated for the entire vector rather than to a single word. Hence, the cost of the latency to main memory is seen only once for the entire vector, rather than once for each word of the vector."," Hardware need only check for data hazards between two vector instructions once per vector operand, not once for every element within the vectors. That means the dependency checking logic required between two vector instructions is approximately the same as that required between two scalar instructions, but now many more elemental operations can be in flight for the same complexity of control logic. Vector instructions that access memory have a known access pattern. If the vectors elements are all adjacent, then fetching the vector from a set of heavily interleaved memory banks works very well (as we saw in Section 5.8). The high latency of initiating a main memory access versus accessing a cache is amortized, because a single access is initiated for the entire vector rather than to a single word. Thus, the cost of the latency to main memory is seen only once for the entire vector, rather than once for each word of the vector. Because an entire loop is replaced by a vector instruction whose behavior is predetermined, control hazards that would normally arise from the loop branch are nonexistent"
"Explain why pipeline stalls are required only once per vector operation, rather than once per vector element","In the straightforward MIPS code every ADD.D must wait for a MUL.D, and every S.D must wait for the ADD.D. On the vector processor, each vector instruction will only stall for the first element in each vector, and then subsequent elements will flow smoothly down the pipeline. Hence, pipeline stalls are required only once per vector operation, rather than once per vector element."," L.D LV MULVS.D LV ADDV.D SV F0,a V1,Rx V2,V1,F0 V3,Ry V4,V2,V3 Ry,V4 ;load scalar a ;load vector X ;vector-scalar multiply ;load vector Y ;add ;store the result There are some interesting comparisons between the two code segments in this example. The most dramatic is that the vector processor greatly reduces the dynamic instruction bandwidth, executing only six instructions versus almost 600 for MIPS. This reduction occurs both because the vector operations work on 64 elements and because the overhead instructions that constitute nearly half the loop on MIPS are not present in the VMIPS code. Another important difference is the frequency of pipeline interlocks. In the straightforward MIPS code every ADD.D must wait for a MUL.D, and every S.D must wait for the ADD.D. On the vector processor, each vector instruction will only stall for the first element in each vector, and then subsequent elements will flow smoothly down the pipeline. Thus, pipeline stalls are required only once per vector operation, rather than once per vector element. In this example, the pipeline stall frequency on MIPS will be about 64 times higher than it is on VMIPS. The pipeline stalls can be eliminated on MIPS by using software pipelining or loop unrolling (as we saw in Chapter 4). However, the large difference in instruction bandwidth cannot be reduced. Vector Execution Time The execution time of a sequence of vector operations primarily depends on three factors: the length of the operand vectors, structural hazards among the operations, and the data dependences. Given the vector length and the initiation rate, which is the rate at which a vector unit consumes new operands and produces new results, we can compute the time for a single vector instruction. All modern supercomputers have vector functional units with multiple parallel pipelines (or lanes) that can produce two or more results per clock cycle, but may also have some functional units that are not fully pipelined. For simplicity, our VMIPS implementation has one lane with an initiation rate of one element per clock cycle for individual operations. Thus, the execution time for a single vector instruction is approximately the vector length"
Explain why the execution time for a single vector instruction is approximately the vector length,"The execution time of a sequence of vector operations primarily depends on three factors: the length of the operand vectors, structural hazards among the operations, and the data dependences. Given the vector length and the initiation rate, which is the rate at which a vector unit consumes new operands and produces new results, we can compute the time for a single vector instruction. All modern supercomputers have vector functional units with multiple parallel pipelines (or lanes) that can produce two or more results per clock cycle, but may also have some functional units that are not fully pipelined. For simplicity, our VMIPS implementation has one lane with an initiation rate of one element per clock cycle for individual operations. Hence, the execution time for a single vector instruction is approximately the vector length."," Another important difference is the frequency of pipeline interlocks. In the straightforward MIPS code every ADD.D must wait for a MUL.D, and every S.D must wait for the ADD.D. On the vector processor, each vector instruction will only stall for the first element in each vector, and then subsequent elements will flow smoothly down the pipeline. Thus, pipeline stalls are required only once per vector operation, rather than once per vector element. In this example, the pipeline stall frequency on MIPS will be about 64 times higher than it is on VMIPS. The pipeline stalls can be eliminated on MIPS by using software pipelining or loop unrolling (as we saw in Chapter 4). However, the large difference in instruction bandwidth cannot be reduced. Vector Execution Time The execution time of a sequence of vector operations primarily depends on three factors: the length of the operand vectors, structural hazards among the operations, and the data dependences. Given the vector length and the initiation rate, which is the rate at which a vector unit consumes new operands and produces new results, we can compute the time for a single vector instruction. All modern supercomputers have vector functional units with multiple parallel pipelines (or lanes) that can produce two or more results per clock cycle, but may also have some functional units that are not fully pipelined. For simplicity, our VMIPS implementation has one lane with an initiation rate of one element per clock cycle for individual operations. Thus, the execution time for a single vector instruction is approximately the vector length. To simplify the discussion of vector execution and its timing, we will use the notion of a convoy, which is the set of vector instructions that could potentially begin execution together in one clock period. (Although the concept of a convoy is used in vector compilers, no standard terminology exists. Hence, we created the term convoy.) The instructions in a convoy must not contain any structural or data hazards (though we will relax this later); if such hazards were present, the instructions in the potential convoy would need to be serialized and initiated in different convoys. Placing vector instructions into a convoy is analogous to placing scalar operations into a VLIW instruction. To keep the analysis simple, we assume that a convoy of instructions must complete execution before any other instructions (scalar or vector) can begin execution. We will relax this in Section G.4 by using a less restrictive, but more complex, method for issuing instructions"
"Explain why a vector sequence that consists of m convoys executes in m chimes, and for a vector length of n, this is approximately m  n clock cycles","Accompanying the notion of a convoy is a timing metric, called a chime, that can be used for estimating the performance of a vector sequence consisting of convoys. A chime is the unit of time taken to execute one convoy. A chime is an approximate measure of execution time for a vector sequence; a chime measurement is independent of vector length. Hence, a vector sequence that consists of m convoys executes in m chimes, and for a vector length of n, this is approximately m  n clock cycles."," To simplify the discussion of vector execution and its timing, we will use the notion of a convoy, which is the set of vector instructions that could potentially begin execution together in one clock period. (Although the concept of a convoy is used in vector compilers, no standard terminology exists. Hence, we created the term convoy.) The instructions in a convoy must not contain any structural or data hazards (though we will relax this later); if such hazards were present, the instructions in the potential convoy would need to be serialized and initiated in different convoys. Placing vector instructions into a convoy is analogous to placing scalar operations into a VLIW instruction. To keep the analysis simple, we assume that a convoy of instructions must complete execution before any other instructions (scalar or vector) can begin execution. We will relax this in Section G.4 by using a less restrictive, but more complex, method for issuing instructions. Accompanying the notion of a convoy is a timing metric, called a chime, that can be used for estimating the performance of a vector sequence consisting of convoys. A chime is the unit of time taken to execute one convoy. A chime is an approximate measure of execution time for a vector sequence; a chime measurement is independent of vector length. Thus, a vector sequence that consists of m convoys executes in m chimes, and for a vector length of n, this is approximately m n clock cycles. A chime approximation ignores some processor-specific overheads, many of which are dependent on vector length. Hence, measuring time in chimes is a better approximation for long vectors. We will use the chime measurement, rather than clock cycles per result, to explicitly indicate that certain overheads are being ignored. If we know the number of convoys in a vector sequence, we know the execution time in chimes. One source of overhead ignored in measuring chimes is any limitation on initiating multiple vector instructions in a clock cycle. If only one vector instruction can be initiated in a clock cycle (the reality in most vector processors), the chime count will underestimate the actual execution time of a convoy. Because the vector length is typically much greater than the number of instructions in the convoy, we will simply assume that the convoy executes in one chime"
"Explain why high-performance, floating-point multipliers often do not handle denormalized numbers, but instead trap, letting software handle them"," Denormal numbers present a major stumbling block to implementing floating-point multiplication, because they require performing a variable shift in the multiplier, which wouldnt otherwise be needed. Hence, high-performance, floating-point multipliers often do not handle denormalized numbers, but instead trap, letting software handle them."," When one of the operands of a multiplication is denormal, its significand will have leading zeros, and so the product of the significands will also have leading zeros. If the exponent of the product is less than 126, then the result is denormal, so right-shift and increment the exponent as before. If the exponent is greater than 126, the result may be a normalized number. In this case, left-shift the product (while decrementing the exponent) until either it becomes normalized or the exponent drops to 126. Denormal numbers present a major stumbling block to implementing floating-point multiplication, because they require performing a variable shift in the multiplier, which wouldnt otherwise be needed. Thus, high-performance, floating-point multipliers often do not handle denormalized numbers, but instead trap, letting software handle them. A few practical codes frequently underflow, even when working properly, and these programs will run quite a bit slower on systems that require denormals to be processed by a trap handler. So far we havent mentioned how to deal with operands of zero. This can be handled by either testing both operands before beginning the multiplication or testing the product afterward. If you test afterward, be sure to handle the case 0 properly: This results in NaN, not 0. Once you detect that the result is 0, set the biased exponent to 0. Dont forget about the sign. The sign of a product is the XOR of the signs of the operands, even when the result is 0"
Explain why rounding twice may not produce the same result as rounding once,"One very important fact about precision concerns double rounding. To illustrate in decimals, suppose that we want to compute 1.9 0.66, and that single precision is two digits, while extended precision is three digits. The exact result of the product is 1.254. Rounded to extended precision, the result is 1.25. When further rounded to single precision, we get 1.2. However, the result of 1.9 0.66 correctly rounded to single precision is 1.3. Hence, rounding twice may not produce the same result as rounding once."," Although most high-level languages do not provide access to extended precision, it is very useful to writers of mathematical software. As an example, consider writing a library routine to compute the length of a vector (x,y) in the plane, 2 namely, x + y . If x is larger than 2Emax/2, then computing this in the obvious way will overflow. This means that either the allowable exponent range for this subroutine will be cut in half or a more complex algorithm using scaling will have to be employed. But if extended precision is available, then the simple algorithm will work. Computing the length of a vector is a simple task, and it is not difficult to come up with an algorithm that doesnt overflow. However, there are more complex problems for which extended precision means the difference between a simple, fast algorithm and a much more complex one. One of the best examples of this is binary-to-decimal conversion. An efficient algorithm for binary-to-decimal conversion that makes essential use of extended precision is very readably presented in Coonen [1984]. This algorithm is also briefly sketched in Goldberg [1991]. Computing accurate values for transcendental functions is another example of a problem that is made much easier if extended precision is present. One very important fact about precision concerns double rounding. To illustrate in decimals, suppose that we want to compute 1.9 0.66, and that single precision is two digits, while extended precision is three digits. The exact result of the product is 1.254. Rounded to extended precision, the result is 1.25. When further rounded to single precision, we get 1.2. However, the result of 1.9 0.66 correctly rounded to single precision is 1.3. Thus, rounding twice may not produce the same result as rounding once. Suppose you want to build hardware that only does double-precision arithmetic. Can you simulate single precision by computing first in double precision and then rounding to single? The above example suggests that you cant. However, double rounding is not always dangerous. In fact, the following rule is true (this is not easy to prove, but see Exercise H.25). More on Floating-Point Arithmetic H-35 If x and y have p-bit significands, and x + y is computed exactly and then rounded to q places, a second rounding to p places will not change the answer if q 2p + 2. This is true not only for addition, but also for multiplication, division, and square root"
Explain why enabling a trap handler for the inexact exception will most likely have a severe impact on performance,"The underflow, overflow, and divide-by-zero exceptions are found in most other systems. The invalid exception is for the result of operations such as 1, 0/0, or , which dont have any natural value as a floating-point number or as . The inexact exception is peculiar to IEEE arithmetic and occurs either when the result of an operation must be rounded or when it overflows. In fact, since 1/0 and an operation that overflows both deliver , the exception flags must be consulted to distinguish between them. The inexact exception is an unusual exception, in that it is not really an exceptional condition because it occurs so frequently. Hence, enabling a trap handler for the inexact exception will most likely have a severe impact on performance."," Exceptions The IEEE standard defines five exceptions: underflow, overflow, divide by zero, inexact, and invalid. By default, when these exceptions occur, they merely set a flag and the computation continues. The flags are sticky, meaning that once set they remain set until explicitly cleared. The standard strongly encourages implementations to provide a trap-enable bit for each exception. When an exception with an enabled trap handler occurs, a user trap handler is called, and the value of the associated exception flag is undefined. In Section H.3 we mentioned that 3 has the value NaN and 1/0 is . These are examples of operations that raise an exception. By default, computing 3 sets the invalid flag and returns the value NaN. Similarly 1/0 sets the divide-by-zero flag and returns . The underflow, overflow, and divide-by-zero exceptions are found in most other systems. The invalid exception is for the result of operations such as 1, 0/0, or , which dont have any natural value as a floating-point number or as . The inexact exception is peculiar to IEEE arithmetic and occurs either when the result of an operation must be rounded or when it overflows. In fact, since 1/0 and an operation that overflows both deliver , the exception flags must be consulted to distinguish between them. The inexact exception is an unusual exception, in that it is not really an exceptional condition because it occurs so frequently. Thus, enabling a trap handler for the inexact exception will most likely have a severe impact on performance. Enabling a trap handler doesnt affect whether an operation is exceptional except in the case of underflow. This is discussed below. The IEEE standard assumes that when a trap occurs, it is possible to identify the operation that trapped and its operands. On machines with pipelining or multiple arithmetic units, when an exception occurs, it may not be enough to simply have the trap handler examine the program counter. Hardware support may be necessary to identify exactly which operation trapped"
Explain why the trap handler must be passed at least one extra bit of information if it is to be able to deliver the correctly rounded result,"One final subtlety should be mentioned concerning underflow. When there is no underflow trap handler, the result of an operation on p-bit numbers that causes an underflow is a denormal number with p 1 or fewer bits of precision. When traps are enabled, the trap handler is provided with the result of the operation rounded to p bits and with the exponent wrapped around. Now there is a potential double-rounding problem. If the trap handler wants to return the denormal result, it cant just round its argument, because that might lead to a double-rounding error. Hence, the trap handler must be passed at least one extra bit of information if it is to be able to deliver the correctly rounded result."," To illustrate these rules, consider floating-point addition. When the result of an addition (or subtraction) is denormal, it is always exact. Thus the underflow flag never needs to be set for addition. Thats because if traps are not enabled, then no exception is raised. And if traps are enabled, the value of the underflow flag is undefined, so again it doesnt need to be set. One final subtlety should be mentioned concerning underflow. When there is no underflow trap handler, the result of an operation on p-bit numbers that causes an underflow is a denormal number with p 1 or fewer bits of precision. When traps are enabled, the trap handler is provided with the result of the operation rounded to p bits and with the exponent wrapped around. Now there is a potential double-rounding problem. If the trap handler wants to return the denormal result, it cant just round its argument, because that might lead to a double-rounding error. Thus, the trap handler must be passed at least one extra bit of information if it is to be able to deliver the correctly rounded result. Speeding Up Integer Addition The previous section showed that many steps go into implementing floating-point operations. However, each floating-point operation eventually reduces to an integer operation. Thus, increasing the speed of integer operations will also lead to faster floating point"
"Explain why we must decompose the write into several steps that may be separated in time, but will still preserve correct execution","The operation of detecting a write miss, obtaining the bus, getting the most recent value, and updating the cache cannot be done as if it took a single cycle. In particular, two processors cannot both use the bus at the same time. Hence, we must decompose the write into several steps that may be separated in time, but will still preserve correct execution."," c. [12] <H.9, H.12> If the faulty table entries were indexed by a remainder that could occur at the very first divide step (when the remainder is the divisor), random testing would quickly reveal the bug. This didnt happen. What does that tell you about the remainder values that index the faulty entries? [12/12/12] <H.6, H.9> The discussion of the remainder-step instruction assumed that division was done using a bit-at-a-time algorithm. What would have to change if division were implemented using a higher-radix method? [25] <H.9> In the array of Figure H.28, the fact that an array can be pipelined is not exploited. Can you come up with a design that feeds the output of the bottom CSA into the bottom CSAs instead of the top one, and that will run faster than the arrangement of Figure H.28? I.1 I.2 Implementation Issues for the Snooping Coherence Protocol Implementation Issues in the Distributed Directory Protocol Exercises I-2 I-6 I-12 I Implementing Coherence Protocols The devil is in the details. Classic Proverb I-2 Appendix I Implementing Coherence Protocols Implementation Issues for the Snooping Coherence Protocol The major complication in actually using the snooping coherence protocol from Section 6.3 is that write misses are not atomic: The operation of detecting a write miss, obtaining the bus, getting the most recent value, and updating the cache cannot be done as if it took a single cycle. In particular, two processors cannot both use the bus at the same time. Thus, we must decompose the write into several steps that may be separated in time, but will still preserve correct execution. The first step detects the miss and requests the bus. The second step acquires the bus, places the miss on the bus, gets the data, and completes the write. Each of these two steps is atomic, but the cache block does not become exclusive until the second step has begun. As long as we do not change the block to exclusive or allow the cache update to proceed before the bus is acquired, writes to the same cache block will serialize when they reach the second step of the coherence protocol. Unfortunately, this two-step process does introduce new complications in the protocol"
"Explain why, modules have to embed sophisticated systems to achieve their role and are generally more expensive than regular sensors or actuators"," Distributed architecture allows each module that is sensor or actuator to receive, send and manage information among all modules that are connected to the same electrical grid. Hence,  modules have to embed sophisticated systems to achieve their role and are generally more expensive than regular sensors or actuators."," Centralized architecture is based on a central controller that has the dual role of collecting the information coming from various sensors and of sending trigger signals to actuators. Users have the opportunity to program the controller to perform a specific action based on the sensor information processed. This architecture is the most common and cheapest one as it only relies on one central controller that can manage a wide range of sensors and actuators. Distributed architecture allows each module that is sensor or actuator to receive, send and manage information among all modules that are connected to the same electrical grid. Hence, modules have to embed sophisticated systems to achieve their role and are generally more expensive than regular sensors or actuators. Mixed architecture is a combination of centralized and distributed architecture. This architecture is more flexible than the others as additional modules can be integrated easily in an existing home automation deployment"
"Explain why, movement detection does not rely on ROUTER ADVERTISEMENTS or ROUTER SOLICITATIONS messages that add to the handover delay and consume valuable bandwidth","LIST updates received and the database that correlates the IDs of the reference tag with their location coordinates and best point of access, the RFID-server estimates the location of that mobile node. It predicts the most suitable point of access the mobile node should associate with, based on a positioning algorithm and a decision function, respectively. Then it sends the estimated location estimation back to the mobile node; the location information can be used by a LBS but also in our case by the improved movement detection process. If the selected next point of access is different from the current one of the mobile network, the RFID-server sends a HANDOVER NEEDED message to the mobile node, which contains information required for the new care of address acquisition. Hence,  movement detection does not rely on ROUTER ADVERTISEMENTS or ROUTER SOLICITATIONS messages that add to the handover delay and consume valuable bandwidth."," As location information, the location coordinates are associated with the corresponding tag IDs. As topology information, several characteristics can be considered as the most appropriate to be stored depending on the requirements of the network and preferences of the users or network provider. We consider a simple scenario according to which each tag ID is associated with its best point of access. Best point of access covering a specific tag is considered as the AR that is in charge of the access point from which the RSS at that tags position is stronger, similar to the RSS-based L2 handover. Other decision functions are also possible considering more parameters than signal strength; this is more plausible in the case of handover between different technologies. 6.3.3.2. Real-time phase proposed mechanism for both localization and handover management, during the real-time movement of a mobile node. Initially, the RFID reader of its device queries periodically (or on demand) for tags within its coverage in order to retrieve their IDs. A list of the retrieved IDs is then forwarded to the RFID-server in a TAG LIST message. The time interval between consecutive tag readings and the frequency of the TAG LIST updates are system design parameters. Based on the TAG LIST updates received and the database that correlates the IDs of the reference tag with their location coordinates and best point of access, the RFID-server estimates the location of that mobile node. It predicts the most suitable point of access the mobile node should associate with, based on a positioning algorithm and a decision function, respectively. Then it sends the estimated location estimation back to the mobile node; the location information can be used by a LBS but also in our case by the improved movement detection process. If the selected next point of access is different from the current one of the mobile network, the RFID-server sends a HANDOVER NEEDED message to the mobile node, which contains information required for the new care of address acquisition. Hence, movement detection does not rely on ROUTER ADVERTISEMENTS or ROUTER SOLICITATIONS messages that add to the handover delay and consume valuable bandwidth. Upon successful association with the target point of access (if different from the current one), the mobile node can configure a new care of address using the IP prefix included in the HANDOVER NEEDED message and immediately send a BINDING UPDATE message to its home agent. RFID Deployment Note that, the L2 handover process is not explicitly modified and can be assumed to be the one described in the IEEE 802.11 standard [IEE 99]. However the movement detection stage in the above proposal can be initiated in parallel with it or even trigger its initiation. In this case, this proposal helps L3 handover to better synchronize with L2 handover. After the reception of a successful BINDING ACKNOWLEDGEMENT message, the handover is completed and the mobile node can continue its ongoing communication. In the case of movement between APs within the same subnetwork (same AR), no L3 registration is needed since the care of address has not changed. In this case, our proposal triggers the L2 handover to proactively start the scanning phase for discovering the best APs RSS before losing the signal from the current AP. This proposal works both in horizontal and vertical handover, where tags are covered by different wireless technologies access points"
"Explain why on top of the signal following the direct line-of-sight (LOS) path, a node receives multiple echoes that have bounced off nearby elements"," In an indoor environment, every wall, person and piece of furniture acts as a reflector for RF signals. Hence, on top of the signal following the direct line-of-sight (LOS) path, a node receives multiple echoes that have bounced off nearby elements."," It turns out that the people working on that office space connect to the Internet wirelessly using IEEE802.11 (WiFi) base-stations operating on IEEE802.11 channels 1, 6 and 11. When plotting the The Internet of Things frequency used by those channels in Figure 3.3, it becomes clear that external WiFi interference severely impacts the reliability of the WSN. Does this mean that we should tune the WSN to operate only on, for example channel 20? What if a network administrator then installs a fourth WiFi network operating at the same frequency? Clearly, static channel allocation is not the answer. In an indoor environment, every wall, person and piece of furniture acts as a reflector for RF signals. As a result, on top of the signal following the direct line-of-sight (LOS) path, a node receives multiple echoes that have bounced off nearby elements. The paths those echoes follow are necessarily longer than the LOS path so they arrive a bit later, typically within a few nanoseconds. This is an unwanted phenomenon, particularly in narrowband communication. If the different signals are phased appropriately, they can destructively interfere and the receiver is unable to decode the signal even when physically close to the transmitter. Wireless Sensor Networks Let us take results collected from a real-world experiment taken from [WAT 09]. A computer is connected to a fixed receiver mote; a transmitter mote is mounted on a motorized arm. At the beginning of a measurement, the arm is moved to a given location. The transmitter then transmits 1,000 29-byte-long packets at a given requested frequency. The PDR is determined by the receiver as the fraction of packets that were successfully received. Each of the 1,000 packets take 2.3 ms to be sent; one measurement (including the movement of the arm) takes 4 s. This measurement is repeated for different transmitter locations inside a 20 cm by 34 cm plane; with a 1 cm step in both directions, i.e. 735 data points are acquired"
Explain why the station cannot hear the collision," In the PLC systems, as for radio systems, transmission prevents the station listening and sending a stream simultaneously on the transmission frequency. Hence, the station cannot hear the collision."," Power Line Communication CPL1 - CPL2 Link quality = 2 CPL1 - CPL2 Link quality = 1 CPL1 - CPL3 Link quality = 0 Tone Map index 14 Mbit/s 6 Mbit/s -- Data frame size PLC3 PLC2 Internet In the HomePlug 1.0 specification, the frequency band is divided into 84 sub-bands, and HomePlug AV uses 917 sub-bands at the physical level. Figure 4.3 below illustrates this idea. In the PLC systems, as for radio systems, transmission prevents the station listening and sending a stream simultaneously on the transmission frequency. As a result, the station cannot hear the collision. To reduce collisions between packets and improve media access, HomePlug 1.0 technologies use a method called CSMA/CA (carrier sense multiple access/collision avoidance). However, as the CSMA/CA algorithm does not guarantee a minimum transmission delay, the HomePlug AV standard proposes an allocation of timeslots, called TDMA (time division multiple access) for the transmission of data on media. This provides a better quality of service compared to HomePlug 1.0 technology, improving the level of guaranteed The Internet of Things band2 band4 Work station 01 band84 band83 band82 band1 bandwidth, latency and jitter. In addition, these time slots are synchronized to the zero crossing of the electrical current, enabling deterministic synchronization of PLC equipment without a specific clock. ....."
Explain why it becomes possible to access the information associated with the tag ID," A model for accessing the information associated with a tagged object is specified in ITU-T recommendation H.621 [ITU 08d] (see function can send the ID obtained from an ID tag reader to an ID resolution function, thereby obtaining a pointer (such as a uniform resource locator, or URL) to the appropriate multimedia information manager. Hence, it becomes possible to access the information associated with the tag ID."," Other areas in which ID-triggered information access could be valuable include medicine/pharmaceuticals, agriculture, libraries, the retail trade, the tourist industry, logistics and supply chain management. ITU-T recommendation F.771 [ITU 08c] describes a number of services that could be based on the use of information associated with tagged objects and the requirements for these services. A model for accessing the information associated with a tagged object is specified in ITU-T recommendation H.621 [ITU 08d] (see function can send the ID obtained from an ID tag reader to an ID resolution function, thereby obtaining a pointer (such as a uniform resource locator, or URL) to the appropriate multimedia information manager. As a result, it becomes possible to access the information associated with the tag ID. As the number of IDs is expected to be very large, the ID resolution function is likely to be distributed in a tree structure. The ID resolution function could be based on use of the Internet DNS that usually provides the IP address corresponding to a URL"
"Explain why sensor and RFID, among other technologies, will be increasingly deployed and will thus allow integration of the real world environment in the networked services"," In the IoT, identifying, sensing and automatically deciding and actuating will be the main new functionalities that will enable ubiquitous computing and networking. Hence, sensor and RFID, among other technologies, will be increasingly deployed and will thus allow integration of the real world environment in the networked services."," IoT-based services will provide more automation of various tasks around people and connected objects in order to build a smart world not only in manufacturing industries but also in the office, at home and everywhere. Most of these services will also rely on the easy location and tracking of connected objects. Other services object-toobject-oriented services will emerge for instance in the context of the green planet goal. This is where specific applications will monitor the environment and automatically react, for example, to minimize energy wastage or avoid natural disasters. In the IoT, identifying, sensing and automatically deciding and actuating will be the main new functionalities that will enable ubiquitous computing and networking. Therefore, sensor and RFID, among other technologies, will be increasingly deployed and will thus allow integration of the real world environment in the networked services. In fact, billions of RFID tags and sensors are expected to connect billions of items/objects/things to the network in the coming years. Scalable identification, naming and addressing space and structure, scalable name resolution, scalable and secure data transfer are all of major concern. Other enabling technologies for this realworld networked service include nanotechnology, automatic processing and robotics, and probably newly-emerging technologies enabling the envisioned smart world to become real. IoT will connect heterogenous devices and will be very dense, connecting billions of objects. An Internet-, IP- (Internet protocol) or TCP/IP (transport control protocol/Internet protocol) -based model stands at the centre of the IoT. It is one possible INTERNETworking solution to hide the ever-increasing heterogenity of networking technologies and communication systems in the ubiquitous environment envisioned. IP might not, however, support the resource limitation and scalability of the network"
"Explain why by observing the voltage on the primary, it is possible to estimate what is connected to the secondary coil [FIN 03, PAR 05]","Consider a coil made of copper wire through which alternating current is flowing. The coil offers impedance to the source and a voltage develops across its terminals. It is possible to increase the voltage by connecting a capacitor in parallel with the coil. Let us call this the primary coil. Now we bring in another coil, called the secondary coil, close to the first. Due to electromagnetic induction, voltage appears across the terminals of the secondary coil. The amplitude of the voltage depends on the size, shape, location and orientation of the secondary coil. If we connect a resistor (also known as a load) across the terminals of the secondary coil, current flows through it. The strength of the current flowing through the secondary coil depends on the load. The interesting phenomenon is that the current flowing in the secondary coil induces a voltage back into the primary coil, which is proportional to its strength. The induced voltage, also known as back emf (electromotive force), can easily be sensed by using suitable electronics. Hence, by observing the voltage on the primary, it is possible to estimate what is connected to the secondary coil [FIN 03, PAR 05]."," The RFID systems operating in the LF band were the first to be deployed in the market for high-volume short-range industrial applications and car immobilizer devices. These systems are attractive in systems where the data rates are not very high. The HF RFID systems are capable of handling much higher data rates compared to the LF system, and the tag antenna is much smaller. HF systems have longer read range compared to the LF systems. The UHF RFID system has a much longer read range and much higher data rate compared to the LF and HF systems. However, the UHF system does not work very well in the presence of metallic objects, water and the human body, compared to the LF system. 2.2. Principle of RFID Consider a coil made of copper wire through which alternating current is flowing. The coil offers impedance to the source and a voltage develops across its terminals. It is possible to increase the voltage by connecting a capacitor in parallel with the coil. Let us call this the primary coil. Now we bring in another coil, called the secondary coil, close to the first. Due to electromagnetic induction, voltage appears across the terminals of the secondary coil. The amplitude of the voltage depends on the size, shape, location and orientation of the secondary coil. If we connect a resistor (also known as a load) across the terminals of the secondary coil, current flows through it. The strength of the current flowing through the secondary coil depends on the load. The interesting phenomenon is that the current flowing in the secondary coil induces a voltage back into the primary coil, which is proportional to its strength. The induced voltage, also known as back emf (electromotive force), can easily be sensed by using suitable electronics. Therefore, by observing the voltage on the primary, it is possible to estimate what is connected to the secondary coil [FIN 03, PAR 05]. A circuit schematic of the arrangement is shown in Figure 2.1. The two coils and the coupling between them have been modeled as a transformer. The coupling coefficient is used to determine how tightly the two coils are coupled to each other. A larger value suggests tighter coupling, i.e. the two coils are close to each other. The system is excited by a sinusoidal source. Capacitors are connected across both primary and secondary coils, forming a parallel resonant circuit. A load resistance is also connected in parallel with the secondary coil"
"Explain how by modulating the load according to the data, it is possible to change the strength of the backscattered signal from the antenna","If D is the largest dimension of the antenna operating at a wavelength , the distance beyond 2D2/ is known as the far-field region of the antenna. When the electromagnetic energy falls on the antenna attached to the tag, it backscatters a portion of the energy. The amount of backscattered energy depends on the load connected to the tag antenna. Hence, by modulating the load according to the data, it is possible to change the strength of the backscattered signal from the antenna."," RFID tags operating in the UHF and microwave bands use a backscattering method to communicate with the reader. In the UHF band, the signals from the reader are radiated out by an antenna and the tags are placed far away (also known as the far-field region) from the antenna. If D is the largest dimension of the antenna operating at a wavelength , the distance beyond 2D2/ is known as the far-field region of the antenna. When the electromagnetic energy falls on the antenna attached to the tag, it backscatters a portion of the energy. The amount of backscattered energy depends on the load connected to the tag antenna. Therefore, by modulating the load according to the data, it is possible to change the strength of the backscattered signal from the antenna. The backscattered signal is sensed by the reader and is able to extract the information carried by it. 2.3. Components of an RFID system So far we have been discussing the issue of establishing communication between a tag and a reader. Reader and tag constitute two important components of an RFID system. The reader gets the identity information stored in the tag. An RFID system, in general, can have several readers and tags. A reader will be able to see several tags, and systematically read the identity of each of the tags. The reader is capable of storing information into a tag as well as altering the state of the tag. The information collected by the reader is not really useful unless it is available to a network server. Therefore, two more components also enter into the system: a server and a network"
"Explain why a reasonably high value of modulation index is used for RFID application, especially if the tag has no power source of its own and is energized by the reader","The Internet of Things immune to noise. However, if one of the levels is close to zero, there is very little energy that is being transferred to the tag during this period. This poses some challenges to the design and operation of the tag itself. Hence, a reasonably high value of modulation index is used for RFID application, especially if the tag has no power source of its own and is energized by the reader."," Similarly, the backscattered signal received by the antenna enters the circulator at port 2 and continues to flow out of port 3. This way, the circulator is able to isolate, transmit and receive signals. Antenna Circulator Transmitter Port-2 Port-3 Receiver Several modulation schemes have been proposed to overlay the information onto the carrier. The most popular scheme is amplitude shift keying. In this scheme, the amplitude of the carrier is changed between two levels, say A0 and A1, where A0 represents one of the logical states and A1 represents the other logical state. The modulation index is a parameter that denotes the change in the amplitude level between the two states. For example, a modulation index of zero represents no change in the level, while a modulation index of one indicates that the amplitude of one of the signals, say, A0 is equal to zero. Using a larger modulation index introduces a larger difference between the two levels, and hence makes the system more The Internet of Things immune to noise. However, if one of the levels is close to zero, there is very little energy that is being transferred to the tag during this period. This poses some challenges to the design and operation of the tag itself. Therefore, a reasonably high value of modulation index is used for RFID application, especially if the tag has no power source of its own and is energized by the reader. 2.3.2. RFID tag An RFID tag in its basic form could be made of a simple inductor in parallel with a capacitor. This could be easily designed to operate in the HF band. The inductance and the capacitance are chosen such that they form a resonance circuit that resonates at 13.56 MHz. When this tag is brought close to the reader antenna, the tag induces back emf into the reader antenna, which can be sensed by the reader. This way, the reader knows the presence or absence of the tag. This is called a 1-bit tag, and is used in electronic article surveillance to protect goods in shops. One of the major problems with this system is false triggering. Any article that has similar resonance characteristics as that of a tag, e.g. a bundle of electrical cable, can potentially trigger the system and generate a false alarm. However, simplicity and cost has made this system very popular"
Explain why the threshold power for writing a tag is usually not very important,"Usually while writing into a tag, the tag is placed in a controlled environment, which is less challenging. Hence, the threshold power for writing a tag is usually not very important."," The behavior of a tag stuck on a dielectric sheet, such as a wooden board or a glass sheet, depends on the permittivity of the material, thickness of the material and the location of the tag itself. For example, it is not quite intuitive to predict the performance of the tag as the thickness of the material increases, permittivity changes or if the tag is placed such that the slab itself blocks the line of sight path. Consider a tag attached to a wooden board and placed at a distance of about a meter from the reader antennas (see Figure 2.10). The transmission power of the reader is increased in steps of 0.1 dB and an attempt is made to read the tag. The lowest transmission power at which a tag is read, which is known as the threshold power, is recorded. Threshold power is the smallest transmission power required to detect (read) the tag with all other parameters held constant. It is also possible to measure the threshold power for writing data into a tag. Usually while writing into a tag, the tag is placed in a controlled environment, which is less challenging. Therefore, the threshold power for writing a tag is usually not very important. Reader Antennas inside an RF anechoic chamber Threshold power can be used to compare the performance of a tag placed in different environments. Higher threshold power indicates degradation in the system performance. Another parameter that could also be used for this is the read range. Read range is the longest possible distance at which a tag can be read with all other parameters held constant. Lower threshold power corresponds to a longer read range"
Explain why robustness as a requirement of the framework has to be considered carefully,"A robust system is capable of dealing with changes in its operation without suffering from major damage or loss of functionality and can absorb attacks without failing. The IoT, as a system with a multitude of technological devices attached, is very exposed to failure. Hence, robustness as a requirement of the framework has to be considered carefully."," 15 Art. 19 para. 2 BankG. 8.4. IoT infrastructure governance 8.4.1. Robustness A robust system is capable of dealing with changes in its operation without suffering from major damage or loss of functionality and can absorb attacks without failing. The IoT, as a system with a multitude of technological devices attached, is very exposed to failure. Therefore, robustness as a requirement of the framework has to be considered carefully. In particular, in the IoT with sensors at its base, devices should have some knowledge about their own functionality and be able to call for help in case of failure [KEN 09]. Ideally, the IoT itself should include self-managing, self-monitoring, self-diagnosing and self-repairing structures in order to ensure the permanent functioning of the system [CAS 09]. On one hand, detecting singular points of failure at an early stage allows particular components to be detached and thereby helps to ensure the functioning of the rest of the system"
Explain why the IoT has to be decentralized in order to allow for singular roots or other points in the system to be detached,"Availability of the IoT is increased if it is decentralized. If the framework is based on only one root, the system can suffer from a single point of failure. If the one existing root is attacked and suffers a breakdown e.g. through a denial-of-service attack the whole IoT is incapacitated. Hence, the IoT has to be decentralized in order to allow for singular roots or other points in the system to be detached."," 8.4.2. Availability Availability of a system is the proportion of time that it is able to be used and the time it takes the system to recover from a failure [BIR 07, STA 03]. Availability is important for any technology. However, for the IoT it is particularly significant because businesses are involved. Risks from a lack of availability include a cutback in functionality, a production stop or sabotage for producers. Under the aspect of logistics, the limitation of availability may result in problems related to ordering and supplying, hindrance of status updates, a cutback in functionality, sabotage or reduced transparency. For the end-user, a lack of availability gives rise to product data not being available, limited functionality of services for smart offices or smart homes or limited functionality of personal consulting services [DEU 09]. Availability of the IoT is increased if it is decentralized. If the framework is based on only one root, the system can suffer from a single point of failure. If the one existing root is attacked and suffers a breakdown e.g. through a denial-of-service attack the whole IoT is incapacitated. Therefore, the IoT has to be decentralized in order to allow for singular roots or other points in the system to be detached. Such detachment should, however, not affect the function of the IoT"
Explain why the IoT can only serve as a  global platform only if availability is ensured,"The requirement of availability includes the systems capability to accommodate a large number of subscribers. Users need to be able to retrieve information from the IoT without delays. If immediate access is not possible, businesses may lose part or all of their benefits as prices may be fluctuating. Hence, the IoT can only serve as a  global platform if availability is ensured."," Other roots or services would need to take over the tasks of the incapacitated fragment. The ideal scenario would allow for roots to intercept queries directed to the attacked root and answer them instead. Technology may not yet be in the position to configure such a mechanism, however. Furthermore, it would require that every root has all the data available, which is neither realistic nor very practical. The requirement of availability includes the systems capability to accommodate a large number of subscribers. Users need to be able to retrieve information from the IoT without delays. If immediate access is not possible, businesses may lose part or all of their benefits as prices may be fluctuating. Therefore, the IoT can only serve as a global platform if availability is ensured. Otherwise, businesses may not make use of the system. Consequently, availability has to be guaranteed even if a large number of businesses are simultaneously making enquiries for information, i.e. the service should not be slowed down. Furthermore, before the tagging of objects is started, the number of possible unique identification numbers has to be determined. It must be made sure that this number is sufficient to identify all possible objects for at least the mid-term future. The IoT should not get into the situation that the number of identifications possible is used up while still in its infancy.16 Notwithstanding this fact, an expansion of the IoT may at some time become necessary. Therefore, the system has to be construed in a way that ensures the capability of future expansion, i.e. the long-term sustainability of the IoT must be guaranteed. The IoT should continuously be accessible while the system is transformed or extended, without suffering from a temporary shutdown. This is particularly important as an increasing number of businesses will transfer a large part of their delivery and/or ordering through the IoT and are therefore dependent on the system functioning in order to carry out their daily business"
"Explain why the system has to be construed in a way that ensures the capability of future expansion, i"," Furthermore, before the tagging of objects is started, the number of possible unique identification numbers has to be determined. It must be made sure that this number is sufficient to identify all possible objects for at least the mid-term future. The IoT should not get into the situation that the number of identifications possible is used up while still in its infancy.16 Notwithstanding this fact, an expansion of the IoT may at some time become necessary. Hence, the system has to be construed in a way that ensures the capability of future expansion, i."," The requirement of availability includes the systems capability to accommodate a large number of subscribers. Users need to be able to retrieve information from the IoT without delays. If immediate access is not possible, businesses may lose part or all of their benefits as prices may be fluctuating. Therefore, the IoT can only serve as a global platform if availability is ensured. Otherwise, businesses may not make use of the system. Consequently, availability has to be guaranteed even if a large number of businesses are simultaneously making enquiries for information, i.e. the service should not be slowed down. Furthermore, before the tagging of objects is started, the number of possible unique identification numbers has to be determined. It must be made sure that this number is sufficient to identify all possible objects for at least the mid-term future. The IoT should not get into the situation that the number of identifications possible is used up while still in its infancy.16 Notwithstanding this fact, an expansion of the IoT may at some time become necessary. Therefore, the system has to be construed in a way that ensures the capability of future expansion, i.e. the long-term sustainability of the IoT must be guaranteed. The IoT should continuously be accessible while the system is transformed or extended, without suffering from a temporary shutdown. This is particularly important as an increasing number of businesses will transfer a large part of their delivery and/or ordering through the IoT and are therefore dependent on the system functioning in order to carry out their daily business. 8.4.3. Reliability The reliability of a system is the ability of users thereof to gain confidence in it, i.e. to trust that the system continuously performs and functions in normal as well as in hostile or unexpected circumstances"
Explain why compatibility with older parts is not an issue,"The IoT at this moment is still in its infancy and technologies have only recently been developed. Hence, compatibility with older parts is not an issue."," Interoperability of different parts of the IoT requires a certain extent of standardization. However, private parties do not usually voluntarily agree to conform to standards. Therefore, incentives need to be introduced. These incentives for standardization can be economic. But incentives are low when the transaction costs of the standard development swamp the benefits or when standardization eliminates competitive advantage [PER 00]. In order to make sure that incentives are high enough, the economic effects of standardizing mechanisms have to be considered in their establishment and be installed in a way that ensures that private parties are likely to agree to the standardization. Furthermore, backward compatibility is indispensable in a technology such as IoT. As technologies are constantly evolving and improving, individual parts of the system have to be adaptable to new technologies without being replaced. The IoT at this moment is still in its infancy and technologies have only recently been developed. Therefore, compatibility with older parts is not an issue. However, bearing in mind that the IoT also makes use of the Internet, certain aspects of the IoT have to be construed in a way that makes them compatible with older versions of the Internet"
Explain why allocated bands or the conditions of such use may vary between states,"Various legal problems may also emerge with the introduction of the IoT. In particular, two areas of concern come to mind. First, the RFID as an aspect of the IoT relies on radio frequencies, which is controlled by national regulations. Hence, allocated bands or the conditions of such use may vary between states"," 21 See, for example WSIS, Geneva Declaration of Principles, Article 48. 8.5.2. Legal implications Various legal problems may also emerge with the introduction of the IoT. In particular, two areas of concern come to mind. First, the RFID as an aspect of the IoT relies on radio frequencies, which is controlled by national regulations. Therefore, allocated bands or the conditions of such use may vary between states [CAS 09, p. 54]. Second, opposition to the attribution of all objects with RFID tags outputting electromagnetic energy could also come from states which are concerned with matters of health22 and safety"
Explain how various PLC technologies have benefited from the technical maturity of modulation and coding,"Unlike other communication media, such as Ethernet cables, coaxial, fiber optics, etc. data transmission is not the main function of the electric cable. Transport data must be added to the electricity cables to feed the electrical energy (200V/50 Hz in Europe and, 100V/60 Hz in the United States and Japan). Moreover, the electric cable is a shared media and is sensitive to radio noise. Hence, various PLC technologies have benefited from the technical maturity of modulation and coding."," Broadband or high speed PLC technology uses the 1-30 MHz frequency band. They can provide data rates ranging from 1 to 200 Mbit/s, depending on the standards used. They are perfectly suited for network deployments requiring high bandwidth and high quality of service. Unlike other communication media, such as Ethernet cables, coaxial, fiber optics, etc. data transmission is not the main function of the electric cable. Transport data must be added to the electricity cables to feed the electrical energy (200V/50 Hz in Europe and, 100V/60 Hz in the United States and Japan). Moreover, the electric cable is a shared media and is sensitive to radio noise. Thus, various PLC technologies have benefited from the technical maturity of modulation and coding. They have better control of access to physical media and provide a robust signal with respect to the electromagnetic noise environment. A set of mechanisms have been integrated at the physical and data link layer to improve data transmission and media access. for a high and low bit-rate PLC technologies with the two categories (high bit-rate and low bitrate) that will be described in the next sections"
Explain why the response frame transmitted by the receiving station can determine whether the data were received correctly by sending a positive acknowledgment to the originating station,"HomePlug PLC technologies use a two-frame format, as shown in section on data (Payload) and an end delimiter frame. Then, the short frame has a response delimiter used by the automatic repeat request process. Hence, the response frame transmitted by the receiving station can determine whether the data were received correctly by sending a positive acknowledgment to the originating station."," ...... band84 band83 band82 band3 OFDM symbols band2 band1 OFDM symbols OFDM symbols 01 10 OFDM symbol phase modulation HomePlug PLC technologies use a two-frame format, as shown in section on data (Payload) and an end delimiter frame. Then, the short frame has a response delimiter used by the automatic repeat request process. Thus, the response frame transmitted by the receiving station can determine whether the data were received correctly by sending a positive acknowledgment to the originating station. However a negative acknowledgment is sent if the data were corrupted or incorrect and will result in data retransmission. In the HomePlug AV standard, an additional response, selective acknowledgment, was added to compensate for the fact that PLC stations between two stations are not necessarily symmetrical in terms of data rate. If the frame goes beyond its maximum size (160 OFDM symbols for data in the HomePlug 1.0 standard), mechanisms of fragmentation and reassembly are implemented"
Explain how the DSL modem access is extended to the home electrical grid that serves as the backbone of home Internet access and any electrical plug can be used to get access to the services that the box is delivering,"Indeed, current PLC technologies offer data rates that are sufficient to deploy broadband services using the existing electrical media. Thus, the simple connection of PLC enclosures through a socket allows the power grid to create robust and secure communications networks. For example, these technologies (HomePlug 1.0, HomePlug AV, etc.) allow the broadcast of audio and video content, or provide the ability to share an Internet connection with an optimum quality of service. As presented in Figure 4.7, a typical architecture for this kind of high bandwidth application relies on the connection of a DSL modem (Internet box) with a PLC interface to an electrical plug. Hence, the DSL modem access is extended to the home electrical grid that serves as the backbone of home Internet access and any electrical plug can be used to get access to the services that the box is delivering."," Most of these services rely on Internet access provided by ISPs through modems or more complex Internet boxes. Currently, the diversity of PLC-related technologies can meet these expectations. Indeed, current PLC technologies offer data rates that are sufficient to deploy broadband services using the existing electrical media. Thus, the simple connection of PLC enclosures through a socket allows the power grid to create robust and secure communications networks. For example, these technologies (HomePlug 1.0, HomePlug AV, etc.) allow the broadcast of audio and video content, or provide the ability to share an Internet connection with an optimum quality of service. As presented in Figure 4.7, a typical architecture for this kind of high bandwidth application relies on the connection of a DSL modem (Internet box) with a PLC interface to an electrical plug. Thus, the DSL modem access is extended to the home electrical grid that serves as the backbone of home Internet access and any electrical plug can be used to get access to the services that the box is delivering. Wireless interfaces that are embedded with PLC devices are available today. Thus the electrical grid can serve to extend the Internet connectivity with wireless access points by a simple connection to any electrical plug in the home. It is interesting to note that such PLC-WiFi devices benefit from getting energy and Internet connectivity within a single cable that can leverage their deployment in an in-door installation"
Explain why it is important to design the PLC infrastructure with regards to these factors by adding PLC signal repeaters,"Despite the ease of deployment offered by PLC technologies in the home environment, some critical factors concerning their performance and security have to be taken into account. First, as stated in section 4.2, the bandwidth delivered through PLC technologies decreases with length of the signal propagation and of number of PLC devices that are connected to the electrical grid. Hence, it is important to design the PLC infrastructure with regards to these factors by adding PLC signal repeaters."," Besides of the above described architecture, high-speed PLC technologies can be deployed with no Internet connectivity to deliver services for an internal usage. Thus the home electrical grid can serve as a transmission channel for various applications as multimedia file sharing, internal telecommunication system, real-time video streaming, etc. Figure 4.8 depicts this sort of usage where a server and a network attached storage equipment are connected to the home electrical grid via a PLC device and provides access to different multimedia content simultaneously to a PC client that is connected to the home electrical grid through a PLC device. Despite the ease of deployment offered by PLC technologies in the home environment, some critical factors concerning their performance and security have to be taken into account. First, as stated in section 4.2, the bandwidth delivered through PLC technologies decreases with length of the signal propagation and of number of PLC devices that are connected to the electrical grid. Thus, it is important to design the PLC infrastructure with regards to these factors by adding PLC signal repeaters. Moreover, as PLC signals can bypass electric meters of an individual installation, it is important to protect the PLC networks deployed with the security mechanisms described in section 2.3"
Explain how the remote operator can be alerted of different events and can react accordingly,"The Internet of Things 4.10 depicts this kind of installation applied for a home security service. The home automation deployment is based on centralized architecture where a smoke detector and a motion detector send their information to a controller, which also serves to monitor the installation. A 56 K modem is connected to the controller via the home electrical grid and communicates its data flow to a remote operator via the local public switched telephone network. Hence, the remote operator can be alerted of different events and can react accordingly."," In this scenario, a smoke detector and a video motion detector are connected to the home electrical grid through PLC devices. These sensors send their information to a central controller that processes the incoming information and sends a signal to an actuator if a fire or an intrusion in the home is detected. The actuator is a simple speaker that produces different sounds depending on the event detected. for in-home applications The scenario presented before can be extended with a modem to offer remote access to the home automation deployment. This feature is commonly used for providing remote services for home security, personal health assistance, home installation monitoring, etc. Figure The Internet of Things 4.10 depicts this kind of installation applied for a home security service. The home automation deployment is based on centralized architecture where a smoke detector and a motion detector send their information to a controller, which also serves to monitor the installation. A 56 K modem is connected to the controller via the home electrical grid and communicates its data flow to a remote operator via the local public switched telephone network. Thus, the remote operator can be alerted of different events and can react accordingly. 4.4. Internet of things using PLC technology Today many different systems must communicate in the house; computer systems, telecommunications, electronics, home automation, everything merges into a world called digital convergence. The concept of digital convergence is related to the digital home and is evolving as an essential service provided to users by ISPs and industry. Access to mobility through the deployment of wireless technologies and the development of chips radio frequency identification (RFID) will soon allow the creation of an Internet of Things (IoT) to accompany users in each of their domestic activities"
Explain how the interconnection of different communicating objects that respect the UPnP protocol can easily be achieved in the home environment using PLC technologies,"Another example of interoperability effort is the Universal Plug and Play (UPnP) [MIL 01] which is a network protocol compatible with TCP/IP and UDP. It proposes is to foster communication between any number of devices on the local area network. UPnP uses an open architecture, allowing independence vis--vis of the media used. An UPnP service works by including a list of actions that the service responds to and then produces a list of variables that characterize the service performance. Each device can dynamically join a network, obtain an IP address, announce its name, specify its options on request and query other devices on their presence and capabilities. Hence, the interconnection of different communicating objects that respect the UPnP protocol can easily be achieved in the home environment using PLC technologies."," 4.4.2. Interoperability environment connecting in home One of the biggest problems of new technologies is the interconnection of different materials or technologies that are not designed to be compatible. This is the case when connecting between different types of PLC technologies or different communicating objects that belong to different constructors. To embrace these challenges, the Digital Living Network Alliance Consortium was created in the US. At present, it brings together around 200 members including leading companies involved in production of electronics, mobile devices and personal computers. Its purpose is to promote common standards and interoperability between products from different companies to create a network of electronic devices in the home. Although the standard is still in a development stage, Digital Living Network Alliance certification is already available. Another example of interoperability effort is the Universal Plug and Play (UPnP) [MIL 01] which is a network protocol compatible with TCP/IP and UDP. It proposes is to foster communication between any number of devices on the local area network. UPnP uses an open architecture, allowing independence vis--vis of the media used. An UPnP service works by including a list of actions that the service responds to and then produces a list of variables that characterize the service performance. Each device can dynamically join a network, obtain an IP address, announce its name, specify its options on request and query other devices on their presence and capabilities. Thus, the interconnection of different communicating objects that respect the UPnP protocol can easily be achieved in the home environment using PLC technologies. Coexistence between different network technologies, whether wired or wireless, can create critical disturbance. For example, propagation of the PLC signal on power cables creates an electromagnetic field that can disrupt not only the other communication systems, such as radio networks, but also the various PLC technologies themselves. However, cohabitation between PLC and wired technologies (Ethernet cable, fiber optics, cable TV, telephone cable, etc.) does not generate disturbance since the frequency bands used by these technologies are all located outside the frequency used by PLC technologies"
Explain why using devices that are conform to Digital Living Network Alliance or UPnP protocol should allow this kind of cross-PLC technology deployment,"As presented earlier, high and low bit-rate PLC technologies can be deployed independently in the home environment. However, merging these technologies in a single deployment architecture should allow users to benefit from a wider range of facilities and usage. For example, it should be possible to allow deployment of a low bit-rate sensor network that could interact with content servers on a high bitrate network. There is no cohabitation problem between low and high bit-rate PLC technologies because they transmit on two different frequency bands. Technology devices cannot, however, communicate with each other. Hence, using devices that are conform to Digital Living Network Alliance or UPnP protocol should allow this kind of cross-PLC technology deployment."," Coexistence between different network technologies, whether wired or wireless, can create critical disturbance. For example, propagation of the PLC signal on power cables creates an electromagnetic field that can disrupt not only the other communication systems, such as radio networks, but also the various PLC technologies themselves. However, cohabitation between PLC and wired technologies (Ethernet cable, fiber optics, cable TV, telephone cable, etc.) does not generate disturbance since the frequency bands used by these technologies are all located outside the frequency used by PLC technologies. As presented earlier, high and low bit-rate PLC technologies can be deployed independently in the home environment. However, merging these technologies in a single deployment architecture should allow users to benefit from a wider range of facilities and usage. For example, it should be possible to allow deployment of a low bit-rate sensor network that could interact with content servers on a high bitrate network. There is no cohabitation problem between low and high bit-rate PLC technologies because they transmit on two different frequency bands. Technology devices cannot, however, communicate with each other. Thus, using devices that are conform to Digital Living Network Alliance or UPnP protocol should allow this kind of cross-PLC technology deployment. The coexistence of PLC and wireless technology is also possible, since the frequency bands used are different. The high bit-rate PLC technologies operate in the band 1 to 30 MHz and various IEEE 802.11 standards in those of 2.4 and 5 GHz. Moreover, some hybrids PLC/WiFi equipment is already available in the market, such as the NetPlug Turbo Thesys equipment that provides an electrical outlet with an Ethernet interface and an antenna for IEEE 802.11. Thus it is possible to deploy a hybrid network that employs any high bit-rate PLC technologies to create an Ethernet backbone through the home electrical grid (where each electrical outlet can be used with hybrid PLC/WiFi equipment to provide wireless connectivity). Furthermore, this hybrid deployment can provide an optimum performance and coverage to different communicating objects in the home"
Explain why passive tags compared to active tags are less expensive and have unlimited lifetime but have reduced read range capability,"Various types of tags exist that differ significantly, mainly in their power supply and computational capabilities. They range from dump passive tags, which operate without battery but respond simply to readers queries, to smart active tags that contain radio transceiver, memory and a power supply. Hence, passive tags compared to active tags are less expensive and have unlimited lifetime but have reduced read range capability."," 6.3. Localization and handover management relying on RFID Radio frequency identification (RFID) is an attractive technology for a wide range of applications. In this section we suggest employing it for achieving accurate localization and time-efficient movement detection, both of which are critical for the success of mobile and wireless communications. After providing a brief technology overview regarding key features of RFID (for further details see 6.3.1. A technology overview of RFID RFID is an automatic ID system that consists of two basic hardware components: a tag and a reader. A tag has an ID stored in its memory that is represented by a bit string. The reader, which is typically a powerful device with memory and computational resources, is able to read the IDs of tags located within its vicinity by running a simple link-layer protocol over the wireless channel. Various types of tags exist that differ significantly, mainly in their power supply and computational capabilities. They range from dump passive tags, which operate without battery but respond simply to readers queries, to smart active tags that contain radio transceiver, memory and a power supply. Thus, passive tags compared to active tags are less expensive and have unlimited lifetime but have reduced read range capability. Due to their low cost, passive tags are anticipated to be a popular choice, especially for large-scale deployment, as in the Internet of Things (IoT). Communication between a reader and a passive tag is done using either magnetic or electromagnetic coupling. Coupling is the transfer of energy from one medium to another medium, and tags use it to obtain power from the reader to transfer data. There are two main types of coupling inductive and backscatter depending on whether the tags are operating in the near-field or far-field of the interrogator, respectively. A key difference between them is that far-field communication has a longer read range compared to near-field communication. RFID systems operate in the industry, scientific and medical frequency band that ranges from 100 KHz to 5.8 GHz, but they are further subdivided into four categories according to their operating frequency: low frequency (LF), high frequency (HF), ultrahigh frequency (UHF) and microwave"
Explain software - defined networking ( sdn ) technology,"Software-defined networking (SDN) technology is an approach to network management that enables dynamic, programmatically efficient network configuration in order to improve network performance and monitoring, making it more like cloud computing than traditional network management.SDN is meant to address the static architecture of traditional networks.SDN attempts to centralize network intelligence in one network component by disassociating the forwarding process of network packets (data plane) from the routing process (control plane).","Software-defined networking (SDN) technology is an approach to network management that enables dynamic, programmatically efficient network configuration in order to improve network performance and monitoring, making it more like cloud computing than traditional network management. SDN is meant to address the static architecture of traditional networks. SDN attempts to centralize network intelligence in one network component by disassociating the forwarding process of network packets (data plane) from the routing process (control plane). The control plane consists of one or more controllers, which are considered the brain of the SDN network where the whole intelligence is incorporated. However, centralization has its own drawbacks when it comes to security, scalability and elasticity and this is the main issue of SDN.SDN was commonly associated with the OpenFlow protocol (for remote communication with network plane elements for the purpose of determining the path of network packets across network switches) since the latter's emergence in 2011. However, since 2012, proprietary  systems also used the term. These include Cisco Systems' Open Network Environment and Nicira's network virtualization platform.
SD-WAN applies similar technology to a wide area network (WAN)."
Explain the advantage and disadvantage of  pay - as - you - go model,"Cloud computing relies on sharing of resources to achieve coherence and typically uses a pay-as-you-go model, which can help in reducing capital expenses but may also lead to unexpected operating expenses for users.","Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user. Large clouds often have functions distributed over multiple locations, each location being a data center. Cloud computing relies on sharing of resources to achieve coherence and typically uses a ""pay-as-you-go"" model, which can help in reducing capital expenses but may also lead to unexpected operating expenses for users.

"
Explain internet access,"Internet access is the ability of individuals and organizations to connect to the Internet using computer terminals, computers, and other devices; and to access services such as email and the World Wide Web.Internet access is sold by Internet service providers (ISPs) delivering connectivity at a wide range of data transfer rates via various networking technologies.","Internet access is the ability of individuals and organizations to connect to the Internet using computer terminals, computers, and other devices; and to access services such as email and the World Wide Web. Internet access is sold by Internet service providers (ISPs) delivering connectivity at a wide range of data transfer rates via various networking technologies. Many organizations, including a growing number of municipal entities, also provide cost-free wireless access and landlines.
Availability of Internet access was once limited, but has grown rapidly. In 1995, only 0.04 percent of the world's population had access, with well over half of those living in the United States, and consumer use was through dial-up. By the first decade of the 21st century, many consumers in developed nations used faster broadband technology, and by 2014, 41 percent of the world's population had access, broadband was almost ubiquitous worldwide, and global average connection speeds exceeded one megabit per second."
Explain encryption,"In cryptography, encryption is the process of encoding information.This process converts the original representation of the information, known as plaintext, into an alternative form known as ciphertext.","In cryptography, encryption is the process of encoding information. This process converts the original representation of the information, known as plaintext, into an alternative form known as ciphertext. Ideally, only authorized parties can decipher a ciphertext back to plaintext and access the original information. Encryption does not itself prevent interference but denies the intelligible content to a would-be interceptor. 
For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is possible to decrypt the message without possessing the key but, for a well-designed encryption scheme, considerable computational resources and skills are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients but not to unauthorized users. 
Historically, various forms of encryption have been used to aid in cryptography. Early encryption techniques were often used in military messaging. Since then, new techniques have emerged and become commonplace in all areas of modern computing. Modern encryption schemes use the concepts of public-key and symmetric-key. Modern encryption techniques ensure security because modern computers are inefficient at cracking the encryption."
Explain browser isolation,"Browser isolation is a cybersecurity model which aims to physically isolate an internet users browsing activity (and the associated cyber risks) away from their local networks and infrastructure.Browser isolation technologies approach this model in different ways, but they all seek to achieve the same goal, effective isolation of the web browser and a users browsing activity as a method of securing web browsers from browser-based security exploits, as well as web-borne threats such as ransomware and other malware.","Browser isolation is a cybersecurity model which aims to physically isolate an internet user's browsing activity (and the associated cyber risks) away from their local networks and infrastructure.  Browser isolation technologies approach this model in different ways, but they all seek to achieve the same goal, effective isolation of the web browser and a user's browsing activity as a method of securing web browsers from browser-based security exploits, as well as web-borne threats such as ransomware and other malware. When a browser isolation technology is delivered to its customers as a cloud hosted service, this is known as remote browser isolation (RBI), a model which enables organizations to deploy a browser isolation solution to their users without managing the associated server infrastructure. There are also client side approaches to browser isolation, based on client-side hypervisors, which do not depend on servers in order to isolate their users browsing activity and the associated risks, instead the activity is virtually isolated on the local host machine. Client-side solutions break the security through physical isolation model, but they do allow the user to avoid the server overhead costs associated with remote browser isolation solutions."
Explain database normalization or database normalisation ( see spelling differences ),Database normalization or database normalisation (see spelling differences) is the process of structuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity.It was first proposed by British computer scientist Edgar F. Codd as part of his relational model.,"Database normalization or database normalisation (see spelling differences) is the process of structuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by British computer scientist Edgar F. Codd as part of his relational model.
Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints. It is accomplished by applying some formal rules either by a process of synthesis (creating a new database design) or decomposition (improving an existing database design)."
Explain data independence,Data independence is the type of data transparency that matters for a centralized DBMS.It refers to the immunity of user applications to changes made in the definition and organization of data.,"Data independence is the type of data transparency  that matters for a centralized  DBMS. It refers to the immunity of user applications to changes made in the definition and organization of data. Application programs should not, ideally, be exposed to details of data representation and storage. The DBMS provides an abstract view of the data that hides such details.
There are two types of data independence: physical and logical data independence.
The data independence and operation independence together gives the feature of data abstraction. There are two levels of data independence."
Explain object  relational impedance mismatch,"The objectrelational impedance mismatch is a set of conceptual and technical difficulties that are often encountered when a relational database management system (RDBMS) is being served by an application program (or multiple application programs) written in an object-oriented programming language or style, particularly because objects or class definitions must be mapped to database tables defined by a relational schema.The term objectrelational impedance mismatch is derived from the electrical engineering term impedance matching.","The objectrelational impedance mismatch is a set of conceptual and technical difficulties that are often encountered when a relational database management system (RDBMS) is being served by an application program (or multiple application programs) written in an object-oriented programming language or style, particularly because objects or class definitions must be mapped to database tables defined by a relational schema.
The term objectrelational impedance mismatch is derived from the electrical engineering term impedance matching."
Explain database design,Database design is the organization of data according to a database model.Database management system manages the data accordingly.,"Database design is the organization of data according to a database model. The designer determines what data must be stored and how the data elements interrelate. With this information, they can begin to fit the data to the database model.
Database management system manages the data accordingly.
Database design involves classifying data and identifying interrelationships. This theoretical representation of the data is called an ontology. The ontology is the theory behind the database's design.

"
Explain data vault modeling,"Data vault modeling is a database modeling method that is designed to provide long-term historical storage of data coming in from multiple operational systems.It is also a method of looking at historical data that deals with issues such as auditing, tracing of data, loading speed and resilience to change as well as emphasizing the need to trace where all the data in the database came from.This is summarized in the statement that a data vault stores a single version of the facts (also expressed by Dan Linstedt as all the data, all of the time) as opposed to the practice in other data warehouse methods of storing a single version of the truth where data that does not conform to the definitions is removed or cleansed.The modeling method is designed to be resilient to change in the business environment where the data being stored is coming from, by explicitly separating structural information from descriptive attributes.","Data vault modeling is a database modeling method that is designed to provide long-term historical storage of data coming in from multiple operational systems. It is also a method of looking at historical data that deals with issues such as auditing, tracing of data, loading speed and resilience to change as well as emphasizing the need to trace where all the data in the database came from. This means that every row in a data vault must be accompanied by record source and load date attributes, enabling an auditor to trace values back to the source. It was developed by Daniel (Dan) Linstedt in 2000.
Data vault modeling makes no distinction between good and bad data (""bad"" meaning not conforming to business rules). This is summarized in the statement that a data vault stores ""a single version of the facts"" (also expressed by Dan Linstedt as ""all the data, all of the time"") as opposed to the practice in other data warehouse methods of storing ""a single version of the truth"" where data that does not conform to the definitions is removed or ""cleansed"".
The modeling method is designed to be resilient to change in the business environment where the data being stored is coming from, by explicitly separating structural information from descriptive attributes. Data vault is designed to enable parallel loading as much as possible, so that very large implementations can scale out without the need for major redesign.

"
Explain database tuning,"Database tuning describes a group of activities used to optimize and homogenize the performance of a database.It usually overlaps with query tuning, but refers to design of the database files, selection of the database management system (DBMS) application, and configuration of the databases environment (operating system, CPU, etc.).Database tuning aims to maximize use of system resources to perform work as efficiently and rapidly as possible.","Database tuning describes a group of activities used to optimize and homogenize the performance of a database. It usually overlaps with query tuning, but refers to design of the database files, selection of the database management system (DBMS) application, and configuration of the database's environment (operating system, CPU, etc.).
Database tuning aims to maximize use of system resources to perform work as efficiently and rapidly as possible. Most systems are designed to manage their use of system resources, but there is still much room to improve their efficiency by customizing their settings and configuration for the database and the DBMS."
Explain the universal decimal classification ( udc ),"The Universal Decimal Classification (UDC) is a bibliographic and library classification representing the systematic arrangement of all branches of human knowledge organized as a coherent system in which knowledge fields are related and inter-linked.The UDC is an analytico-synthetic and faceted classification system featuring detailed vocabulary and syntax that enables powerful content indexing and information retrieval in large collections.Since 1991, the UDC has been owned and managed by the UDC Consortium, a non-profit international association of publishers with headquarters in The Hague, Netherlands.Unlike other library classification schemes that have started their life as national systems, the UDC was conceived and maintained as an international scheme.UDC Summary, an abridged Web version of the scheme, is available in over 50 languages.The classification has been modified and extended over the years to cope with increasing output in all areas of human knowledge, and is still under continuous review to take account of new developments.Albeit originally designed as an indexing and retrieval system, due to its logical structure and scalability, UDC has become one of the most widely used knowledge organization systems in libraries, where it is used for either shelf arrangement, content indexing or both.","The Universal Decimal Classification (UDC) is a bibliographic and library classification representing the systematic arrangement of all branches of human knowledge organized as a coherent system in which knowledge fields are related and inter-linked. The UDC is an analytico-synthetic and faceted classification system featuring detailed vocabulary and syntax that enables powerful content indexing and information retrieval in large collections. Since 1991, the UDC has been owned and managed by the UDC Consortium, a non-profit international association of publishers with headquarters in The Hague, Netherlands.
Unlike other library classification schemes that have started their life as national systems, the UDC was conceived and maintained as an international scheme. Its translation into other languages started at the beginning of the 20th century and has since been published in various printed editions in over 40 languages. UDC Summary, an abridged Web version of the scheme, is available in over 50 languages. The classification has been modified and extended over the years to cope with increasing output in all areas of human knowledge, and is still under continuous review to take account of new developments.Albeit originally designed as an indexing and retrieval system, due to its logical structure and scalability, UDC has become one of the most widely used knowledge organization systems in libraries, where it is used for either shelf arrangement, content indexing or both. UDC codes can describe any type of document or object to any desired level of detail. These can include textual documents and other media such as films, video and sound recordings, illustrations, maps as well as realia such as museum objects."
Explain domain - driven design ( ddd ),"Domain-driven design (DDD) is a major software design approach, focusing on modelling software to match a domain according to input from that domains experts.Domain-driven design is predicated on the following goals: placing the projects primary focus on the core domain and domain logic; basing complex designs on a model of the domain; initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.","Domain-driven design (DDD) is a major software design approach, focusing on modelling software to match a domain according to input from that domain's experts.Under domain-driven design, the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if software processes loan applications, it might have classes like LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.
Domain-driven design is predicated on the following goals:

placing the project's primary focus on the core domain and domain logic;
basing complex designs on a model of the domain;
initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.Criticisms of domain-driven design argue that developers must typically implement a great deal of isolation and encapsulation to maintain the model as a pure and helpful construct. While domain-driven design provides benefits such as maintainability, Microsoft recommends it only for complex domains where the model provides clear benefits in formulating a common understanding of the domain.The term was coined by Eric Evans in his book of the same title published in 2003."
Explain the purpose of a system (POSIWID),"The purpose of a system is what it does (POSIWID) is a systems thinking heuristic coined by Stafford Beer, who observed that there is no point in claiming that the purpose of a system is to do what it constantly fails to do.The term is widely used by systems theorists, and is generally invoked to counter the notion that the purpose of a system can be read from the intentions of those who design, operate, or promote it.","The purpose of a system is what it does (POSIWID) is a systems thinking heuristic coined by Stafford Beer, who observed that there is ""no point in claiming that the purpose of a system is to do what it constantly fails to do."" The term is widely used by systems theorists, and is generally invoked to counter the notion that the purpose of a system can be read from the intentions of those who design, operate, or promote it. When a system's side effects or unintended consequences reveal that its behavior is poorly understood, then the POSIWID perspective can balance political understandings of system behavior with a more straightforwardly descriptive view."
Explain type theory,"In mathematics, logic, and computer science, a type theory is the formal presentation of a specific type system, and in general type theory is the academic study of type systems.Most computerized proof-writing systems use a type theory for their foundation.","In mathematics, logic, and computer science, a type theory is the formal presentation of a specific type system, and in general type theory is the academic study of type systems. Some type theories serve as alternatives to set theory as a foundation of mathematics. Two influential type theories that were proposed as foundations are Alonzo Church's typed -calculus and Per Martin-Lf's intuitionistic type theory.  Most computerized proof-writing systems use a type theory for their foundation.  A common one is Thierry Coquand's Calculus of Inductive Constructions."
Explain IPC,"Typically, applications can use IPC, categorized as clients and servers, where the client requests data and the server responds to client requests.Many applications are both clients and servers, as commonly seen in distributed computing.","In computer science, inter-process communication or interprocess communication (IPC) refers specifically to the mechanisms an operating system provides to allow the processes to manage shared data. Typically, applications can use IPC, categorized as clients and servers, where the client requests data and the server responds to client requests. Many applications are both clients and servers, as commonly seen in distributed computing.
IPC is very important to the design process for microkernels and nanokernels, which reduce the number of functionalities provided by the kernel. Those functionalities are then obtained by communicating with servers via IPC, leading to a large increase in communication when compared to a regular monolithic kernel. IPC interfaces generally encompass variable analytic framework structures. These processes ensure compatibility between the multi-vector protocols upon which IPC models rely.An IPC mechanism is either synchronous or asynchronous. Synchronization primitives may be used to have synchronous behavior with an asynchronous IPC mechanism."
Explain the cut command,"The cut command removes the selected data from its original position, while the copy command creates a duplicate; in both cases the selected data is kept in temporary storage (the clipboard).Terms like cloning, copy forward, carry forward, or re-use refer to the dissemination of such information through documents, and may be subject to regulation by administrative bodies.","In humancomputer interaction and user interface design, cut, copy, and paste are related commands that offer an interprocess communication technique for transferring data through a computer's user interface. The cut command removes the selected data from its original position, while the copy command creates a duplicate; in both cases the selected data is kept in temporary storage (the clipboard). The data from the clipboard is later inserted wherever a paste command is issued. The data remains available to any application supporting the feature, thus allowing easy data transfer between applications.
The command names are an interface metaphor based on the physical procedure used in manuscript editing to create a page layout.
This interaction technique has close associations with related techniques in graphical user interfaces (GUIs) that use pointing devices such as a computer mouse (by drag and drop, for example). Typically, clipboard support is provided by an operating system as part of its GUI and widget toolkit.
The capability to replicate information with ease, changing it between contexts and applications, involves privacy concerns because of the risks of disclosure when handling sensitive information. Terms like cloning, copy forward, carry forward, or re-use refer to the dissemination of such information through documents, and may be subject to regulation by administrative bodies."
Explain concurrent computing,Concurrent computing is a form of computing in which several computations are executed concurrentlyduring overlapping time periodsinstead of sequentiallywith one completing before the next starts.,"Concurrent computing is a form of computing in which several computations are executed concurrentlyduring overlapping time periodsinstead of sequentiallywith one completing before the next starts.
This is a property of a systemwhether a program, computer, or a networkwhere there is a separate execution point or ""thread of control"" for each process. A concurrent system is one where a computation can advance without waiting for all other computations to complete.Concurrent computing is a form of modular programming.  In its paradigm an overall computation is factored into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare."
Explain the fetch - and - add cpu instruction ( faa ),"In computer science, the fetch-and-add CPU instruction (FAA) atomically increments the contents of a memory location by a specified value.That is, fetch-and-add performs the operation increment the value at address x by a, where x is a memory location and a is some value, and return the original value at xin such a way that if this operation is executed by one process in a concurrent system, no other process will ever see an intermediate result.Fetch-and-add can be used to implement concurrency control structures such as mutex locks and semaphores.","In computer science, the fetch-and-add CPU instruction (FAA) atomically increments the contents of a memory location by a specified value.
That is, fetch-and-add performs the operation

increment the value at address x by a, where x is a memory location and a is some value, and return the original value at xin such a way that if this operation is executed by one process in a concurrent system, no other process will ever see an intermediate result.
Fetch-and-add can be used to implement concurrency control structures such as mutex locks and semaphores."
Explain serializable,"In concurrency control of databases, transaction processing (transaction management), and various transactional applications (e.g., transactional memory and software transactional memory), both centralized and distributed, a transaction schedule is serializable if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e. without overlapping in time.","In concurrency control of databases, transaction processing (transaction management), and various transactional applications (e.g., transactional memory and software transactional memory), both centralized and distributed, a transaction schedule is serializable if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e. without overlapping in time. Transactions are normally executed concurrently (they overlap), since this is the most efficient way. Serializability is the major correctness criterion for concurrent transactions' executions. It is considered the highest level of isolation between transactions, and plays an essential role in concurrency control. As such it is supported in all general purpose database systems. Strong strict two-phase locking (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.
Serializability theory provides the formal framework to reason about and analyze serializability and its techniques. Though it is mathematical in nature, its fundamentals are informally (without mathematics notation) introduced below."
Explain deadlock,"In concurrent computing, deadlock is any situation in which no member of some group of entities can proceed because each waits for another member, including itself, to take action, such as sending a message or, more commonly, releasing a lock.Deadlocks are a common problem in multiprocessing systems, parallel computing, and distributed systems, because in these contexts systems often use software or hardware locks to arbitrate shared resources and implement process synchronization.In a communications system, deadlocks occur mainly due to loss or corruption of signals rather than contention for resources.","In concurrent computing, deadlock is any situation in which no member of some group of entities can proceed because each waits for another member, including itself, to take action, such as sending a message or, more commonly, releasing a lock. Deadlocks are a common problem in multiprocessing systems, parallel computing, and distributed systems, because in these contexts systems often use software or hardware locks to arbitrate shared resources and implement process synchronization.In an operating system, a deadlock occurs when a process or thread enters a waiting state because a requested system resource is held by another waiting process, which in turn is waiting for another resource held by another waiting process. If a process remains indefinitely unable to change its state because resources requested by it are being used by another process that itself is waiting, then the system is said to be in a deadlock.In a communications system, deadlocks occur mainly due to loss or corruption of signals rather than contention for resources.

"
Explain a deterministic algorithm,"In computer science, a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states.Formally, a deterministic algorithm computes a mathematical function; a function has a unique value for any input in its domain, and the algorithm is a process that produces this particular value as output.","In computer science, a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states. Deterministic algorithms are by far the most studied and familiar kind of algorithm, as well as one of the most practical, since they can be run on real machines efficiently.
Formally, a deterministic algorithm computes a mathematical function; a function has a unique value for any input in its domain, and the algorithm is a process that produces this particular value as output."
Explain fair - share scheduling,"Fair-share scheduling is a scheduling algorithm for computer operating systems in which the CPU usage is equally distributed among system users or groups, as opposed to equal distribution of resources among processes.One common method of logically implementing the fair-share scheduling strategy is to recursively apply the round-robin scheduling strategy at each level of abstraction (processes, users, groups, etc.)","Fair-share scheduling is a scheduling algorithm for computer operating systems in which the CPU usage is equally distributed among system users or groups, as opposed to equal distribution of resources among processes.One common method of logically implementing the fair-share scheduling strategy is to recursively apply the round-robin scheduling strategy at each level of abstraction (processes, users, groups, etc.)  The time quantum required by round-robin is arbitrary, as any equal division of time will produce the same results.
This was first developed by Judy Kay and Piers Lauder through their research at Sydney University in the 1980s.
For example, if four users (A,B,C,D) are concurrently executing one process each, the scheduler will logically divide the available CPU cycles such that each user gets 25% of the whole (100% / 4 = 25%). If user B starts a second process, each user will still receive 25% of the total cycles, but each of user B's processes will now be attributed 12.5% of the total CPU cycles each, totalling user B's fair share of 25%. On the other hand, if a new user starts a process on the system, the scheduler will reapportion the available CPU cycles such that each user gets 20% of the whole (100% / 5 = 20%).
Another layer of abstraction allows us to partition users into groups, and apply the fair share algorithm to the groups as well. In this case, the available CPU cycles are divided first among the groups, then among the users within the groups, and then among the processes for that user. For example, if there are three groups (1,2,3) containing three, two, and four users respectively, the available CPU cycles will be distributed as follows:
100% / 3 groups = 33.3% per group
Group 1: (33.3% / 3 users) = 11.1% per user
Group 2: (33.3% / 2 users) = 16.7% per user
Group 3: (33.3% / 4 users) = 8.3% per user"
Explain starvation,"In computer science, resource starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work.Starvation may be caused by errors in a scheduling or mutual exclusion algorithm, but can also be caused by resource leaks, and can be intentionally caused via a denial-of-service attack such as a fork bomb.When starvation is impossible in a concurrent algorithm, the algorithm is called starvation-free, lockout-freed or said to have finite bypass.","In computer science, resource starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work. Starvation may be caused by errors in a scheduling or mutual exclusion algorithm, but can also be caused by resource leaks, and can be intentionally caused via a denial-of-service attack such as a fork bomb.
When starvation is impossible in a concurrent algorithm, the algorithm is called starvation-free, lockout-freed or said to have finite bypass. This property is an instance of liveness, and is one of the two requirements for any mutual exclusion algorithm; the other being correctness. The name ""finite bypass"" means that any process (concurrent part) of the algorithm is bypassed at most a finite number times before being allowed access to the shared resource."
Explain finite bypass,"When starvation is impossible in a concurrent algorithm, the algorithm is called starvation-free, lockout-freed or said to have finite bypass.The name finite bypass means that any process (concurrent part) of the algorithm is bypassed at most a finite number times before being allowed access to the shared resource.","In computer science, resource starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work. Starvation may be caused by errors in a scheduling or mutual exclusion algorithm, but can also be caused by resource leaks, and can be intentionally caused via a denial-of-service attack such as a fork bomb.
When starvation is impossible in a concurrent algorithm, the algorithm is called starvation-free, lockout-freed or said to have finite bypass. This property is an instance of liveness, and is one of the two requirements for any mutual exclusion algorithm; the other being correctness. The name ""finite bypass"" means that any process (concurrent part) of the algorithm is bypassed at most a finite number times before being allowed access to the shared resource."
Explain multitasking,"Several processes may be associated with the same program; for example, opening up several instances of the same program often results in more than one process being executed.Each CPU (core) executes a single task at a time.","In computing, a process is the instance of a computer program that is being executed by one or many threads. There are many different process models, some of which are light weight, but almost all processes (even entire virtual machines) are rooted in an operating system (OS) process which comprises the program code, assigned system resources, physical and logical access permissions, and data structures to initiate, control and coordinate execution activity. Depending on the OS, a process may be made up of multiple threads of execution that execute instructions concurrently.While a computer program is a passive collection of instructions typically stored in a file on disk, a process is the execution of those instructions after being loaded from the disk into memory. Several processes may be associated with the same program; for example, opening up several instances of the same program often results in more than one process being executed.
Multitasking is a method to allow multiple processes to share processors (CPUs) and other system resources. Each CPU (core) executes a single task at a time. However, multitasking allows each processor to switch between tasks that are being executed without having to wait for each task to finish (preemption). Depending on the operating system implementation, switches could be performed when tasks initiate and wait for completion of input/output operations, when a task voluntarily yields the CPU, on hardware interrupts, and when the operating system scheduler decides that a process has expired its fair share of CPU time (e.g, by the Completely Fair Scheduler of the Linux kernel).
A common form of multitasking is provided by CPU's time-sharing that is a method for interleaving the execution of users' processes and threads, and even of independent kernel tasks  although the latter feature is feasible only in preemptive kernels such as Linux. Preemption has an important side effect for interactive processes that are given higher priority with respect to CPU bound processes, therefore users are immediately assigned computing resources at the simple pressing of a key or when moving a mouse. Furthermore, applications like video and music reproduction are given some kind of real-time priority, preempting any other lower priority process. In time-sharing systems, context switches are performed rapidly, which makes it seem like multiple processes are being executed simultaneously on the same processor. This simultaneous execution of multiple processes is called concurrency.
For security and reliability, most modern operating systems prevent direct communication between independent processes, providing strictly mediated and controlled inter-process communication functionality."
Explain work stealing,"In parallel computing, work stealing is a scheduling strategy for multithreaded computer programs.It solves the problem of executing a dynamically multithreaded computation, one that can spawn new threads of execution, on a statically multithreaded computer, with a fixed number of processors (or cores).When a processor runs out of work, it looks at the queues of the other processors and steals their work items.Work stealing contrasts with work sharing, another popular scheduling approach for dynamic multithreading, where each work item is scheduled onto a processor when it is spawned.","In parallel computing, work stealing is a scheduling strategy for multithreaded computer programs. It solves the problem of executing a dynamically multithreaded computation, one that can ""spawn"" new threads of execution, on a statically multithreaded computer, with a fixed number of processors (or cores). It does so efficiently in terms of execution time, memory usage, and inter-processor communication.
In a work stealing scheduler, each processor in a computer system has a queue of work items (computational tasks, threads) to perform. Each work item consists of a series of instructions, to be executed sequentially, but in the course of its execution, a work item may also spawn new work items that can feasibly be executed in parallel with its other work. These new items are initially put on the queue of the processor executing the work item. When a processor runs out of work, it looks at the queues of the other processors and ""steals"" their work items. In effect, work stealing distributes the scheduling work over idle processors, and as long as all processors have work to do, no scheduling overhead occurs.Work stealing contrasts with work sharing, another popular scheduling approach for dynamic multithreading, where each work item is scheduled onto a processor when it is spawned. Compared to this approach, work stealing reduces the amount of process migration between processors, because no such migration occurs when all processors have work to do.The idea of work stealing goes back to the implementation of the Multilisp programming language and work on parallel functional programming languages in the 1980s. It is employed in the scheduler for the Cilk programming language, the Java fork/join framework, the .NET Task Parallel Library, and the Rust Tokio runtime."
Explain production scheduling,"In manufacturing, the purpose of scheduling is to keep due dates of customers and then minimize the production time and costs, by telling a production facility when to make, with which staff, and on which equipment.Production scheduling aims to maximize the efficiency of the operation, utilize maximum resources available and reduce costs.","Scheduling is the process of arranging, controlling and optimizing work and workloads in a production process or manufacturing process. Scheduling is used to allocate plant and machinery resources, plan human resources, plan production processes and purchase materials.
It is an important tool for manufacturing and engineering, where it can have a major impact on the productivity of a process. In manufacturing, the purpose of scheduling is to keep due dates of customers and then minimize the production time and costs, by telling a production facility when to make, with which staff, and on which equipment. Production scheduling aims to maximize the efficiency of the operation, utilize maximum resources available and reduce costs.
In some situations, scheduling can involve random attributes, such as random processing times, random due dates, random weights, and stochastic machine breakdowns. In this case, the scheduling problems are referred to as ""stochastic scheduling."""
Explain swinging,"Swinging, sometimes called wife-swapping, husband-swapping, or partner-swapping, is a sexual activity in which both singles and partners in a committed relationship sexually engage with others for recreational purposes.Swinging is a form of non-monogamy and is an open relationship.Some people may engage in swinging to add variety into their otherwise conventional sex-lives or due to their curiosity.","Swinging, sometimes called wife-swapping, husband-swapping, or partner-swapping, is a sexual activity in which both singles and partners in a committed relationship sexually engage with others for recreational purposes. Swinging is a form of non-monogamy and is an open relationship. People may choose a swinging lifestyle for a variety of reasons. Practitioners cite an increased quality and quantity of sex. Some people may engage in swinging to add variety into their otherwise conventional sex-lives or due to their curiosity. Some couples see swinging as a healthy outlet and means to strengthen their relationship.The phenomenon of swinging, or its wider discussion and practice, is regarded by some as arising from the freer attitudes to sexual activity after the sexual revolution of the 1960s, the invention and availability of the contraceptive pill, and the emergence of treatments for many of the sexually transmitted diseases that were known at that time. The adoption of safe sex practices became more common in the late 1980s. It is also a recurring theme in pornography. 
The swingers community sometimes refers to itself as ""the lifestyle"", or as ""the alternative lifestyle""."
Explain slab allocation,"Slab allocation is a memory management mechanism intended for the efficient memory allocation of objects.In comparison with earlier mechanisms, it reduces fragmentation caused by allocations and deallocations.This technique is used for retaining allocated memory containing a data object of a certain type for reuse upon subsequent allocations of objects of the same type.It is analogous to an object pool, but only applies to memory, not other resources.Slab allocation was first introduced in the Solaris 2.4 kernel by Jeff Bonwick.","Slab allocation is a memory management mechanism intended for the efficient memory allocation of objects. In comparison with earlier mechanisms, it reduces fragmentation caused by allocations and deallocations. 
This technique is used for retaining allocated memory containing a data object of a certain type for reuse upon subsequent allocations of objects of the same type.  It is analogous to an object pool, but only applies to memory, not other resources.
Slab allocation was first introduced in the Solaris 2.4 kernel by Jeff Bonwick. It is now widely used by many Unix and Unix-like operating systems including FreeBSD and Linux."
Explain the function of page replacement algorithms,"In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out, sometimes called swap out, or write to disk, when a page of memory needs to be allocated.A page replacement algorithm looks at the limited information about accesses to the pages provided by hardware, and tries to guess which pages should be replaced to minimize the total number of page misses, while balancing this with the costs (primary storage and processor time) of the algorithm itself.","In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out, sometimes called swap out, or write to disk, when a page of memory needs to be allocated. Page replacement happens when a requested page is not in memory (page fault) and a free page cannot be used to satisfy the allocation, either because there are none, or because the number of free pages is lower than some threshold.
When the page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), and this involves waiting for I/O completion. This determines the quality of the page replacement algorithm: the less time waiting for page-ins, the better the algorithm. A page replacement algorithm looks at the limited information about accesses to the pages provided by hardware, and tries to guess which pages should be replaced to minimize the total number of page misses, while balancing this with the costs (primary storage and processor time) of the algorithm itself.
The page replacing problem is a typical online problem from the competitive analysis perspective in the sense that the optimal deterministic algorithm is known."
Explain copying,"Copying is basic but has subtleties and can have significant overhead.There are several ways to copy an object, most commonly by a copy constructor or cloning.While in simple cases copying can be done by allocating a new, uninitialized object and copying all fields (attributes) from the original object, in more complex cases this does not result in desired behavior.","In object-oriented programming, object copying is creating a copy of an existing object, a unit of data in object-oriented programming. The resulting object is called an object copy or simply copy of the original object. Copying is basic but has subtleties and can have significant overhead. There are several ways to copy an object, most commonly by a copy constructor or cloning. Copying is done mostly so the copy can be modified or moved, or the current value preserved. If either of these is unneeded, a reference to the original data is sufficient and more efficient, as no copying occurs.
Objects in general store composite data. While in simple cases copying can be done by allocating a new, uninitialized object and copying all fields (attributes) from the original object, in more complex cases this does not result in desired behavior."
Explain hip replacement,"Hip replacement is a surgical procedure in which the hip joint is replaced by a prosthetic implant, that is, a hip prosthesis.A total hip replacement (total hip arthroplasty or THA) consists of replacing both the acetabulum and the femoral head while hemiarthroplasty generally only replaces the femoral head.","Hip replacement is a surgical procedure in which the hip joint is replaced by a prosthetic implant, that is, a hip prosthesis. Hip replacement surgery can be performed as a total replacement or a hemi (half) replacement. Such joint replacement orthopaedic surgery is generally conducted to relieve arthritis pain or in some hip fractures. A total hip replacement (total hip arthroplasty or THA) consists of replacing both the acetabulum and the femoral head while hemiarthroplasty generally only replaces the femoral head. Hip replacement is one of the most common orthopaedic operations, though patient satisfaction varies widely. Approximately 58% of total hip replacements are estimated to last 25 years. The average cost of a total hip replacement in 2012 was $40,364 in the United States, and about $7,700 to $12,000 in most European countries."
Explain copying,"Copying is basic but has subtleties and can have significant overhead.There are several ways to copy an object, most commonly by a copy constructor or cloning.While in simple cases copying can be done by allocating a new, uninitialized object and copying all fields (attributes) from the original object, in more complex cases this does not result in desired behavior.","In object-oriented programming, object copying is creating a copy of an existing object, a unit of data in object-oriented programming. The resulting object is called an object copy or simply copy of the original object. Copying is basic but has subtleties and can have significant overhead. There are several ways to copy an object, most commonly by a copy constructor or cloning. Copying is done mostly so the copy can be modified or moved, or the current value preserved. If either of these is unneeded, a reference to the original data is sufficient and more efficient, as no copying occurs.
Objects in general store composite data. While in simple cases copying can be done by allocating a new, uninitialized object and copying all fields (attributes) from the original object, in more complex cases this does not result in desired behavior.

"
Explain local privilege escalation,"It is a local privilege escalation bug that exploits a race condition in the implementation of the copy-on-write mechanism in the kernels memory-management subsystem.Because of the race condition, with the right timing, a local attacker can exploit the copy-on-write mechanism to turn a read-only mapping of a file into a writable mapping.","Dirty COW (Dirty copy-on-write) is a computer security vulnerability of the Linux kernel that affected all Linux-based operating systems, including Android devices, that used older versions of the Linux kernel created before 2018. It is a local privilege escalation bug that exploits a race condition in the implementation of the copy-on-write mechanism in the kernel's memory-management subsystem. Computers and devices that still use the older kernels remain vulnerable.
The vulnerability was discovered by Phil Oester. 
Because of the race condition, with the right timing, a local attacker can exploit the copy-on-write mechanism to turn a read-only mapping of a file into a writable mapping. Although it is a local privilege escalation, remote attackers can use it in conjunction with other exploits that allow remote execution of non-privileged code to achieve remote root access on a computer. The attack itself does not leave traces in the system log.The vulnerability has the Common Vulnerabilities and Exposures designation CVE-2016-5195. Dirty Cow was one of the first security issues transparently fixed in Ubuntu by the Canonical Live Patch service.It has been demonstrated that the vulnerability can be utilized to root any Android device up to (and excluding) Android version 7 (Nougat)."
Explain why pattern matching is used to control the execution of a for loop and to obtain the next value for processing,"Although a frequent use of the pattern matching form of instanceof is in an if statement, it is by no means limited to that use. It can also be employed in the conditional portion of the loop statements. As an example, imagine that you are processing a collection of objects, perhaps contained in an array. Furthermore, at the start of the array are several strings, and you want to process those strings, but not any of the remaining objects in the list. The following sequence accomplishes this task with a for loop in which the condition uses instanceof to confirm that an object in the array is a String and to obtain that string for processing within the loop. Hence, pattern matching is used to control the execution of a for loop and to obtain the next value for processing."," One other point: A logical expression cannot introduce the same pattern variable more than once. For example, in a logical AND, it is an error if both operands create the same pattern variable. Pattern Matching in Other Statements Although a frequent use of the pattern matching form of instanceof is in an if statement, it is by no means limited to that use. It can also be employed in the conditional portion of the loop statements. As an example, imagine that you are processing a collection of objects, perhaps contained in an array. Furthermore, at the start of the array are several strings, and you want to process those strings, but not any of the remaining objects in the list. The following sequence accomplishes this task with a for loop in which the condition uses instanceof to confirm that an object in the array is a String and to obtain that string for processing within the loop. Thus, pattern matching is used to control the execution of a for loop and to obtain the next value for processing. Object[] someObjs = { new String(""Alpha""), new String(""Beta""), new String(""Omega""), 21/09/21 5:48 PM 476PART I Integer.valueOf(10) }; int i; // This loop iterates until an element is not a String, or the end // of the array is reached"
Explain why a sealed class and its subclasses define a mutually dependent logical unit,"If a class is specified in a permits clause for a sealed class, then that class must directly extend the sealed class. Otherwise, a compile-time error will result. Hence, a sealed class and its subclasses define a mutually dependent logical unit."," } Now, any class may inherit Beta. However, the only direct subclasses of MySealedClass remain Alpha and Beta. A primary reason for non-sealed is to enable a superclass to specify a limited set of direct subclasses that provide a baseline of well-defined functionality but allow those subclasses to be freely extended. 21/09/21 5:48 PM 478PART I If a class is specified in a permits clause for a sealed class, then that class must directly extend the sealed class. Otherwise, a compile-time error will result. Thus, a sealed class and its subclasses define a mutually dependent logical unit. Additionally, it is illegal to declare a class that does not extend a sealed class as non-sealed. A key requirement of a sealed class is that every subclass that it permits must be accessible. Furthermore, if a sealed class is contained in a named module, then each subclass must also be in the same named module. In this case, a subclass can be in a different package from the sealed class. If the sealed class is in the unnamed module, then the sealed class and all permitted subclasses must be in the same package"
"Explain why a class that is not part of the permits clause cannot implement a sealed interface, and an interface not included in the permits clause cannot extend it","A sealed interface is declared in the same way as a sealed class, by the use of sealed. A sealed interface uses its permits clause to specify the classes allowed to implement it and/or the interfaces allowed to extend it. Hence, a class that is not part of the permits clause cannot implement a sealed interface, and an interface not included in the permits clause cannot extend it."," } final class Beta extends MySealedClass { // ... } Sealed Interfaces A sealed interface is declared in the same way as a sealed class, by the use of sealed. A sealed interface uses its permits clause to specify the classes allowed to implement it and/or the interfaces allowed to extend it. Thus, a class that is not part of the permits clause cannot implement a sealed interface, and an interface not included in the permits clause cannot extend it. Here is a simple example of a sealed interface that permits only the classes Alpha and Beta to implement it: public sealed interface MySealedIF permits Alpha, Beta { void myMeth(); } 21/09/21 5:48 PM A class that implements a sealed interface must itself be specified as either final, sealed, or non-sealed. For example, here Alpha is marked non-sealed and Beta is specified as final: public non-sealed class Alpha implements MySealedIF { public void myMeth() { System.out.println(""In Alpha's myMeth().""); } // .."
Explain why a sealed interface and its implementing classes form a logical unit," Here is a key point: Any class specified in a sealed interfaces permits clause must implement the interface. Hence, a sealed interface and its implementing classes form a logical unit."," } public final class Beta implements MySealedIF { public void myMeth() { System.out.println(""Inside Beta's myMeth().""); } // ... } Here is a key point: Any class specified in a sealed interfaces permits clause must implement the interface. Thus, a sealed interface and its implementing classes form a logical unit. A sealed interface can also specify which other interfaces can extend the sealed interface"
Explain why you can use a string literal to initialize a String object,"For each string literal in your program, Java automatically constructs a String object. Hence, you can use a string literal to initialize a String object."," String Length The length of a string is the number of characters that it contains. To obtain this value, call the length( ) method, shown here: int length( ) The following fragment prints ""3"", since there are three characters in the string s: char[] chars = { 'a', 'b', 'c' }; String s = new String(chars); System.out.println(s.length()); Special String Operations Because strings are a common and important part of programming, Java has added special support for several string operations within the syntax of the language. These operations include the automatic creation of new String instances from string literals, concatenation of multiple String objects by use of the + operator, and the conversion of other data types to a string representation. There are explicit methods available to perform all of these functions, but Java does them automatically as a convenience for the programmer and to add clarity. String Literals The earlier examples showed how to explicitly create a String instance from an array of characters by using the new operator. However, there is an easier way to do this using a string literal. For each string literal in your program, Java automatically constructs a String object. Thus, you can use a string literal to initialize a String object. For example, the following code fragment creates two equivalent strings: char[] chars = { 'a', 'b', 'c' }; String s1 = new String(chars); String s2 = ""abc""; // use string literal Because a String object is created for every string literal, you can use a string literal any place you can use a String object. For example, you can call methods directly on a quoted string as if it were an object reference, as the following statement shows. It calls the length( ) method on the string ""abc"". As expected, it prints ""3"". System.out.println(""abc"".length()); 21/09/21 6:14 PM String Concatenation In general, Java does not allow operators to be applied to String objects. The one exception to this rule is the + operator, which concatenates two strings, producing a String object as the result. This allows you to chain together a series of + operations. For example, the following fragment concatenates three strings: String age = ""9""; String s = ""He is "" + age + "" years old.""; System.out.println(s); This displays the string ""He is 9 years old."" One practical use of string concatenation is found when you are creating very long strings. Instead of letting long strings wrap around within your source code, you can break them into smaller pieces, using the + to concatenate them. Here is an example: // Using concatenation to prevent long lines"
"Explain why the feature release counter for JDK 10 is 10, the one for JDK 11 is 11, and so on","The feature release counter specifies the number of the release. This counter is updated with each feature release. To smooth the transition from the previous version scheme, the feature release counter began at 10. Hence, the feature release counter for JDK 10 is 10, the one for JDK 11 is 11, and so on."," class ExecDemoFini { public static void main(String[] args) { Runtime r = Runtime.getRuntime(); Process p = null; try { p = r.exec(""notepad""); p.waitFor(); } catch (Exception e) { System.out.println(""Error executing notepad.""); } System.out.println(""Notepad returned "" + p.exitValue()); } } 21/09/21 5:48 PM While a subprocess is running, you can write to and read from its standard input and output. The getOutputStream( ) and getInputStream( ) methods return the handles to standard in and out of the subprocess. Alternatively, beginning with JDK 17, you can also use outputWriter( ) and inputReader( ) to obtain a writer and reader. (I/O is examined in detail in Chapter 22.) Runtime.Version encapsulates version information (which includes the version number) pertaining to the Java environment. You can obtain an instance of Runtime.Version for the current platform by calling Runtime.version( ). Originally added by JDK 9, Runtime.Version was substantially changed with the release of JDK 10 to better accommodate the faster, timebased release cadence. As discussed earlier in this book, starting with JDK 10, a feature release is anticipated to occur on a strict schedule, with the time between feature releases expected to be six months. Runtime.Version is a value-based class. (See Chapter 13 for a description of value-based classes.) In the past, the JDK version number used the well-known major.minor approach. This mechanism did not, however, provide a good fit with the time-based release schedule. As a result, a different meaning was given to the elements of a version number. Today, the first four elements specify counters, which occur in the following order: feature release counter, interim release counter, update release counter, and patch release counter. Each number is separated by a period. However, trailing zeros, along with their preceding periods, are removed. Although additional elements may also be included, only the meaning of the first four are predefined. The feature release counter specifies the number of the release. This counter is updated with each feature release. To smooth the transition from the previous version scheme, the feature release counter began at 10. Thus, the feature release counter for JDK 10 is 10, the one for JDK 11 is 11, and so on. The interim release counter indicates the number of a release that occurs between feature releases. At the time of this writing, the value of the interim release counter will be zero because interim releases are not expected to be part of the increased release cadence"
Explain why they are available for all collections,"Algorithms are another important part of the collection mechanism. Algorithms operate on collections and are defined as static methods within the Collections class. Hence, they are available for all collections."," The Collections Framework was designed to meet several goals. First, the framework had to be high-performance. The implementations for the fundamental collections 21/09/21 5:49 PM java.util Part 1: The Collections Framework (dynamic arrays, linked lists, trees, and hash tables) are highly efficient. You seldom, if ever, need to code one of these data engines manually. Second, the framework had to allow different types of collections to work in a similar manner and with a high degree of interoperability. Third, extending and/or adapting a collection had to be easy. Toward this end, the entire Collections Framework is built upon a set of standard interfaces. Several standard implementations (such as LinkedList, HashSet, and TreeSet) of these interfaces are provided that you may use as-is. You may also implement your own collection, if you choose. Various special-purpose implementations are created for your convenience, and some partial implementations are provided that make creating your own collection class easier. Finally, mechanisms were added that allow the integration of standard arrays into the Collections Framework. Algorithms are another important part of the collection mechanism. Algorithms operate on collections and are defined as static methods within the Collections class. Thus, they are available for all collections. Each collection class need not implement its own versions. The algorithms provide a standard means of manipulating collections. Another item closely associated with the Collections Framework is the Iterator interface"
Explain why an iterator provides a means of enumerating the contents of a collection," An iterator offers a general-purpose, standardized way of accessing the elements within a collection, one at a time. Hence, an iterator provides a means of enumerating the contents of a collection."," Another item closely associated with the Collections Framework is the Iterator interface. An iterator offers a general-purpose, standardized way of accessing the elements within a collection, one at a time. Thus, an iterator provides a means of enumerating the contents of a collection. Because each collection provides an iterator, the elements of any collection class can be accessed through the methods defined by Iterator. Thus, with only small changes, the code that cycles through a set can also be used to cycle through a list, for example. JDK 8 added another type of iterator called a spliterator. In brief, spliterators are iterators that provide support for parallel iteration. The interfaces that support spliterators are Spliterator and several nested interfaces that support primitive types. Also available are iterator interfaces designed for use with primitive types, such as PrimitiveIterator and PrimitiveIterator.OfDouble"
"Explain why with only small changes, the code that cycles through a set can also be used to cycle through a list, for example","An iterator offers a general-purpose, standardized way of accessing the elements within a collection, one at a time. Thus, an iterator provides a means of enumerating the contents of a collection. Because each collection provides an iterator, the elements of any collection class can be accessed through the methods defined by Iterator. Hence, with only small changes, the code that cycles through a set can also be used to cycle through a list, for example."," Another item closely associated with the Collections Framework is the Iterator interface. An iterator offers a general-purpose, standardized way of accessing the elements within a collection, one at a time. Thus, an iterator provides a means of enumerating the contents of a collection. Because each collection provides an iterator, the elements of any collection class can be accessed through the methods defined by Iterator. Thus, with only small changes, the code that cycles through a set can also be used to cycle through a list, for example. JDK 8 added another type of iterator called a spliterator. In brief, spliterators are iterators that provide support for parallel iteration. The interfaces that support spliterators are Spliterator and several nested interfaces that support primitive types. Also available are iterator interfaces designed for use with primitive types, such as PrimitiveIterator and PrimitiveIterator.OfDouble"
Explain why you can process the contents of a map as a collection,"Maps store key/value pairs. Although maps are part of the Collections Framework, they are not collections in the strict use of the term. You can, however, obtain a collection-view of a map. Such a view contains the elements from the map stored in a collection. Hence, you can process the contents of a map as a collection, if you choose."," In addition to collections, the framework defines several map interfaces and classes. Maps store key/value pairs. Although maps are part of the Collections Framework, they are not collections in the strict use of the term. You can, however, obtain a collection-view of a map. Such a view contains the elements from the map stored in a collection. Thus, you can process the contents of a map as a collection, if you choose. The collection mechanism was retrofitted to some of the original classes defined by java.util so that they too could be integrated into the new system. It is important to understand that although the addition of collections altered the architecture of many of the original utility classes, it did not cause the deprecation of any. Collections simply provide a better way of doing several things"
Explain why the head of the queue will be the smallest value,"If no comparator is specified when a PriorityQueue is constructed, then the default comparator for the type of data stored in the queue is used. The default comparator will order the queue in ascending order. Hence, the head of the queue will be the smallest value."," PriorityQueue defines the seven constructors shown here: PriorityQueue( ) PriorityQueue(int capacity) PriorityQueue(Comparator<? super E> comp) PriorityQueue(int capacity, Comparator<? super E> comp) PriorityQueue(Collection<? extends E> c) PriorityQueue(PriorityQueue<? extends E> c) PriorityQueue(SortedSet<? extends E> c) The first constructor builds an empty queue. Its starting capacity is 11. The second constructor builds a queue that has the specified initial capacity. The third constructor specifies a comparator, and the fourth builds a queue with the specified capacity and comparator. The last three constructors create queues that are initialized with the elements of the collection passed in c. In all cases, the capacity grows automatically as elements are added. 21/09/21 5:49 PM 594PART II If no comparator is specified when a PriorityQueue is constructed, then the default comparator for the type of data stored in the queue is used. The default comparator will order the queue in ascending order. Thus, the head of the queue will be the smallest value. However, by providing a custom comparator, you can specify a different ordering scheme"
Explain why Spliterator supports parallel programming,"A spliterator cycles through a sequence of elements, and in this regard, it is similar to the iterators just described. However, the techniques required to use it differ. Furthermore, it offers substantially more functionality than does either Iterator or ListIterator. Perhaps the most important aspect of Spliterator is its ability to provide support for parallel iteration of portions of the sequence. Hence, Spliterator supports parallel programming."," int sum = 0; for(int v : vals) sum += v; System.out.println(""Sum of values: "" + sum); } } 21/09/21 5:49 PM 600PART II The output from the program is shown here: Contents of vals: 1 2 3 4 5 Sum of values: 15 As you can see, the for loop is substantially shorter and simpler to use than the iteratorbased approach. However, it can only be used to cycle through a collection in the forward direction, and you cant modify the contents of the collection. Spliterators JDK 8 added another type of iterator called a spliterator that is defined by the Spliterator interface. A spliterator cycles through a sequence of elements, and in this regard, it is similar to the iterators just described. However, the techniques required to use it differ. Furthermore, it offers substantially more functionality than does either Iterator or ListIterator. Perhaps the most important aspect of Spliterator is its ability to provide support for parallel iteration of portions of the sequence. Thus, Spliterator supports parallel programming. (See Chapter 29 for information on concurrency and parallel programming.) However, you can use Spliterator even if you wont be using parallel execution. One reason you might want to do so is because it offers a streamlined approach that combines the hasNext and next operations into one method. Spliterator is a generic interface that is declared like this: interface Spliterator<T> Here, T is the type of elements being iterated. Spliterator declares the methods shown in Table 20-10"
"Explain why like a map, a dictionary can be thought of as a list of key/value pairs","Dictionary is an abstract class that represents a key/value storage repository and operates much like Map. Given a key and value, you can store the value in a Dictionary object. Once the value is stored, you can retrieve it by using its key. Hence, like a map, a dictionary can be thought of as a list of key/value pairs."," Table 20-17 The Methods Defined by Stack required to bring it to the top of the stack. Here is an example that creates a stack, pushes several Integer objects onto it, and then pops them off again: // Demonstrate the Stack class. import java.util.*; class StackDemo { static void showpush(Stack<Integer> st, int a) { st.push(a); System.out.println(""push("" + a + "")""); System.out.println(""stack: "" + st); } static void showpop(Stack<Integer> st) { System.out.print(""pop -> ""); Integer a = st.pop(); System.out.println(a); System.out.println(""stack: "" + st); } public static void main(String[] args) { Stack<Integer> st = new Stack<Integer>(); System.out.println(""stack: "" + st); showpush(st, 42); showpush(st, 66); showpush(st, 99); showpop(st); showpop(st); showpop(st); try { showpop(st); } catch (EmptyStackException e) { System.out.println(""empty stack""); } } } 21/09/21 5:49 PM The following is the output produced by the program; notice how the exception handler for EmptyStackException is used so that you can gracefully handle a stack underflow: stack: [ ] push(42) stack: [42] push(66) stack: [42, 66] push(99) stack: [42, 66, 99] pop -> 99 stack: [42, 66] pop -> 66 stack: [42] pop -> 42 stack: [ ] pop -> empty stack Dictionary Dictionary is an abstract class that represents a key/value storage repository and operates much like Map. Given a key and value, you can store the value in a Dictionary object. Once the value is stored, you can retrieve it by using its key. Thus, like a map, a dictionary can be thought of as a list of key/value pairs. Although not currently deprecated, Dictionary is classified as obsolete, because it is fully superseded by Map. However, Dictionary is still in use and thus is discussed here. With the advent of JDK 5, Dictionary was made generic. It is declared as shown here: class Dictionary<K, V> Here, K specifies the type of keys, and V specifies the type of values. The abstract methods defined by Dictionary are listed in Table 20-18"
Explain why Hashtable is integrated into the Collections Framework,"With the advent of collections, Hashtable was reengineered to also implement the Map interface. Hence, Hashtable is integrated into the Collections Framework."," REMEMBER The Dictionary class is obsolete. You should implement the Map interface to obtain key/value storage functionality. Hashtable Hashtable was part of the original java.util and is a concrete implementation of a Dictionary. However, with the advent of collections, Hashtable was reengineered to also implement the Map interface. Thus, Hashtable is integrated into the Collections Framework. It is similar to HashMap, but is synchronized. Like HashMap, Hashtable stores key/value pairs in a hash table. However, neither keys nor values can be null. When using a Hashtable, you specify an object that is used as a key, and the value that you want linked to that key. The key is then hashed, and the resulting hash code is used as the index at which the value is stored within the table"
"Explain why to schedule a task, you will first create a TimerTask object and then schedule it for execution using an instance of Timer","Timer and TimerTask work together. Timer is the class that you will use to schedule a task for execution. The task being scheduled must be an instance of TimerTask. Hence, to schedule a task, you will first create a TimerTask object and then schedule it for execution using an instance of Timer."," Using these classes, you can create a thread that runs in the background, waiting for a specific time. When the time arrives, the task linked to that thread is executed. Various options allow you to schedule a task for repeated execution, and to schedule a task to run on a specific date. Although it was always possible to manually create a task that would be executed at a specific time using the Thread class, Timer and TimerTask greatly simplify this process. Timer and TimerTask work together. Timer is the class that you will use to schedule a task for execution. The task being scheduled must be an instance of TimerTask. Thus, to schedule a task, you will first create a TimerTask object and then schedule it for execution using an instance of Timer. TimerTask implements the Runnable interface; thus, it can be used to create a thread of execution. Its constructor is shown here: protected TimerTask( ) 21/09/21 5:50 PM Method boolean cancel( ) Terminates the task. Returns true if an execution of the task is prevented. Otherwise, returns false"
Explain why the overall rate of execution is fixed,"Task is scheduled for execution at the time specified by targetTime. The task is then executed repeatedly at the interval passed in repeat. The repeat parameter is specified in milliseconds. The time of each repetition is relative to the first execution, not the preceding execution. Hence, the overall rate of execution is fixed."," Thus, the overall rate of execution is fixed. void scheduleAtFixedRate( TimerTask TTask, Date targetTime, long repeat) TTask is scheduled for execution at the time specified by targetTime. The task is then executed repeatedly at the interval passed in repeat. The repeat parameter is specified in milliseconds. The time of each repetition is relative to the first execution, not the preceding execution. Thus, the overall rate of execution is fixed. /* Set an initial delay of 1 second, then repeat every half second"
Explain why a number read by nextDouble( ) need not specify a decimal point,"The numbers are read by calling nextDouble( ). This method reads any number that can be converted into a double value, including an integer value, such as 2, and a floating-point value like 3.4. Hence, a number read by nextDouble( ) need not specify a decimal point."," 2 4 done Average is 2.65 The program reads numbers until it encounters a token that does not represent a valid double value. When this occurs, it confirms that the token is the string ""done"". If it is, the program terminates normally. Otherwise, it displays an error. Notice that the numbers are read by calling nextDouble( ). This method reads any number that can be converted into a double value, including an integer value, such as 2, and a floating-point value like 3.4. Thus, a number read by nextDouble( ) need not specify a decimal point. This same general principle applies to all next methods. They will match and read any data format that can represent the type of value being requested. One thing that is especially nice about Scanner is that the same technique used to read from one source can be used to read from another. For example, here is the preceding program reworked to average a list of numbers contained in a text file: // Use Scanner to compute an average of the values in a file"
"Explain why even if you are reading the data a byte at a time out of the InputStream, you will be manipulating fast memory most of the time","I/O stream is always a good idea. That way, the low-level system can read blocks of data from the disk or network and store the results in your buffer. Hence, even if you are reading the data a byte at a time out of the InputStream, you will be manipulating fast memory most of the time."," BufferedInputStream has two constructors: BufferedInputStream(InputStream inputStream) BufferedInputStream(InputStream inputStream, int bufSize) The first form creates a buffered stream using a default buffer size. In the second, the size of the buffer is passed in bufSize. Use of sizes that are multiples of a memory page, a disk block, and so on, can have a significant positive impact on performance. This is, however, implementation-dependent. An optimal buffer size is generally dependent on the host operating system, the amount of memory available, and how the machine is configured. To make good use of buffering doesnt necessarily require quite this degree of sophistication. A good guess for a size is around 8,192 bytes, and attaching even a rather small buffer to an 21/09/21 5:51 PM Input/Output: Exploring java.io I/O stream is always a good idea. That way, the low-level system can read blocks of data from the disk or network and store the results in your buffer. Thus, even if you are reading the data a byte at a time out of the InputStream, you will be manipulating fast memory most of the time. Buffering an input stream also provides the foundation required to support moving backward in the stream of the available buffer. Beyond the read( ) and skip( ) methods implemented in any InputStream, BufferedInputStream also supports the mark( ) and reset( ) methods. This support is reflected by BufferedInputStream.markSupported( ) returning true"
Explain why printf( ) can be used in place of println( ) when writing to the console whenever formatted output is desired," Because System.out is a PrintStream, you can call printf( ) on System.out. Hence, printf( ) can be used in place of println( ) when writing to the console whenever formatted output is desired."," 21/09/21 5:51 PM Input/Output: Exploring java.io In general, printf( ) works in a manner similar to the format( ) method specified by Formatter. The fmtString consists of two types of items. The first type is composed of characters that are simply copied to the output buffer. The second type contains format specifiers that define the way the subsequent arguments, specified by args, are displayed. For complete information on formatting output, including a description of the format specifiers, see the Formatter class in Chapter 20. Because System.out is a PrintStream, you can call printf( ) on System.out. Thus, printf( ) can be used in place of println( ) when writing to the console whenever formatted output is desired. For example, the following program uses printf( ) to output numeric values in various formats. In the past, such formatting required a bit of work. With the addition of printf( ), this is now an easy task. // Demonstrate printf()"
"Explain why if null is returned, no console I/O is possible","If a console is available, then a reference to it is returned. Otherwise, null is returned. A console will not be available in all cases. Hence, if null is returned, no console I/O is possible."," The Console Class The Console class is used to read from and write to the console, if one exists. It implements the Flushable interface. Console is primarily a convenience class because most of its functionality is available through System.in and System.out. However, its use can simplify some types of console interactions, especially when reading strings from the console. Console supplies no constructors. Instead, a Console object is obtained by calling System.console( ), which is shown here: static Console console( ) 21/09/21 5:51 PM Input/Output: Exploring java.io If a console is available, then a reference to it is returned. Otherwise, null is returned. A console will not be available in all cases. Thus, if null is returned, no console I/O is possible. Console defines the methods shown in Table 22-5. Notice that the input methods, such as readLine( ), throw IOError if an input error occurs. IOError is a subclass of Error. It indicates an I/O failure that is beyond the control of your program. Thus, you will not normally catch an IOError. Frankly, if an IOError is thrown while accessing the console, it usually means there has been a catastrophic system failure"
Explain why of( ) gives you a way to construct a new Path instance,"Beginning with JDK 11, an important new static factory method called of( ) was added to Path. It returns a Path instance from either a path name or a URI. Hence, of( ) gives you a way to construct a new Path instance."," The number of elements in a path can be obtained by calling getNameCount( ). If you want to obtain a string representation of the entire path, simply call toString( ). Notice that you can resolve a relative path into an absolute path by using the resolve( ) method. Beginning with JDK 11, an important new static factory method called of( ) was added to Path. It returns a Path instance from either a path name or a URI. Thus, of( ) gives you a way to construct a new Path instance. One other point: When updating legacy code that uses the File class defined by java.io, it is possible to convert a File instance into a Path instance by calling toPath( ) on the File object. Furthermore, it is possible to obtain a File instance by calling the toFile( ) method defined by Path"
Explain why the Files methods use a Path to specify the file that is being operated upon,"Many of the actions that you perform on a file are provided by static methods within the Files class. The file to be acted upon is specified by its Path. Hence, the Files methods use a Path to specify the file that is being operated upon."," Returns the invoking Path as an absolute path. Table 23-3 A Sampling of Methods Specified by Path The Files Class Many of the actions that you perform on a file are provided by static methods within the Files class. The file to be acted upon is specified by its Path. Thus, the Files methods use a Path to specify the file that is being operated upon. Files contains a wide array of functionality. For example, it has methods that let you open or create a file that has the specified path. You can obtain information about a Path, such as whether it is executable, hidden, or read-only"
"Explain why to write to the mapped buffer, the channel must be opened as read/write"," This is because a mapped buffer can either be read-only or read/write. Hence, to write to the mapped buffer, the channel must be opened as read/write."," Here is the preceding program reworked so that a mapped file is used. Notice that in the call to newByteChannel( ), the open option StandardOpenOption.READ has been added. This is because a mapped buffer can either be read-only or read/write. Thus, to write to the mapped buffer, the channel must be opened as read/write. MappedByteBuffer map(FileChannel.MapMode how, long pos, long size) throws IOException // Write to a mapped file"
"Explain why the iterator( ) method can be called only once, and a for-each loop can be executed only once","DirectoryStream is to use a for-each style for loop. It is important to understand, however, that the iterator implemented by DirectoryStream<Path> can be obtained only once for each instance. Hence, the iterator( ) method can be called only once, and a for-each loop can be executed only once."," It will throw an IOException if an I/O error occurs and a NotDirectoryException (which is a subclass of IOException) if the specified path is not a directory. A SecurityException is also possible if access to the directory is not permitted. DirectoryStream<Path> implements AutoCloseable, so it can be managed by a try-with-resources statement. It also implements Iterable<Path>. This means that you can obtain the contents of the directory by iterating over the DirectoryStream object. When iterating, each directory entry is represented by a Path instance. An easy way to iterate over a DirectoryStream is to use a for-each style for loop. It is important to understand, however, that the iterator implemented by DirectoryStream<Path> can be obtained only once for each instance. Thus, the iterator( ) method can be called only once, and a for-each loop can be executed only once. The following program displays the contents of a directory called MyDir: // Display a directory"
Explain why ServerSocket is for servers," The ServerSocket class is designed to be a ""listener,"" which waits for clients to connect before doing anything. Hence, ServerSocket is for servers."," There are two kinds of TCP sockets in Java. One is for servers, and the other is for clients. The ServerSocket class is designed to be a ""listener,"" which waits for clients to connect before doing anything. Thus, ServerSocket is for servers. The Socket class is for clients. It is designed to connect to server sockets and initiate protocol exchanges. Because client sockets are the most commonly used by Java applications, they are examined here. 21/09/21 5:54 PM 800PART II The creation of a Socket object implicitly establishes a connection between the client and server. There are no methods or constructors that explicitly expose the details of establishing that connection. Here are two constructors used to create client sockets: Socket(String hostName, int port) throws UnknownHostException, IOException Creates a socket connected to the named host and port"
Explain why knowledge of Javas traditional approach to networking is important for all programmers,"The preceding material introduced Javas traditional support for networking provided by java.net. This API is available in all versions of Java and is widely used. Hence, knowledge of Javas traditional approach to networking is important for all programmers."," NOTE The use of datagrams may not be allowed on your computer. (For example, a firewall may prevent their use.) If this is the case, the preceding example cannot be used. Also, the port numbers used in the program work on the authors system, but may have to be adjusted for your environment. The preceding material introduced Javas traditional support for networking provided by java.net. This API is available in all versions of Java and is widely used. Thus, knowledge of Javas traditional approach to networking is important for all programmers. However, beginning with JDK 11, a new networking package called java.net.http, in the module java.net.http, has been added. It provides enhanced, updated networking support for HTTP clients. This new API is generally referred to as the HTTP Client API. For many types of HTTP networking, the capabilities defined by the API in java.net.http can provide superior solutions. In addition to offering a streamlined, easy-to-use API, other advantages include support for asynchronous communication, HTTP/2, and flow control"
Explain why a discussion of event handling must begin with the event classes," Event Classes are the classes that represent events are at the core of Javas event handling mechanism. Hence, a discussion of event handling must begin with the event classes."," Here is one more key point about events: An event handler must return quickly. For the most part, a GUI program should not enter a mode of operation in which it maintains control for an extended period. Instead, it must perform specific actions in response to events and then return control to the run-time system. Failure to do this can cause your program to appear sluggish or even non-responsive. If your program needs to perform a repetitive task, such as scrolling a banner, it must do so by starting a separate thread. In short, when your program receives an event, it must process it immediately, and then return. Event Classes The classes that represent events are at the core of Javas event handling mechanism. Thus, a discussion of event handling must begin with the event classes. It is important to understand, however, that Java defines several types of events and that not all event classes can be discussed 22/09/21 6:38 PM 822PART II in this chapter. Arguably, the most widely used events at the time of this writing are those defined by the AWT and those defined by Swing. This chapter focuses on the AWT events. (Most of these events also apply to Swing.) Several Swing-specific events are described in At the root of the Java event class hierarchy is EventObject, which is in java.util. It is the superclass for all events. Its one constructor is shown here: EventObject(Object src) Here, src is the object that generates this event"
Explain why your code does not need to be concerned with the differences in the way color is supported by various hardware devices,"Java supports color in a portable, device-independent fashion. The AWT color system allows you to specify any color you want. It then finds the best match for that color, given the limits of the display hardware currently executing your program. Hence, your code does not need to be concerned with the differences in the way color is supported by various hardware devices."," addMouseListener(new MouseAdapter() { public void mouseReleased(MouseEvent me) { int w = (d.width + inc) > max?min :(d.width + inc); int h = (d.height + inc) > max?min :(d.height + inc); setSize(new Dimension(w, h)); } }); // Anonymous inner class to handle window close events. addWindowListener(new WindowAdapter() { public void windowClosing(WindowEvent we) { System.exit(0); } }); } public void paint(Graphics g) { Insets i = getInsets(); d = getSize(); g.drawLine(i.left, i.top, d.width-i.right, d.height-i.bottom); g.drawLine(i.left, d.height-i.bottom, d.width-i.right, i.top); } public static void main(String[] args) { ResizeMe appwin = new ResizeMe(); 22/09/21 6:38 PM appwin.setSize(new Dimension(200, 200)); appwin.setTitle(""ResizeMe""); appwin.setVisible(true); } } Java supports color in a portable, device-independent fashion. The AWT color system allows you to specify any color you want. It then finds the best match for that color, given the limits of the display hardware currently executing your program. Thus, your code does not need to be concerned with the differences in the way color is supported by various hardware devices. Color is encapsulated by the Color class. As you saw earlier, Color defines several constants (for example, Color.black) to specify a number of common colors. You can also create your own colors, using one of the Color constructors. Three commonly used forms are shown here: Working with Color Color(int red, int green, int blue) Color(int rgbValue) Color(float red, float green, float blue) The first constructor takes three integers that specify the color as a mix of red, green, and blue. These values must be between 0 and 255, as in this example: new Color(255, 100, 100); // light red The second color constructor takes a single integer that contains the mix of red, green, and blue packed into an integer. The integer is organized with red in bits 16 to 23, green in bits 8 to 15, and blue in bits 0 to 7. Here is an example of this constructor: int newRed = (0xff000000 | (0xc0 << 16) | (0x00 << 8) | 0x00); Color darkRed = new Color(newRed); The third constructor, Color(float, float, float), takes three float values (between 0.0 and 1.0) that specify the relative mix of red, green, and blue"
Explain why Java provides ways for you to create a new image object and ways to load one,"There are three common operations that occur when you work with images: creating an image, loading an image, and displaying an image. In Java, the Image class is used to refer to images in memory and to images that must be loaded from external sources. Hence, Java provides ways for you to create a new image object and ways to load one."," File Formats Originally, web images could only be in GIF format. The GIF image format was created by CompuServe in 1987 to make it possible for images to be viewed while online, so it was well suited to the Internet. GIF images can have only up to 256 colors each. This limitation caused the major browser vendors to add support for JPEG images in 1995. The JPEG format was created by a group of photographic experts to store full-color-spectrum, continuous-tone images. These images, when properly created, can be of much higher fidelity as well as more highly compressed than a GIF encoding of the same source image. Another file format is 28-ch28.indd 929 CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 The Java Library PNG. It too is an alternative to GIF. In almost all cases, you will never care or notice which format is being used in your programs. The Java image classes abstract the differences behind a clean interface. Image Fundamentals: Creating, Loading, and Displaying There are three common operations that occur when you work with images: creating an image, loading an image, and displaying an image. In Java, the Image class is used to refer to images in memory and to images that must be loaded from external sources. Thus, Java provides ways for you to create a new image object and ways to load one. It also provides a means by which an image can be displayed. Lets look at each. Creating an Image Object You might expect that you create a memory image using something like the following: Image test = new Image(200, 100); // Error -- wont work Not so. Because images must eventually be painted on a window to be seen, the Image class doesnt have enough information about its environment to create the proper data format for the screen. Therefore, the Component class in java.awt has a factory method called createImage( ) that is used to create Image objects. (Remember that all of the AWT components are subclasses of Component, so all support this method.) The createImage( ) method has the following two forms: Image createImage(ImageProducer imgProd) Image createImage(int width, int height) The first form returns an image produced by imgProd, which is an object of a class that implements the ImageProducer interface. (We will look at image producers later.) The second form returns a blank (that is, empty) image that has the specified width and height"
"Explain why over the years, the concurrent API has evolved and expanded to meet the needs of the contemporary computing environment"," Furthermore, both JDK 8 and JDK 9 added features related to other parts of the concurrent API. Hence, over the years, the concurrent API has evolved and expanded to meet the needs of the contemporary computing environment."," Although the original concurrent API was impressive in its own right, it was significantly expanded by JDK 7. The most important addition was the Fork/Join Framework. The Fork/Join Framework facilitates the creation of programs that make use of multiple processors (such as those found in multicore systems). Thus, it streamlines the development of programs in 29-ch29.indd 955 CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 The Java Library which two or more pieces execute with true simultaneity (that is, true parallel execution), not just time-slicing. As you can easily imagine, parallel execution can dramatically increase the speed of certain operations. Because multicore systems are now commonplace, the inclusion of the Fork/Join Framework was as timely as it was powerful. The Fork/Join Framework was further enhanced by JDK 8. Furthermore, both JDK 8 and JDK 9 added features related to other parts of the concurrent API. Thus, over the years, the concurrent API has evolved and expanded to meet the needs of the contemporary computing environment. The original concurrent API was quite large, and the additions made over the years have increased its size substantially. As you might expect, many of the issues surrounding the concurrency utilities are quite complex. It is beyond the scope of this book to discuss all of its facets. The preceding notwithstanding, it is important for all programmers to have a general, working knowledge of key aspects of the concurrent API. Even in programs that are not intensively parallel, features such as synchronizers, callable threads, and executors, are applicable to a wide variety of situations. Perhaps most importantly, because of the rise of multicore computers, solutions involving the Fork/Join Framework are becoming more common. For these reasons, this chapter presents an overview of several core features defined by the concurrency utilities and shows a number of examples that demonstrate their use. It concludes with an introduction to the Fork/Join Framework"
"Explain why to access the resource, a thread must be granted a permit from the semaphore","The synchronization object that many readers will immediately recognize is Semaphore, which implements a classic semaphore. A semaphore controls access to a shared resource through the use of a counter. If the counter is greater than zero, then access is allowed. If it is zero, then access is denied. What the counter is counting are permits that allow access to the shared resource. Hence, to access the resource, a thread must be granted a permit from the semaphore."," 21/09/21 5:55 PM 958PART II Using Synchronization Objects Synchronization objects are supported by the Semaphore, CountDownLatch, CyclicBarrier, Exchanger, and Phaser classes. Collectively, they enable you to handle several formerly difficult synchronization situations with ease. They are also applicable to a wide range of programseven those that contain only limited concurrency. Because the synchronization objects will be of interest to nearly all Java programs, each is examined here in some detail. Semaphore The synchronization object that many readers will immediately recognize is Semaphore, which implements a classic semaphore. A semaphore controls access to a shared resource through the use of a counter. If the counter is greater than zero, then access is allowed. If it is zero, then access is denied. What the counter is counting are permits that allow access to the shared resource. Thus, to access the resource, a thread must be granted a permit from the semaphore. In general, to use a semaphore, the thread that wants access to the shared resource tries to acquire a permit. If the semaphores count is greater than zero, then the thread acquires a permit, which causes the semaphores count to be decremented. Otherwise, the thread will be blocked until a permit can be acquired. When the thread no longer needs access to the shared resource, it releases the permit, which causes the semaphores count to be incremented. If there is another thread waiting for a permit, then that thread will acquire a permit at that time. Javas Semaphore class implements this mechanism"
Explain why each thread waits until all threads have completed the phase (and the main thread is ready),"Now look at MyThread. First, notice that the constructor is passed a reference to the phaser that it will use and then registers with the new thread as a party on that phaser. Thus, each new MyThread becomes a party registered with the passed-in phaser. Also notice that each thread has three phases. In this example, each phase consists of a placeholder that simply displays the name of the thread and what it is doing. Obviously, in real-world code, the thread would be performing more meaningful actions. Between the first two phases, the thread calls arriveAndAwaitAdvance( ). Hence, each thread waits until all threads have completed the phase (and the main thread is ready)."," At that point, all three MyThreads have also deregistered. Since this results in there being no registered parties when the phaser advances to the next phase, the phaser is terminated. Now look at MyThread. First, notice that the constructor is passed a reference to the phaser that it will use and then registers with the new thread as a party on that phaser. Thus, each new MyThread becomes a party registered with the passed-in phaser. Also notice that each thread has three phases. In this example, each phase consists of a placeholder that simply displays the name of the thread and what it is doing. Obviously, in real-world code, the thread would be performing more meaningful actions. Between the first two phases, the thread calls arriveAndAwaitAdvance( ). Thus, each thread waits until all threads have completed the phase (and the main thread is ready). After all threads have arrived (including the main thread), the phaser moves on to the next phase. After the third phase, each thread deregisters itself with a call to arriveAndDeregister( ). As the comments in MyThread explain, the calls to sleep( ) are used for the purposes of illustration to ensure that the output is not jumbled because of the multithreading. They are not needed to make the phaser work properly. If you remove them, the output may look a bit jumbled, but the phases will still be synchronized correctly. One other point: Although the preceding example used three threads that were all of the same type, this is not a requirement. Each party that uses a phaser can be unique, with each performing some separate task"
Explain why execute( ) starts the specified thread," At the core of an executor is the Executor interface. It defines the following method: Using an Executor void execute(Runnable thread) The thread specified by thread is executed. Hence, execute( ) starts the specified thread."," The concurrent API supplies a feature called an executor that initiates and controls the execution of threads. As such, an executor offers an alternative to managing threads through the Thread class. At the core of an executor is the Executor interface. It defines the following method: Using an Executor void execute(Runnable thread) The thread specified by thread is executed. Thus, execute( ) starts the specified thread. The ExecutorService interface extends Executor by adding methods that help manage and control the execution of threads. For example, ExecutorService defines shutdown( ), shown here, which stops the invoking ExecutorService"
Explain why four tasks share the two threads that are in the pool,"Before going any further, a simple example that uses an executor will be of value. The following program creates a fixed thread pool that contains two threads. It then uses that pool to execute four tasks. Hence, four tasks share the two threads that are in the pool."," Here are some examples: static ExecutorService newCachedThreadPool( ) static ExecutorService newFixedThreadPool(int numThreads) static ScheduledExecutorService newScheduledThreadPool(int numThreads) newCachedThreadPool( ) creates a thread pool that adds threads as needed but reuses threads if possible. newFixedThreadPool( ) creates a thread pool that consists of a specified 21/09/21 5:55 PM 978PART II number of threads. newScheduledThreadPool( ) creates a thread pool that supports thread scheduling. Each returns a reference to an ExecutorService that can be used to manage the pool. A Simple Executor Example Before going any further, a simple example that uses an executor will be of value. The following program creates a fixed thread pool that contains two threads. It then uses that pool to execute four tasks. Thus, four tasks share the two threads that are in the pool. After the tasks finish, the pool is shut down and the program ends. // A simple example that uses an Executor"
Explain why ForkJoinTasks are very efficient when compared to threads,"ForkJoinPool. This mechanism allows a large number of tasks to be managed by a small number of actual threads. Hence, ForkJoinTasks are very efficient when compared to threads."," ForkJoinTask<V> is an abstract class that defines a task that can be managed by a ForkJoinPool. The type parameter V specifies the result type of the task. ForkJoinTask differs from Thread in that ForkJoinTask represents lightweight abstraction of a task, rather than a thread of execution. ForkJoinTasks are executed by threads managed by a thread pool of type ForkJoinPool. This mechanism allows a large number of tasks to be managed by a small number of actual threads. Thus, ForkJoinTasks are very efficient when compared to threads. ForkJoinTask defines many methods. At the core are fork( ) and join( ), shown here: ForkJoinTask<V> final ForkJoinTask<V> fork( ) final V join( ) The fork( ) method submits the invoking task for asynchronous execution of the invoking task"
Explain why there is no need to explicitly shut down a ForkJoinPool,"ForkJoinPool uses daemon threads. A daemon thread is automatically terminated when all user threads have terminated. Hence, there is no need to explicitly shut down a ForkJoinPool."," ForkJoinPool manages the execution of its threads using an approach called work-stealing. Each worker thread maintains a queue of tasks. If one worker threads queue is empty, it will take a task from another worker thread. This adds to overall efficiency and helps maintain a balanced load. (Because of demands on CPU time by other processes in the system, even two worker threads with identical tasks in their respective queues may not complete at the same time.) 21/09/21 5:55 PM 992PART II One other point: ForkJoinPool uses daemon threads. A daemon thread is automatically terminated when all user threads have terminated. Thus, there is no need to explicitly shut down a ForkJoinPool. However, with the exception of the common pool, it is possible to do so by calling shutdown( ). The shutdown( ) method has no effect on the common pool. The Divide-and-Conquer Strategy As a general rule, users of the Fork/Join Framework will employ a divide-and-conquer strategy that is based on recursion. This is why the two subclasses of ForkJoinTask are called RecursiveAction and RecursiveTask. It is anticipated that you will extend one of these classes when creating your own fork/join task"
Explain why the summation of each subtask is added to the running total,"This statement waits until each task ends. It then adds the results of each and assigns the total to sum. Hence, the summation of each subtask is added to the running total."," for(int i=0; i < nums.length; i++) nums[i] = (double) (((i%2) == 0) ? i : -i) ; Sum task = new Sum(nums, 0, nums.length); // Start the ForkJoinTasks. Notice that, in this case, // invoke() returns a result. double summation = fjp.invoke(task); System.out.println(""Summation "" + summation); } } Heres the output from the program: Summation -2500.0 There are a couple of interesting items in this program. First, notice that the two subtasks are executed by calling fork( ), as shown here: subTaskA.fork(); subTaskB.fork(); In this case, fork( ) is used because it starts a task but does not wait for it to finish. (Thus, it asynchronously runs the task.) The result of each task is obtained by calling join( ), as shown here: sum = subTaskA.join() + subTaskB.join(); This statement waits until each task ends. It then adds the results of each and assigns the total to sum. Thus, the summation of each subtask is added to the running total. Finally, 21/09/21 5:55 PM compute( ) ends by returning sum, which will be the final total when the first invocation returns. There are other ways to approach the handling of the asynchronous execution of the subtasks. For example, the following sequence uses fork( ) to start subTaskA and uses invoke( ) to start and wait for subTaskB: subTaskA.fork(); sum = subTaskB.invoke() + subTaskA.join(); Another alternative is to have subTaskB call compute( ) directly, as shown here: subTaskA.fork(); sum = subTaskB.compute() + subTaskA.join(); Executing a Task Asynchronously The preceding programs have called invoke( ) on a ForkJoinPool to initiate a task. This approach is commonly used when the calling thread must wait until the task has completed (which is often the case) because invoke( ) does not return until the task has terminated"
Explain why both the calling thread and the task execute simultaneously,"However, you can start a task asynchronously. In this approach, the calling thread continues to execute. Hence, both the calling thread and the task execute simultaneously."," There are other ways to approach the handling of the asynchronous execution of the subtasks. For example, the following sequence uses fork( ) to start subTaskA and uses invoke( ) to start and wait for subTaskB: subTaskA.fork(); sum = subTaskB.invoke() + subTaskA.join(); Another alternative is to have subTaskB call compute( ) directly, as shown here: subTaskA.fork(); sum = subTaskB.compute() + subTaskA.join(); Executing a Task Asynchronously The preceding programs have called invoke( ) on a ForkJoinPool to initiate a task. This approach is commonly used when the calling thread must wait until the task has completed (which is often the case) because invoke( ) does not return until the task has terminated. However, you can start a task asynchronously. In this approach, the calling thread continues to execute. Thus, both the calling thread and the task execute simultaneously. To start a task asynchronously, use execute( ), which is also defined by ForkJoinPool. It has the two forms shown here: void execute(ForkJoinTask<?> task) void execute(Runnable task) In both forms, task specifies the task to run. Notice that the second form lets you specify a Runnable rather than a ForkJoinTask task. Thus, it forms a bridge between Javas traditional approach to multithreading and the Fork/Join Framework. It is important to remember that the threads used by a ForkJoinPool are daemon. Thus, they will end when the main thread ends. As a result, you may need to keep the main thread alive until the tasks have finished. Cancelling a Task A task can be cancelled by calling cancel( ), which is defined by ForkJoinTask. It has this general form: boolean cancel(boolean interuptOK) It returns true if the task on which it was called is cancelled. It returns false if the task has ended or cant be cancelled. At this time, the interruptOK parameter is not used by the default implementation. In general, cancel( ) is intended to be called from code outside the task because a task can easily cancel itself by returning"
Explain why a stream represents a sequence of objects,"The stream API uses some of Javas most advanced features. To fully understand and utilize it requires a solid understanding of generics and lambda expressions. The basic concepts of parallel execution and a working knowledge of the Collections Framework are also needed. Stream Basics Lets begin by defining the term stream as it applies to the stream API: a stream is a conduit for data. Hence, a stream represents a sequence of objects."," Put simply, the stream API provides a powerful means of handling data in an efficient, yet easy to use way. Before continuing, an important point needs to be made: The stream API uses some of Javas most advanced features. To fully understand and utilize it requires a solid understanding of generics and lambda expressions. The basic concepts of parallel execution and a working knowledge of the Collections Framework are also needed. (See Chapters 14, 15, 20, and 29.) Stream Basics Lets begin by defining the term stream as it applies to the stream API: a stream is a conduit for data. Thus, a stream represents a sequence of objects. A stream operates on a data source, such as an array or a collection. A stream, itself, never provides storage for the data. It simply moves data, possibly filtering, sorting, or otherwise operating on that data in the process. As a general rule, however, a stream operation by itself does not modify the data source. For example, sorting a stream does not change the order of the source. Rather, sorting a stream results in the creation of a new stream that produces the sorted result"
Explain why intermediate operations can be used to create a pipeline that performs a sequence of actions,"In both tables, notice that many of the methods are notated as being either terminal or intermediate. The difference between the two is very important. A terminal operation consumes the stream. It is used to produce a result, such as finding the minimum value in the stream, or to execute some action, as is the case with the forEach( ) method. Once a stream has been consumed, it cannot be reused. Intermediate operations produce another stream. Hence, intermediate operations can be used to create a pipeline that performs a sequence of actions."," (Terminal operation.) Counts the number of elements in the stream and returns the result. (Terminal operation.) Produces a stream that contains those elements from the invoking stream that satisfy the predicate specified by pred. (Intermediate operation.) For each element in the invoking stream, the code specified by action is executed. (Terminal operation.) <R> Stream<R> map(Function<? super T, ? extends R> mapFunc) Applies mapFunc to the elements from the invoking stream, yielding a new stream that contains those elements. (Intermediate operation.) DoubleStream mapToDouble( ToDoubleFunction<? super T> mapFunc) Applies mapFunc to the elements from the invoking stream, yielding a new DoubleStream that contains those elements. (Intermediate operation.) IntStream mapToInt( ToIntFunction<? super T> mapFunc) Applies mapFunc to the elements from the invoking stream, yielding a new IntStream that contains those elements. (Intermediate operation.) LongStream mapToLong( ToLongFunction<? super T> mapFunc) Applies mapFunc to the elements from the invoking stream, yielding a new LongStream that contains those elements. (Intermediate operation.) Optional<T> max( Comparator<? super T> comp) Using the ordering specified by comp, finds and returns the maximum element in the invoking stream. (Terminal operation.) Using the ordering specified by comp, finds and returns the minimum element in the invoking stream. (Terminal operation.) Method 30-ch30.indd 1007 CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 The Java Library Description T reduce(T identityVal, BinaryOperator<T> accumulator) Returns a result based on the elements in the invoking stream. This is called a reduction operation. (Terminal operation.) Produces a new stream that contains the elements of the invoking stream sorted in natural order. (Intermediate operation.) Creates an array from the elements in the invoking stream. (Terminal operation.) Creates an unmodifiable List from the elements in the invoking stream (Terminal operation.) Table 30-2 A Sampling of Methods Declared by Stream In both tables, notice that many of the methods are notated as being either terminal or intermediate. The difference between the two is very important. A terminal operation consumes the stream. It is used to produce a result, such as finding the minimum value in the stream, or to execute some action, as is the case with the forEach( ) method. Once a stream has been consumed, it cannot be reused. Intermediate operations produce another stream. Thus, intermediate operations can be used to create a pipeline that performs a sequence of actions. One other point: intermediate operations do not take place immediately. Instead, the specified action is performed when a terminal operation is executed on the new stream created by an intermediate operation. This mechanism is referred to as lazy behavior, and the intermediate operations are referred to as lazy. The use of lazy behavior enables the stream API to perform more efficiently. Another key aspect of streams is that some intermediate operations are stateless and some are stateful. In a stateless operation, each element is processed independently of the others"
Explain why you can specify a regular expression that represents a general form that can match several different specific character sequences,"The java.util.regex package supports regular expression processing. Beginning with JDK 9, java.util.regex is in the java.base module. As the term is used here, a regular expression is a string of characters that describes a character sequence. This general description, called a pattern, can then be used to find matches in other character sequences. Regular expressions can specify wildcard characters, sets of characters, and various quantifiers. Hence, you can specify a regular expression that represents a general form that can match several different specific character sequences."," The regular expression package lets you perform sophisticated pattern matching operations. This chapter provides an introduction to this package along with extensive examples. Reflection is the ability of software to analyze itself. It is an essential part of the Java Beans technology that is covered in Chapter 35. Remote Method Invocation (RMI) allows you to build Java applications that are distributed among several machines. This chapter provides a simple client/server example that uses RMI. The text formatting capabilities of java.text have many uses. The one examined here formats date and time strings. The date and time API supplies an up-to-date approach to handling date and time. Regular Expression Processing The java.util.regex package supports regular expression processing. Beginning with JDK 9, java.util.regex is in the java.base module. As the term is used here, a regular expression is a string of characters that describes a character sequence. This general description, called a pattern, can then be used to find matches in other character sequences. Regular expressions can specify wildcard characters, sets of characters, and various quantifiers. Thus, you can specify a regular expression that represents a general form that can match several different specific character sequences. There are two classes that support regular expression processing: Pattern and Matcher"
Explain why you can pass a string to matcher( ),"Here str is the character sequence that the pattern will be matched against. This is called the input sequence. CharSequence is an interface that defines a read-only set of characters. It is implemented by the String class, among others. Hence, you can pass a string to matcher( )."," 31-ch31.indd 1031 CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 The Java Library Pattern The Pattern class defines no constructors. Instead, a pattern is created by calling the compile( ) factory method. One of its forms is shown here: static Pattern compile(String pattern) Here, pattern is the regular expression that you want to use. The compile( ) method transforms the string in pattern into a pattern that can be used for pattern matching by the Matcher class. It returns a Pattern object that contains the pattern. Once you have created a Pattern object, you will use it to create a Matcher. This is done by calling the matcher( ) method defined by Pattern. It is shown here: Matcher matcher(CharSequence str) Here str is the character sequence that the pattern will be matched against. This is called the input sequence. CharSequence is an interface that defines a read-only set of characters. It is implemented by the String class, among others. Thus, you can pass a string to matcher( ). Matcher The Matcher class has no constructors. Instead, you create a Matcher by calling the matcher( ) factory method defined by Pattern, as just explained. Once you have created a Matcher, you will use its methods to perform various pattern matching operations. Several are described here"
Explain why lightweight components are more efficient and more flexible,"With very few exceptions, Swing components are lightweight. This means that they are written entirely in Java and do not map directly to platform-specific peers. Hence, lightweight components are more efficient and more flexible."," More than anything else, it is these two features that define the essence of Swing. Each is examined here. Swing Components Are Lightweight With very few exceptions, Swing components are lightweight. This means that they are written entirely in Java and do not map directly to platform-specific peers. Thus, lightweight components are more efficient and more flexible. Furthermore, because lightweight components do not translate into native peers, the look and feel of each component is determined by Swing, not by the underlying operating system. As a result, each component will work in a consistent manner across all platforms. Swing Supports a Pluggable Look and Feel Swing supports a pluggable look and feel (PLAF). Because each Swing component is rendered by Java code rather than by native peers, the look and feel of a component is under the control of Swing. This fact means that it is possible to separate the look and feel of a component from the logic of the component, and this is what Swing does. Separating out the look and feel provides a significant advantage: it becomes possible to change the way that a 22/09/21 6:42 PM component is rendered without affecting any of its other aspects. In other words, it is possible to plug in a new look and feel for any given component without creating any side effects in the code that uses that component. Moreover, it becomes possible to define entire sets of look-and-feels that represent different GUI styles. To use a specific style, its look and feel is simply plugged in. Once this is done, all components are automatically rendered using that style"
Explain why a container is a special type of component that is designed to hold other components,"A Swing GUI consists of two key items: components and containers. However, this distinction is mostly conceptual because all containers are also components. The difference between the two is found in their intended purpose: As the term is commonly used, a component is an independent visual control, such as a push button or slider. A container holds a group of components. Hence, a container is a special type of component that is designed to hold other components."," To support the Model-Delegate architecture, most Swing components contain two objects. The first represents the model. The second represents the UI delegate. Models are defined by interfaces. For example, the model for a button is defined by the ButtonModel interface. UI delegates are classes that inherit ComponentUI. For example, the UI delegate for a button is ButtonUI. Normally, your programs will not interact directly with the UI delegate. Components and Containers A Swing GUI consists of two key items: components and containers. However, this distinction is mostly conceptual because all containers are also components. The difference between the two is found in their intended purpose: As the term is commonly used, a component is an independent visual control, such as a push button or slider. A container holds a group of components. Thus, a container is a special type of component that is designed to hold other components. Furthermore, in order for a component to be displayed, it must be held within a container. Thus, all Swing GUIs will have at least one container. Because containers are components, a container can also hold other containers. This enables Swing to define what is called a containment hierarchy, at the top of which must be a top-level container. Lets look a bit more closely at components and containers"
Explain why all Swing GUIs will have at least one container,"A Swing GUI consists of two key items: components and containers. However, this distinction is mostly conceptual because all containers are also components. The difference between the two is found in their intended purpose: As the term is commonly used, a component is an independent visual control, such as a push button or slider. A container holds a group of components. Thus, a container is a special type of component that is designed to hold other components. Furthermore, in order for a component to be displayed, it must be held within a container. Hence, all Swing GUIs will have at least one container."," To support the Model-Delegate architecture, most Swing components contain two objects. The first represents the model. The second represents the UI delegate. Models are defined by interfaces. For example, the model for a button is defined by the ButtonModel interface. UI delegates are classes that inherit ComponentUI. For example, the UI delegate for a button is ButtonUI. Normally, your programs will not interact directly with the UI delegate. Components and Containers A Swing GUI consists of two key items: components and containers. However, this distinction is mostly conceptual because all containers are also components. The difference between the two is found in their intended purpose: As the term is commonly used, a component is an independent visual control, such as a push button or slider. A container holds a group of components. Thus, a container is a special type of component that is designed to hold other components. Furthermore, in order for a component to be displayed, it must be held within a container. Thus, all Swing GUIs will have at least one container. Because containers are components, a container can also hold other containers. This enables Swing to define what is called a containment hierarchy, at the top of which must be a top-level container. Lets look a bit more closely at components and containers"
Explain why you can use lightweight containers such as JPanel to create subgroups of related controls that are contained within an outer container,"The second type of containers supported by Swing are lightweight containers. Lightweight containers do inherit JComponent. An example of a lightweight container is JPanel, which is a general-purpose container. Lightweight containers are often used to organize and manage groups of related components because a lightweight container can be contained within another container. Hence, you can use lightweight containers such as JPanel to create subgroups of related controls that are contained within an outer container."," A top-level container is not contained within any other container. Furthermore, every containment hierarchy must begin with a top-level container. The one most commonly used for applications is JFrame. In the past, the one used for applets was JApplet. As explained in removal. As a result, JApplet is also deprecated for removal. Furthermore, beginning with JDK 11, applet support has been removed. The second type of containers supported by Swing are lightweight containers. Lightweight containers do inherit JComponent. An example of a lightweight container is JPanel, which is a general-purpose container. Lightweight containers are often used to organize and manage groups of related components because a lightweight container can be contained within another container. Thus, you can use lightweight containers such as JPanel to create subgroups of related controls that are contained within an outer container. Containers The Top-Level Container Panes Each top-level container defines a set of panes. At the top of the hierarchy is an instance of JRootPane. JRootPane is a lightweight container whose purpose is to manage the other panes. It also helps manage the optional menu bar. The panes that comprise the root pane are called the glass pane, the content pane, and the layered pane"
"Explain why the layered pane lets you specify a Z-order for a component, although this is not something that you will usually need to do","The layered pane is an instance of JLayeredPane. The layered pane allows components to be given a depth value. This value determines which component overlays another. (Hence, the layered pane lets you specify a Z-order for a component, although this is not something that you will usually need to do."," By default, it is a transparent instance of JPanel. The glass pane enables you to manage mouse events that affect the entire container (rather than an individual control) or to paint over any other component, for example. In most cases, you wont need to use the glass pane directly, but it is there if you need it. 22/09/21 6:42 PM 1066PART III The layered pane is an instance of JLayeredPane. The layered pane allows components to be given a depth value. This value determines which component overlays another. (Thus, the layered pane lets you specify a Z-order for a component, although this is not something that you will usually need to do.) The layered pane holds the content pane and the (optional) menu bar. Although the glass pane and the layered panes are integral to the operation of a top-level container and serve important purposes, much of what they provide occurs behind the scene"
"Explain why although event handlers are defined by your program, they are called on a thread that was not created by your program"," The handler is executed on the event dispatching thread provided by Swing and not on the main thread of the application. Hence, although event handlers are defined by your program, they are called on a thread that was not created by your program."," An event is passed to the application by calling an event handler defined by the application. However, the handler is executed on the event dispatching thread provided by Swing and not on the main thread of the application. Thus, although event handlers are defined by your program, they are called on a thread that was not created by your program. To avoid problems (including the potential for deadlock), all Swing GUI components must be created and updated from the event dispatching thread, not the main thread of the application. However, main( ) is executed on the main thread. Thus, main( ) cannot directly instantiate a SwingDemo object. Instead, it must create a Runnable object that executes on the event dispatching thread and have this object create the GUI"
Explain why main( ) cannot directly instantiate a SwingDemo object,"To avoid problems (including the potential for deadlock), all Swing GUI components must be created and updated from the event dispatching thread, not the main thread of the application. However, main( ) is executed on the main thread. Hence, main( ) cannot directly instantiate a SwingDemo object."," However, the handler is executed on the event dispatching thread provided by Swing and not on the main thread of the application. Thus, although event handlers are defined by your program, they are called on a thread that was not created by your program. To avoid problems (including the potential for deadlock), all Swing GUI components must be created and updated from the event dispatching thread, not the main thread of the application. However, main( ) is executed on the main thread. Thus, main( ) cannot directly instantiate a SwingDemo object. Instead, it must create a Runnable object that executes on the event dispatching thread and have this object create the GUI. To enable the GUI code to be created on the event dispatching thread, you must use one of two methods that are defined by the SwingUtilities class. These methods are invokeLater( ) and invokeAndWait( ). They are shown here: static void invokeLater(Runnable obj) static void invokeAndWait(Runnable obj) throws InterruptedException, InvocationTargetException Here, obj is a Runnable object that will have its run( ) method called by the event dispatching thread. The difference between the two methods is that invokeLater( ) returns immediately, but invokeAndWait( ) waits until obj.run( ) returns. You can use one of these methods to call a method that constructs the GUI for your Swing application, or whenever you need to modify the state of the GUI from code not executed by the event dispatching thread. You will normally want to use invokeLater( ), as the preceding program does. However, when the 22/09/21 6:42 PM initial GUI for an applet is constructed, invokeAndWait( ) is required. Thus, you will see its use in legacy applet code"
"Explain why JButton provides the addActionListener( ) method, which is used to add an action listener"," When a push button is pressed, it generates an ActionEvent. Hence, JButton provides the addActionListener( ) method, which is used to add an action listener."," Swing push buttons are instances of JButton. JButton supplies several constructors. The one used here is The msg parameter specifies the string that will be displayed inside the button. When a push button is pressed, it generates an ActionEvent. Thus, JButton provides the addActionListener( ) method, which is used to add an action listener. (JButton also provides removeActionListener( ) to remove a listener, but this method is not used by the program.) As explained in Chapter 25, the ActionListener interface defines only one method: actionPerformed( ). It is shown again here for your convenience: void actionPerformed(ActionEvent ae) This method is called when a button is pressed. In other words, it is the event handler that is called when a button press event has occurred. Next, event listeners for the buttons action events are added by the code shown here: JButton(String msg) // Add action listener for Alpha"
Explain why an object of type ImageIcon can be passed as an argument to the Icon parameter of JLabels constructor," Notice that icons are specified by objects of type Icon, which is an interface defined by Swing. The easiest way to obtain an icon is to use the ImageIcon class. ImageIcon implements Icon and encapsulates an image. Hence, an object of type ImageIcon can be passed as an argument to the Icon parameter of JLabels constructor."," JLabel and ImageIcon JLabel is Swings easiest-to-use component. It creates a label and was introduced in the preceding chapter. Here, we will look at JLabel a bit more closely. JLabel can be used to display text and/or an icon. It is a passive component in that it does not respond to user input. JLabel defines several constructors. Here are three of them: JLabel(Icon icon) JLabel(String str) JLabel(String str, Icon icon, int align) 33-ch33.indd 1079 CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 Introducing GUI Programming with Swing Here, str and icon are the text and icon used for the label. The align argument specifies the horizontal alignment of the text and/or icon within the dimensions of the label. It must be one of the following values: LEFT, RIGHT, CENTER, LEADING, or TRAILING. These constants are defined in the SwingConstants interface, along with several others used by the Swing classes. Notice that icons are specified by objects of type Icon, which is an interface defined by Swing. The easiest way to obtain an icon is to use the ImageIcon class. ImageIcon implements Icon and encapsulates an image. Thus, an object of type ImageIcon can be passed as an argument to the Icon parameter of JLabels constructor. There are several ways to provide the image, including reading it from a file or downloading it from a URL. Here is the ImageIcon constructor used by the example in this section: ImageIcon(String filename) It obtains the image in the file named filename. The icon and text associated with the label can be obtained by the following methods: Icon getIcon( ) String getText( ) The icon and text associated with a label can be set by these methods: void setIcon(Icon icon) void setText(String str) Here, icon and str are the icon and text, respectively. Therefore, using setText( ) it is possible to change the text inside a label during program execution"
Explain why the viewport displays the visible portion of the component being scrolled,"The viewable area of a scroll pane is called the viewport. It is a window in which the component being scrolled is displayed. Hence, the viewport displays the visible portion of the component being scrolled."," class CitiesPanel extends JPanel { } class ColorsPanel extends JPanel { public ColorsPanel() { JCheckBox cb1 = new JCheckBox(""Red""); add(cb1); JCheckBox cb2 = new JCheckBox(""Green""); add(cb2); JCheckBox cb3 = new JCheckBox(""Blue""); add(cb3); } } class FlavorsPanel extends JPanel { public FlavorsPanel() { JComboBox<String> jcb = new JComboBox<String>(); jcb.addItem(""Vanilla""); jcb.addItem(""Chocolate""); 22/09/21 6:43 PM 1094PART III jcb.addItem(""Strawberry""); add(jcb); } } JScrollPane JScrollPane is a lightweight container that automatically handles the scrolling of another component. The component being scrolled can be either an individual component, such as a table, or a group of components contained within another lightweight container, such as a JPanel. In either case, if the object being scrolled is larger than the viewable area, horizontal and/or vertical scroll bars are automatically provided, and the component can be scrolled through the pane. Because JScrollPane automates scrolling, it usually eliminates the need to manage individual scroll bars. The viewable area of a scroll pane is called the viewport. It is a window in which the component being scrolled is displayed. Thus, the viewport displays the visible portion of the component being scrolled. The scroll bars scroll the component through the viewport. In its default behavior, a JScrollPane will dynamically add or remove a scroll bar as needed. For example, if the component is taller than the viewport, a vertical scroll bar is added. If the component will completely fit within the viewport, the scroll bars are removed. JScrollPane defines several constructors. The one used in this chapter is shown here: JScrollPane(Component comp) The component to be scrolled is specified by comp. Scroll bars are automatically displayed when the content of the pane exceeds the dimensions of the viewport"
Explain why a JMenuItem defines a selection that can be chosen by the user,"JMenu object defines a menu. That is, each JMenu object contains one or more selectable items. The items displayed by a JMenu are objects of JMenuItem. Hence, a JMenuItem defines a selection that can be chosen by the user."," A menu that is typically activated by right-clicking the mouse. Table 34-1 The Core Swing Menu Classes Here is a brief overview of how the classes fit together. To create the top-level menu for an application, you first create a JMenuBar object. This class is, loosely speaking, a container for menus. To the JMenuBar instance, you will add instances of JMenu. Each JMenu object defines a menu. That is, each JMenu object contains one or more selectable items. The items displayed by a JMenu are objects of JMenuItem. Thus, a JMenuItem defines a selection that can be chosen by the user. As an alternative or adjunct to menus that descend from the menu bar, you can also create stand-alone, popup menus. To create a popup menu, first create an object of type JPopupMenu. Then, add JMenuItems to it. A popup menu is normally activated by clicking the right mouse button when the mouse is over a component for which a popup menu has been defined"
Explain why you can determine which item was selected by examining the action command,"As mentioned in passing previously, when a menu item is selected, an action event is generated. The action command string associated with that action event will, by default, be the name of the selection. Hence, you can determine which item was selected by examining the action command."," Another key point is that JMenuItem is a superclass of JMenu. This allows the creation of submenus, which are, essentially, menus within menus. To create a submenu, you first create and populate a JMenu object and then add it to another JMenu object. You will see this process in action in the following section. As mentioned in passing previously, when a menu item is selected, an action event is generated. The action command string associated with that action event will, by default, be the name of the selection. Thus, you can determine which item was selected by examining 22/09/21 6:43 PM the action command. Of course, you can also use separate anonymous inner classes or lambda expressions to handle each menu items action events. In this case, the menu selection is already known, and there is no need to examine the action command string to determine which item was selected. Menus can also generate other types of events. For example, each time that a menu is activated, selected, or canceled, a MenuEvent is generated that can be listened for via a MenuListener. Other menu-related events include MenuKeyEvent, MenuDragMouseEvent, and PopupMenuEvent. In many cases, however, you need only watch for action events, and in this chapter, we will use only action events"
Explain why a mnemonic allows you to use the keyboard to select an item from a menu that is already being displayed,"In real applications, a menu usually includes support for keyboard shortcuts because they give an experienced user the ability to select menu items rapidly. Keyboard shortcuts come in two forms: mnemonics and accelerators. As it applies to menus, a mnemonic defines a key that lets you select an item from an active menu by typing the key. Hence, a mnemonic allows you to use the keyboard to select an item from a menu that is already being displayed."," Add Mnemonics and Accelerators to Menu Items The menu created in the preceding example is functional, but it is possible to make it better. In real applications, a menu usually includes support for keyboard shortcuts because they give an experienced user the ability to select menu items rapidly. Keyboard shortcuts come in two forms: mnemonics and accelerators. As it applies to menus, a mnemonic defines a key that lets you select an item from an active menu by typing the key. Thus, a mnemonic allows you to use the keyboard to select an item from a menu that is already being displayed. An accelerator is a key that lets you select a menu item without having to first activate the menu. A mnemonic can be specified for both JMenuItem and JMenu objects. There are two ways to set the mnemonic for JMenuItem. First, it can be specified when an object is constructed using this constructor: JMenuItem(String name, int mnem) In this case, the name is passed in name and the mnemonic is passed in mnen. Second, you can set the mnemonic by calling setMnemonic( ). To specify a mnemonic for JMenu, you must call setMnemonic( ). This method is inherited by both classes from AbstractButton and is shown next: void setMnemonic(int mnem) Here, mnem specifies the mnemonic. It should be one of the constants defined in java.awt.event.KeyEvent, such as KeyEvent.VK_F or KeyEvent.VK_Z. (There is another version of setMnemonic( ) that takes a char argument, but it is considered obsolete.) Mnemonics are not case sensitive, so in the case of VK_A, typing either a or A will work"
Explain why the popup trigger is usually caused by right-clicking the mouse on a popup menuenabled component," A popup menu is normally activated by clicking the right mouse button when the mouse pointer is over a component for which a popup menu is defined. Hence, the popup trigger is usually caused by right-clicking the mouse on a popup menuenabled component."," Lets examine each of these steps closely. A popup menu is normally activated by clicking the right mouse button when the mouse pointer is over a component for which a popup menu is defined. Thus, the popup trigger is usually caused by right-clicking the mouse on a popup menuenabled component. To listen for the popup trigger, implement the MouseListener interface and then register the listener by calling the addMouseListener( ) method. As described in Chapter 25, MouseListener defines the methods shown here: void mouseClicked(MouseEvent me) void mouseEntered(MouseEvent me) void mouseExited(MouseEvent me) void mousePressed(MouseEvent me) void mouseReleased(MouseEvent me) Of these, two are very important relative to the popup menu: mousePressed( ) and mouseReleased( ). Depending on the installed look and feel, either of these two events can trigger a popup menu. For this reason, it is often easier to use a MouseAdapter to implement the MouseListener interface and simply override mousePressed( ) and mouseReleased( ). The MouseEvent class defines several methods, but only four are commonly needed when activating a popup menu. They are shown here: int getX( ) int getY( ) boolean isPopupTrigger( ) Component getComponent( ) The current X,Y location of the mouse relative to the source of the event is found by calling getX( ) and getY( ). These are used to specify the upper-left corner of the popup menu when it is displayed. The isPopupTrigger( ) method returns true if the mouse event represents a popup trigger and false otherwise. You will use this method to determine when to pop up the menu. To obtain a reference to the component that generated the mouse event, call getComponent( )"
Explain why data members of a Bean specified as transient will not be serialized," When using automatic serialization, you can prevent a field from being saved through the use of the transient keyword. Hence, data members of a Bean specified as transient will not be serialized."," The easiest way to serialize a Bean is to have it implement the java.io.Serializable interface, which is simply a marker interface. Implementing java.io.Serializable makes serialization automatic. Your Bean need take no other action. Automatic serialization can also be inherited. Therefore, if any superclass of a Bean implements java.io.Serializable, then automatic serialization is obtained. When using automatic serialization, you can prevent a field from being saved through the use of the transient keyword. Thus, data members of a Bean specified as transient will not be serialized. If a Bean does not implement java.io.Serializable, you must provide serialization yourself, such as by implementing java.io.Externalizable. Otherwise, containers cannot save the configuration of your component"
"Explain why you can specify a reference to a module, package, class, or interface in addition to a reference to a specific method or field","In the first form, anchor is a link to an absolute or relative URL. In the second form, mod-name/pkg-name.class-name#member-name specifies the name of the item, and text is the text displayed for that item. The text parameter is optional, and if not used, then the item specified by mod-name/pkg-name.class-name#member-name is displayed. The member name, too, is optional. Hence, you can specify a reference to a module, package, class, or interface in addition to a reference to a specific method or field."," Thus, the tag can be used only in documentation for a method. JDK 16 added an inline tag version: {@return explanation} This form must be at the top of the methods documentation comment. @see The @see tag provides a reference to additional information. Two commonly used forms are shown here: @see anchor @see mod-name/pkg-name.class-name#member-name text In the first form, anchor is a link to an absolute or relative URL. In the second form, mod-name/pkg-name.class-name#member-name specifies the name of the item, and text is the text displayed for that item. The text parameter is optional, and if not used, then the item specified by mod-name/pkg-name.class-name#member-name is displayed. The member name, too, is optional. Thus, you can specify a reference to a module, package, class, or interface in addition to a reference to a specific method or field. The name can be fully qualified or partially qualified. However, the dot that precedes the member name (if it exists) must be replaced by a hash character. There is a third form of @see that lets you simply specify a text-based description. @serial @serial description Here, description is the comment for that field. Two other forms, shown here, let you indicate if a class or package will be part of the Serialized Form documentation page"
"Explain why if a code fragment can be executed by JShell, then that fragment represents valid Java code","In addition to statements and variable declarations, JShell lets you declare classes and methods, and use import statements. Examples are shown in the following sections. One other point: Any code that is valid for JShell will also be valid for compilation by javac, assuming the necessary framework is provided to create a complete program. Hence, if a code fragment can be executed by JShell, then that fragment represents valid Java code."," In addition to demonstrating the use of a variable, the preceding example illustrates another important aspect of JShell: it maintains state information. In this case, count is assigned the value 10 in one statement and then this value is used in the expression 1.0 / count in the subsequent call to println( ) in a second statement. Between these two statements, JShell stores counts value. In general, JShell maintains the current state and effect of the code snippets that you enter. This lets you experiment with larger code fragments that span multiple lines. Before moving on, lets try one more example. In this case, we will create a for loop that uses the count variable. Begin by entering this line at the prompt: for(count = 0; count < 5; count++) This indicates that additional code is required to finish the statement. In this case, the target of the for loop must be provided. Enter the following: At this point, JShell responds with the following prompt: After entering this line, the for statement is complete and both lines are executed. You will see the following output: 1 3 21/09/21 6:00 PM 1194PART V In addition to statements and variable declarations, JShell lets you declare classes and methods, and use import statements. Examples are shown in the following sections. One other point: Any code that is valid for JShell will also be valid for compilation by javac, assuming the necessary framework is provided to create a complete program. Thus, if a code fragment can be executed by JShell, then that fragment represents valid Java code. In other words, JShell code is Java code. List, Edit, and Rerun Code JShell supports a large number of commands that let you control the operation of JShell. At this point, three are of particular interest because they let you list the code that you have entered, edit a line of code, and rerun a code snippet. As the subsequent examples become longer, you will find these commands to be very helpful"
"Explain why even if you are an experienced pro, you will still find JShell helpful whenever you need to explore new areas","JShell is not just for beginners. It also excels when prototyping code. Hence, even if you are an experienced pro, you will still find JShell helpful whenever you need to explore new areas."," JShell provides several commands that let you list various elements of your work. They are shown here: Command /types /imports /methods /vars For example, if you entered the following lines: int start = 0; int end = 10; int count = 5; and then entered the /vars command, you would see | | | int start = 0; int end = 10; int count = 5; Another often useful command is /history. It lets you view the history of the current session. The history contains a list of what you have typed at the command prompt. Exploring JShell Further The best way to get proficient with JShell is to work with it. Try entering several different Java constructs and watching the way that JShell responds. As you experiment with JShell, you will find the usage patterns that work best for you. This will enable you to find effective ways to integrate JShell into your learning or development process. Also, keep in mind that JShell is not just for beginners. It also excels when prototyping code. Thus, even if you are an experienced pro, you will still find JShell helpful whenever you need to explore new areas. Simply put: JShell is an important tool that further enhances the overall Java development experience"
Explain why Mini-batch GD will end up walking around a bit closer to the minimum than SGD," The algorithms progress in parameter space is less erratic than with SGD, especially with fairly large mini-batches. Hence, Mini-batch GD will end up walking around a bit closer to the minimum than SGD."," To perform Linear Regression using SGD with Scikit-Learn, you can use the SGDRe gressor class, which defaults to optimizing the squared error cost function. The fol lowing code runs 50 epochs, starting with a learning rate of 0.1 (eta0=0.1), using the default learning schedule (different from the preceding one), and it does not use any regularization (penalty=None; more details on this shortly): from sklearn.linear_model import SGDRegressor sgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1) sgd_reg.fit(X, y.ravel()) Once again, you find a solution very close to the one returned by the Normal Equa tion: >>> sgd_reg.intercept_, sgd_reg.coef_ (array([ 4.18380366]), array([ 2.74205299])) Mini-batch Gradient Descent The last Gradient Descent algorithm we will look at is called Mini-batch Gradient Descent. It is quite simple to understand once you know Batch and Stochastic Gradi ent Descent: at each step, instead of computing the gradients based on the full train ing set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini- | Download from finelybook www.finelybook.com batch GD computes the gradients on small random sets of instances called minibatches. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs. The algorithms progress in parameter space is less erratic than with SGD, especially with fairly large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum than SGD. But, on the other hand, it may be harder for it to escape from local minima (in the case of problems that suffer from local minima, unlike Linear Regression as we saw earlier). Figure 4-11 shows the paths taken by the three Gradient Descent algorithms in parameter space during training. They all end up near the minimum, but Batch GDs path actually stops at the minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, dont forget that Batch GD takes a lot of time to take each step, and Stochas tic GD and Mini-batch GD would also reach the minimum if you used a good learn ing schedule"
Explain why it does not scale as well as bag ging or pasting," There is one important drawback to this sequential learning techni que: it cannot be parallelized (or only partially), since each predic tor can only be trained after the previous predictor has been trained and evaluated. Hence, it does not scale as well as bag ging or pasting."," Once all predictors are trained, the ensemble makes predictions very much like bag ging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set. There is one important drawback to this sequential learning techni que: it cannot be parallelized (or only partially), since each predic tor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bag ging or pasting. Lets take a closer look at the AdaBoost algorithm. Each instance weight w(i) is initially set to m . A first predictor is trained and its weighted error rate r1 is computed on the training set; see Equation 7-1"
Explain why all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space,"In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated (as discussed earlier for MNIST). Hence, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space."," Main Approaches for Dimensionality Reduction Before we dive into specific dimensionality reduction algorithms, lets take a look at the two main approaches to reducing dimensionality: projection and Manifold Learning. Projection In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated (as discussed earlier for MNIST). As a result, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space. This sounds very abstract, so lets look at an example. In Figure 8-2 you can see a 3D data set represented by the circles. | Download from finelybook www.finelybook.com Notice that all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of the high-dimensional (3D) space. Now if we project every training instance perpendicularly onto this subspace (as represented by the short lines con necting the instances to the plane), we get the new 2D dataset shown in Figure 8-3"
Explain why the 2D projec tion looks very much like the original 3D dataset,"Once you have identified all the principal components, you can reduce the dimen sionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible. For example, in Figure 8-2 the 3D dataset is projected down to the 2D plane defined by the first two principal com ponents, preserving a large part of the datasets variance. Hence, the 2D projec tion looks very much like the original 3D dataset."," Equation 8-1. Principal components matrix = 1 2 T The following Python code uses NumPys svd() function to obtain all the principal components of the training set, then extracts the first two PCs: X_centered = X - X.mean(axis=0) U, s, V = np.linalg.svd(X_centered) c1 = V.T[:, 0] c2 = V.T[:, 1] PCA assumes that the dataset is centered around the origin. As we will see, Scikit-Learns PCA classes take care of centering the data for you. However, if you implement PCA yourself (as in the pre ceding example), or if you use other libraries, dont forget to center the data first. Projecting Down to d Dimensions Once you have identified all the principal components, you can reduce the dimen sionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible. For example, in Figure 8-2 the 3D dataset is projected down to the 2D plane defined by the first two principal com ponents, preserving a large part of the datasets variance. As a result, the 2D projec tion looks very much like the original 3D dataset. To project the training set onto the hyperplane, you can simply compute the dot product of the training set matrix X by the matrix Wd, defined as the matrix contain PCA Download from finelybook www.finelybook.com ing the first d principal components (i.e., the matrix composed of the first d columns of VT), as shown in Equation 8-2"
"Explain why the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution"," Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. Hence, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution."," With these tools, you will be able to train very deep nets: welcome to Deep Learning! Vanishing/Exploding Gradients Problems As we discussed in Chapter 10, the backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way. Once the algorithm has computed the gradient of the cost function with regards to each Download from finelybook www.finelybook.com parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step. Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the vanishing gradients problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the exploding gradients prob lem, which is mostly encountered in recurrent neural networks (see Chapter 14). More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds"
"Explain why each neuron in the coding layer typically ends up representing a useful feature (if you could speak only a few words per month, you would probably try to make them worth listening to)","Another kind of constraint that often leads to good feature extraction is sparsity: by adding an appropriate term to the cost function, the autoencoder is pushed to reduce the number of active neurons in the coding layer. For example, it may be pushed to have on average only 5% significantly active neurons in the coding layer. This forces the autoencoder to represent each input as a combination of a small number of acti vations. Hence, each neuron in the coding layer typically ends up representing a useful feature (if you could speak only a few words per month, you would probably try to make them worth listening to)."," Implementing the dropout version, which is more common, is not much harder: from tensorflow.contrib.layers import dropout keep_prob = 0.7 is_training = tf.placeholder_with_default(False, shape=(), name='is_training') X = tf.placeholder(tf.float32, shape=[None, n_inputs]) X_drop = dropout(X, keep_prob, is_training=is_training) [...] hidden1 = activation(tf.matmul(X_drop, weights1) + biases1) [...] reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE [...] During training we must set is_training to True (as explained in Chapter 11) using the feed_dict: sess.run(training_op, feed_dict={X: X_batch, is_training: True}) However, during testing it is not necessary to set is_training to False, since we set that as the default in the call to the placeholder_with_default() function. | Download from finelybook www.finelybook.com Another kind of constraint that often leads to good feature extraction is sparsity: by adding an appropriate term to the cost function, the autoencoder is pushed to reduce the number of active neurons in the coding layer. For example, it may be pushed to have on average only 5% significantly active neurons in the coding layer. This forces the autoencoder to represent each input as a combination of a small number of acti vations. As a result, each neuron in the coding layer typically ends up representing a useful feature (if you could speak only a few words per month, you would probably try to make them worth listening to). In order to favor sparse models, we must first measure the actual sparsity of the cod ing layer at each training iteration. We do so by computing the average activation of each neuron in the coding layer, over the whole training batch. The batch size must not be too small, or else the mean will not be accurate"
"Explain why rounding the codings to 0 or 1 wont distort them too much, and this will improve the reliability of the hashes","In order to preserve a high signal-to-noise ratio, the autoencoder will learn to feed large values to the coding layer (so that the noise becomes negligible). In turn, this means that the logistic function of the coding layer will likely satu rate at 0 or 1. Hence, rounding the codings to 0 or 1 wont distort them too much, and this will improve the reliability of the hashes."," One neat trick proposed by Salakhutdinov and Hinton is to add Gaussian noise (with zero mean) to the inputs of the coding layer, during training only. In order to preserve a high signal-to-noise ratio, the autoencoder will learn to feed large values to the coding layer (so that the noise becomes negligible). In turn, this means that the logistic function of the coding layer will likely satu rate at 0 or 1. As a result, rounding the codings to 0 or 1 wont distort them too much, and this will improve the reliability of the hashes. Download from finelybook www.finelybook.com Compute the hash of every image, and see if images with identical hashes look alike. Since MNIST and CIFAR10 are labeled, a more objective way to measure the performance of the autoencoder for semantic hashing is to ensure that images with the same hash generally have the same class. One way to do this is to measure the average Gini purity (introduced in Chapter 6) of the sets of images with identical (or very similar) hashes"
Explain why each predictor will be trained on a random subset of the input features,"The BaggingClassifier class supports sampling the features as well. This is con trolled by two hyperparameters: max_features and bootstrap_features. They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Hence, each predictor will be trained on a random subset of the input features."," , 1. ], [ 0.48958333, 0.51041667]]) Random Patches and Random Subspaces The BaggingClassifier class supports sampling the features as well. This is con trolled by two hyperparameters: max_features and bootstrap_features. They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the Random Patches method.7 Keeping all training instances (i.e., bootstrap=False and max_sam ples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_fea tures smaller than 1.0) is called the Random Subspaces method.8 7 Ensembles on Random Patches, G. Louppe and P. Geurts (2012)"
"Explain why we cannot trust our subjective experience: per ception is not trivial at all, and to understand it we must look at how the sensory modules work","Although IBMs Deep Blue supercomputer beat the chess world champion Garry Kas parov back in 1996, until quite recently computers were unable to reliably perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing spoken words. Why are these tasks so effortless to us humans? The answer lies in the fact that perception largely takes place outside the realm of our consciousness, within special ized visual, auditory, and other sensory modules in our brains. By the time sensory information reaches our consciousness, it is already adorned with high-level features; for example, when you look at a picture of a cute puppy, you cannot choose not to see the puppy, or not to notice its cuteness. Nor can you explain how you recognize a cute puppy; its just obvious to you. Hence, we cannot trust our subjective experience: per ception is not trivial at all, and to understand it we must look at how the sensory modules work."," Next, try again using synchronous updates. Do synchronous updates produce a better model? Is training faster? Split the DNN vertically and place each vertical slice on a different device, and train the model again. Is training any faster? Is the performance any different? Solutions to these exercises are available in Appendix A. | Download from finelybook www.finelybook.com Convolutional Neural Networks Although IBMs Deep Blue supercomputer beat the chess world champion Garry Kas parov back in 1996, until quite recently computers were unable to reliably perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing spoken words. Why are these tasks so effortless to us humans? The answer lies in the fact that perception largely takes place outside the realm of our consciousness, within special ized visual, auditory, and other sensory modules in our brains. By the time sensory information reaches our consciousness, it is already adorned with high-level features; for example, when you look at a picture of a cute puppy, you cannot choose not to see the puppy, or not to notice its cuteness. Nor can you explain how you recognize a cute puppy; its just obvious to you. Thus, we cannot trust our subjective experience: per ception is not trivial at all, and to understand it we must look at how the sensory modules work. Convolutional neural networks (CNNs) emerged from the study of the brains visual cortex, and they have been used in image recognition since the 1980s. In the last few years, thanks to the increase in computational power, the amount of available training data, and the tricks presented in Chapter 11 for training deep nets, CNNs have man aged to achieve superhuman performance on some complex visual tasks. They power image search services, self-driving cars, automatic video classification systems, and more. Moreover, CNNs are not restricted to visual perception: they are also successful at other tasks, such as voice recognition or natural language processing (NLP); however, we will focus on visual applications for now"
"Explain why, community evolution is a challenging task and often means to evaluating detected communities in the absence of ground truth","Similar to real-world friendships, social media interactions evolve over time. People join or leave groups; groups expand, shrink, dissolve, or split over time. Studying the temporal behavior of communities is necessary for a deep understanding of communities in social media. 3. How can we evaluate detected communities? As emphasized in our botnet example, the list of community members (i.e., ground truth) is rarely known. Hence,  community evolution is a challenging task and often means to evaluating detected communities in the absence of ground truth."," Similar to real-world friendships, social media interactions evolve over time. People join or leave groups; groups expand, shrink, dissolve, or split over time. Studying the temporal behavior of communities is necessary for a deep understanding of communities in social media. 3. How can we evaluate detected communities? As emphasized in our botnet example, the list of community members (i.e., ground truth) is rarely known. Hence, community evolution is a challenging task and often means to evaluating detected communities in the absence of ground truth. Social Communities ZACHARYS KARATE CLUB Broadly speaking, a real-world community is a body of individuals with common economic, social, or political interests/characteristics, often living in relative proximity. A virtual community comes into existence when likeminded users on social media form a link and start interacting with each other. In other words, formation of any community requires (1) a set of at least two nodes sharing some interest and (2) interactions with respect to that interest"
"Explain why, we use the label of the majority a community against the label of each member of the community to evaluate the algorithm"," Purity In purity, we assume that the majority of a community represents the community. Hence,  we use the label of the majority a community against the label of each member of the community to evaluate the algorithm."," P+R Computed for the same example, we get F = 0.54. Purity In purity, we assume that the majority of a community represents the community. Hence, we use the label of the majority a community against the label of each member of the community to evaluate the algorithm. For P1: WQS CUUS2079-06 CUUS2079-Zafarani 6.3 Community Evaluation instance, in Figure 6.15, the majority in Community 1 is ; therefore, we assume majority label for that community. The purity is then defined as the fraction of instances that have labels equal to their communitys majority label. Formally, Purity = k 1  max |Ci L j |, N i=1 j where k is the number of communities, N is the total number of nodes, L j is the set of instances with label j in all communities, and Ci is the set of members in community i. In the case of our example, purity is = 0.75. Normalized Mutual Information Purity can be easily manipulated to generate high values; consider when nodes represent singleton communities (of size 1) or when we have very large pure communities (ground truth = majority label). In both cases, purity does not make sense because it generates high values"
"Explain why, we discuss two techniques: one for nominal attributes and one for ordinal attributes","Consider a network where individuals are friends with people of different ages. Unlike races, individuals are more likely to be friends with others close in age, but not necessarily with ones of the exact same age. Hence,  we discuss two techniques: one for nominal attributes and one for ordinal attributes."," P1: CUUS2079-08 CUUS2079-Zafarani Gutter: 0.75in January 13, 2014 percent of the links exist between members of the same race (from [Currarini et al., 2009]). Consider a network where individuals are friends with people of different ages. Unlike races, individuals are more likely to be friends with others close in age, but not necessarily with ones of the exact same age. Hence, we discuss two techniques: one for nominal attributes and one for ordinal attributes. 8.1.1 Measuring Assortativity for Nominal Attributes Consider a scenario where we have nominal attributes assigned to nodes"
"Explain why, one can formulate many of these behaviors as a link prediction problem","As discussed previously, most behaviors result in newly formed links in social media. It can be a link to a user, as in befriending behavior; a link to an entity, as in buying behavior; or a link to a community, as in joining behavior. Hence,  one can formulate many of these behaviors as a link prediction problem."," r Cascade Models (Chapter 7). Cascade models are examples of scenarios where an innovation, product, or information cascades through a network. The discussion with respect to cascade models is similar, to the threshold models with the exception that cascade models are sender-centric. That is, the sender decides to activate the receiver, whereas threshold models are receiver-centric, in which receivers get activated by multiple senders. Therefore, the computation of ICM parameters needs to be done from the senders point of view in cascade models. Note that both threshold and cascade models are examples of individual behavior modeling. 10.1.3 Individual Behavior Prediction As discussed previously, most behaviors result in newly formed links in social media. It can be a link to a user, as in befriending behavior; a link to an entity, as in buying behavior; or a link to a community, as in joining behavior. Hence, one can formulate many of these behaviors as a link prediction problem. Next, we discuss link prediction in social media. Link Prediction Link prediction assumes a graph G(V, E). Let e(u, v) E represent an interaction (edge) between nodes u and v, and let t(e) denote the time of P1: CUUS2079-10 CUUS2079-Zafarani Gutter: 0.75in January 13, 2014 the interaction. Let G[t1 , t2 ] represent the subgraph of G such that all edges are created between t1 and t2 (i.e., for all edges e in this subgraph, t1 < t(e) < t2 ). Now given four time stamps t1 < t1 < t2 < t2 , a link prediction algorithm is given the subgraph G[t1 , t1 ] (training interval) and is expected to predict edges in G[t2 , t2 ] (testing interval). Note that, just like new edges, new nodes can be introduced in social networks; therefore, G[t2 , t2 ] may contain nodes not present in G[t1 , t1 ]. Hence, a link prediction algorithm is generally constrained to predict edges only for pairs of nodes that are present during the training period. One can add extra constraints such as predicting links only for nodes that are incident to at least k edges (i.e., have degree greater or equal to k) during both testing and training intervals"
"Explain why, we can determine the correlation between features and migration behavior","Given two snapshots of a network, we know if users migrated or not. We can also compute the values for the aforementioned features. Hence,  we can determine the correlation between features and migration behavior."," These three features are correlated with the site attention migration behavior and one expects changes in them when migrations happen. Feature-Behavior Association Given two snapshots of a network, we know if users migrated or not. We can also compute the values for the aforementioned features. Hence, we can determine the correlation between features and migration behavior. Let vector Y Rn indicate whether any of our n users have migrated or not. Let X t R3n be the features collected (activity, friends, rank) for any one of these users at time stamp t. Then, the correlation between features X t and labels Y can be computed via logistic regression. How can we verify that this correlation is not random? Next, we discuss how we verify that this correlation is statistically significant"
"Explain why we are facing an exacerbated problem of big data  drowning in data, but thirsty for knowledge","With the rise of social media, the web has become a vibrant and lively realm in which billions of individuals all around the globe interact, share, post, and conduct numerous daily activities. Information is collected, curated, and published by citizen journalists and simultaneously shared or consumed by thousands of individuals, who give spontaneous feedback. Social media enables us to be connected and interact with each other anywhere and anytime allowing us to observe human behavior in an unprecedented scale with a new lens. This social media lens provides us with golden opportunities to understand individuals at scale and to mine human behavioral patterns otherwise impossible. As a byproduct, by understanding individuals better, we can design better computing systems tailored to individuals needs that will serve them and society better. This new social media world has no geographical boundaries and incessantly churns out oceans of data. Hence, we are facing an exacerbated problem of big data  drowning in data, but thirsty for knowledge."," We have truly enjoyed our collaboration in this arduous journey. We will certainly miss our weekly meetings and many missed deadlines. P1: qVa CUUS2079-01 CUUS2079-Zafarani 1 Introduction With the rise of social media, the web has become a vibrant and lively realm in which billions of individuals all around the globe interact, share, post, and conduct numerous daily activities. Information is collected, curated, and published by citizen journalists and simultaneously shared or consumed by thousands of individuals, who give spontaneous feedback. Social media enables us to be connected and interact with each other anywhere and anytime allowing us to observe human behavior in an unprecedented scale with a new lens. This social media lens provides us with golden opportunities to understand individuals at scale and to mine human behavioral patterns otherwise impossible. As a byproduct, by understanding individuals better, we can design better computing systems tailored to individuals needs that will serve them and society better. This new social media world has no geographical boundaries and incessantly churns out oceans of data. As a result, we are facing an exacerbated problem of big data drowning in data, but thirsty for knowledge. Can data mining come to the rescue? Unfortunately, social media data is significantly different from the traditional data that we are familiar with in data mining. Apart from enormous size, the mainly user-generated data is noisy and unstructured, with abundant social relations such as friendships and followers-followees. This new type of data mandates new computational data analysis approaches that can combine social theories with statistical and data mining methods. The pressing demand for new techniques ushers in and entails a new interdisciplinary field social media mining. SOCIAL MEDIA CITIZEN JOURNALISM 1.1 What is Social Media Mining Social media shatters the boundaries between the real world and the virtual world. We can now integrate social theories with computational methods to study how individuals (also known as social atoms) interact and how 16:30 Trim: 6.125in 9.25in CUUS2079-Zafarani SOCIAL MOLECULE SOCIAL MEDIA MINING DATA SCIENTIST Gutter: 0.75in January 13, 2014 communities (i.e., social molecules) form. The uniqueness of social media data calls for novel data mining techniques that can effectively handle usergenerated content with rich social relations. The study and development of these new techniques are under the purview of social media mining, an emerging discipline under the umbrella of data mining. Social Media Mining is the process of representing, analyzing, and extracting actionable patterns from social media data"
Explain why a community detection algorithm following this approach should assign members with similar characteristics to the same community,"The intuition behind member-based community detection is that members with the same (or similar) characteristics are more often in the same community. Hence, a community detection algorithm following this approach should assign members with similar characteristics to the same community."," Member-based community detection, uses community detection algorithms that group members based on attributes or measures such as similarity, degree, or reachability. In group-based community detection, we are interested in finding communities that are modular, balanced, dense, robust, or hierarchical. 6.1.2 Member-Based Community Detection The intuition behind member-based community detection is that members with the same (or similar) characteristics are more often in the same community. Therefore, a community detection algorithm following this approach should assign members with similar characteristics to the same community. P1: WQS CUUS2079-06 CUUS2079-Zafarani January 13, 2014 Group-Based Community Detection Member-Based Community Detection detection groups members based on their characteristics. Here, we divide the network based on color. In group-based community detection, we find communities based on group properties. Here, groups are formed based on the density of interactions among their members"
Explain why new computational methods are needed to mine the data," Social media data differs from traditional data we are familiar with in data mining. Hence, new computational methods are needed to mine the data."," January 14, 2014 P1: WQS CUUS2079-FM CUUS2079-Zafarani January 14, 2014 P1: WQS CUUS2079-FM CUUS2079-Zafarani Contents Preface Acknowledgments page xi xv 1 Introduction 1.1 What is Social Media Mining 1.2 New Challenges for Mining 1.3 Book Overview and Readers Guide 1.4 Summary 1.5 Bibliographic Notes 1.6 Exercises Part I Essentials 2 Graph Essentials 2.1 Graph Basics 2.2 Graph Representation 2.3 Types of Graphs 2.4 Connectivity in Graphs 2.5 Special Graphs 2.6 Graph Algorithms 2.7 Summary 2.8 Bibliographic Notes 2.9 Exercises 1 3 7 14 20 26 46 48 3 Network Measures 3.1 Centrality 3.2 Transitivity and Reciprocity 3.3 Balance and Status 3.4 Similarity 3.5 Summary 52 69 76 P1: WQS CUUS2079-FM CUUS2079-Zafarani January 14, 2014 Contents 3.6 Bibliographic Notes 3.7 Exercises 78 4 Network Models 4.1 Properties of Real-World Networks 4.2 Random Graphs 4.3 Small-World Model 4.4 Preferential Attachment Model 4.5 Summary 4.6 Bibliographic Notes 4.7 Exercises 80 93 102 5 Data Mining Essentials 5.1 Data 5.2 Data Preprocessing 5.3 Data Mining Algorithms 5.4 Supervised Learning 5.5 Unsupervised Learning 5.6 Summary 5.7 Bibliographic Notes 5.8 Exercises 113 134 Part II Communities and Interactions 6 Community Analysis 6.1 Community Detection 6.2 Community Evolution 6.3 Community Evaluation 6.4 Summary 6.5 Bibliographic Notes 6.6 Exercises 7 Information Diffusion in Social Media 7.1 Herd Behavior 7.2 Information Cascades 7.3 Diffusion of Innovations 7.4 Epidemics 7.5 Summary 7.6 Bibliographic Notes 7.7 Exercises 168 179 200 P1: WQS CUUS2079-FM CUUS2079-Zafarani January 14, 2014 Part III Applications 8 Influence and Homophily 8.1 Measuring Assortativity 8.2 Influence 8.3 Homophily 8.4 Distinguishing Influence and Homophily 8.5 Summary 8.6 Bibliographic Notes 8.7 Exercises 234 242 9 Recommendation in Social Media 9.1 Challenges 9.2 Classical Recommendation Algorithms 9.3 Recommendation Using Social Context 9.4 Evaluating Recommendations 9.5 Summary 9.6 Bibliographic Notes 9.7 Exercises 258 269 10 Behavior Analytics 10.1 Individual Behavior 10.2 Collective Behavior 10.3 Summary 10.4 Bibliographic Notes 10.5 Exercises 290 Notes Bibliography Index P1: WQS CUUS2079-FM CUUS2079-Zafarani x P1: WQS CUUS2079-FM CUUS2079-Zafarani Preface We live in an age of big data. With hundreds of millions of people spending countless hours on social media to share, communicate, connect, interact, and create user-generated data at an unprecedented rate, social media has become one unique source of big data. This novel source of rich data provides unparalleled opportunities and great potential for research and development. Unfortunately, more data does not necessarily beget more good, only more of the right (or relevant) data that enables us to glean gems. Social media data differs from traditional data we are familiar with in data mining. Thus, new computational methods are needed to mine the data. Social media data is noisy, free-format, of varying length, and multimedia"
Explain why a central node for one measure may be deemed unimportant by other measures," The centrality measures discussed thus far have different views on what a central node is. Hence, a central node for one measure may be deemed unimportant by other measures."," Hence, node v 2 has the highest closeness centrality. The centrality measures discussed thus far have different views on what a central node is. Thus, a central node for one measure may be deemed unimportant by other measures. Example 3.9. Consider the graph in Figure 3.6. For this graph, we compute the top three central nodes based on degree, eigenvector, Katz, PageRank, betweenness, and closeness centrality methods. These nodes are listed in Table 3.1"
Explain why the distribution of the number of friends denotes the degree distribution of the network," The degree of a node in social media often denotes the number of friends an individual has. Hence, the distribution of the number of friends denotes the degree distribution of the network."," The last observation is directly related to node degrees in social media. The degree of a node in social media often denotes the number of friends an individual has. Thus, the distribution of the number of friends denotes the degree distribution of the network. It turns out that in all provided observations, the distribution of values follows a power-law distribution. For instance, let k denote the degree of a node (i.e., the number of friends an individual has). Let pk denote the fraction of individuals with degree k (i.e., frequency of|v|observing k ). Then, in the power-law distribution pk = ak b , POWER-LAW DISTRIBUTION Trim: 6.125in 9.25in Top: 0.5in Network Models Log ( pk) Degree (a) Power-Law Degree Distribution (b) Log-Log Plot of Power-Law Degree Distribution where b is the power-law exponent and a is the power-law intercept. A power-law degree distribution is shown in Figure 4.1(a)"
Explain why random graphs are considered generally incapable of generating networks with high clustering coefficients without compromising other required properties,"In random graphs, the clustering coefficient is equal to the probability p; therefore, by appropriately selecting p, we can generate networks with a high clustering coefficient. Note that selecting a large p is undesirable because doing so will generate a very dense graph, which is unrealistic, as in the real-world, networks are often sparse. Hence, random graphs are considered generally incapable of generating networks with high clustering coefficients without compromising other required properties."," Note that in random graphs, the expected local clustering coefficient is equivalent to the global clustering coefficient. In random graphs, the clustering coefficient is equal to the probability p; therefore, by appropriately selecting p, we can generate networks with a high clustering coefficient. Note that selecting a large p is undesirable because doing so will generate a very dense graph, which is unrealistic, as in the real-world, networks are often sparse. Thus, random graphs are considered generally incapable of generating networks with high clustering coefficients without compromising other required properties. Average Path Length Proposition 4.8. The average path length l in a random graph is ln |V | , (4.16) ln c Proof. (Sketch) The proof is similar to the proof provided in determining when phase transition happens (see Section 4.2.1). Let D denote the expected diameter size in the random graph. Starting with any node in a l P1: WQS CUUS2079-04 CUUS2079-Zafarani January 13, 2014 Table 4.4. A Comparison between Real-World Networks and Simulated Random Graphs. In this table, C denotes the average clustering coefficient. The last two columns show the average path length and the clustering coefficient for the random graph simulated for the real-world network. Note that average path lengths are modeled properly, whereas the clustering coefficient is underestimated Original Network Network Film Actors Medline Coauthorship 1,520,251 E. Coli C. Elegans Average Average Path Degree Length 7.35 3.65 2.9 2.65 Simulated Random Graph C Average Path Length 0.79 0.56 0.32 0.28 2.99 4.91 3.04 2.25 1.8 104 0.05 random graph and its expected degree c, one can visit approximately c nodes by traveling one edge, c2 nodes by traveling two edges, and cD nodes by traveling diameter number of edges. After this step, almost all nodes should be visited. In this case, we have cD |V |"
Explain why the label for each instance is not known to the clustering algorithm,"Unsupervised learning is the unsupervised division of instances into groups of similar objects. In this topic, we focus on clustering. In clustering, the data is often unlabeled. Hence, the label for each instance is not known to the clustering algorithm."," n In the case of regression, however, it is unreasonable to assume that the label can be predicted precisely because the labels are real values. A small variation in the prediction would result in extremely low accuracy. For instance, if we train a model to predict the temperature of a city in a given day and the model predicts the temperature to be 71.1 degrees Fahrenheit and the actual observed temperature is 71, then the model is highly accurate; however, using the accuracy measure, the model is 0% accurate. In general, for regression, we check if the predictions are highly correlated with the ground truth using correlation analysis, or we can fit lines to both ground P1: Sqe CUUS2079-05 CUUS2079-Zafarani 5.5 Unsupervised Learning Table 5.3. Distance Measures Measure Name Mahalanobis Manhattan (L i norm) L p -norm  Description (X Y )T 1 (X i  i X, Y are features vectors and is the covariance matrix of the dataset X, Y are features vectors |xi yi |n ) n truth and prediction results and check if these lines are close. The smaller the distance between these lines, the more accurate the models learned from the data. 5.5 Unsupervised Learning Unsupervised learning is the unsupervised division of instances into groups of similar objects. In this topic, we focus on clustering. In clustering, the data is often unlabeled. Thus, the label for each instance is not known to the clustering algorithm. This is the main difference between supervised and unsupervised learning. Any clustering algorithm requires a distance measure. Instances are put into different clusters based on their distance to other instances. The most popular distance measure for continuous features is the Euclidean distance:  d(X, Y ) = (x1 y1 )2 + (x2 y2 )2 + + (xn yn )2   n  =  (xi yi )2 , i=1 where X = (x1 , x2 , . . . , xn ) and Y = (y1 , y2 , . . . , yn ) are n-dimensional feature vectors in R n . A list of some commonly used distance measures is provided in Table 5.3"
Explain why the same query issued by different individuals should result in different recommendations," Recommendation systems are designed to recommend individual-based choices. Hence, the same query issued by different individuals should result in different recommendations."," Recommendation vs. Search When individuals seek recommendations, they often use web search engines. However, search engines are rarely tailored to individuals needs and often retrieve the same results as long as the search query stays the RECOMMENDER SYSTEM Trim: 6.125in 9.25in Top: 0.5in CUUS2079-Zafarani January 13, 2014 same. To receive accurate recommendation from a search engine, one needs to send accurate keywords to the search engine. For instance, the query best 2013 movie to watch issued by an 8-year old and an adult will result in the same set of movies, whereas their individual tastes dictate different movies. Recommendation systems are designed to recommend individual-based choices. Thus, the same query issued by different individuals should result in different recommendations. These systems commonly employ browsing history, product purchases, user profile information, and friends information to make customized recommendations. As simple as it this process may look, a recommendation system algorithm actually has to deal with many challenges. 9.1 Challenges Recommendation systems face many challenges, some of which are presented next: r Cold-Start Problem. Many recommendation systems use historical data or information provided by the user to recommend items, products, and the like. However, when individuals first join sites, they have not yet bought any product: they have no history. This makes it hard to infer what they are going to like when they start on a site. The problem is referred to as the cold-start problem. As an example, consider an online movie rental store. This store has no idea what recently joined users prefer to watch and therefore cannot recommend something close to their tastes. To address this issue, these sites often ask users to rate a couple of movies before they begin recommend others to them. Other sites ask users to fill in profile information, such as interests. This information serves as an input to the recommendation algorithm"
"Explain why if the hypothesis is valid, a factor that plays a role in users joining a community is the number of their friends who are already members of the community"," One such hypothesis is that individuals are inclined toward an activity when their friends are engaged in the same activity. Hence, if the hypothesis is valid, a factor that plays a role in users joining a community is the number of their friends who are already members of the community."," To determine factors that affect community-joining behavior, we can design hypotheses based on different factors that describe when communityjoining behavior takes place. We can verify these hypotheses by using data available on social media. The factors used in the validated hypotheses describe the behavior under study most accurately. One such hypothesis is that individuals are inclined toward an activity when their friends are engaged in the same activity. Thus, if the hypothesis is valid, a factor that plays a role in users joining a community is the number of their friends who are already members of the community. In data mining terms, this translates to using the number of friends of an individual in a community as a feature to predict whether the individual joins the P1: CUUS2079-10 CUUS2079-Zafarani Gutter: 0.75in January 13, 2014 Behavior Analytics 0.02 0.01 5 25 45 Number of Friends m Already in the Community (from Backstrom et al. [2006]). DIMINISHING RETURNS community (i.e., class attribute). Figure 10.2 depicts the probability of joining a community with respect to the number of friends an individual has who are already members of the community. The probability increases as more friends are in a community, but a diminishing returns property is also observed, meaning that when enough friends are inside the community, more friends have no or only marginal effects on the likelihood of the individuals act of joining the community"
Explain why we focus on individuals whose attention has migrated,"Site migration is rarely observed since users often abandon their accounts rather than closing them. A more observable behavior is attention migration, which is clearly observable on most social media sites. Moreover, when a user commits site migration, it is often too late to perform preventive measures. However, when attention migration is detected, it is still possible to take actions to retain the user or expedite his or her attention migration to guarantee site migration. Hence, we focus on individuals whose attention has migrated."," The interval could be measured at different granularity, such as days, weeks, months, and years. It is common to set = 1 month. To analyze the migration of populations across sites, we can analyze migrations of individuals and then measure the rate at which the population of these individuals is migrating across sites. Since this method analyzes migrations at the individual level, we can use the methodology outlined in Section 10.1.1 for individual behavior analysis as follows. P1: CUUS2079-10 CUUS2079-Zafarani Gutter: 0.75in January 13, 2014 The Observable Behavior Site migration is rarely observed since users often abandon their accounts rather than closing them. A more observable behavior is attention migration, which is clearly observable on most social media sites. Moreover, when a user commits site migration, it is often too late to perform preventive measures. However, when attention migration is detected, it is still possible to take actions to retain the user or expedite his or her attention migration to guarantee site migration. Thus, we focus on individuals whose attention has migrated. To observe attention migrations, several steps need to be taken. First, users are required to be identified on multiple networks so that their activity on multiple sites can be monitored simultaneously. For instance, username huan.liu1 on Facebook is username liuhuan on Twitter. This identification can be done by collecting information from sites where individuals list their multiple identities on social media sites. On social networking sites such as Google+ or Facebook, this happens regularly. The second step is collecting multiple snapshots of social media sites. At least two snapshots are required to observe migrations. After these two steps, we can observe whether attention migrations have taken place or not. In other words, we can observe if users have become inactive on one of the sites over time. Figure 10.6 depicts these migrations for some well-known social media sites"
"Explain why, smoothing (blurring) is achieved in the frequency domain by high-frequency attenuation; that is, by lowpass filtering","The remainder of this chapter deals with various filtering techniques in the frequency domain, beginning with lowpass filters. Edges and other sharp intensity transitions (such as noise) in an image contribute significantly to the high frequency content of its Fourier transform. Hence,  smoothing (blurring) is achieved in the frequency domain by high-frequency attenuation; that is, by lowpass filtering."," The procedure used to generate H(u,v) is: (1) multiply ( , ) by ( 1) to center the frequency domain filter; (2) compute the forward DFT of the result in (1) to generate H(u,v); (3) set the real part of H(u,v) to 0 to account for parasitic real parts (we know that H has to be purely imaginary because is real and odd); and (4) multiply the result by ( 1) multiplication of H(u,v) by ( 1) + , which is implicit when h(x, y) was manually placed in the center of ( , ) . Figure 4.38(a) shows a perspective plot of H(u, v), and Fig. 4.38(b) shows H(u,v) as an image. Note the antisymmetry in this image about its center, a result of H(u,v) being odd. Function H(u,v) is used as any other frequency domain filter transfer function. Figure 4.38(c) is the result of using the filter transfer function just obtained to filter the image in Fig. 4.37(a) in the frequency domain, using the step-by-step filtering procedure outlined earlier. As expected from a derivative filter, edges were enhanced and all the constant intensity areas were reduced to zero (the grayish tone is due to scaling for display). Figure 4.38(d) shows the result of filtering the same image in the spatial domain with the Sobel kernel h(x, y), using the procedure discussed in Section 3.6 . The results are identical. 4.8 Image Smoothing Using Lowpass Frequency Domain Filters The remainder of this chapter deals with various filtering techniques in the frequency domain, beginning with lowpass filters. Edges and other sharp intensity transitions (such as noise) in an image contribute significantly to the high frequency content of its Fourier transform. Hence, smoothing (blurring) is achieved in the frequency domain by high-frequency attenuation; that is, by lowpass filtering. In this section, we consider three types of lowpass filters: ideal, Butterworth, and Gaussian. These three categories cover the range from very sharp (ideal) to very smooth (Gaussian) filtering. The shape of a Butterworth filter is controlled by a parameter called the filter order. For large values of this parameter, the Butterworth filter approaches the ideal filter. For lower values, the Butterworth filter is more like a Gaussian filter. Thus, the Butterworth filter provides a transition between two extremes. All filtering in this section follows the procedure outlined in the previous section, so all filter transfer functions, H(u, v), are understood to be of size ; that is, the discrete frequency variables are in the range Eqs. (4-100) and (4-101) "
"Explain why the HSI model is a useful tool for developing image processing algorithms based on color descriptions that are natural and intuitive to humans, who, after all, are the developers and users of these algorithms","When humans view a color object, we describe it by its hue, saturation, and brightness. Recall from the discussion in Section 7.1 that hue is a color attribute that describes a pure color (pure yellow, orange, or red), whereas saturation gives a measure of the degree to which a pure color is diluted by white light. Brightness is a subjective descriptor that is practically impossible to measure. It embodies the achromatic notion of intensity and is one of the key factors in describing color sensation. We do know that intensity (gray level) is a most useful descriptor of achromatic images. This quantity definitely is measurable and easily interpretable. The model we are about to present, called the HSI (hue, saturation, intensity) color model, decouples the intensity component from the color-carrying information (hue and saturation) in a color image. Hence, the HSI model is a useful tool for developing image processing algorithms based on color descriptions that are natural and intuitive to humans, who, after all, are the developers and users of these algorithms."," These color systems are ideally suited for hardware implementations. In addition, the RGB system matches nicely with the fact that the human eye is strongly perceptive to red, green, and blue primaries. Unfortunately, the RGB, CMY, and other similar color models are not well suited for describing colors in terms that are practical for human interpretation. For example, one does not refer to the color of an automobile by giving the percentage of each of the primaries composing its color. Furthermore, we do not think of color images as being composed of three primary images that combine to form a single image. When humans view a color object, we describe it by its hue, saturation, and brightness. Recall from the discussion in Section 7.1 that hue is a color attribute that describes a pure color (pure yellow, orange, or red), whereas saturation gives a measure of the degree to which a pure color is diluted by white light. Brightness is a subjective descriptor that is practically impossible to measure. It embodies the achromatic notion of intensity and is one of the key factors in describing color sensation. We do know that intensity (gray level) is a most useful descriptor of achromatic images. This quantity definitely is measurable and easily interpretable. The model we are about to present, called the HSI (hue, saturation, intensity) color model, decouples the intensity component from the color-carrying information (hue and saturation) in a color image. As a result, the HSI model is a useful tool for developing image processing algorithms based on color descriptions that are natural and intuitive to humans, who, after all, are the developers and users of these algorithms. We can summarize by saying that RGB is ideal for image color generation (as in image capture by a color camera or image display on a monitor screen), but its use for color description is much more limited. The material that follows provides an effective way to do this. We know from Example 7.1 that an RGB color image is composed three gray-scale intensity images (representing red, green, and blue), so it should come as no surprise that we can to extract intensity from an RGB image. This becomes clear if we take the color cube from Fig. 7.7 and stand it on the black, (0, 0, 0), vertex, with the white, (1, 1, 1), vertex directly above it [see Fig. 7.10(a) ]"
Explain why the code rate is constant and known in advance,"There are three basic ways to threshold a transformed subimage or, stated differently, to create a subimage threshold masking function of the form given in Eq. (8-22) : (1) A single global threshold can be applied to all subimages; (2) a different threshold can be used for each subimage, or; (3) the threshold can be varied as a function of the location of each coefficient within the subimage. In the first approach, the level of compression differs from image to image, depending on the number of coefficients that exceed the global threshold. In the second, called N-largest coding, the same number of coefficients is discarded for each subimage. Hence, the code rate is constant and known in advance."," Threshold Coding Implementation Zonal coding usually is implemented by using a single fixed mask for all subimages. Threshold coding, however, is inherently adaptive in the sense that the location of the transform coefficients retained for each subimage vary from one subimage to another. In fact, threshold coding is the adaptive transform coding approach most often used in practice because of its computational simplicity. The underlying concept is that, for any subimage, the transform coefficients of largest magnitude make the most significant contribution to reconstructed subimage quality, as demonstrated in the last example. Because the locations of the maximum coefficients vary from one subimage to another, the elements of ( , ) ( , ) normally are reordered (in a predefined manner) to form a 1-D, run-length coded sequence. Figure 8.26(c) shows a typical threshold mask for one subimage of a hypothetical image. This mask provides a convenient way to visualize the threshold coding process for the corresponding subimage, as well as to mathematically describe the process using Eq. (8-23) . When the mask is applied [via Eq. (8-23) ] to the subimage for which it was derived, and the resulting array is reordered to form an -element coefficient sequence in accordance with the zigzag ordering pattern of Fig. 8.26(d) , the reordered 1-D sequence contains several long runs of 0s. [The zigzag pattern becomes evident by starting at 0 in Fig. 8.26(d) and following the numbers in sequence.] These runs normally are run-length coded. The nonzero or retained coefficients, corresponding to the mask locations that contain a 1, are represented using a variable-length code. There are three basic ways to threshold a transformed subimage or, stated differently, to create a subimage threshold masking function of the form given in Eq. (8-22) : (1) A single global threshold can be applied to all subimages; (2) a different threshold can be used for each subimage, or; (3) the threshold can be varied as a function of the location of each coefficient within the subimage. In the first approach, the level of compression differs from image to image, depending on the number of coefficients that exceed the global threshold. In the second, called N-largest coding, the same number of coefficients is discarded for each subimage. As a result, the code rate is constant and known in advance. The third technique, like the first, results in a variable code rate, but offers the advantage that thresholding and quantization can be combined by replacing ( , ) ( , ) in Eq. (8-23) The N in N-largest coding is not an image dimension, but refers to the number of coefficients that are kept. ( , ) ( , ) where ^( , ) is a thresholded and quantized approximation of T(u, v), and Z(u, v) is an element of the following transform normalization array: (0, 0) (1, 0) = ( 1, 0) ( 1, 1) Before a normalized (thresholded and quantized) subimage transform, ^( , ), can be inverse transformed to obtain an approximation of subimage g(x, y), it must be multiplied by Z(u, v). The resulting denormalized array, denoted ( , ) is an approximation of ^( , ): ( , ) = ^( , ) ( , ) The inverse transform of ( , ) yields the decompressed subimage approximation"
Explain why all standards require the periodic insertion of I-frames into the compressed video codestream,"The first of these, called motion compensation, is the subject of the remainder of this section. Before proceeding, however, we should note that when there is insufficient interframe correlation to make predictive coding effective, the second problem is typically addressed using a block-oriented 2-D transform, like JPEGs DCT-based coding (see the previous section). Frames compressed in this way (i.e., without a prediction residual) are called intraframes or Independent frames (I-frames). They can be decoded without access to other frames in the video to which they belong. I-frames usually resemble JPEG encoded images, and are ideal starting points for the generation of prediction residuals. Moreover, they provide a high degree of random access, ease of editing, and resistance to the propagation of transmission error. Hence, all standards require the periodic insertion of I-frames into the compressed video codestream."," 2. By switching to an alternate coding method when there is insufficient interframe correlation (similarity between frames) to make predictive coding advantageous. The first of these, called motion compensation, is the subject of the remainder of this section. Before proceeding, however, we should note that when there is insufficient interframe correlation to make predictive coding effective, the second problem is typically addressed using a block-oriented 2-D transform, like JPEGs DCT-based coding (see the previous section). Frames compressed in this way (i.e., without a prediction residual) are called intraframes or Independent frames (I-frames). They can be decoded without access to other frames in the video to which they belong. I-frames usually resemble JPEG encoded images, and are ideal starting points for the generation of prediction residuals. Moreover, they provide a high degree of random access, ease of editing, and resistance to the propagation of transmission error. As a result, all standards require the periodic insertion of I-frames into the compressed video codestream. rectangular regions (typically of size 4 4 to 16 16) called macroblocks. (Only one macroblock is shown in Fig. 8.33 .) The movement of each macroblock with respect to its most likely position in the previous (or subsequent) video frame, called the reference frame, is encoded in a motion vector. The vector describes the motion by defining the horizontal and vertical displacement from the most likely position. The displacements typically are specified to the nearest pixel, pixel, or pixel precision. If subpixel precision is used, the predictions must be interpolated [e.g., using bilinear interpolation (see Section 2.4 )] from a combination of pixels in the reference frame. An encoded frame that is based on the previous frame (a forward prediction in Fig. 8.33 ) is called a Predictive frame (P-frame); one that is based on the subsequent frame (a backward prediction in Fig. 8.33 ) is called a Bidirectional frame (B-frame). B-frames require the compressed codestream to be reordered so that frames are presented to the decoder in the proper decoding sequence, rather than the natural display order"
Explain why a number of heuristic methods that attempt to find approximations to the minimum have been proposed over the years," Unfortunately, finding this minimum is an NP-hard problem for which no practical solution is known. Hence, a number of heuristic methods that attempt to find approximations to the minimum have been proposed over the years."," (10-85) arg min = is the mean vector (or centroid) of the samples in set and arg is the vector norm of the argument. Typically, the equation says that we are interested in finding the sets , to mean } such that the sum of the distances from each point in a set to the mean of that set is minimum. Unfortunately, finding this minimum is an NP-hard problem for which no practical solution is known. As a result, a number of heuristic methods that attempt to find approximations to the minimum have been proposed over the years. In this section, we discuss what is generally considered to be the standard k-means algorithm, which is based on the Euclidean distance (see Section 2.6 ). Given a set { , } of vector observation and a specified value of k, the algorithm is as follows: 1. Initialize the algorithm: Specify an initial set of means, = 1, 2, , . 2. Assign samples to clusters: Assign each sample to the cluster set whose mean is the closest (ties are resolved arbitrarily, but samples are assigned to only one cluster): if = 1, 2, , ( ); = 1, 2, , 3. Update the cluster centers (means): = | | = 1, 2, , 4. Test for completion: Compute the Euclidean norms of the differences between the mean vectors in the current and previous steps. Compute the residual error, E, as the sum of the k norms. Stop if go back to Step 2"
"Explain why to determine if a kernel is separable, all we have to do is determine if its rank is 1","We know from matrix theory that a matrix resulting from the product of a column vector and a row vector always has a rank of 1. By definition, a separable kernel is formed by such a product. Hence, to determine if a kernel is separable, all we have to do is determine if its rank is 1."," For an image of size and a kernel of size , implementation of Eq. (3-44) requires on the order of multiplications and additions. This is because it follows directly from that equation that each pixel in the output (filtered) image depends on all the coefficients in the filter kernel. But, if the kernel is separable and we use Eq. (3-52) , then the first convolution, , requires on the order of MNm multiplications and additions because is of size 1. The result is of size , so the convolution of with the ( + ) multiplication and addition operations. Thus, the computational advantage of performing convolution with a separable, as opposed to a nonseparable, kernel is defined as = + ) (3-53) + For a kernel of modest size, say 11 11, the computational advantage (and thus execution-time advantage) is a respectable 5.2. For kernels with hundreds of elements, execution times can be reduced by a factor of a hundred or more, which is significant. We will illustrate the use of such large kernels in Example 3.18 . We know from matrix theory that a matrix resulting from the product of a column vector and a row vector always has a rank of 1. By definition, a separable kernel is formed by such a product. Therefore, to determine if a kernel is separable, all we have to do is determine if its rank is 1. Typically, we find the rank of a matrix using a pre-programmed function in the computer language being used. For example, if you use MATLAB, function rank will do the job"
"Explain why the maximum possible intensity change also is finite, and the shortest distance over which that change can occur is between adjacent pixels"," We are dealing with digital quantities whose values are finite. Hence, the maximum possible intensity change also is finite, and the shortest distance over which that change can occur is between adjacent pixels."," 3. Must be zero along intensity ramps. We are dealing with digital quantities whose values are finite. Therefore, the maximum possible intensity change also is finite, and the shortest distance over which that change can occur is between adjacent pixels. A basic definition of the first-order derivative of a one-dimensional function f(x) is the difference = ( + 1) ( ) We will return to Eq. (3-57) we accept it as a definition"
"Explain why using examples from image enhancement in this chapter not only saves having an extra chapter in the book but, more importantly, is an effective tool for introducing newcomers to filtering techniques in the frequency domain","Sharpening are traditionally associated with image enhancement, as are techniques for contrast manipulation. By its very nature, beginners in digital image processing find enhancement to be interesting and relatively simple to understand. Hence, using examples from image enhancement in this chapter not only saves having an extra chapter in the book but, more importantly, is an effective tool for introducing newcomers to filtering techniques in the frequency domain."," Our focus in the sections that follow is on the Fourier transform and its properties. As we progress through this chapter, it will become evident that Fourier techniques are useful in a broad range of image processing applications. We conclude the chapter with a discussion of the FFT. About the Examples in this Chapter As in Chapter 3 sharpening are traditionally associated with image enhancement, as are techniques for contrast manipulation. By its very nature, beginners in digital image processing find enhancement to be interesting and relatively simple to understand. Therefore, using examples from image enhancement in this chapter not only saves having an extra chapter in the book but, more importantly, is an effective tool for introducing newcomers to filtering techniques in the frequency domain. We will use frequency domain processing methods for other applications in Chapters 5 , and 12 4.2 Preliminary Concepts We pause briefly to introduce several of the basic concepts that underlie the material in later sections. Complex Numbers A complex number, C, is defined as = (4-3) where R and I are real numbers and = 1 . Here, R denotes the real part of the complex number and I its imaginary part. Real numbers are a subset of complex numbers in which = 0 . The conjugate of a complex number C, denoted * *, Complex numbers can be viewed geometrically as points on a plane (called the complex plane) whose abscissa is the real axis (values of R) and whose ordinate is the imaginary axis (values of I). That is, the complex number + is point (R, I) in the coordinate system of the complex plane"
"Explain why the primary colors of pigments are magenta, cyan, and yellow, and the secondary colors are red, green, and blue","Differentiating between the primary colors of light and the primary colors of pigments or colorants is important. In the latter, a primary color is defined as one that subtracts or absorbs a primary color of light, and reflects or transmits the other two. Hence, the primary colors of pigments are magenta, cyan, and yellow, and the secondary colors are red, green, and blue."," FIGURE 7.4 Primary and secondary colors of light and pigments. (Courtesy of the General Electric Co., Lighting Division.) Differentiating between the primary colors of light and the primary colors of pigments or colorants is important. In the latter, a primary color is defined as one that subtracts or absorbs a primary color of light, and reflects or transmits the other two. Therefore, the primary colors of pigments are magenta, cyan, and yellow, and the secondary colors are red, green, and blue. These colors are shown in Fig. 7.4(b) In practice, pigments seldom are pure. This results in a muddy brown instead of black when primaries, or primaries and secondaries, are combined. We will discuss this issue in Section 7.2 Color television reception is an example of the additive nature of light colors. The interior of CRT (cathode ray tube) color TV screens used well into the 1990s is composed of a large array of triangular dot patterns of electron-sensitive phosphor. When excited, each dot in a triad produces light in one of the primary colors. The intensity of the red-emitting phosphor dots is modulated by an electron gun inside the tube, which generates pulses corresponding to the red energy seen by the TV camera. The green and blue phosphor dots in each triad are modulated in the same manner. The effect, viewed on the television receiver, is that the three primary colors from each phosphor triad are received and added together by the color-sensitive cones in the eye and perceived as a full-color image. Thirty successive image changes per second in all three colors complete the illusion of a continuous image display on the screen. CRT displays started being replaced in the late 1990s by flat-panel digital technologies, such as liquid crystal displays (LCDs) and plasma devices. Although they are fundamentally different from CRTs, these and similar technologies use the same principle in the sense that they all require three subpixels (red, green, and blue) to generate a single color pixel. LCDs use properties of polarized light to block or pass light through the LCD screen and, in the case of active matrix display technologies, thin film transistors (TFTs) are used to provide the proper signals to address each pixel on the screen. Light filters are used to produce the three primary colors of light at each pixel triad location. In plasma units, pixels are tiny gas cells coated with phosphor to produce one of the three primary colors. The individual cells are addressed in a manner analogous to LCDs. This individual pixel triad coordinate addressing capability is the foundation of digital displays"
"Explain why the transform coefficients of maximum variance carry the most image information, and should be retained in the coding process"," Zonal Coding Implementation Zonal coding is based on the information theory concept of viewing information as uncertainty. Hence, the transform coefficients of maximum variance carry the most image information, and should be retained in the coding process."," The difference images are scaled by 4. Zonal Coding Implementation Zonal coding is based on the information theory concept of viewing information as uncertainty. Therefore, the transform coefficients of maximum variance carry the most image information, and should be retained in the coding process. The variances themselves can be calculated directly from the ensemble of transformed subimage arrays (as in the preceding example) or based on an assumed image model (say, a Markov autocorrelation function). In either case, the zonal sampling process can be viewed, in accordance with Eq. (8-23) , as multiplying each T(u, v) by the corresponding element in a zonal mask, which is constructed by placing a 1 in the locations of maximum variance and a 0 in all other locations. Coefficients of maximum variance usually are located around the origin of an image transform, resulting in the typical zonal mask shown in Fig. 8.26(a) . FIGURE 8.26 A typical (a) zonal mask, (b) zonal bit allocation, (c) threshold mask, and (d) thresholded coefficient ordering sequence. Shading highlights the coefficients that are retained"
Explain why edge detection typically is followed by linking algorithms designed to assemble edge pixels into meaningful edges and/or region boundaries,"Ideally, edge detection should yield sets of pixels lying only on edges. In practice, these pixels seldom characterize edges completely because of noise, breaks in the edges caused by nonuniform illumination, and other effects that introduce discontinuities in intensity values. Hence, edge detection typically is followed by linking algorithms designed to assemble edge pixels into meaningful edges and/or region boundaries."," The price paid for the improved performance of the Canny algorithm is a significantly more complex implementation than the two approaches discussed earlier. In some applications, such as real-time industrial image processing, cost and speed requirements usually dictate the use of simpler techniques, principally the thresholded gradient approach. When edge quality is the driving force, the Marr-Hildreth and Canny algorithms, especially the latter, offer superior alternatives. Linking Edge Points Ideally, edge detection should yield sets of pixels lying only on edges. In practice, these pixels seldom characterize edges completely because of noise, breaks in the edges caused by nonuniform illumination, and other effects that introduce discontinuities in intensity values. Therefore, edge detection typically is followed by linking algorithms designed to assemble edge pixels into meaningful edges and/or region boundaries. In this section, we discuss two fundamental approaches to edge linking that are representative of techniques used in practice. The first requires knowledge about edge points in a local region (e.g., a 3 3 neighborhood), and the second is a global approach that works with an entire edge map. As it turns out, linking points along the boundary of a region is also an important aspect of some of the segmentation methods discussed in the next chapter, and in extracting features from a segmented image, as we will do in Chapter 12 . Thus, you will encounter additional edge-point linking methods in the next two chapters. Local Processing A simple approach for linking edge points is to analyze the characteristics of pixels in a small neighborhood about every point (x, y) that has been declared an edge point by one of the techniques discussed in the preceding sections. All points that are similar according to predefined criteria are linked, forming an edge of pixels that share common properties according to the specified criteria"
"Explain why we conclude that if we add a bias to the spatial convolution computation performed by a CNN at any fixed position (x, y) in the input, the result can be expressed in a form identical to the computation performed by an artificial neuron in a fully connected neural net","The form of the first line of this equation is identical to Eq. (13-54) . Hence, we conclude that if we add a bias to the spatial convolution computation performed by a CNN at any fixed position (x, y) in the input, the result can be expressed in a form identical to the computation performed by an artificial neuron in a fully connected neural net."," For example, the map on the top of the first column highlights the two principal edges on the top of the character. The second map highlights the edges of the entire inner region, and the third highlights a blob-like nature of the digit, almost as if it had been blurred by a lowpass kernel. The other three images show other features. Although the pooled feature maps are lower-resolution versions of the original feature maps, they still retained the key characteristics of the features in the latter. If you look at the first two feature maps in the second layer, and compare them with the first two in the first layer, you can see that they could be interpreted as higher-level abstractions of the top part of the character, in the sense that they show an area flanked on both sides by areas of opposite intensity. These abstractions are not always easy to analyze visually, but as you will see in later examples, they can be very effective. The vectorized version of the last pooled layer is self-explanatory. The output of the fully connected neural net shows dark for low values and white for the highest value, indicating that the input was properly recognized as a number 6. Later in this section, we will show that the simple CNN architecture in Fig. 13.42 is capable of recognizing the correct class of over 70,000 numerical samples with nearly perfect accuracy. FIGURE 13.44 Visual summary of an input image propagating through the CNN in Fig. 13.42 . Shown as images are all the results of convolution (feature maps) and pooling (pooled feature maps) for both layers of the network. (Example 13.17 contains more details about this figure.) Neural Computations in a CNN Recall from Fig. 13.29 that the basic computation performed by an artificial neuron is a sum of products between weights and values from a previous layer. To this we add a bias and call the result the net (total) input to the neuron, which we denoted by . As we showed in Eq. (13-54) is a single sum. The computations performed in a CNN to generate a single value in a feature map is 2-D convolution. As you learned in Chapter 3 , this is a double sum of products between the coefficients of a kernel and the corresponding elements of the image array overlapped by the kernel. With reference to Fig. 13.40 , let w denote a kernel formed by arranging the weights in the shape of the receptive field we discussed in connection with that figure. For notational consistency with Section 13.5 , let , denote image or pooled feature values, depending on the layer. The convolution value at any point (x, y) in the input is given by , , where l and k span the dimensions of the kernel. Suppose that w is of size 3 3. Then, we can then expand this equation into the following sum of products: = = = + ++ , , , We could relabel the subscripts on w and , and write instead , ++ = = The results of Eqs. (13-84) are identical. If we add a bias to the latter equation and call the result z we have (13-86) = + = + The form of the first line of this equation is identical to Eq. (13-54) . Therefore, we conclude that if we add a bias to the spatial convolution computation performed by a CNN at any fixed position (x, y) in the input, the result can be expressed in a form identical to the computation performed by an artificial neuron in a fully connected neural net. We need the x, y only to account for the fact that we are working in 2-D. If we think of z as the net input to a neuron, the analogy with the neurons discussed in Section 13.5 is completed by passing z through an activation function, , to get the output of the neuron: = ( ) This is exactly how the value of any point in a feature map (such as the point labeled A in Fig. 13.40 Now consider point B in that figure. As mentioned earlier, its value is given by adding three convolution equations: ( ) , ( ) , ( ) , ( ) , ( ) , ( ) , ( ) ( ) , , ( ) ( ) , , + ( ) ( ) , , where the superscripts refer to the three pooled feature maps in Fig. 13.40 . The values of l, k, x, and y are the same in all three equations because all three kernels are of the same size and they move in unison. We could expand this equation and obtain a sum of products that is lengthier than for point A in Fig. 13.40 only one summation, exactly as before. The preceding result tells us that the equations used to obtain the value of an element of any feature map in a CNN can be expressed in the form of the computation performed by an artificial neuron. This holds for any feature map, regardless of how many convolutions are involved in the computation of the elements of that feature map, in which case we would simply be dealing with the sum of more convolution equations. The implication is that we can use the basic form of Eqs. (13-86) and (13-87) to describe how the value of an element in any feature map of a CNN is obtained. This means we do not have to account explicitly for the number of different pooled feature maps (and hence the number of different kernels) used in a pooling layer. The result is a significant simplification of the equations that describe forward and backpropagation in a CNN"
"Explain why when going in the reverse direction, we upsample (e","In a forward pass, we went from a convolution layer to a pooled layer. In backpropagation, we are going in the opposite direction. But the pooled feature maps are smaller than their corresponding feature maps (see Fig. 13.40 ). Hence, when going in the reverse direction, we upsample (e."," By definition, the first term of the double summation of Eq. (13-96) Substituting Eq. (13-92) into Eq. (13-91) ( + 1) , = (13-97) ( + 1) , () , , we obtain (13-98) () , = , then = = The derivative of the expression inside the brackets is zero unless respect to , () = , () is and = , = , and because the derivative of ( + 1) with as (13-99) () = ( + 1) , Values of x, y, u, and v are specified outside of the terms inside the brackets. Once the values of these variables are fixed, can write Eq. (13-99) ( + 1)( ()), and we , () = = ( , ()) , ( + 1) , (13-100) The double sum expression in the second line of this equation is in the form of a convolution, but the displacements are the negatives of those in Eq. (13-91) as , , ( + 1) ( + 1)] The negatives in the subscripts indicate that w is reflected about both spatial axes. This is the same as rotating w by 180, as we explained in connection with Eq. (3-44) (13-101) equivalently as , ())[ ( + 1)rot180( , (13-102) But the kernels do not depend on x and y, so we can write this equation as , ())[ , (13-103) As in Section 13.5 , our final objective is to compute the change in E with respect to the weights and biases. Following a similar procedure as above, we obtain = , () , = ()( , () , , = , , , ( 1)) where the last line follows from Eq. (13-92) . This line is in the form of a convolution but, comparing it to Eq. (13-91) , we see there is a sign reversal between the summation variables and their corresponding subscripts. To put it in the form of a convolution, we write the last line of Eq. (13-104) , = () = () , , ), = () , , = () () and 1) ), it follows that (13-107) , () = () = () () Equations (13-107) and (13-108) update the weights and bias of each convolution layer in a CNN. As we have mentioned before, it is understood that the represents all the weights of a layer. The variables l and k span the spatial dimensions of the 2-D kernels, all of which are of the same size. In a forward pass, we went from a convolution layer to a pooled layer. In backpropagation, we are going in the opposite direction. But the pooled feature maps are smaller than their corresponding feature maps (see Fig. 13.40 ). Therefore, when going in the reverse direction, we upsample (e.g., by pixel replication) each pooled feature map to match the size of the feature map that generated it. Each pooled feature map corresponds to a unique feature map, so the path of backpropagation is clearly defined. With reference to Fig. 13.40 how to update the weights of this network. When we get to the interface between the neural net and the CNN, we have to reverse the vectorization method used to generate input vectors. That is, before we can proceed with backpropagation using Eqs. (13-107) and (13-108) , we have to regenerate the individual pooled feature maps from the single vector propagated back by the fully connected neural net"
Explain why digital image processing encompasses a wide and varied field of applications,"However, unlike humans, who are limited to the visual band of the electromagnetic (EM) spectrum, imaging machines cover almost the entire EM spectrum, ranging from gamma to radio waves. They can operate on images generated by sources that humans are not accustomed to associating with images. These include ultrasound, electron microscopy, and computer-generated images. Hence, digital image processing encompasses a wide and varied field of applications."," 1.1 What is Digital Image Processing? An image may be defined as a two-dimensional function, f(x, y), where x and y are spatial (plane) coordinates, and the amplitude of f at any pair of coordinates (x, y) is called the intensity or gray level of the image at that point. When x, y, and the intensity values of f are all finite, discrete quantities, we call the image a digital image. The field of digital image processing refers to processing digital images by means of a digital computer. Note that a digital image is composed of a finite number of elements, each of which has a particular location and value. These elements are called picture elements, image elements, pels, and pixels. Pixel is the term used most widely to denote the elements of a digital image. We will consider these definitions in more formal terms in Chapter 2 Vision is the most advanced of our senses, so it is not surprising that images play the single most important role in human perception. However, unlike humans, who are limited to the visual band of the electromagnetic (EM) spectrum, imaging machines cover almost the entire EM spectrum, ranging from gamma to radio waves. They can operate on images generated by sources that humans are not accustomed to associating with images. These include ultrasound, electron microscopy, and computer-generated images. Thus, digital image processing encompasses a wide and varied field of applications. There is no general agreement among authors regarding where image processing stops and other related areas, such as image analysis and computer vision, start. Sometimes, a distinction is made by defining image processing as a discipline in which both the input and output of a process are images. We believe this to be a limiting and somewhat artificial boundary. For example, under this definition, even the trivial task of computing the average intensity of an image (which yields a single number) would not be considered an image processing operation. On the other hand, there are fields such as computer vision whose ultimate goal is to use computers to emulate human vision, including learning and being able to make inferences and take actions based on visual inputs. This area itself is a branch of artificial intelligence (AI) whose objective is to emulate human intelligence. The field of AI is in its earliest stages of infancy in terms of development, with progress having been much slower than originally anticipated. The area of image analysis (also called image understanding) is in between image processing and computer vision"
Explain why only the emission light reaches the eye or other detector,"Ultraviolet light is used in fluorescence microscopy, one of the fastest growing areas of microscopy. Fluorescence is a phenomenon discovered in the middle of the nineteenth century, when it was first observed that the mineral fluorspar fluoresces when ultraviolet light is directed upon it. The ultraviolet light itself is not visible, but when a photon of ultraviolet radiation collides with an electron in an atom of a fluorescent material, it elevates the electron to a higher energy level. Subsequently, the excited electron relaxes to a lower level and emits light in the form of a lower-energy photon in the visible (red) light region. Important tasks performed with a fluorescence microscope are to use an excitation light to irradiate a prepared specimen, and then to separate the much weaker radiating fluorescent light from the brighter excitation light. Hence, only the emission light reaches the eye or other detector."," Imaging in the Ultraviolet Band Applications of ultraviolet light are varied. They include lithography, industrial inspection, microscopy, lasers, biological imaging, and astronomical observations. We illustrate imaging in this band with examples from microscopy and astronomy. Ultraviolet light is used in fluorescence microscopy, one of the fastest growing areas of microscopy. Fluorescence is a phenomenon discovered in the middle of the nineteenth century, when it was first observed that the mineral fluorspar fluoresces when ultraviolet light is directed upon it. The ultraviolet light itself is not visible, but when a photon of ultraviolet radiation collides with an electron in an atom of a fluorescent material, it elevates the electron to a higher energy level. Subsequently, the excited electron relaxes to a lower level and emits light in the form of a lower-energy photon in the visible (red) light region. Important tasks performed with a fluorescence microscope are to use an excitation light to irradiate a prepared specimen, and then to separate the much weaker radiating fluorescent light from the brighter excitation light. Thus, only the emission light reaches the eye or other detector. The resulting fluorescing areas shine against a dark background with sufficient contrast to permit detection. The darker the background of the nonfluorescing material, the more efficient the instrument. Fluorescence microscopy is an excellent method for studying materials that can be made to fluoresce, either in their natural form (primary fluorescence) or when treated with chemicals capable of fluorescing (secondary fluorescence). Figures 1.8(a) and (b) show results typical of the capability of fluorescence microscopy. Figure 1.8(a) shows a fluorescence microscope image of normal corn, and Fig. 1.8(b) shows corn infected by smut, a disease of cereals, corn, grasses, onions, and sorghum that can be caused by any one of more than 700 species of parasitic fungi. Corn smut is particularly harmful because corn is one of the principal food sources in the world. As another illustration, Fig. 1.8(c) shows the Cygnus Loop imaged in the high-energy region of the ultraviolet band"
"Explain why radio waves have photons with low energies, microwaves have more energy than radio waves, infrared still more, then visible, ultraviolet, X-rays, and finally gamma rays, the most energetic of all","Electromagnetic waves can be visualized as propagating sinusoidal waves with wavelength, or they can be thought of as a stream of massless particles, each traveling in a wavelike pattern and moving at the speed of light. Each massless particle contains a certain amount (or bundle) of energy, called a photon. We see that energy is proportional to frequency, so the higher-frequency (shorter wavelength) electromagnetic phenomena carry more energy per photon. Hence, radio waves have photons with low energies, microwaves have more energy than radio waves, infrared still more, then visible, ultraviolet, X-rays, and finally gamma rays, the most energetic of all."," The energy of the various components of the electromagnetic spectrum is given by the expression = where h is Plancks constant. The units of wavelength are meters, with the terms microns (denoted m and equal to 10 m) and nanometers (denoted nm and equal to 10 m) being used just as frequently. Frequency is measured in Hertz (Hz), with one Hz being equal to one cycle of a sinusoidal wave per second. A commonly used unit of energy is the electron-volt. Electromagnetic waves can be visualized as propagating sinusoidal waves with wavelength (Fig. 2.11 ), or they can be thought of as a stream of massless particles, each traveling in a wavelike pattern and moving at the speed of light. Each massless particle contains a certain amount (or bundle) of energy, called a photon. We see from Eq. (2-2) that energy is proportional to frequency, so the higher-frequency (shorter wavelength) electromagnetic phenomena carry more energy per photon. Thus, radio waves have photons with low energies, microwaves have more energy than radio waves, infrared still more, then visible, ultraviolet, X-rays, and finally gamma rays, the most energetic of all. High-energy electromagnetic radiation, especially in the X-ray and gamma ray bands, is particularly harmful to living organisms. Light is a type of electromagnetic radiation that can be sensed by the eye. The visible (color) spectrum is shown expanded in Fig"
"Explain why to study these molecules, we would need a source capable of emitting energy in the far (high-energy) ultraviolet band or soft (lowenergy) X-ray bands","In principle, if a sensor can be developed that is capable of detecting energy radiated in a band of the electromagnetic spectrum, we can image events of interest in that band. Note, however, that the wavelength of an electromagnetic wave required to see an object must be of the same size as, or smaller than, the object. For example, a water molecule has a diameter on the order of 10 m. Hence, to study these molecules, we would need a source capable of emitting energy in the far (high-energy) ultraviolet band or soft (lowenergy) X-ray bands."," Chromatic (color) light spans the electromagnetic energy spectrum from approximately 0.43 to 0.79 m, as noted previously. In addition to frequency, three other quantities are used to describe a chromatic light source: radiance, luminance, and brightness. Radiance is the total amount of energy that flows from the light source, and it is usually measured in watts (W). Luminance, measured in lumens (lm), gives a measure of the amount of energy an observer perceives from a light source. For example, light emitted from a source operating in the far infrared region of the spectrum could have significant energy (radiance), but an observer would hardly perceive it; its luminance would be almost zero. Finally, as discussed in Section 2.1 , brightness is a subjective descriptor of light perception that is practically impossible to measure. It embodies the achromatic notion of intensity and is one of the key factors in describing color sensation. In principle, if a sensor can be developed that is capable of detecting energy radiated in a band of the electromagnetic spectrum, we can image events of interest in that band. Note, however, that the wavelength of an electromagnetic wave required to see an object must be of the same size as, or smaller than, the object. For example, a water molecule has a diameter on the order of 10 m. Thus, to study these molecules, we would need a source capable of emitting energy in the far (high-energy) ultraviolet band or soft (lowenergy) X-ray bands. Although imaging is based predominantly on energy from electromagnetic wave radiation, this is not the only method for generating images. For example, we saw in Section 1.3 that sound reflected from objects can be used to form ultrasonic images. Other sources of digital images are electron beams for electron microscopy, and software for generating synthetic images used in graphics and visualization"
Explain why a velocity image might be coded as having both positive and negative values,"Image intensities can become negative during processing, or as a result of interpretation. For example, in radar images, objects moving toward the radar often are interpreted as having negative velocities while objects moving away are interpreted as having positive velocities. Hence, a velocity image might be coded as having both positive and negative values."," FIGURE 2.15 An example of digital image acquisition. (a) Illumination (energy) source. (b) A scene. (c) Imaging system. (d) Projection of the scene onto the image plane. (e) Digitized image. A Simple Image Formation Model As introduced in Section 1.1 , we denote images by two-dimensional functions of the form f(x, y). The value of f at spatial coordinates (x, y) is a scalar quantity whose physical meaning is determined by the source of the image, and whose values are proportional to energy radiated by a physical source (e.g., electromagnetic waves). As a consequence, f(x, y) must be nonnegative and finite; that is, Image intensities can become negative during processing, or as a result of interpretation. For example, in radar images, objects moving toward the radar often are interpreted as having negative velocities while objects moving away are interpreted as having positive velocities. Thus, a velocity image might be coded as having both positive and negative values. When storing and displaying images, we normally scale the intensities so that the smallest negative value becomes 0 (see Section 2.6 0 ( , )< Function f(x, y) is characterized by two components: (1) the amount of source illumination incident on the scene being viewed, and (2) the amount of illumination reflected by the objects in the scene. Appropriately, these are called the illumination and reflectance components, and are denoted by i(x, y) and r(x, y), respectively. The two functions combine as a product to form f(x, y): ( , )= ( , ) ( , ) 0 ( , )< 0 ( , )1 where Thus, reflectance is bounded by 0 (total absorption) and 1 (total reflectance). The nature of i(x, y) is determined by the illumination source, and r(x, y) is determined by the characteristics of the imaged objects. These expressions are applicable also to images formed via transmission of the illumination through a medium, such as a chest X-ray. In this case, we would deal with a transmissivity instead of a reflectivity function, but the limits would be the same as in Eq. (2-6) product in Eq. (2-4) . EXAMPLE 2.1: Some typical values of illumination and reflectance"
Explain why an edge is a local concept that is based on a measure of intensity-level discontinuity at a point,"The concept of an edge is found frequently in discussions dealing with regions and boundaries. However, there is a key difference between these two concepts. The boundary of a finite region forms a closed path and is thus a global concept. As we will discuss in detail in Chapter 10 , edges are formed from pixels with derivative values that exceed a preset threshold. Hence, an edge is a local concept that is based on a measure of intensity-level discontinuity at a point."," If R happens to be an entire image, then its boundary (or border) is defined as the set of pixels in the first and last rows and columns of the image. This extra definition is required because an image has no neighbors beyond its border. Normally, when we refer to a region, we are referring to a subset of an image, and any pixels in the boundary of the region that happen to coincide with the border of the image are included implicitly as part of the region boundary. The concept of an edge is found frequently in discussions dealing with regions and boundaries. However, there is a key difference between these two concepts. The boundary of a finite region forms a closed path and is thus a global concept. As we will discuss in detail in Chapter 10 , edges are formed from pixels with derivative values that exceed a preset threshold. Thus, an edge is a local concept that is based on a measure of intensity-level discontinuity at a point. It is possible to link edge points into edge segments, and sometimes these segments are linked in such a way that they correspond to boundaries, but this is not always the case. The one exception in which edges and boundaries correspond is in binary images. Depending on the type of connectivity and edge operators used (we will discuss these in Chapter 10 ), the edge extracted from a binary region will be the same as the region boundary. This is intuitive. Conceptually, until we arrive at Chapter 10 , it is helpful to think of edges as intensity discontinuities, and of boundaries as closed paths. Distance Measures For pixels p, q, and s, with coordinates (x, y), (u, v), and (w, z), respectively, D is a distance function or metric if a. ( , ) 0 ( ( , ) = 0 iff b. ( , ) = ( , ), and c. ( , ) ( , ) + ( , ) "
Explain why we expect image values to be in the range from 0 to 255,"A few comments about implementing image arithmetic operations are in order before we leave this section. In practice, most images are displayed using 8 bits (even 24-bit color images consist of three separate 8-bit channels). Hence, we expect image values to be in the range from 0 to 255."," FIGURE 2.33 Shading correction. (a) Shaded test pattern. (b) Estimated shading pattern. (c) Product of (a) by the reciprocal of (b). (See Section 3.5 FIGURE 2.34 (a) Digital dental X-ray image. (b) ROI mask for isolating teeth with fillings (white corresponds to 1 and black corresponds to 0). (c) Product of (a) and (b). A few comments about implementing image arithmetic operations are in order before we leave this section. In practice, most images are displayed using 8 bits (even 24-bit color images consist of three separate 8-bit channels). Thus, we expect image values to be in the range from 0 to 255. When images are saved in a standard image format, such as TIFF or JPEG, conversion to this range is automatic. When image values exceed the allowed range, clipping or scaling becomes necessary. For example, the values in the difference of two 8-bit images can range from a minimum of 255 to a maximum of 255, and the values of the sum of two such images can range from 0 to 510. When converting images to eight bits, many software applications simply set all negative values to 0 and set to 255 all values that exceed this limit. Given a digital image g resulting from one or more arithmetic (or other) operations, an approach guaranteeing that the full range of a values is captured into a fixed number of bits is as follows. First, we perform the operation = (2-31) which creates an image whose minimum value is 0. Then, we perform the operation = [ which creates a scaled image, )] (2-32) = 255 gives us a scaled image whose intensities span the full 8-bit scale from 0 to 255. Similar comments apply to 16-bit images or higher. This approach can be used for all arithmetic operations. When performing division, we have the extra requirement that a small number should be added to the pixels of the divisor image to avoid division by 0. Set and Logical Operations In this section, we introduce some important set and logical operations. We also introduce the concept of a fuzzy set"
Explain why a method that is quite useful for enhancing X-ray images may not be the best approach for enhancing infrared images,"Although intensity transformation and spatial filtering methods span a broad range of applications, most of the examples in this chapter are applications to image enhancement. Enhancement is the process of manipulating an image so that the result is more suitable than the original for a specific application. The word specific is important, because it establishes at the outset that enhancement techniques are problem-oriented. Hence, for example, a method that is quite useful for enhancing X-ray images may not be the best approach for enhancing infrared images."," FIGURE 3.2 Intensity transformation functions. (a) Contrast stretching function. (b) Thresholding function. About the Examples in this Chapter Although intensity transformation and spatial filtering methods span a broad range of applications, most of the examples in this chapter are applications to image enhancement. Enhancement is the process of manipulating an image so that the result is more suitable than the original for a specific application. The word specific is important, because it establishes at the outset that enhancement techniques are problem-oriented. Thus, for example, a method that is quite useful for enhancing X-ray images may not be the best approach for enhancing infrared images. There is no general theory of image enhancement. When an image is processed for visual interpretation, the viewer is the ultimate judge of how well a particular method works. When dealing with machine perception, enhancement is easier to quantify. For example, in an automated character-recognition system, the most appropriate enhancement method is the one that results in the best recognition rate, leaving aside other considerations such as computational requirements of one method versus another. Regardless of the application or method used, image enhancement is one of the most visually appealing areas of image processing. Beginners in image processing generally find enhancement applications interesting and relatively simple to understand. Therefore, using examples from image enhancement to illustrate the spatial processing methods developed in this chapter not only saves having an extra chapter in the book dealing with image enhancement but, more importantly, is an effective approach for introducing newcomers to image processing techniques in the spatial domain. As you progress through the remainder of the book, you will find that the material developed in this chapter has a scope that is much broader than just image enhancement"
"Explain why when the values of a kernel are symmetric about its center, correlation and convolution yield the same result","Spatial correlation is illustrated graphically in Fig. 3.34 , and it is described mathematically by Eq. (3-40) . Correlation consists of moving the center of a kernel over an image, and computing the sum of products at each location. The mechanics of spatial convolution are the same, except that the correlation kernel is rotated by 180. Hence, when the values of a kernel are symmetric about its center, correlation and convolution yield the same result."," It certainly is possible to work with kernels of even size, or mixed even and odd sizes. However, working with odd sizes simplifies indexing and is also more intuitive because the kernels have centers falling on integer values, and they are spatially symmetric. Spatial Correlation and Convolution Spatial correlation is illustrated graphically in Fig. 3.34 , and it is described mathematically by Eq. (3-40) . Correlation consists of moving the center of a kernel over an image, and computing the sum of products at each location. The mechanics of spatial convolution are the same, except that the correlation kernel is rotated by 180. Thus, when the values of a kernel are symmetric about its center, correlation and convolution yield the same result. The reason for rotating the kernel will become clear in the following discussion. The best way to explain the differences between the two concepts is by example. FIGURE 3.34 The mechanics of linear spatial filtering using a 3 3 kernel. The pixels are shown as squares to simplify the graphics. Note that the origin of the image is at the top left, but the origin of the kernel is at its center. Placing the origin at the center of spatially symmetric kernels simplifies writing expressions for linear filtering"
"Explain why the appearance of an image depends on the frequencies of its sinusoidal components change the frequencies of those components, and you will change the appearance of the image","A function satisfying some mild conditions can be expressed as the sum of sinusoids of different frequencies and amplitudes. Hence, the appearance of an image depends on the frequencies of its sinusoidal components change the frequencies of those components, and you will change the appearance of the image."," 2. An impulse of strength A in the spatial domain is a constant of value A in the frequency domain, and vice versa. As explained in Chapter 4 , a function (e.g., an image) satisfying some mild conditions can be expressed as the sum of sinusoids of different frequencies and amplitudes. Thus, the appearance of an image depends on the frequencies of its sinusoidal components change the frequencies of those components, and you will change the appearance of the image. What makes this a powerful concept is that it is possible to associate certain frequency bands with image characteristics. For example, regions of an image with intensities that vary slowly (e.g., the walls in an image of a room) are characterized by sinusoids of low frequencies. Similarly, edges and other sharp intensity transitions are characterized by high frequencies. Thus, reducing the high-frequency components of an image will tend to blur it. Linear filtering is concerned with finding suitable ways to modify the frequency content of an image. In the spatial domain we do this via convolution filtering. In the frequency domain we do it with multiplicative filters. The latter is a much more intuitive approach, which is one of the reasons why it is virtually impossible to truly understand spatial filtering without having at least some rudimentary knowledge of the frequency domain"
Explain why reducing the high-frequency components of an image will tend to blur it,"A function satisfying some mild conditions can be expressed as the sum of sinusoids of different frequencies and amplitudes. Thus, the appearance of an image depends on the frequencies of its sinusoidal components change the frequencies of those components, and you will change the appearance of the image. What makes this a powerful concept is that it is possible to associate certain frequency bands with image characteristics. For example, regions of an image with intensities that vary slowly (e.g., the walls in an image of a room) are characterized by sinusoids of low frequencies. Similarly, edges and other sharp intensity transitions are characterized by high frequencies. Hence, reducing the high-frequency components of an image will tend to blur it."," 2. An impulse of strength A in the spatial domain is a constant of value A in the frequency domain, and vice versa. As explained in Chapter 4 , a function (e.g., an image) satisfying some mild conditions can be expressed as the sum of sinusoids of different frequencies and amplitudes. Thus, the appearance of an image depends on the frequencies of its sinusoidal components change the frequencies of those components, and you will change the appearance of the image. What makes this a powerful concept is that it is possible to associate certain frequency bands with image characteristics. For example, regions of an image with intensities that vary slowly (e.g., the walls in an image of a room) are characterized by sinusoids of low frequencies. Similarly, edges and other sharp intensity transitions are characterized by high frequencies. Thus, reducing the high-frequency components of an image will tend to blur it. Linear filtering is concerned with finding suitable ways to modify the frequency content of an image. In the spatial domain we do this via convolution filtering. In the frequency domain we do it with multiplicative filters. The latter is a much more intuitive approach, which is one of the reasons why it is virtually impossible to truly understand spatial filtering without having at least some rudimentary knowledge of the frequency domain"
Explain why image differentiation enhances edges and other discontinuities (such as noise) and de-emphasizes areas with slowly varying intensities,"Sharpening highlights transitions in intensity. Uses of image sharpening range from electronic printing and medical imaging to industrial inspection and autonomous guidance in military systems. In Section 3.5 , we saw that image blurring could be accomplished in the spatial domain by pixel averaging (smoothing) in a neighborhood. Because averaging is analogous to integration, it is logical to conclude that sharpening can be accomplished by spatial differentiation. In fact, this is the case, and the following discussion deals with various ways of defining and implementing operators for sharpening by digital differentiation. The strength of the response of a derivative operator is proportional to the magnitude of the intensity discontinuity at the point at which the operator is applied. Hence, image differentiation enhances edges and other discontinuities (such as noise) and de-emphasizes areas with slowly varying intensities."," median filtering over lowpass filtering in situations such as this, we show in Fig. 3.49(b) the result of filtering the noisy image with a Gaussian lowpass filter, and in Fig. 3.49(c) the result of using a median filter. The lowpass filter blurred the image and its noise reduction performance was poor. The superiority in all respects of median over lowpass filtering in this case is evident. FIGURE 3.49 (a) X-ray image of a circuit board, corrupted by salt-and-pepper noise. (b) Noise reduction using a 19 19 Gaussian lowpass filter kernel with 3.6 Sharpening (Highpass) Spatial Filters Sharpening highlights transitions in intensity. Uses of image sharpening range from electronic printing and medical imaging to industrial inspection and autonomous guidance in military systems. In Section 3.5 , we saw that image blurring could be accomplished in the spatial domain by pixel averaging (smoothing) in a neighborhood. Because averaging is analogous to integration, it is logical to conclude that sharpening can be accomplished by spatial differentiation. In fact, this is the case, and the following discussion deals with various ways of defining and implementing operators for sharpening by digital differentiation. The strength of the response of a derivative operator is proportional to the magnitude of the intensity discontinuity at the point at which the operator is applied. Thus, image differentiation enhances edges and other discontinuities (such as noise) and de-emphasizes areas with slowly varying intensities. As noted in Section 3.5 , smoothing is often referred to as lowpass filtering, a term borrowed from frequency domain processing. In a similar manner, sharpening is often referred to as highpass filtering. In this case, high frequencies (which are responsible for fine details) are passed, while low frequencies are attenuated or rejected. Foundation In the two sections that follow, we will consider in some detail sharpening filters that are based on first- and second-order derivatives, respectively. Before proceeding with that discussion, however, we stop to look at some of the fundamental properties of these derivatives in a digital context. To simplify the explanation, we focus attention initially on one-dimensional derivatives. In particular, we are interested in the behavior of these derivatives in areas of constant intensity, at the onset and end of discontinuities (step and ramp discontinuities), and along intensity ramps. As you will see in Chapter 10 points, lines, and edges in an image"
Explain why we obtain a highpass filter kernel in the spatial domain by subtracting a lowpass filter kernel from a unit impulse with the same center as the kernel,"A constant in the frequency domain is an impulse in the spatial domain. Hence, we obtain a highpass filter kernel in the spatial domain by subtracting a lowpass filter kernel from a unit impulse with the same center as the kernel."," shows the transfer function of a 1-D ideal lowpass filter in the frequency domain [this is the same as Fig. 3.38(a) We know from earlier discussions in this chapter that lowpass filters attenuate or delete high frequencies, while passing low frequencies. A highpass filter behaves in exactly the opposite manner. As Fig. 3.58(b) shows, a highpass filter deletes or attenuates all frequencies below a cut-off value, , and passes all frequencies above this value. Comparing Figs. 3.58(a) and (b) , we see that a highpass filter transfer function is obtained by subtracting a lowpass function from 1. This operation is in the frequency domain. As you know from Section 3.4 , a constant in the frequency domain is an impulse in the spatial domain. Thus, we obtain a highpass filter kernel in the spatial domain by subtracting a lowpass filter kernel from a unit impulse with the same center as the kernel. An image filtered with this kernel is the same as an image obtained by subtracting a lowpass-filtered image from the original image. The unsharp mask defined by Eq. (3-64) Problem 3.53 ). Recall from the discussion of Eq. (3-42) implement equivalent operations (see FIGURE 3.58 Transfer functions of ideal 1-D filters in the frequency domain (u denotes frequency). (a) Lowpass filter. (b) Highpass filter. (c) Bandreject filter. (d) Bandpass filter. (As before, we show only positive frequencies for simplicity.) Figure 3.58(c) shows the transfer function of a bandreject filter. This transfer function can be constructed from the sum of a lowpass and a highpass function with different cut-off frequencies (the highpass function can be constructed from a different lowpass function)"
"Explain why we may interpret infinite-valued membership functions as being the foundation of a fuzzy logic, and the sets generated using them may be viewed as fuzzy sets","We see an immediate difficulty with this formulation: A person 20 years of age is considered young, but a person whose age is 20 years and 1 second is not a member of the set of young people. This illustrates a fundamental problem with crisp sets that limits the use of classical set theory in many practical applications. What we need is more flexibility in what we mean by young; that is, we need a way to express a gradual transition from young to not young. Figure 3.64(b) shows one possibility. The key feature of this function is that it is infinite valued, thus allowing a continuous transition between young and not young. This makes it possible to have degrees of youngness. We can make statements now such as a person being young (upper flat end of the curve), relatively young (toward the beginning of the ramp), 50% young (in the middle of the ramp), not so young (toward the end of the ramp), and so on (note that decreasing the slope of the curve in Fig. 3.64(b) introduces more vagueness in what we mean by young.) These types of vague (fuzzy) statements are more in line with what humans use when talking imprecisely about age. Hence, we may interpret infinite-valued membership functions as being the foundation of a fuzzy logic, and the sets generated using them may be viewed as fuzzy sets."," FIGURE 3.64 Membership functions used to generate (a) a crisp set, and (b) a fuzzy set. We see an immediate difficulty with this formulation: A person 20 years of age is considered young, but a person whose age is 20 years and 1 second is not a member of the set of young people. This illustrates a fundamental problem with crisp sets that limits the use of classical set theory in many practical applications. What we need is more flexibility in what we mean by young; that is, we need a way to express a gradual transition from young to not young. Figure 3.64(b) shows one possibility. The key feature of this function is that it is infinite valued, thus allowing a continuous transition between young and not young. This makes it possible to have degrees of youngness. We can make statements now such as a person being young (upper flat end of the curve), relatively young (toward the beginning of the ramp), 50% young (in the middle of the ramp), not so young (toward the end of the ramp), and so on (note that decreasing the slope of the curve in Fig. 3.64(b) introduces more vagueness in what we mean by young.) These types of vague (fuzzy) statements are more in line with what humans use when talking imprecisely about age. Thus, we may interpret infinite-valued membership functions as being the foundation of a fuzzy logic, and the sets generated using them may be viewed as fuzzy sets. These ideas are formalized in the following section. Principles of Fuzzy Set Theory Fuzzy set theory was introduced by L. A. Zadeh in a paper more than four decades ago (Zadeh [1965]). As the following discussion shows, fuzzy sets provide a formalism for dealing with imprecise information"
Explain why it often is the case that understanding when a function is odd or even plays a key role in our ability to interpret image results based on DFTs,"The preceding discussion indicates that evenness and oddness of sequences depend also on the length of the sequences. For example, we showed already that the sequence {0, 1, 0, 1} is odd. However, the sequence {0, 1, 0, 1, 0} is neither odd nor even, although the basic structure appears to be odd. This is an important issue in interpreting DFT results. We will show later in this section that the DFTs of even and odd functions have some very important characteristics. Hence, it often is the case that understanding when a function is odd or even plays a key role in our ability to interpret image results based on DFTs."," When M is odd, the first term still has to be 0, but the remaining terms form pairs with equal value but opposite signs. The preceding discussion indicates that evenness and oddness of sequences depend also on the length of the sequences. For example, we showed already that the sequence {0, 1, 0, 1} is odd. However, the sequence {0, 1, 0, 1, 0} is neither odd nor even, although the basic structure appears to be odd. This is an important issue in interpreting DFT results. We will show later in this section that the DFTs of even and odd functions have some very important characteristics. Thus, it often is the case that understanding when a function is odd or even plays a key role in our ability to interpret image results based on DFTs. The same basic considerations hold in 2-D. For example, the 6 6 2-D array with center at location (3,3), shown bold in the figure [remember, we start counting at (0,0)], 0 0 0 0 0 0 0 2 0 is odd, as you can prove using Eq. (4-83) . However, adding another row or column of 0s would give a result that is neither odd nor even. In general, inserting a 2-D array of even dimensions into a larger array of zeros, also of even dimensions, preserves the symmetry of the smaller array, provided that the centers coincide. Similarly, a 2-D array of odd dimensions can be inserted into a larger array of zeros of odd dimensions without affecting the symmetry. Note that the inner structure of the preceding array is a Sobel kernel (see Fig. 3.56 filtering purposes"
Explain why the Butterworth filter provides a transition between two extremes,"In this section, we consider three types of lowpass filters: ideal, Butterworth, and Gaussian. These three categories cover the range from very sharp (ideal) to very smooth (Gaussian) filtering. The shape of a Butterworth filter is controlled by a parameter called the filter order. For large values of this parameter, the Butterworth filter approaches the ideal filter. For lower values, the Butterworth filter is more like a Gaussian filter. Hence, the Butterworth filter provides a transition between two extremes."," 4.8 Image Smoothing Using Lowpass Frequency Domain Filters The remainder of this chapter deals with various filtering techniques in the frequency domain, beginning with lowpass filters. Edges and other sharp intensity transitions (such as noise) in an image contribute significantly to the high frequency content of its Fourier transform. Hence, smoothing (blurring) is achieved in the frequency domain by high-frequency attenuation; that is, by lowpass filtering. In this section, we consider three types of lowpass filters: ideal, Butterworth, and Gaussian. These three categories cover the range from very sharp (ideal) to very smooth (Gaussian) filtering. The shape of a Butterworth filter is controlled by a parameter called the filter order. For large values of this parameter, the Butterworth filter approaches the ideal filter. For lower values, the Butterworth filter is more like a Gaussian filter. Thus, the Butterworth filter provides a transition between two extremes. All filtering in this section follows the procedure outlined in the previous section, so all filter transfer functions, H(u, v), are understood to be of size ; that is, the discrete frequency variables are in the range Eqs. (4-100) and (4-101) . 1 and 1 . where P and Q are the padded sizes given by Ideal Lowpass Filters A 2-D lowpass filter that passes without attenuation all frequencies within a circle of radius from the origin, and cuts off all frequencies outside this, circle is called an ideal lowpass filter (ILPF); it is specified by the transfer function where if ( , ) if ( , ) > is a positive constant, and D(u,v) is the distance between a point (u, v) in the frequency domain and the center of the frequency rectangle; that is, ( , ) = ( /2) + ( /2) where, as before, P and Q are the padded sizes from Eqs. (4-102) / . Figure 4.39(a) transfer function H(u,v) and Fig. 4.39(b) shows it displayed as an image. As mentioned in Section 4.3 , the name ideal indicates that all frequencies on or inside a circle of radius are passed without attenuation, whereas all frequencies outside the circle are completely attenuated (filtered out). The ideal lowpass filter transfer function is radially symmetric about the origin. This means that it is defined completely by a radial cross section, as Fig. 4.39(c) cross section 360"
Explain why restoration techniques are oriented toward modeling the degradation and applying the inverse process in order to recover the original image,"As in image enhancement, the principal goal of restoration techniques is to improve an image in some predefined sense. Although there are areas of overlap, image enhancement is largely a subjective process, while image restoration is for the most part an objective process. Restoration attempts to recover an image that has been degraded by using a priori knowledge of the degradation phenomenon. Hence, restoration techniques are oriented toward modeling the degradation and applying the inverse process in order to recover the original image."," We know only but our manner of perceiving them. Immanuel Kant As in image enhancement, the principal goal of restoration techniques is to improve an image in some predefined sense. Although there are areas of overlap, image enhancement is largely a subjective process, while image restoration is for the most part an objective process. Restoration attempts to recover an image that has been degraded by using a priori knowledge of the degradation phenomenon. Thus, restoration techniques are oriented toward modeling the degradation and applying the inverse process in order to recover the original image. In this chapter, we consider linear, space invariant restoration models that are applicable in a variety of restoration situations. We also discuss fundamental techniques of image reconstruction from projections, and their application to computed tomography (CT), one of the most important commercial applications of image processing, especially in health care. Upon completion of this chapter, readers should: Be familiar with the characteristics of various noise models used in image processing, and how to estimate from image data the parameters that define those models"
"Explain why the transform coefficients of a wavelet-based transform, as inner products measuring the similarity of the function being transformed and the associated wavelet basis functions, provide both frequency and temporal information"," A principle consequence of the preceding comments is that each wavelet basis function is characterized by a unique spectrum and location in time. Hence, the transform coefficients of a wavelet-based transform, as inner products measuring the similarity of the function being transformed and the associated wavelet basis functions, provide both frequency and temporal information."," 6.5(b) (d) . Note the width of the basis function in Fig. 6.5(c) is half of that in (d), while the width of its spectrum is double that of (d). It is shifted higher in frequency by a factor of two. The same can be said for the basis function and spectrum in Fig. 6.5(b) when compared to (c). This halving of support in time and doubling of support in frequency produces Heisenberg cells of differing widths and heights, but of equal area. Moreover, each row of cells on the right of Fig. 6.5 represents a unique scale s and range of frequencies. The cells within a row are shifted with respect to one another in time. In accordance with Eq. (4-71) and Table 4.4 of , if ( ) is shifted in time by , { ( )} = ( ) Thus, ||{ ( )}|| = || ( )|| and the spectra of the time-shifted wavelets are identical. This is demonstrated by the basis functions in Figs. 6.5(a) and (b) . Note their Heisenberg cells are identical in size and differ only in position. A principle consequence of the preceding comments is that each wavelet basis function is characterized by a unique spectrum and location in time. Thus, the transform coefficients of a wavelet-based transform, as inner products measuring the similarity of the function being transformed and the associated wavelet basis functions, provide both frequency and temporal information. They furnish the equivalent of a musical score for the function being transformed, revealing not only what notes to play but also when to play them. This is true for all the wavelet bases depicted in the bottom half of Fig. 6.3 . The bases in the top half of the figure provide only the notes; temporal information is lost in the transformation process or is difficult to extract from the transform coefficients (e.g., from the phase component of a Fourier transform). 6.5 Basis Images Since inverse transformation kernel s(x,y,u,v) in Eq. (6-32) values of f(x, y) or T(u,v), Eq. (6-32) can be alternately written as the matrix sum = , = 0, 1, , (0, 0, , ) ( 1, 0, , ) 1, , ) 1, , ) = (0, matrices of size (6-76) for 1. If the underlying s(x,y,u,v) are real-valued, separable, and symmetric, , , , (6-75) ( , ) = where F is an and (6-77) are as previously defined by Eq. (6-20) . In the context of digital image processing, F is a 2-D image and the , are called basis images. They can be arranged in an array, as shown in Fig. 6.6(a) , to provide a concise visual representation of the 2-D basis functions they represent"
Explain why the wavelets and scaling function serve as an othonormal or biorthonormal basis of the DWT expansion," The discrete wavelet transform (DWT) uses those wavelets, together with a single scaling function, to represent a function or image as a linear combination of the wavelets and scaling function. Hence, the wavelets and scaling function serve as an othonormal or biorthonormal basis of the DWT expansion."," 6.10 Wavelet Transforms In 1987, wavelets were shown to be the foundation of a powerful new approach to signal processing and analysis called multiresolution theory (Mallat [1987]) . Multiresolution theory incorporates and unifies techniques from a variety of disciplines, including subband coding from signal processing, quadrature mirror filtering from digital speech recognition, and pyramidal image processing. As its name implies, it is concerned with the representation and analysis of signals (or images) at more than one resolution. A scaling function is used to create a series of approximations of a function or image, each differing by a factor of 2 in resolution from its nearest neighboring approximations, and complementary functions, called wavelets, are used to encode the differences between adjacent approximations. The discrete wavelet transform (DWT) uses those wavelets, together with a single scaling function, to represent a function or image as a linear combination of the wavelets and scaling function. Thus, the wavelets and scaling function serve as an othonormal or biorthonormal basis of the DWT expansion. The Daubechies and Biorthogonal B-splines of Figs. 6.3(f) and (g) and the Haar basis functions of the previous section are but three of the many bases that can be used in DWTs. , wavelets are small waves with bandpass spectra as defined in Eq. (6-72) The discrete wavelet transform, like all transforms considered in this chapter, generates linear expansions of functions with respect to sets of orthonormal or biorthonormal expansion functions"
"Explain why when we call an object red, orange, or yellow, we are referring to its hue","The characteristics generally used to distinguish one color from another are brightness, hue, and saturation. As indicated earlier in this section, brightness embodies the achromatic notion of intensity. Hue is an attribute associated with the dominant wavelength in a mixture of light waves. Hue represents dominant color as perceived by an observer. Hence, when we call an object red, orange, or yellow, we are referring to its hue."," CRT displays started being replaced in the late 1990s by flat-panel digital technologies, such as liquid crystal displays (LCDs) and plasma devices. Although they are fundamentally different from CRTs, these and similar technologies use the same principle in the sense that they all require three subpixels (red, green, and blue) to generate a single color pixel. LCDs use properties of polarized light to block or pass light through the LCD screen and, in the case of active matrix display technologies, thin film transistors (TFTs) are used to provide the proper signals to address each pixel on the screen. Light filters are used to produce the three primary colors of light at each pixel triad location. In plasma units, pixels are tiny gas cells coated with phosphor to produce one of the three primary colors. The individual cells are addressed in a manner analogous to LCDs. This individual pixel triad coordinate addressing capability is the foundation of digital displays. The characteristics generally used to distinguish one color from another are brightness, hue, and saturation. As indicated earlier in this section, brightness embodies the achromatic notion of intensity. Hue is an attribute associated with the dominant wavelength in a mixture of light waves. Hue represents dominant color as perceived by an observer. Thus, when we call an object red, orange, or yellow, we are referring to its hue. Saturation refers to the relative purity or the amount of white light mixed with a hue. The pure spectrum colors are fully saturated. Colors such as pink (red and white) and lavender (violet and white) are less saturated, with the degree of saturation being inversely proportional to the amount of white light added. Hue and saturation taken together are called chromaticity and, therefore, a color may be characterized by its brightness and chromaticity. The amounts of red, green, and blue needed to form any particular color are called the tristimulus values, and are denoted, X, Y, and Z, respectively. A color is then specified by its trichromatic coefficients, defined as = (7-1) + + + (7-2) and = We see from these equations that + (7-4) Our use of x, y, and z in this context follows convention. These should not be confused with our use of (x, y) throughout the book to denote spatial coordinates"
"Explain why when publishers talk about four-color printing, they are referring to the three CMY colors, plus a portion of black","So, in order to produce true black (which is the predominant color in printing), a fourth color, black, denoted by K, is added, giving rise to the CMYK color model. The black is added in just the proportions needed to produce true black. Hence, when publishers talk about four-color printing, they are referring to the three CMY colors, plus a portion of black."," According to Fig. 7.4 , equal amounts of the pigment primaries, cyan, magenta, and yellow, should produce black. In practice, because C, M, and Y inks seldom are pure colors, combining these colors for printing black produces instead a muddy-looking brown. So, in order to produce true black (which is the predominant color in printing), a fourth color, black, denoted by K, is added, giving rise to the CMYK color model. The black is added in just the proportions needed to produce true black. Thus, when publishers talk about four-color printing, they are referring to the three CMY colors, plus a portion of black. The conversion from CMY to CMYK begins by letting = min( , If (7-6) The C, M, and Y on the right side of Eqs. (7-6) left of Eqs. (7-7) -(7-12) (7-7) (7-8) (7-9) are in the CMYK system"
Explain why intensity values of 255 in an 8-bit image coming from such a system automatically imply a problem with the weld,"In the preceding simple example, the grayscale was divided into intervals and a different color was assigned to each, with no regard for the meaning of the gray levels in the image. Interest in that case was simply to view the different gray levels constituting the image. Intensity slicing assumes a much more meaningful and useful role when subdivision of the grayscale is based on physical characteristics of the image. For instance, Fig. 7.19(a) shows an X-ray image of a weld (the broad, horizontal dark region) containing several cracks and porosities (the bright streaks running horizontally through the middle of the image). When there is a porosity or crack in a weld, the full strength of the X-rays going through the object saturates the imaging sensor on the other side of the object. Hence, intensity values of 255 in an 8-bit image coming from such a system automatically imply a problem with the weld."," FIGURE 7.18 (a) Grayscale image of the Picker Thyroid Phantom. (b) Result of intensity slicing using eight colors. (Courtesy of Dr. J. L. Blankenship, Oak Ridge National Laboratory.) In the preceding simple example, the grayscale was divided into intervals and a different color was assigned to each, with no regard for the meaning of the gray levels in the image. Interest in that case was simply to view the different gray levels constituting the image. Intensity slicing assumes a much more meaningful and useful role when subdivision of the grayscale is based on physical characteristics of the image. For instance, Fig. 7.19(a) shows an X-ray image of a weld (the broad, horizontal dark region) containing several cracks and porosities (the bright streaks running horizontally through the middle of the image). When there is a porosity or crack in a weld, the full strength of the X-rays going through the object saturates the imaging sensor on the other side of the object. Thus, intensity values of 255 in an 8-bit image coming from such a system automatically imply a problem with the weld. If human visual analysis is used to inspect welds (still a common procedure today), a simple color coding that assigns one color to level 255 and another to all other intensity levels can simplify the inspectors job considerably. Figure 7.19(b) shows the result. No explanation is required to arrive at the conclusion that human error rates would be lower if images were displayed in the form of Fig. 7.19(b) , instead of the form in Fig. 7.19(a) . In other words, if an intensity value, or range of values, one is looking for is known, intensity slicing is a simple but powerful aid in visualization, especially if numerous images have to be inspected on a routine basis"
Explain why any string of Huffman encoded symbols can be decoded by examining the individual symbols of the string in a left-to-right manner,"Huffmans procedure creates the optimal code for a set of symbols and probabilities subject to the constraint that the symbols be coded one at a time. After the code has been created, coding and/or error-free decoding is accomplished in a simple lookup table manner. The code itself is an instantaneous uniquely decodable block code. It is called a block code because each source symbol is mapped into a fixed sequence of code symbols. It is instantaneous because each code word in a string of code symbols can be decoded without referencing succeeding symbols. It is uniquely decodable because any string of code symbols can be decoded in only one way. Hence, any string of Huffman encoded symbols can be decoded by examining the individual symbols of the string in a left-to-right manner."," The second step in Huffmans procedure is to code each reduced source, starting with the smallest source and working back to the original source. The minimal length binary code for a two-symbol source, of course, are the symbols 0 and 1. As Fig. 8.8 shows, these symbols are assigned to the two symbols on the right. (The assignment is arbitrary; reversing the order of the 0 and 1 would work just as well.) As the reduced source symbol with probability 0.6 was generated by combining two symbols in the reduced source to its left, the 0 used to code it is now assigned to both of these symbols, and a 0 and 1 are arbitrarily appended to each to distinguish them from each other. This operation is then repeated for each reduced source until the original source is reached. The final code appears at the far left in Fig. 8.8 . The average length of this code is avg = (0.4)(1) + (0.3)(2) + (0.1)(3) + (0.1)(4) + (0.06)(5) + (0.04)(5) = 2.2 bits/pixel and the entropy of the source is 2.14 bits/symbol. Huffmans procedure creates the optimal code for a set of symbols and probabilities subject to the constraint that the symbols be coded one at a time. After the code has been created, coding and/or error-free decoding is accomplished in a simple lookup table manner. The code itself is an instantaneous uniquely decodable block code. It is called a block code because each source symbol is mapped into a fixed sequence of code symbols. It is instantaneous because each code word in a string of code symbols can be decoded without referencing succeeding symbols. It is uniquely decodable because any string of code symbols can be decoded in only one way. Thus, any string of Huffman encoded symbols can be decoded by examining the individual symbols of the string in a left-to-right manner. For the binary code of Fig. 8.8 , a left-to-right scan of the encoded string 010100111100 reveals that the first valid code word is 01010, which is the code for symbol . The next valid code is 011, which corresponds to symbol completely decoded message to be . FIGURE 8.7 Huffman source reductions"
Explain why the probabilities adapt to the local statistics of the symbols being coded,"With accurate input symbol probability models, that is, models that provide the true probabilities of the symbols being coded, arithmetic coders are near optimal in the sense of minimizing the average number of code symbols required to represent the symbols being coded. As in both Huffman and Golomb coding, however, inaccurate probability models can lead to non-optimal results. A simple way to improve the accuracy of the probabilities employed is to use an adaptive, context dependent probability model. Adaptive probability models update symbol probabilities as symbols are coded or become known. Hence, the probabilities adapt to the local statistics of the symbols being coded."," Source Symbol FIGURE 8.12 Arithmetic coding procedure. Initial Subinterval [0.0, 0.2) [0.2, 0.4) [0.4, 0.8) [0.8, 1.0) Adaptive Context Dependent Probability Estimates With accurate input symbol probability models, that is, models that provide the true probabilities of the symbols being coded, arithmetic coders are near optimal in the sense of minimizing the average number of code symbols required to represent the symbols being coded. As in both Huffman and Golomb coding, however, inaccurate probability models can lead to non-optimal results. A simple way to improve the accuracy of the probabilities employed is to use an adaptive, context dependent probability model. Adaptive probability models update symbol probabilities as symbols are coded or become known. Thus, the probabilities adapt to the local statistics of the symbols being coded. Context-dependent models provide probabilities that are based on a predefined neighborhood of pixels, called the context, around the symbols being coded. Normally, a causal context (one limited to symbols that have already been coded) is used. Both the Q-coder (Pennebaker et al. [1988]) and MQ-coder (ISO/IEC [2000]), two well-known arithmetic coding techniques that have been incorporated into the JBIG, JPEG-2000, and other important image compression standards, use probability models that are both adaptive and context dependent.The Q-coder dynamically updates symbol probabilities during the interval renormalizations that are part of the arithmetic coding process. Adaptive context dependent models also have been used in Golomb coding, for example, in the JPEG-LS compression standard. coding often is used when binary symbols are to be coded. As each symbol (or bit) begins the coding process, its context is formed in the Context determination block of Fig. 8.13(a) . Figures 8.13(b) through (d) show three possible contexts that can be used: (1) the immediately preceding symbol, (2) a group of preceding symbols, and (3) some number of preceding symbols plus symbols on the previous scan line. For the three cases shown, the Probability estimation block must manage 2 (or 2), 2 (or 256), and 2 (or 32) is used, conditional probabilities (0 | = 0) (the probability that the symbol being coded is a 0 given that the preceding symbol is a 0), (1 || = 0), (0 || = 1) and (1 | = 1) must be tracked. The appropriate probabilities are then passed to the Arithmetic coding block as a function of the current context, and contexts and their associated probabilities. For instance, if the context in Fig. 8.13(b) drive the generation of the arithmetically coded output sequence in accordance with the process illustrated in Fig. 8.12 . The probabilities associated with the context involved in the current coding step are then updated to reflect the fact that another symbol within that context has been processed"
"Explain why inputs and outputs of our morphological procedures are images, not individual sets","We work with sets of foreground pixels embedded in a set of background pixels to form a complete image. Hence, inputs and outputs of our morphological procedures are images, not individual sets."," Remember, set A can represent (be the union of) multiple disjoint sets of foreground pixels (i.e., objects). As noted, we work with sets of foreground pixels embedded in a set of background pixels to form a complete image, I. Thus, inputs and outputs of our morphological procedures are images, not individual sets. We could make this fact explicit by writing Eq. (9-3) ( ) as (9-4) where I is a rectangular array of foreground and background pixels. The contents of the first braces say the same thing as Eq. (9-3) with the added clarification that A is a subset of (i.e., is contained in) I. The union with the operation inside the second set of braces adds the pixels that are not in subset A (i.e., , which is the set of background pixels) to the result from the first braces, requiring also that the background pixels be part of the rectangle defined by I. In words, all this equation says is that erosion of I by B is the set of all points, z, such that B, translated by z, is contained in A. The equation also makes explicit that A is contained in I, that the result is embedded in a set of background pixels, and that the entire process is of the same size as I. Of course, we do not use the cumbersome notation of Eq. (9-4) use the notation when a morphological operation uses only foreground elements, and when the operation uses foreground and background elements. This distinction may seem trivial, but suppose that we want to perform erosion with Eq. (9-3) , using the foreground elements of the structuring element in the last column in Fig. 9.2 . This structuring element also has background elements, but Eq. (9-3) assumes that B only has foreground elements. In fact, erosion is defined only for operations between foreground elements, so writing would be meaningless without the explanation embedded in Eq. (9-4) . To avoid confusion, we use A in morphological expressions when the operation involves only foreground elements, and I when the operation also involves background and/or dont-care elements. We also avoid using standard morphological symbols like when working with mixed SEs"
Explain why we take this opportunity to illustrate how to solve a problem by combining several of the morphological techniques discussed up to this point,"Pruning methods are an essential complement to thinning and skeletonizing algorithms, because these procedures tend to leave spurs (parasitic components) that need to be cleaned up by postprocessing. We begin the discussion with a pruning problem, then develop a solution based on the material introduced in the preceding sections. Hence, we take this opportunity to illustrate how to solve a problem by combining several of the morphological techniques discussed up to this point."," The fourth column contains two partial skeletons, and the final result at the bottom of the column. The final skeleton not only is thicker than it needs to be but, more important, it is not connected. This result is not unexpected, as nothing in the preceding formulation of the morphological skeleton guarantees connectivity. Morphology produces an elegant formulation in terms of erosions and openings of the given set. However, heuristic formulations (see Section 12.2 skeleton must be maximally thin, connected, and minimally eroded. The entries in the fifth and sixth columns deal with reconstructing the original set from its skeleton subsets. The fifth column are the dilations of ( ), set A which, according to Eq. (9-32) . Finally, the last column shows reconstruction of Pruning Pruning methods are an essential complement to thinning and skeletonizing algorithms, because these procedures tend to leave spurs (parasitic components) that need to be cleaned up by postprocessing. We begin the discussion with a pruning problem, then develop a solution based on the material introduced in the preceding sections. Thus, we take this opportunity to illustrate how to solve a problem by combining several of the morphological techniques discussed up to this point. A common approach in the automated recognition of hand-printed characters is to analyze the shape of the skeleton of a character"
Explain why we can expect second-order derivatives to enhance fine detail (including noise) much more than first-order derivatives,"Consider the properties of the first and second derivatives as we traverse the profile from left to right. Initially, the first-order derivative is nonzero at the onset and along the entire intensity ramp, while the second-order derivative is nonzero only at the onset and end of the ramp. Because the edges of digital images resemble this type of transition, we conclude that first-order derivatives produce thick edges, and second-order derivatives much thinner ones. Next we encounter the isolated noise point. Here, the magnitude of the response at the point is much stronger for the second- than for the first-order derivative. This is not unexpected, because a secondorder derivative is much more aggressive than a first-order derivative in enhancing sharp changes. Hence, we can expect second-order derivatives to enhance fine detail (including noise) much more than first-order derivatives."," second-order derivatives behave as they encounter a point, a line, and the edges of objects. In this diagram the transition in the ramp spans four pixels, the noise point is a single pixel, the line is three pixels thick, and the transition of the step edge takes place between adjacent pixels. The number of intensity levels was limited to eight for simplicity. Consider the properties of the first and second derivatives as we traverse the profile from left to right. Initially, the first-order derivative is nonzero at the onset and along the entire intensity ramp, while the second-order derivative is nonzero only at the onset and end of the ramp. Because the edges of digital images resemble this type of transition, we conclude that first-order derivatives produce thick edges, and second-order derivatives much thinner ones. Next we encounter the isolated noise point. Here, the magnitude of the response at the point is much stronger for the second- than for the first-order derivative. This is not unexpected, because a secondorder derivative is much more aggressive than a first-order derivative in enhancing sharp changes. Thus, we can expect second-order derivatives to enhance fine detail (including noise) much more than first-order derivatives. The line in this example is rather thin, so it too is fine detail, and we see again that the second derivative has a larger magnitude. Finally, note in both the ramp and step edges that the second derivative has opposite signs (negative to positive or positive to negative) as it transitions into and out of an edge. This double-edge effect is an important characteristic that can be used to locate edges, as we will show later in this section. As we move into the edge, the sign of the second derivative is used also to determine whether an edge is a transition from light to dark (negative second derivative), or from dark to light (positive second derivative) In summary, we arrive at the following conclusions: (1) First-order derivatives generally produce thicker edges. (2) Second-order derivatives have a stronger response to fine detail, such as thin lines, isolated points, and noise. (3) Second-order derivatives produce a double-edge response at ramp and step transitions in intensity. (4) The sign of the second derivative can be used to determine whether a transition into an edge is from light to dark or dark to light. The approach of choice for computing first and second derivatives at every pixel location in an image is to use spatial convolution. For the 3 3 filter kernel in Fig. 10.3 , the procedure is to compute the sum of products of the kernel coefficients with the intensity values in the region encompassed by the kernel, as we explained in Section 3.4 kernel is . That is, the response of the filter at the center point of the notation for the kernel coefficients"
Explain why you will encounter additional edge-point linking methods in the next two chapters,"Ideally, edge detection should yield sets of pixels lying only on edges. In practice, these pixels seldom characterize edges completely because of noise, breaks in the edges caused by nonuniform illumination, and other effects that introduce discontinuities in intensity values. Therefore, edge detection typically is followed by linking algorithms designed to assemble edge pixels into meaningful edges and/or region boundaries. In this section, we discuss two fundamental approaches to edge linking that are representative of techniques used in practice. The first requires knowledge about edge points in a local region (e.g., a 3 3 neighborhood), and the second is a global approach that works with an entire edge map. As it turns out, linking points along the boundary of a region is also an important aspect of some of the segmentation methods discussed in the next chapter, and in extracting features from a segmented image, as we will do in Chapter 12 . Hence, you will encounter additional edge-point linking methods in the next two chapters."," The price paid for the improved performance of the Canny algorithm is a significantly more complex implementation than the two approaches discussed earlier. In some applications, such as real-time industrial image processing, cost and speed requirements usually dictate the use of simpler techniques, principally the thresholded gradient approach. When edge quality is the driving force, the Marr-Hildreth and Canny algorithms, especially the latter, offer superior alternatives. Linking Edge Points Ideally, edge detection should yield sets of pixels lying only on edges. In practice, these pixels seldom characterize edges completely because of noise, breaks in the edges caused by nonuniform illumination, and other effects that introduce discontinuities in intensity values. Therefore, edge detection typically is followed by linking algorithms designed to assemble edge pixels into meaningful edges and/or region boundaries. In this section, we discuss two fundamental approaches to edge linking that are representative of techniques used in practice. The first requires knowledge about edge points in a local region (e.g., a 3 3 neighborhood), and the second is a global approach that works with an entire edge map. As it turns out, linking points along the boundary of a region is also an important aspect of some of the segmentation methods discussed in the next chapter, and in extracting features from a segmented image, as we will do in Chapter 12 . Thus, you will encounter additional edge-point linking methods in the next two chapters. Local Processing A simple approach for linking edge points is to analyze the characteristics of pixels in a small neighborhood about every point (x, y) that has been declared an edge point by one of the techniques discussed in the preceding sections. All points that are similar according to predefined criteria are linked, forming an edge of pixels that share common properties according to the specified criteria"
Explain why we can find the (continuous-valued) eigenvector corresponding to the second smallest eigenvalue using either a generalized or a standard eigenvalue solver," We can convert the preceding generalized eigenvalue formulation into a standard eigenvalue problem by writing Eq. (10-105) as (see Problem 10.47): = (10-106) where ) (10-107) and = (10-108) from which it follows that = (10-109) Hence, we can find the (continuous-valued) eigenvector corresponding to the second smallest eigenvalue using either a generalized or a standard eigenvalue solver."," = (10-103) be the sum of the weights from node to all other nodes in V. Using these definitions, we can write Eq. (10-98) as ( , )= ( , ) ( , ) + ( , ) (, ) , + (10-104) The objective is to find a vector, x, that minimizes Ncut(A, B). A closed-form solution that minimizes Eq. (10-104) can be found, but only if the elements of x are allowed to be real, continuous numbers instead of being constrained to be 1. The solution derived by Shi and Malik [2000] is given by solving the generalized eigensystem expression ( (10-105) where D is a diagonal matrix with main-diagonal elements , = 1, 2, , , and W is a weight matrix with elements w(i, j), as defined earlier. Solving Eq. (10-105) gives K eigenvalues and K eigenvectors, each corresponding to one eigenvalue. The solution to our problem is the eigenvector corresponding the second smallest eigenvalue. We can convert the preceding generalized eigenvalue formulation into a standard eigenvalue problem by writing Eq. (10-105) as (see Problem 10.47): = (10-106) where ) (10-107) and = (10-108) from which it follows that = (10-109) Thus, we can find the (continuous-valued) eigenvector corresponding to the second smallest eigenvalue using either a generalized or a standard eigenvalue solver. The desired (discrete) vector x can be generated from the resulting, continuous valued solution vector by finding a splitting point that divides the values of the continuous eigenvector elements into two parts. We do this by finding the splitting point that yields the smallest value of Ncut(A, B), since this is the quantity we are trying to minimize. To simplify the search, we divide the range of values in the continuous vector into Q evenly spaced values, evaluate Eq. (10-104) for each value, and choose the splitting point that yields the smallest value of Ncut(A, B). Then, all values of the eigenvector with values above the split point are assigned the value 1; all others are assigned the value 1. The result is the desired vector x. Then, partition A is the set nodes in V corresponding to 1s in x; the remaining nodes correspond to partition B. This partitioning is carried out only if the stability criterion discussed in the following paragraph is met"
"Explain why in practice, we often see watershed segmentation applied to the gradient of an image, rather than to the image itself","One of the principal applications of watershed segmentation is in the extraction of nearly uniform (blob-like) objects from the background. Regions characterized by small variations in intensity have small gradient values. Hence, in practice, we often see watershed segmentation applied to the gradient of an image, rather than to the image itself."," Because of neighboring contrast, the leftmost basin in Fig. 10.60(c) appears black, but it is a few shades lighter than the black background. The mid-gray in the second basin is a natural gray from the image in (a). One of the principal applications of watershed segmentation is in the extraction of nearly uniform (blob-like) objects from the background. Regions characterized by small variations in intensity have small gradient values. Thus, in practice, we often see watershed segmentation applied to the gradient of an image, rather than to the image itself. In this formulation, the regional minima of catchment basins correlate nicely with the small value of the gradient corresponding to the objects of interest. Dam Construction Dam construction is based on binary images, which are members of 2-D integer space points is to use morphological dilation (see Section 9.2)"
"Explain why even when the MOG snake yields good results, it makes sense to consider a GVF approach to determine if savings in computation can be achieved for the given application","MOG-based snake. And, if we had more images to segment with the same parameters and edge map, the 160 iterations would become even less meaningful because the GVF forces would be computed only once. Hence, even when the MOG snake yields good results, it makes sense to consider a GVF approach to determine if savings in computation can be achieved for the given application."," As Fig. 11.8(d) shows, a MOG-based snake did a good job of segmenting the rose image in 400 iterations. In this example, we show that we can obtain an equivalent result in less than 100 iterations using a GVF-based snake. Figure 11.12(a) 11.12(b) is the result of 90 iterations with a MOG-based snake with the same parameters as in Example 11.3 . Figure is the result after the same number of iterations, but using a GVF force field obtained with the same edge map as in Example 11.3 . The GVF force field was generated in 160 iterations using Eq. (11-55) with = 0.2 . If we add the 160 iterations to the 90 required by the GVF snake to converge, we would still save close to half the number of iterations required by the MOG-based snake. And, if we had more images to segment with the same parameters and edge map, the 160 iterations would become even less meaningful because the GVF forces would be computed only once. Thus, even when the MOG snake yields good results, it makes sense to consider a GVF approach to determine if savings in computation can be achieved for the given application. FIGURE 11.12 (a) MOG-based snake after 90 iterations. The snake is beginning to attach itself to the boundary, but it has a long way to go before it fully converges to the boundary. (b) GVF-based snake after the same number of iterations"
"Explain why we think of features as attributes that are going to help us assign unique labels to objects in an image or, more generally, are going to be of value in differentiating between entire images or families of images","Although there is no universally accepted, formal definition of what constitutes an image feature, there is little argument that, intuitively, we generally think of a feature as a distinctive attribute or description of something we want to label or differentiate. For our purposes, the key words here are label and differentiate. The something of interest in this chapter refers either to individual image objects, or even to entire images or sets of images. Hence, we think of features as attributes that are going to help us assign unique labels to objects in an image or, more generally, are going to be of value in differentiating between entire images or families of images."," Have a feel for the types of features that have a good chance of success in a given application. 12.1 Background Although there is no universally accepted, formal definition of what constitutes an image feature, there is little argument that, intuitively, we generally think of a feature as a distinctive attribute or description of something we want to label or differentiate. For our purposes, the key words here are label and differentiate. The something of interest in this chapter refers either to individual image objects, or even to entire images or sets of images. Thus, we think of features as attributes that are going to help us assign unique labels to objects in an image or, more generally, are going to be of value in differentiating between entire images or families of images. There are two principal aspects of image feature extraction: feature detection, and feature description. That is, when we refer to feature extraction, we are referring to both detecting the features and then describing them. To be useful, the extraction process must encompass both. The terminology you are likely to encounter in image processing and analysis to describe feature detection and description varies, but a simple example will help clarify our use of these term. Suppose that we use object corners as features for some image processing task. In this chapter, detection refers to finding the corners in a region or image. Description, on the other hand, refers to assigning quantitative (or sometimes qualitative) attributes to the detected features, such as corner orientation, and location with respect to other corners. In other words, knowing that there are corners in an image has limited use without additional information that can help us differentiate between objects in an image, or between images, based on corners and their attributes"
"Explain why for the purpose of analysis, every periodic pattern is associated with only one peak in the spectrum, rather than two","Here, we consider three features of the Fourier spectrum that are useful for texture description: (1) prominent peaks in the spectrum give the principal direction of the texture patterns; (2) the location of the peaks in the frequency plane gives the fundamental spatial period of the patterns; and (3) eliminating any periodic components via filtering leaves nonperiodic image elements, which can then be described by statistical techniques. Recall that the spectrum is symmetric about the origin, so only half of the frequency plane needs to be considered. Hence, for the purpose of analysis, every periodic pattern is associated with only one peak in the spectrum, rather than two."," Spectral Approaches As we discussed in Section 5.4 , the Fourier spectrum is ideally suited for describing the directionality of periodic or semiperiodic 2-D patterns in an image. These global texture patterns are easily distinguishable as concentrations of high-energy bursts in the spectrum. Here, we consider three features of the Fourier spectrum that are useful for texture description: (1) prominent peaks in the spectrum give the principal direction of the texture patterns; (2) the location of the peaks in the frequency plane gives the fundamental spatial period of the patterns; and (3) eliminating any periodic components via filtering leaves nonperiodic image elements, which can then be described by statistical techniques. Recall that the spectrum is symmetric about the origin, so only half of the frequency plane needs to be considered. Thus, for the purpose of analysis, every periodic pattern is associated with only one peak in the spectrum, rather than two. Detection and interpretation of the spectrum features just mentioned often are simplified by expressing the spectrum in polar coordinates to yield a function ( , ), where S is the spectrum function, and r and each direction , ( , ) may be considered a 1-D function for a fixed value of analyzing ( ) . Similarly, for each frequency , ( ) ( ) for a fixed value of r yields the behavior along a circle centered on the origin"
Explain why intensity transitions caused by edges are eliminated,"Using a difference of Gaussians yields edges in an image. But keypoints of interest in SIFT are corner-like features, which are significantly more localized. Hence, intensity transitions caused by edges are eliminated."," Improving the Accuracy of Keypoint Locations When a continuous function is sampled, its true maximum or minimum may actually be located between sample points. The usual approach used to get closer to the true extremum (to achieve subpixel accuracy) is to fit an interpolating function at each extremum point found in the digital function, then look for an improved extremum location in the interpolated function. SIFT uses the linear and quadratic terms of a Taylor series expansion of ( , , ), shifted so that the origin is located at the sample point being examined. In vector form, the expression is ( )= = + + (12-71) operator, / = / / = and H is the Hessian matrix / = / / / / / / / / The location of the extremum, ^, is found by taking the derivative of Eq. (12-71) us (see Problem 12.40 ): with respect to x and setting it to zero, which gives ^= ( ) The Hessian and gradient of D are approximated using differences of neighboring points, as we did in Section 10.2 . The resulting 3 3 system of linear equations is easily solved computationally. If the offset ^ is greater than 0.5 in any of its three dimensions, we conclude that the extremum lies closer to another sample point, in which case the sample point is changed and the interpolation is performed about that point instead. The final offset ^ is added to the location of its sample point to obtain the interpolated estimate of the location of the extremum. The function value at the extremum, (^), is used by SIFT for rejecting unstable extrema with low contrast, where (^) is obtained by substituting Eq. (12-74) , giving (see Problem 12.40 (^) = + ( ) ^ , any extrema for which (^) was less than 0.03 was rejected, based on all Eliminating Edge Responses Recall from Section 10.2 that using a difference of Gaussians yields edges in an image. But keypoints of interest in SIFT are corner-like features, which are significantly more localized. Thus, intensity transitions caused by edges are eliminated. To quantify the difference between edges and corners, we can look at local curvature. An edge is characterized by high curvature in one direction, and low curvature in the orthogonal direction. Curvature at a point in an image can be estimated from the 2 2 Hessian matrix evaluated at that point. Thus, to estimate local curvature of the DoG at any level in scalar space, we compute the Hessian matrix of D at that level: If you display an image as a topographic map (see Fig. 2.18 ), edges will appear as ridges that have low curvature along the ridge and high curvature perpendicular to it. / / / / where the form on the right uses the same notation as the A term [Eq. (12-61) ] of the Harris matrix (but note that the main diagonals are different). The eigenvalues of H are proportional to the curvatures of D. As we explained in connection with the Harris-Stephens corner detector, we can avoid direct computation of the eigenvalues by formulating tests based on the trace and determinant of H, which are equal to the sum and product of the eigenvalues, respectively. To use notation different from the HS discussion, let and be the eigenvalues of H with the largest and smallest magnitude, respectively. Using the relationship between the eigenvalues of H and its trace and determinant we have (remember, H is is symmetric and of size 2 2): Tr( ) = Det( ) = = ( (12-77) If the determinant is negative, the curvatures have different signs and the keypoint in question cannot be an extremum, so it is discarded"
"Explain why for the locations with multiple peaks of similar magnitude, there will be multiple keypoints created at the same location and scale, but with different orientations","Peaks in the histogram correspond to dominant local directions of local gradients. The highest peak in the histogram is detected and any other local peak that is within 80% of the highest peak is used also to create another keypoint with that orientation. Hence, for the locations with multiple peaks of similar magnitude, there will be multiple keypoints created at the same location and scale, but with different orientations."," (12-80) (12-81) A histogram of orientations is formed from the gradient orientations of sample points in a neighborhood of each keypoint. The histogram has 36 bins covering the 360 range of orientations on the image plane. Each sample added to the histogram is weighed by its gradient magnitude, and by a circular Gaussian function with a standard deviation 1.5 times the scale of the keypoint. Peaks in the histogram correspond to dominant local directions of local gradients. The highest peak in the histogram is detected and any other local peak that is within 80% of the highest peak is used also to create another keypoint with that orientation. Thus, for the locations with multiple peaks of similar magnitude, there will be multiple keypoints created at the same location and scale, but with different orientations. SIFT assigns multiple orientations to only about 15% of points with multiple orientations, but these contribute significant to image matching (to be discussed later and in Chapter 13 ). Finally, a parabola is fit to the three histogram values closest to each peak to interpolate the peak position for better accuracy. shows the same keypoints as Fig. 12.60 superimposed on the image and showing keypoint orientations as arrows"
Explain why its important to review exactly what information your application makes available," Information Disclosure Information disclosure is not a technical problem per se. Its quite possible that your application may provide an attacker with an insightful piece of knowledge that could aid him in taking advantage of the application.Hence, its important to review exactly what information your application makes available."," In all languages, you need to trace back to the origin of the user data and determine if the data goes through any filtering of HTML and/or scripting characters. If it doesnt, an attacker could use your Web application for a CSS attack against another user (taking advantage of your user/customer due to your applications insecurity). Information Disclosure Information disclosure is not a technical problem per se. Its quite possible that your application may provide an attacker with an insightful piece of knowledge that could aid him in taking advantage of the application.Therefore, its important to review exactly what information your application makes available. Some general things to look for in all languages include: Printing sensitive information (passwords, credit card numbers) in full display Many applications do not transmit full credit card numbers; rather, they show only the last four or five digits. Passwords should be obfuscated so a bypasser cannot spot the actual password on a users terminal"
Explain why its not far off base for Web applications to interact with the file system as well," Checking for File System Access/Interaction The Web is basically a graphically based file sharing protocol; the opening and reading of user-specified files is the core of what makes the Web run.Hence, its not far off base for Web applications to interact with the file system as well."," 363_Web_App_06.qxd 3:45 PM Code Auditing and Reverse Engineeering Chapter 6 Because the actual method of information disclosure can widely vary within any language, there are no exact functions or code snippets to look for. Checking for File System Access/Interaction The Web is basically a graphically based file sharing protocol; the opening and reading of user-specified files is the core of what makes the Web run.Therefore, its not far off base for Web applications to interact with the file system as well. Essentially, you should definitively know exactly where, when, and how a Web application accesses the local file system on the server.The danger lies in using filenames that contain tainted data"
Explain why ColdFusion is composed primarily of tags,"In addition to the application server, ColdFusion is also a powerful programming language. When it was created, the idea was to make a language that looked as close as possible to the default language of the NetHTML.Hence, ColdFusion is composed primarily of tags."," When your code is received by the CFML compiler, it converts it into a binary version that is stored in memory. This compiled Java bytecode is able to run faster, and is done in a single step. This is an improvement that occurred in ColdFusion 6.1, as before this, the CFML was first converted into Java source code and then compiled as bytecode. The change in how code becomes bytecode improved performance in ColdFusion considerably, making the latest version the fastest of any previous ones. Using the Benefit of Rapid Development As weve said, in addition to the application server, ColdFusion is also a powerful programming language. When it was created, the idea was to make a language that looked as close as possible to the default language of the NetHTML.Therefore, ColdFusion is composed primarily of tags. Because ColdFusion looks like HTML and has much of the same syntax structure, it is easy to learn and very intuitive. An application can be created in little time and because of the English-like syntax, it is 363_Web_App_10.qxd 11:37 AM Securing ColdFusion Chapter 10 easy to read. For example, to query a database and output the results, you simply need the code shown in Figure 10.2. <CFQUERY name=""qGetUsers"" Datasource=""Users""> Select userif, firstname, lastname From users </CFQUERY> It is quite clear what the code is trying to do.The tag name is CFQUERY, which tells you it will be querying a database.The code inside the tag body is simple, standard SQL. Outputting the data is also simple (see Figure 10.3)"
"Explain why for a few times after the initial registration, whenever users go to that site, they have to re-register until they have cookies from all the servers"," Imagine that an organization decided to add security to its e-commerce site, and chose to use certificates or cookies to identify legitimate users.The organization, which employs a load-balanced multiple Web server architecture, issues cookies specific to each server in their server farm. When users register for the first time and get a certificate, it is only for the server they directly contacted.Hence, for a few times after the initial registration, whenever users go to that site, they have to re-register until they have cookies from all the servers."," Even if the changes made to the existing environment seem minor, it is always best to test them first. Imagine that an organization decided to add security to its e-commerce site, and chose to use certificates or cookies to identify legitimate users.The organization, which employs a load-balanced multiple Web server architecture, issues cookies specific to each server in their server farm. When users register for the first time and get a certificate, it is only for the server they directly contacted.Therefore, for a few times after the initial registration, whenever users go to that site, they have to re-register until they have cookies from all the servers.This is clearly not the way the secuwww.syngress.com 12/18/06 Page 423 rity measure was supposed to work. It was supposed to provide secure automatic authentication and authorization to customers after the initial registration, so they wouldnt have to keep submitting private information like credit card numbers unprotected over the Web. Customers are far less likely to visit a site where they have to manually input information every time, because they see it as a security risk. The process of testing security implementation may seem a daunting task at first, but consider these three major goals your testing needs to accomplish: Establish that the implementation has the desired result. Security must work and must work as planned. Whatever your security goals are, you must ensure they are met. For example, the organization mentioned in the example earlier in this section should have issued certificates that covered the site and not just an individual server if they were seeking seamless and secure access"
Explain why the intruder will have at his disposal a number of automated system modification utilities (the rootkit) to eradicate or conceal any evidence of his success," Because everyone was away, the penetration lasted significantly longer than it would have if staff had been present.This incident eventually culminated with the pursuit, capture, and prosecution of Kevin Mitnick. (However, many security specialists do not believe Mitnick was capable of carrying out the attack. Furthermore, this intrusion was not among the charges for which Mitnick was tried and convicted.) It is said that failing to plan is planning to fail, and failure is the last thing on an intruders mind.Hence, the intruder will have at his disposal a number of automated system modification utilities (the rootkit) to eradicate or conceal any evidence of his success."," Following reconnaissance of the system, the intruder will bide his time until the night, weekend, or holiday when staff is at minimum. Christmas Eve, Christmas, and New Years Eve are among the most popular dates on which intrusion attempts occur. Friday afternoons, in general, are popular, too. Perhaps the most well documented holiday attack was the 1994 Christmas Day intrusion of Tsutomu Shimomuras system in San Diego, California. Around 2:00 P.M., when staff was at a minimum and most people were away with their families (Shimomura himself was in San Francisco, preparing to go on vacation to the Sierra Nevadas), the attacker(s) launched their intrusion attempts and successfully penetrated Shimomuras system. Because everyone was away, the penetration lasted significantly longer than it would have if staff had been present.This incident eventually culminated with the pursuit, capture, and prosecution of Kevin Mitnick. (However, many security specialists do not believe Mitnick was capable of carrying out the attack. Furthermore, this intrusion was not among the charges for which Mitnick was tried and convicted.) It is said that failing to plan is planning to fail, and failure is the last thing on an intruders mind.Thus, the intruder will have at his disposal a number of automated system modification utilities (the rootkit) to eradicate or conceal any evidence of his success.These rootkits will replace many system monitoring utilities with modified versions that will not reveal the intruders presence. In addition, the rootkit may also create secret entryways or back doors by which the intruder may access the victim system whenever he chooses. More advanced rootkits will eliminate specific log entries to hide the intruders presence, rather than delete the log files outright, which would raise suspicions during a security audit. 363_Web_App_05.qxd 11:03 AM Hacking Techniques and Tools Chapter 5 Tools & Traps Nessus The only true way to defend your system is to look at it through the eyes of your enemy: the intruder. A number of automated utilities can probe your networks to look for common exposures and vulnerabilities. One of the foremost freeware tools is a package called Nessus"
"Explain why the emphasis is really in looking for vulnerabilities that involve user-supplied data in some way, shape, or form"," This chapter illustrates a different technique for approaching source code reviews. Rather than trace the program forward through execution, we take the www.syngress.com 12/18/06 Page 201 reverse approach: proceed directly to the potential problem areas, and then trace back through the program to confirm whether they are vulnerable.Technically, were only interested in the execution paths that involve the user; however, trying to follow those paths can be excruciating because data supplied by a user can go every which way after the program starts processing it. So instead, we start at the end and then trace the flow in reverse to see if we encounter a user path.Hence, the emphasis is really in looking for vulnerabilities that involve user-supplied data in some way, shape, or form."," If its a small program with a linear logic flow (that is, the program isnt highly interactive nor does it contain a lot of branching logic), the task may not be that hard; however, if the program is of moderate size, reviewing it can be a headache.This headache is compounded if the source code is distributed among multiple components, contained in multiple files. Starting at the beginning of the program and then stepping through every possible execution path becomes nearly impossible. This chapter illustrates a different technique for approaching source code reviews. Rather than trace the program forward through execution, we take the www.syngress.com 12/18/06 Page 201 reverse approach: proceed directly to the potential problem areas, and then trace back through the program to confirm whether they are vulnerable.Technically, were only interested in the execution paths that involve the user; however, trying to follow those paths can be excruciating because data supplied by a user can go every which way after the program starts processing it. So instead, we start at the end and then trace the flow in reverse to see if we encounter a user path.Thus, the emphasis is really in looking for vulnerabilities that involve user-supplied data in some way, shape, or form. NOTE When reviewing code, we dont need to bother looking at areas where the program internally generates the data, because we assume the program will not try to exploit itself"
"Explain why the problem is that a buffer overflow will overwrite these return addresses, and the computernot knowing any betterwill still attempt to use them","Looking for Buffer Overflows Buffer overflows are one of the top flaws for exploitation on the Internet today. A buffer overflow occurs when a particular operation/function writes more data into a variable (which is actually just a place in memory) than the variable was designed to hold.The result is that the data starts overwriting other memory locations without the computer knowing those locations have been tampered with.To make matters worse, some hardware architectures (such as Intel and Sparc) use the stack (a place in memory for variable storage) to store function return addresses.Hence, the problem is that a buffer overflow will overwrite these return addresses, and the computernot knowing any betterwill still attempt to use them."," NOTE Perl refers to any variable (and thus any command using that variable) containing user data as tainted. Thus, a variable is tainted until it is run through a proper filter/validity check. We will use the term tainted throughout the chapter. Perl actually has an official taint mode, activated by the T command-line switch. When activated, the Perl interpreter will abort the program when a tainted variable is used. Perl programmers should consider using this handy security feature. 3:45 PM Looking for Buffer Overflows Buffer overflows are one of the top flaws for exploitation on the Internet today. A buffer overflow occurs when a particular operation/function writes more data into a variable (which is actually just a place in memory) than the variable was designed to hold.The result is that the data starts overwriting other memory locations without the computer knowing those locations have been tampered with.To make matters worse, some hardware architectures (such as Intel and Sparc) use the stack (a place in memory for variable storage) to store function return addresses.Thus, the problem is that a buffer overflow will overwrite these return addresses, and the computernot knowing any betterwill still attempt to use them. If the attacker is skilled enough to precisely control what values the return pointers are overwritten with, he can control the computers next operation(s). The two flavors of buffer overflows referred to today are stack and heap"
"Explain why data theft is a malicious subclass of data leak, and most controls that prove effective against data leaks can also mitigate data theft (although the converse isnt necessarily true)"," A data leak generally refers to sensitive customer or corporate information electronically leaving the enterprise environment either inadvertently or deliberately. Hence, data theft is a malicious subclass of data leak, and most controls that prove effective against data leaks can also mitigate data theft (although the converse isnt necessarily true)."," 1.2 Data Leakage Data theft is also interesting because its part of a larger insider threat, data leakage, which includes accidental or unintentional data loss in addition to malicious theft. Inadvertent data loss is actually more common than theft. Many insider threats come in both malicious and non-malicious varieties, but security staff sometime has a natural tendency to focus on the former, and in doing so may miss opportunities to neutralize two birds with one stone. A data leak generally refers to sensitive customer or corporate information electronically leaving the enterprise environment either inadvertently or deliberately. Hence data theft is a malicious subclass of data leak, and most controls that prove effective against data leaks can also mitigate data theft (although the converse isnt necessarily true). Through incidents and observed trends over the past year or longer, it has become clear to many enterprises that they are increasingly vulnerable to a serious data leak that could damage them financially, legally, and reputationally. Confidential data is increasingly found in inappropriate places (both inside and outside the enterprise that owns it). Many private sector companies have embarked on enterprise data leak prevention (EDLP) programs to deal with this threat. A market of data loss prevention (DLP) technology vendors has sprung into being to profit from this development"
"Explain why, insider attack may also be distinguished by intent of the users actions","We may distinguish traitors and masqueraders based upon the amount of knowledge each has. A traitor of course has full knowledge of the systems they routinely use and likely the security policies in force. The masquerader may have far less knowledge than the traitor. Furthermore, an insider attack may be due to an innocent mistake by a legitimate user. Hence,  insider attack may also be distinguished by intent of the users actions."," The most familiar example of an insider is a masquerader; an attacker who succeeds in stealing a legitimate users identity and impersonates another user for malicious purposes. Credit card fraudsters are perhaps the best example of masqueraders. Once a bank customers commercial identity is stolen (e.g. their credit card or account information), a masquerader presents those credentials for the malicious purpose of using the victims credit line to steal money. We may distinguish traitors and masqueraders based upon the amount of knowledge each has. A traitor of course has full knowledge of the systems they routinely use and likely the security policies in force. The masquerader may have far less knowledge than the traitor. Furthermore, an insider attack may be due to an innocent mistake by a legitimate user. Hence, insider attack may also be distinguished by intent of the users actions. Traitors and masqueraders are two sides of what we consider to be the insider threat. The distinction is not entirely satisfactory. After all, a disgruntled insider employee may act as a traitor and a masquerader after stealing the identity of a coworker. But for our present purposes, the distinction is clear enough to consider the general themes of past research in insider attack detection. An extensive literature exists reporting on approaches that profile user behavior as a means of detecting insider attack, and identity theft in particular. A traitor is presumed to have full knowledge of the internal systems of an organization to which they belong. They use their own credentials and the access granted by those credentials to perform their malicious deeds. A traitor may exhibit normal behavior and still perpetrate malicious acts. Profiling user behavior in this case may seem less relevant except for identifying subtle but significant changes in a users normal behavior. A masquerader, on the other hand, has stolen someones credentials, and is unlikely to know the behavior of their victim. Thus, even though they control the victims credentials that grant access to whatever the victim is authorized to use, the masquerader is likely to perform actions inconsistent with the victims typical behavior"
"Explain why, approaches to profiling environments and program executions may have relevance to the insider attack detection problem"," Besides user issued commands, inside attackers may inject programs or infect host systems causing changes in underlying system configurations and program behaviors. Hence,  approaches to profiling environments and program executions may have relevance to the insider attack detection problem."," There is a vast literature on data mining methods applied to web user click data for marketing analytics that goes well beyond the scope of this paper. However, some work has been done focusing on web profiling for security problems. Kim, Cho, Seo, Lee, and Cha studied the problem of masquerade detection in a web environment. They focused on anomalous web requests generated by insiders who attempted to violate existing security policies given by the specific organization [17]. They applied SVMs to web server logs and used two different kernels: TinySVM (an implementation of SVM for pattern recognition) and the Radial Basis Function (RBF) kernel. Only simple features were used, i.e. neither session features, nor temporal features were included. Simple features are those related to a Insider Attack and Cyber Security single web sever request such as the IP address, the hour of the day, the HTTP method (get, post, put, delete, options, head, and trace), the requested page ID, the request status code, the number of transferred bytes, etc. The results showed that SVMs achieved near-perfect classification rates using simple features only. However, the method used did not handle concept drift well, and failed to generalize the model for two users due to changes in user behavior. Besides user issued commands, inside attackers may inject programs or infect host systems causing changes in underlying system configurations and program behaviors. Hence, approaches to profiling environments and program executions may have relevance to the insider attack detection problem. Much work in this area is devoted to detection of code injection attacks, too broad a topic to describe here. A few characteristic works are described in the following"
"Explain why, abnormal behavior is a good indicator of a potential masquerade attack as a consequence of identity theft","User profiling as a means of identifying abnormal user behavior is well established as a primary methodology for masquerader attack detection. As we have noted, a masquerader impersonates another persona and it is unlikely the victims behavior will be easily mimicked. Hence,  abnormal behavior is a good indicator of a potential masquerade attack as a consequence of identity theft."," Honeypots and related decoy technologies are proposed as suitable technologies for traitor detection, as well as masquerader detection. Alternatively, it is unclear how well an insider attack may be detected from Unix System Call anomalies, and hence we rate the utility of this audit source as low. We are unaware of any formal study of each audit source validating or refuting these assumptions. This table may serve as a guide for future research in monitoring technologies for insider attack detection. A Survey of Insider Attack Detection Research Two-Class Classifiers: Unix Command Sequences Masquerader High Unfamiliar with local environment and user behavior One-Class: Unix Command quences High Unfamiliar with local environment and user behavior Unix Audit Events Medium Given proper credentials and might not trigger alerts Medium Might not violate system call profile Medium Given proper credentials and might not trigger alerts Medium unless malicious programs access Registry Medium If attack uses network and attribution is possible High Unfamiliar with local information and likely to interact with honeypot Network Activity Audit Honeypots and Decoy Technologies Internal Traitor Medium Can possibly mimic another normal user or train the classifier Medium Can possibly mimic another normal user or train the classifier Low Application level malicious acts may not manifest as unusual events Low Application level malicious acts may not manifest as unusual events Low Application level malicious acts may not manifest as unusual events Medium unless malicious programs access Registry High If attack uses network and attribution is possible Medium Unlikely to interact if aware of the location of honeypots User profiling as a means of identifying abnormal user behavior is well established as a primary methodology for masquerader attack detection. As we have noted, a masquerader impersonates another persona and it is unlikely the victims behavior will be easily mimicked. Hence, abnormal behavior is a good indicator of a potential masquerade attack as a consequence of identity theft. User profiling may also be useful in detecting a traitor, if subtle but significant changes in a users behavior indicate a malicious activity. We believe that it will be important Insider Attack and Cyber Security to derive user profile models that reveal user intent in order to hone in on insider actions that are suspicious and likely malicious. It may not be enough to know of a malicious act merely from knowing that a user has issued an abnormal command sequence unless that sequence could violate a security policy. For example, we conjecture that modeling a users search behavior may be one way of capturing a users intent to seek information for malicious purposes, something that a masquerader, and possibly a traitor, is likely to do early in their attack behavior. Too much searching, or searching in abnormal directories or locations, seems more than odd, it may seem sinister in intent. A major challenge of insider attack detection research is the lack of real data in order to study and measure general solutions and models. It is hard, if not impossible, to collect data from normal users in many different environments. It is especially hard to acquire real data from a masquerader or traitor while performing their malicious actions. It is hard to obtain real intrusions for ground truth test and evaluation for a number of reasons: Researchers generally do not have direct access to real attacks Attacks may be undetected and thus unavailable for study Organizations do not admit that they were attacked and hence shy away from cooperating with researchers Attacks might be mistaken for incompetence Even if such data were available, it is more likely to be out of reach and controlled under the rules of evidence, rather than being a source of valuable information for research purposes. Because of the scarcity of real data, Chinchani et al"
"Explain why, we also believe that any technologies developed to detect insider attack have to include strong privacy-preserving guarantees to avoid making false claims that could harm the reputation of individuals whenever errors occur","The most vexing problem for researchers is to devise detection methods that accurately distinguish between the cases where an insider attack is verified with high confidence versus cases where an insider attack is inferred from partial knowledge of possibly suspicious actions. Distinguishing false positives from true positives in the presence of uncertainty is particularly challenging when peoples reputations are at stake. Hence,  we also believe that any technologies developed to detect insider attack have to include strong privacy-preserving guarantees to avoid making false claims that could harm the reputation of individuals whenever errors occur."," Detecting such unusual events can only occur at the business application level, and application-level knowledge is needed to understand the users intent and confirm whether the intent of user actions is possibly malicious. This may be detectable using host-based sensors and audit sources, and possibly through network-based sensors if the application is accessed remotely and the content flow on the network were exposed for analysis. The most vexing problem for researchers is to devise detection methods that accurately distinguish between the cases where an insider attack is verified with high confidence versus cases where an insider attack is inferred from partial knowledge of possibly suspicious actions. Distinguishing false positives from true positives in the presence of uncertainty is particularly challenging when peoples reputations are at stake. Hence, we also believe that any technologies developed to detect insider attack have to include strong privacy-preserving guarantees to avoid making false claims that could harm the reputation of individuals whenever errors occur. Another important topic for research is the investigation of alternative mitigation strategies. For instance, how does a monitoring or detection system challenge a user when the system detects what it believes are malicious activities? How might a system alert a supervisor of a possible attack without disclosing an employees true identity unless and until an attack has been validated? Beyond the significant challenges in computing accurate user profiles, considerable effort is needed on developing techniques for trapping traitor behaviors"
"Explain why, the design of system-level countermeasures are likely to rely, at least for the foreseeable future, on the application of well-understood survivability principles to specific system designs to derive architectural structures that satisfy operational, cost, and usability constraints","Insider Attack and Cyber Security in non-government domains (e.g., business, industry, commerce), losses caused by insider attacks amounted to 19% of all losses, where 6% was attributable to disgruntled employees and 13% to dishonest ones. These losses trailed only those caused by administrative error (65%), and far exceeded those caused by infrastructure damage due to floods, fires, and earthquakes (8%), outsider attacks (2%), and unspecified other events (6%). Whether all Courtneys statistics are relevant today can be debated: the Internet has enabled outsiders attacks on unprotected or poorly protected systems, causing substantial more losses than those of the centralized systems of the 70s and 80s. However, two facts are not debatable: (1) the handling of insider attacks in large-scale systems and enterprises remains a significant technical challenge, and (2) little assurance theory and design practice exists to provide guidance on designing effective, credible countermeasures for such systems. Much of the relevant theory has focused on insider attacks on individual protocols (e.g., applications of threshold cryptography [3, 4, 12] and passwordbased authenticated key exchange protocols [10, 11], and various corruption models [8]). Hence,  the design of system-level countermeasures are likely to rely, at least for the foreseeable future, on the application of well-understood survivability principles to specific system designs to derive architectural structures that satisfy operational, cost, and usability constraints."," Introduction Insiders are generally understood to be trusted individuals or entities authorized to access and manage system and application resources. By virtue of their largely unrestricted access, insiders can launch extremely potent attacks against information systems. These attacks take advantage of the insiders authority and knowledge to configure and administer systems and the ability to exploit known, and perhaps deliberately induced, vulnerabilities in a largely unconstrained networked environment; e.g., in the Internet. Anecdotal evidence over the past twenty years identified insiders as the most potent source of attacks in computer systems and networks in terms of the loss incurred as a consequence of these attacks. For example, in the summer of 1987, in a presentation given to a National Research Council panel studying the information security posture of the Department of Energy, Bob Courtney1 suggested that, At the time of his presentation, the late Robert Courtney was an industry security consultant and a former manager for data security in IBMs Systems Develpment Division. In 1993, he re- Insider Attack and Cyber Security in non-government domains (e.g., business, industry, commerce), losses caused by insider attacks amounted to 19% of all losses, where 6% was attributable to disgruntled employees and 13% to dishonest ones. These losses trailed only those caused by administrative error (65%), and far exceeded those caused by infrastructure damage due to floods, fires, and earthquakes (8%), outsider attacks (2%), and unspecified other events (6%). Whether all Courtneys statistics are relevant today can be debated: the Internet has enabled outsiders attacks on unprotected or poorly protected systems, causing substantial more losses than those of the centralized systems of the 70s and 80s. However, two facts are not debatable: (1) the handling of insider attacks in large-scale systems and enterprises remains a significant technical challenge, and (2) little assurance theory and design practice exists to provide guidance on designing effective, credible countermeasures for such systems. Much of the relevant theory has focused on insider attacks on individual protocols (e.g., applications of threshold cryptography [3, 4, 12] and passwordbased authenticated key exchange protocols [10, 11], and various corruption models [8]). Hence, the design of system-level countermeasures are likely to rely, at least for the foreseeable future, on the application of well-understood survivability principles to specific system designs to derive architectural structures that satisfy operational, cost, and usability constraints. These constraints require tradeoffs among different interpretations of survivability principles and help optimize these interpretations. Of course, the benefits of using survivability principles would have to be illustrated in realistic examples of specific subsystems and/or applications that survive insider attacks in a demonstrable way. A useful application of survivability principles, which would blend good theory with design practice, would be a user authentication subsystem. Such a subsystem should be resilient to attacks by systems administrators and operators who are free to act simultaneously as insiders and outsiders. We suggest this application area for three reasons"
"Explain why, critical component replicas running different operating systems are more likely to survive an attack than if all replicas run the same operating system","Design diversity ensures that generic flaws cannot arise that would affect multiple separated replicas of a critical component. For example, practice shows that different operating system families (e.g., MS Windows family and Unix/Linux family) are unlikely to be plagued by identical flaws. For any one of the common flaw types (e.g., buffer overflow, failure to validate input parameters or to enforce resource bounds), it is usually not possible to craft a single exploit that would work across diverse platforms. Hence,  critical component replicas running different operating systems are more likely to survive an attack than if all replicas run the same operating system."," 2.2 Independence of Failure Modes and Attack Vulnerabilities Avoidance of a single point of failure and its related design methods discussed above cannot counter multiple non-independent (related) failures or attacks against separated replicas of a system, duties (e.g., roles), or against criticalfunction partitions. For example, generic design flaws may affect all spatially separated replicas of a critical function using the same design, and could make that Insider Attack and Cyber Security function susceptible to compromise. The same generic flaw could lead to identical (and hence non-independent) failures or penetrations of all critical component replicas. Similarly, separated duties (e.g., roles) would not prevent system compromise if two or more insiders collude to violate system integrity by joint malicious actions. Design diversity ensures that generic flaws cannot arise that would affect multiple separated replicas of a critical component. For example, practice shows that different operating system families (e.g., MS Windows family and Unix/Linux family) are unlikely to be plagued by identical flaws. For any one of the common flaw types (e.g., buffer overflow, failure to validate input parameters or to enforce resource bounds), it is usually not possible to craft a single exploit that would work across diverse platforms. Hence, critical component replicas running different operating systems are more likely to survive an attack than if all replicas run the same operating system. Different forms of diversity have been used in practical designs [1,2]. Insider personnel diversity seeks to provide similar benefits as those of design diversity. First, different individuals or entities must be assigned to different duties (e.g., roles) and critical functions. Second, different individuals or entities that have different interests (e.g., financial, corporate reporting lines) are less likely to collude. Third, different individuals must have the required skills to operate the diverse component platforms and applications (e.g., even if a Windows system administrator may have the skills to administer a Unix-based system, s/he may not have the skills to administer a specific applications on either system)"
Explain why a rational insider is likely to be deterred by attack-detection measures as his/her unencumbered access may no longer be the weakest link of the system," This principle is grounded in the key observation that a rational attacker (1) will always avoid system areas where the chances of being detected are high or even non-negligible, and (2) will always seek the weakest link of a system. Hence, a rational insider is likely to be deterred by attack-detection measures as his/her unencumbered access may no longer be the weakest link of the system."," 2.4 Attack Deterrence All of the above principles of survivable system design assumed the certainty of failures and attacks. While this is a safe assumption in case of both physical and human failures, cost of the countermeasurs can be high. For example, the finegrained separation duties will undoubtedly increase the number of administrative personnel. This would lead to an unavoidable recurrent cost. The alternative of coarse-gained or no separation of duty would incur the cost of recovery from an attack, which can also be high. An alternative to incurring such costs would be to decrease the probability of insider attacks by deterrence. The principle of deter- Insider Attack and Cyber Security rence says that it is better to deter an attack than to have to recover from one. This principle is grounded in the key observation that a rational attacker (1) will always avoid system areas where the chances of being detected are high or even non-negligible, and (2) will always seek the weakest link of a system. Hence a rational insider is likely to be deterred by attack-detection measures as his/her unencumbered access may no longer be the weakest link of the system. The importance of this principle cannot be overestimated, particularly in the case of insider attacks. While diversity of insiders interests makes insider collusion unlikely, it cannot prevent it. Hence, the ability to detect attacks would deter rational insiders, even if the detection occurs after the fact. Thus, deterrence becomes indispensable in system areas where the costs of countermeasures to, or recovery from, insider attack are prohibitively high. It typically takes place in the form of auditing [14]"
"Explain why, the ability to detect attacks would deter rational insiders, even if the detection occurs after the fact","The importance of this principle cannot be overestimated, particularly in the case of insider attacks. While diversity of insiders interests makes insider collusion unlikely, it cannot prevent it. Hence,  the ability to detect attacks would deter rational insiders, even if the detection occurs after the fact."," This principle is grounded in the key observation that a rational attacker (1) will always avoid system areas where the chances of being detected are high or even non-negligible, and (2) will always seek the weakest link of a system. Hence a rational insider is likely to be deterred by attack-detection measures as his/her unencumbered access may no longer be the weakest link of the system. The importance of this principle cannot be overestimated, particularly in the case of insider attacks. While diversity of insiders interests makes insider collusion unlikely, it cannot prevent it. Hence, the ability to detect attacks would deter rational insiders, even if the detection occurs after the fact. Thus, deterrence becomes indispensable in system areas where the costs of countermeasures to, or recovery from, insider attack are prohibitively high. It typically takes place in the form of auditing [14]. Auditing of critical function execution deters an insider from execution an attack only if the auditing function is protected. The protection of the auditing function can be implemented in many systems by traditional access control mechanisms and application of the separation of duty principle. Hence any invocation of a critical function by an insider could be audited and hence deter an insider from launching an attack by misusing his/her privileged access permissions. To be effective, however, auditing must be conducted fairly frequently by separate personnel [9, 13]"
Explain why any invocation of a critical function by an insider could be audited and hence deter an insider from launching an attack by misusing his/her privileged access permissions,"Auditing of critical function execution deters an insider from execution an attack only if the auditing function is protected. The protection of the auditing function can be implemented in many systems by traditional access control mechanisms and application of the separation of duty principle. Hence, any invocation of a critical function by an insider could be audited and hence deter an insider from launching an attack by misusing his/her privileged access permissions."," The importance of this principle cannot be overestimated, particularly in the case of insider attacks. While diversity of insiders interests makes insider collusion unlikely, it cannot prevent it. Hence, the ability to detect attacks would deter rational insiders, even if the detection occurs after the fact. Thus, deterrence becomes indispensable in system areas where the costs of countermeasures to, or recovery from, insider attack are prohibitively high. It typically takes place in the form of auditing [14]. Auditing of critical function execution deters an insider from execution an attack only if the auditing function is protected. The protection of the auditing function can be implemented in many systems by traditional access control mechanisms and application of the separation of duty principle. Hence any invocation of a critical function by an insider could be audited and hence deter an insider from launching an attack by misusing his/her privileged access permissions. To be effective, however, auditing must be conducted fairly frequently by separate personnel [9, 13]. 2.5 Least Privilege Authorization When applied to insider access, this principle [17] suggests that insiders have access only to the critical system functions and permissions necessary for carrying out their tasks. Further, it suggests that an insiders authorization to a critical function be restricted to include only the permissions or privileges necessary to carry out that function. This requires that permissions (1) assigned to an insider to perform a duty need be tailored to fit tightly the needs of that duty, and (2) assigned to a critical function be kept to the fewest necessary for the functions operation. This principle complements separation of duty in that permissions assigned to roles are kept to the minimum necessary for that role"
"Explain why, it becomes important to come up with designs that minimize and tradeoff recurrent costs against fixed costs","Architectures that employ the survivability principles discussed above incur extra costs, including fixed, non-recurrent, development, and equipment costs, as well as recurrent system administrative and maintenance costs. The fixed costs (e.g., hardware replicas) are typically marginal compared to the recurrent costs (e.g. personnel salaries and benefits) over a systems lifetime. Hence,  it becomes important to come up with designs that minimize and tradeoff recurrent costs against fixed costs."," However, the application of the least privilege principle at process and object granularity also provides an illustration of unintended and undesirable consequences. Fine granularity of permissions, by definition, requires that multiple permissions must be initialized, reset and checked during system operation and, hence, their interaction must be well understood by system administrators. In view of the generally accepted fact that the largest source of security problems is administrator error and that, among all administrator errors, configuration errors are the main caused of such problems, the application of the least privilege principle appears to be a less than useful approach in this context. Some evidence of 3 Architectures that employ the survivability principles discussed above incur extra costs, including fixed, non-recurrent, development, and equipment costs, as well as recurrent system administrative and maintenance costs. The fixed costs (e.g., hardware replicas) are typically marginal compared to the recurrent costs (e.g. personnel salaries and benefits) over a systems lifetime. Hence, it becomes important to come up with designs that minimize and tradeoff recurrent costs against fixed costs. Spatial separation, replication, and partitioning of critical functions requires substantial additional equipment and development costs, but only marginally more recurrent maintenance and administrative costs. In contrast, separation of duty, design diversity, and attack deterrence imposes substantial recurrent administrative costs not just in terms of personnel but also in terms of desirable administrative skills. Separation of duty and diversity imposes additional recurrent costs since it requires different individuals in different roles to carry out the fine-grain administrative functions. Deterrence requires increased and frequent use of administrative tasks, such as audit-trail review and audit-event correlation and analysis,, and requires special skills and analytical tools"
Explain why an important practice for tracking access paths and reducing the occurrence of unknown access paths is ongoing and thorough account management,"In the cases we examined, accounts that were secretly created by the insider or shared with other coworkers were access paths often used by the insider but unknown by management. In addition, lack of tracking led to unknown access paths for the insider that were overlooked in the termination process, and later used by the insider to attack. Hence, an important practice for tracking access paths and reducing the occurrence of unknown access paths is ongoing and thorough account management."," Fig. 9 emphasizes the importance of diligent tracking and management of access paths into the organizations system and networks. As tracking increases, the likelihood an organization will forget about the existence of specific access paths and who has access to them decreases. If precursor technical activity is detected, unknown access paths can be discovered and disabled, further reducing the number of unknown access paths available to the insider. As the number of unknown access paths decreases, the ability to conceal malicious activity by the insider decreases. As the ability to conceal decreases, the discovery of technical precursors increases. This makes it more and more difficult for the insider to conceal unauthorized or malicious online activity. Conversely however, if technical precursors are not discovered, the insider can accumulate unknown access paths, making it easier for him to conceal his actions. Insider Attack and Cyber Security technical precursor ability to conceal activity discovery of precursors acquiring unknown paths unknown access paths discovering paths forgetting paths known access paths disabling paths tracking Fig. 9: Eliminating Unknown Access Paths In the cases we examined, accounts that were secretly created by the insider or shared with other coworkers were access paths often used by the insider but unknown by management. In addition, lack of tracking led to unknown access paths for the insider that were overlooked in the termination process, and later used by the insider to attack. Therefore, an important practice for tracking access paths and reducing the occurrence of unknown access paths is ongoing and thorough account management. Account management is a complex task, encompassing verification of new accounts, changes to account authorization levels, tracking who has access to shared accounts, and decommissioning of old accounts. Unfortunately, it takes a significant amount of time and resources for an organization to recover from obsolete account management practices. 4.5 Measures Upon Demotion or Termination Termination or demotion was the final precipitating event in many cases we examined. It is important that organizations recognize that such precipitating events may cause the insider to take technical actions to set up and carry out the attack possibly using previously acquired unknown access paths. A clearly-defined process for demotions and terminations in combination with proactive IT best practices for detecting unknown access paths and eliminating unauthorized access paths can reduce the insiders ability and/or desire to attack the organization"
Explain why it is important to evaluate the ELICIT system using other scenarios that occur over longer periods of time," Identifying specific users from observable network events consumed considerable effort. Event attribution proved to be a major challenge: 83% of events ini- Insider Attack and Cyber Security tially had no attribution, and 28.6% of them remained un-attributed, even after the use of two off-line methods to determine the originator of a particular event. The evaluation of the system used scenarios that were executed over a short period of time, less than one day. However, attacks by insiders who violate need-to-know usually occur over days, months, and even decades, such as in the case of Robert Hanssen. Hence, it is important to evaluate the ELICIT system using other scenarios that occur over longer periods of time."," 3.2 Network-Based Sensors ARDA (recently renamed IARPA) sponsored a Cyber Indications and Warning workshop dealing with the insider threat. One of the lessons learned was that in many cases insider threats have authorization to access information but may access information they do not have a need to know. When an insider accesses information that they do not need to know, one may have good evidence of an insider attack. A system for detecting insiders who violate need-to-know, called ELICIT, was developed by Maloof and Stephens [21]. The focus of their work was on detecting activities, such as searching, browsing, downloading, and printing, by monitoring the use of sensitive search terms, printing to a non-local printer, anomalous browsing activity, and retrieving documents outside of ones social network. Five malicious insider scenarios were tested, that represented need-to-know violations. Contextual information about the user identity, past activity, and the activity of peers in the organization or in a social network were incorporated when building the models. HTTP, SMB, SMTP, and FTP traffic was collected from within a corporate intranet network for over 13 months, but no inbound or outbound traffic was gathered. In order to identify the information deemed outside the scope of an insiders duties, a social network was computed for each insider based on the people in their department, whom they e-mailed, and with whom they worked on projects. A Bayesian network for ranking the insider threats was developed using 76 detectors. Subject-matter experts defined the thresholds for these detectors, at which an alarm is set. A single threat score is computed for each user based on the alerts from these detectors. Identifying specific users from observable network events consumed considerable effort. Event attribution proved to be a major challenge: 83% of events ini- Insider Attack and Cyber Security tially had no attribution, and 28.6% of them remained un-attributed, even after the use of two off-line methods to determine the originator of a particular event. The evaluation of the system used scenarios that were executed over a short period of time, less than one day. However, attacks by insiders who violate need-to-know usually occur over days, months, and even decades, such as in the case of Robert Hanssen. Therefore, it is important to evaluate the ELICIT system using other scenarios that occur over longer periods of time. In any event, although interesting, the focus of this system is limited to environments and organizations that have a formal policy restricting access to information on a need-to-know-basis. It is rare that such controls are easily discernible in most organizations. Honeypots are information system resources designed to attract malicious users"
Explain why this data set remains static throughout the experiment; i,"Self training data The self training data comprise 1000 commands. Since the experimental objective is to examine the effect of commands that are novel to the naive Bayes classifier (NBSCs), and these novel commands are always outside the scope of the self data, there is no need for the self data ever to change. Hence, this data set remains static throughout the experiment; i."," 5.2 Synthetic Data There are three types of data in each data set: the self training data (for building the model of normal behavior), the nonself training data (for building the model of abnormal behavior), and the test block (commands generated by the potential masquerader). The generation of each synthetic data set can be viewed as a procedure that takes as input a particular value for each of the four variables in the vector (k, m, s[ci], n[ci]), and constructs self and nonself training data sets, plus a test block. These data are constructed as follows. Self training data The self training data comprise 1000 commands. Since the experimental objective is to examine the effect of commands that are novel to the naive Bayes classifier (NBSCs), and these novel commands are always outside the scope of the self data, there is no need for the self data ever to change. Therefore, this data set remains static throughout the experiment; i.e., there is only one self data set. The length of 1000 was chosen to be consistent with previous work done in masquerade detection (e.g., [6, 7]), as well as its convenience as a multiple of 10. The self training data was populated with commands2 selected from the 11 11 matrix in Table 1 so that the distinct elements in the data set would range widely 2 6 10 11 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 51 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 11 111 112 113 114 115 116 117 118 119 120 121 Nonself-ness - Table 1. Matrix of commands used to populate the self and nonself training data with commands that range in relative frequency"
"Explain why companies can generally capture and analyze an employees email and other communications that use company equipment, log all files and other resources an employee accesses, and retain copies of programs and data an employee creates under the companys auspices","When the insider is an employee, privacy rights are subordinated to business rights. The courts have consistently upheld the right of a company to monitor employees behavior, as long as there is a reasonable business purpose for the monitoring and the monitoring does not violate basic human and civil rights. Hence, companies can generally capture and analyze an employees email and other communications that use company equipment, log all files and other resources an employee accesses, and retain copies of programs and data an employee creates under the companys auspices."," Again, the definition of insider becomes important. When the insider is an employee, privacy rights are subordinated to business rights. The courts have consistently upheld the right of a company to monitor employees behavior, as long as there is a reasonable business purpose for the monitoring and the monitoring does not violate basic human and civil rights. Thus, companies can generally capture and analyze an employees email and other communications that use company equipment, log all files and other resources an employee accesses, and retain copies of programs and data an employee creates under the companys auspices. A company is far more free in tracking its employees system activities than would be law enforcement, for whom probable cause and a search warrant are needed. But not all insiders are employees. Some definitions of insider include people such as account holders who access their bank accounts, patients who use an electronic system to communicate with medical professionals or view or manage their medical records, students at universities, customers who use online shopping systems, and similar users. Each of these users has certain authorized access to a system. Privacy for some classes of users is covered by laws, such as HIPAA for patients in the United States or the European Privacy Directive for many accesses by Europeans. In other cases, the privacy regulations allow monitoring, data collection and retention, and even data sharing if it is documented in a privacy policy (and sometimes even if not). In these cases, then, privacy rights vary"
Explain why the volume of nonmalicious insider activity far outweighs that of malicious activity,"An insider attack recognition tool would be useful to flag attacks or suspicious behavior in time to limit severity. Clearly most insider activity is not malicious; otherwise organizations computer systems would be constantly broken. Hence, the volume of nonmalicious insider activity far outweighs that of malicious activity."," 3.4 Detecting Insider Attacks Insider attacks are difficult to detect, either by human or technical means. One workshop participant observed that most insider attacks are detected only because of some reason to suspect: the insider may have talked (bragged) about the act, for example. In other kinds of crime police investigators sometimes profit from a perpetrator who does something to draw suspicion. An insider attack recognition tool would be useful to flag attacks or suspicious behavior in time to limit severity. Clearly most insider activity is not malicious; otherwise organizations computer systems would be constantly broken. Thus, the volume of nonmalicious insider activity far outweighs that of malicious activity. Such volume of data is hard to analyze"
"Explain why the curity, Office of Science and Technology, which provided financial support for the study in fiscal years 2003 and 2004"," The CERT project team felt that it was important to further utilize the wealth of empirical data from the Insider Threat Study to next concentrate on conveying the ""big picture"" of the insider threat problem - the complex interactions, relative degree of risk, and unintended consequences of policies, practices, technology, insider psychological issues, and organizational culture over time. Hence, the curity, Office of Science and Technology, which provided financial support for the study in fiscal years 2003 and 2004."," The Insider Threat Study provided the first comprehensive analysis of the insider threat problem. CERTs technical security expertise was augmented with expertise from several experts in the areas of psychology, sociology, insider threat, espionage, cyber crime, and specific domains like the financial industry. The results of the study show that to detect insider threats as early as possible or to prevent them altogether, members of management, IT, human resources, security officers, and others in the organization must understand the psychological, organizational, and technical aspects of the problem, as well as how to coordinate their actions over time. The CERT project team felt that it was important to further utilize the wealth of empirical data from the Insider Threat Study to next concentrate on conveying the ""big picture"" of the insider threat problem - the complex interactions, relative degree of risk, and unintended consequences of policies, practices, technology, insider psychological issues, and organizational culture over time. Thus, the curity, Office of Science and Technology, which provided financial support for the study in fiscal years 2003 and 2004. Insider Attack and Cyber Security MERIT project was initiated3. MERIT stands for the Management and Education of the Risk of Insider Threat. As part of MERIT, we are developing a series of models and associated tools that can be used to better communicate the risks of the insider threat"
"Explain why deterrence becomes indispensable in system areas where the costs of countermeasures to, or recovery from, insider attack are prohibitively high","The importance of this principle cannot be overestimated, particularly in the case of insider attacks. While diversity of insiders interests makes insider collusion unlikely, it cannot prevent it. Hence, the ability to detect attacks would deter rational insiders, even if the detection occurs after the fact. Hence, deterrence becomes indispensable in system areas where the costs of countermeasures to, or recovery from, insider attack are prohibitively high."," This principle is grounded in the key observation that a rational attacker (1) will always avoid system areas where the chances of being detected are high or even non-negligible, and (2) will always seek the weakest link of a system. Hence a rational insider is likely to be deterred by attack-detection measures as his/her unencumbered access may no longer be the weakest link of the system. The importance of this principle cannot be overestimated, particularly in the case of insider attacks. While diversity of insiders interests makes insider collusion unlikely, it cannot prevent it. Hence, the ability to detect attacks would deter rational insiders, even if the detection occurs after the fact. Thus, deterrence becomes indispensable in system areas where the costs of countermeasures to, or recovery from, insider attack are prohibitively high. It typically takes place in the form of auditing [14]. Auditing of critical function execution deters an insider from execution an attack only if the auditing function is protected. The protection of the auditing function can be implemented in many systems by traditional access control mechanisms and application of the separation of duty principle. Hence any invocation of a critical function by an insider could be audited and hence deter an insider from launching an attack by misusing his/her privileged access permissions. To be effective, however, auditing must be conducted fairly frequently by separate personnel [9, 13]"
"Explain why it is important to investigate self-protecting monitoring mechanisms, which can confound the attacker, or at least sufficiently complicate the attack process such that the chances of detection increase","It is important to remember that every action causes a reaction, especially in computer security. In the realm of insider threats, we must assume that any protection mechanism we employ will eventually become known to the adversary. This may be because the insider has legitimate access to this information, or because this information becomes known through a variety of means. Thereafter, the first task of a determined adversary will be to disable or blind such mechanisms. Hence, it is important to investigate self-protecting monitoring mechanisms, which can confound the attacker, or at least sufficiently complicate the attack process such that the chances of detection increase."," A different, promising approach in detection involves the aggressive use of decoys, traps, and honeytokens in identifying and localizing insider incidents. These may vary in nature to fit the types of sensitive data that are in use, and can include trapped credit card numbers, unique username/password credentials (or other similar information) transmitted over specific network segments or stored in specific fileservers and/or user account files, interesting-looking emails pointing to honeypot servers, etc. Although such techniques have been used quite successfully in the past to address insider threats, and similar techniques (e.g., network honeypots) are actively being used in different areas of computer security, there has been little systematic work to date on scalable decoy trap-based techniques. One particularly attractive aspect of such technologies is that they seem to impose minimal requirements with respect to access to data for development and validation. Naturally, such mechanisms will have to be designed such that they confound the sophisticated adversary who is aware that decoys are in use by an enterprise. At a minimum, these decoys will have to be indistinguishable from legitimate sensitive information. On the other hand, awareness of the use of decoys in an enterprise could act as a deterrent, or at least as a speed bump for attackers who would face the additional task (beyond penetration and data acquisition) of decoy/honeytoken identification and elimination from the captured data. Finally, it is important to remember that every action causes a reaction, especially in computer security. In the realm of insider threats, we must assume that any protection mechanism we employ will eventually become known to the adversary. This may be because the insider has legitimate access to this information, or because this information becomes known through a variety of means. Thereafter, the first task of a determined adversary will be to disable or blind such mechanisms. Thus, it is important to investigate self-protecting monitoring mechanisms, which can confound the attacker, or at least sufficiently complicate the attack process such that the chances of detection increase. The nature of such self-protection can be manifold: trusted hardware monitors, overlapping monitoring schemes, mutually-protecting monitors, etc. We believe that only a focused research effort in developing hardened protection mechanisms will result in adequately strong defenses. Index Access control, 1, 2, 3, 8, 9, 55, 59, 62, 70, 114, 116, 119, 120, 126, 127, 128, 129, 160, 161, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 205, 211, 217, 220, 222 role based, 65 two person, 4 Administrative controls, 54, 59, 67 Anomaly detection, 4, 76, 78, 80, 81, 86, 89, 90, 112 Attack taxonomy, 3 Attacker Profile, 56 Auditing, 32, 35, 72, 74, 78, 83, 84, 113, 114, 160, 176, 185, 210 Authentication, 150, 171, 181, 182, 183, 184, 185, 193 biometric, 66, 182, 183, 185, 217 Authorization, 7, 34, 35, 81, 126, 127, 150, 160, 166, 167, 171, 172, 175, 176, 177, 185, 186, 187, 188, Behavior tracking, 66 CERT project, 19 Chinese walls, 64 Complexity, 10, 45, 69, 81, 128, 139, 178, 179, 186, 188, 206, 220 Cost, 2, 38, 39, 46, 57, 93, 109, 149, 154, 159, 161, 169, 176, 177, 179, 181, 182, 185, 187, 188, 190, Countermeasures, 24, 29, 30, 43, 139, 153, 154, 155, 160, 199 Data leak, 13, 54, 55, 56, 57, 60, 62 Data loss prevention, 54 Data partitioning, 157 Data scrubbing, 59, 62, 65 Data theft, 53, 54, 55, 56, 57, 58, 60, 63, Detection, 2, 44, 57, 69, 78, 89, 90, 112, Firewall, 2, 13, 62, 209 Fraud, 4, 19, 41, 46, 54, 57, 71, 114, 200, 215, 217 Honey token, 66 Honeypots, 82, 83, 84, 85, 90 Incentive, 165, 169, 188, 201 Information tracing, 115, 116, 117, 118, Insider attack, 11, 27, 53, 72 define threat, 6, 8, 70, 191, 198 fictional case scenario, 36 history, 55 model of sabotage, 52 motive, 6, 37, 56 psychology, 10, 11, 15, 24 recommendations, 61 risk, 19, 21, 54 social engineering, 72 traitor, 69, 70, 71, 84 Malicious behavior, 7, 8, 11, 31, 59, 86, Malicious software, 27, 37, 72, 91, 116, Masquerader, 70, 71, 74, 75, 77, 84, 85, 86, 87, 88, 91, 92, 93, 94, 95, 97, 99, 101, 102, 103, 106, 107, 108, 109, 110, 111 MERIT PROJECT, 20, 21, 22, 23, 24, 26, 29, 40, 41, 42, 43, 44 Misuse, 1, 2, 4, 21, 53, 57, 58, 69, 115, 157, 196, 198, 200, 201, 203, 205, 211 access, 1 defense bypass, 2 Mitigation, 20, 36, 39, 40, 41, 42, 45, 87, 141, 165, 187, 192, 205, 218, Monitoring, 2, 9, 12, 13, 26, 27, 28, 31, 32, 35, 38, 39, 51, 62, 72, 73, 78, 80, 81, 84, 87, 114, 115, 123, 137, 139, 140, 197, 198, 209, 210, 211, 217, 218, 219, 221, host-based, 9, 72, 73, 83, 86, 87 Public key cryptography, 147, 174, 175, 184, 185 Sabotage, 17, 19, 20, 21, 22, 23, 24, 25, 27, 29, 34, 35, 36, 37, 39, 40, 41, 43, 45, 197, 208 Usability, 7, 154, 155, 177, 179, 181, 182, 186, 188, 190 User behavior, 70, 73, 76, 79, 80, 83, 85, 94 intent, 73, 86, 221 profiling, 73 Virtualization, 53, 114, 117, 120, 126, 128, "
"Explain why, instead of increasing its window in a continuous fashion, the sender will now do so in MSS-sized steps","The SWS is silly indeed, and it is troublesome because one cannot simply forbid sending small segments we still have the push ag for situations when data must be transmitted right away. The SWS was rst described in RFC 813 (Clark 1982) together with three proposed algorithms that conjointly solve the problem: two at the receiver side and one at the sender side. The rst algorithm requires the receiver to reduce its advertised window by the size of incoming small segments until the sender can be allowed to transmit a full MSSsized segment. Then, the advertised window will jump to the new value. Hence,  instead of increasing its window in a continuous fashion, the sender will now do so in MSS-sized steps."," This is how it comes about: as already mentioned, a TCP sender should postpone sending data until an MSS-sized segment can be lled, unless the push ag is set. Waiting is, however, not mandated in RFC 793, and an interactive application might have trouble generating enough data for a whole segment. Imagine, for example, a Telnet user who will only enter a command and thereby cause trafc when the result of a previously entered command is visible; if the rst command is buffered at the TCP sender side, the user could wait forever. In such a scenario, it might seem efcient to transmit commands or even characters right away. Now imagine that the sender does this, and the small segment that it generates reaches a receiver, which immediately ACKs all segments that come in. The ACK corresponding to the small segment would lead the sender to transmit another small segment, which the receiver would acknowledge, and so on. The small window size stays in the system, and the sender has caused a prevailing problem by sending a single small segment. 3.2.2 SWS avoidance The SWS is silly indeed, and it is troublesome because one cannot simply forbid sending small segments we still have the push ag for situations when data must be transmitted right away. The SWS was rst described in RFC 813 (Clark 1982) together with three proposed algorithms that conjointly solve the problem: two at the receiver side and one at the sender side. The rst algorithm requires the receiver to reduce its advertised window by the size of incoming small segments until the sender can be allowed to transmit a full MSSsized segment. Then, the advertised window will jump to the new value. Hence, instead of increasing its window in a continuous fashion, the sender will now do so in MSS-sized steps. the sender buffer. An MSS consists of six data blocks; for simplicity, we can say that each of these blocks in the diagram has a size of 1 kB. As in Figure 3.3, the dashed box is the current sender window. The dashed arrow indicates an initial advertised window of 6 kB (one MSS) as a starting point this arrow could, for instance, represent the last (SYN, ACK) message of the three-way handshake that sets up a connection. Let us assume that the receiver has no problem in accepting data as fast as they arrive and would not normally Window = 6 MSS ACK3, Window = 3 MSS 5 4 3 MSS Sender alter the advertised window size, that is, there should always be one MSS in ight. Here is what happens: At the beginning, the sender has 3 kB in the buffer but it has a window of a whole MSS available; while we know that it should wait until the MSS can be lled, it does not do so the rst 3 kB are sent right away, potentially leading to SWS"
Explain why the Internet community has become quite religious about ensuring scalability at all times  and it appears that they are doing the right thing,"Systems can scale in several dimensions depending on the context, scalability could mean that something works with a small or large amount of trafc, or that it will not cease to work if link capacities grow. In the context of computer networks, the most common is related to the number of users, or communication ows, in the system. If something scales, it is expected to work no matter how many users there are. Quite obviously, Internet technology turned out to scale very well, as is illustrated by the continuous growth of the network itself. Hence, the Internet community has become quite religious about ensuring scalability at all times  and it appears that they are doing the right thing."," What makes the design of such a scheme a difcult task is the range of possibilities to choose from: packets can be dropped from the front or the end of the queue, or they can be picked from the middle (which is usually inefcient because it leads to time-consuming memory management operations). There is a large variety of possible methods to monitor the queue size and use it in combination with some randomness to make a decision functions that are applied in this context usually have their advantages as well as corresponding disadvantages. Perhaps the most-important design goal is scalability: if a mechanism that works perfectly with, say, ten ows, but ceases to work in the presence of thousands of ows because router resources do not sufce any longer in such a scenario (e.g. the available memory is exceeded), it cannot be used as a core element of a network that is as large as the Internet. 2.11 Scalability Systems can scale in several dimensions depending on the context, scalability could mean that something works with a small or large amount of trafc, or that it will not cease to work if link capacities grow. In the context of computer networks, the most common is related to the number of users, or communication ows, in the system. If something scales, it is expected to work no matter how many users there are. Quite obviously, Internet technology turned out to scale very well, as is illustrated by the continuous growth of the network itself. Therefore, the Internet community has become quite religious about ensuring scalability at all times and it appears that they are doing the right thing. 2.11.1 The end-to-end argument When we are talking about the Internet, the key element of scalability is most certainly the end-to-end argument. This rule (or rather set of arguments with a common theme) was originally described in (Saltzer et al. 1984). It is often quoted to say that one should move complexities out of the network (towards endpoints, upwards in the protocol stack) and keep the network itself as simple as possible. This interpretation is actually incorrect because it is a little more restrictive than it should be. Still, it is reasonable to consider it as a rst hint: if a system is designed in such a manner, the argument is clearly not violated"
"Explain why mechanisms for Mobile IP come into play, which may require incoming packets to be forwarded via a Home Agent to the new location (Perkins 2002)","Mobility: As users move from one access point (base station, cell. . . depending on the technology in use) to another while desiring permanent connectivity, two noteworthy problems occur: 1. Normally, any kind of link layer technology requires a certain time period for handoff (during which no packets can be transmitted) before normal operation can continue. 2. If the moving device is using an Internet connection, which should be maintained, it should keep the same IP address. Hence, mechanisms for Mobile IP come into play, which may require incoming packets to be forwarded via a Home Agent to the new location (Perkins 2002)."," Mobility: As users move from one access point (base station, cell. . . depending on the technology in use) to another while desiring permanent connectivity, two noteworthy problems occur: 1. Normally, any kind of link layer technology requires a certain time period for handoff (during which no packets can be transmitted) before normal operation can continue. 2. If the moving device is using an Internet connection, which should be maintained, it should keep the same IP address. Therefore, mechanisms for Mobile IP come into play, which may require incoming packets to be forwarded via a Home Agent to the new location (Perkins 2002). This means that packets that are directed to the mobile host experience increased delay, which has an adverse effect on RTT estimation. This list contains only a small subset of network environments there are a large number of other technologies that roughly have a similar inuence on delay and packet loss. ADSL connections, for example, are highly asymmetric and therefore exhibit the problems that were explained above for direct end user satellite connections. DWDMbased networks using optical burst or packet switching may add delay overhead or even drop packets, depending on the path setup and network load conditions (Hassan and Jain 2004). Some of the effects of mobility and wireless connections may be amplied in mobile ad hoc networks, where connections are in constant ux and customized routing schemes may cause delay overhead. Even much more traditional network environments show properties that might have an adverse effect on congestion control for example, link layer MAC functions such as Ethernet CSMA/CD can add delay, and so can path changes in the Internet"
Explain why adding functionality in the transport layer that deduces implicit feedback from measurements based on assumptions about lower layers (e.g. packets will mainly be dropped as a result of congestion) means to work against the underlying reasoning of the OSI model.,"In order to answer this, we need to look at the reason for making ow control a layer 3 function in the OSI standard: congestion occurs inside the network, and (ISO 1994) explicitly says that the network layer is supposed to hide details concerning the inner network from the transport layer. Hence, adding functionality in the transport layer that deduces implicit feedback from measurements based on assumptions about lower layers (e..g. packets will mainly be dropped as a result of congestion) means to work against the underlying reasoning of the OSI model."," Interestingly, no such function is listed for the transport layer in (ISO 1994) but almost any introductory networking book will (correctly) tell you that TCP is a transport layer protocol. Also, TCP is the main entity that realizes congestion control in the Internet complementary AQM mechanisms are helpful but play a less crucial role in practice. Indeed, embedding congestion control in TCP was a violation of the ISO/OSI model. This is not too unusual, as Internet protocols, in general, do not strictly follow the OSI rules as an example, there is nothing wrong with skipping layers in TCP/IP. One reason for this is that the rst Internet protocol standards are simply older than ISO/OSI. The important question on the table is, Was it good design to place congestion control in the transport rather than in the network layer? In order to answer this, we need to look at the reason for making ow control a layer 3 function in the OSI standard: congestion occurs inside the network, and (ISO 1994) explicitly says that the network layer is supposed to hide details concerning the inner network from the transport layer. Therefore, adding functionality in the transport layer that deduces implicit feedback from measurements based on assumptions about lower layers (e.g. packets will mainly be dropped as a result of congestion) means to work against the underlying reasoning of the OSI model. The unifying element of the Internet is said to be the IP datagram; it is a simple intermediate block that can act as a binding network layer element between an enormous number of different technologies on top and underneath. The catchphrase that says it all is: IP over everything, everything over IP. Now, as soon as some technology on top of IP makes implicit assumptions about lower layers, this narrows the eld of usability somewhat which is why we are facing well-known problems with TCP over heterogeneous network infrastructures. Researchers have come up with a plethora of individual TCP tweaks that enhance its behaviour in different environments, but there is one major problem here: owing to the wide acceptance of the whole TCP/IP suite, the binding element is no longer just IP but it is, in fact, TCP/IP in other words, you will need to be compatible with legacy TCP implementations, or you cannot speak with thy neighbour. Today, IP over everything, everything over TCP is more like it"
"Explain why in practice, difculties of fairness only arise when users do not share the similar (or the same amount of) resources"," In a network, having users assert different claims would mean that one user is more important than the other, perhaps because she paid more. This kind of per-user prioritization is a bit of a long lost dream in the history of computer networks. It was called Quality of Service (QoS), large amounts of money and effort were put into it and, bluntly put, nothing happened (we will take a much more detailed look at QoS in Chapter 5, but for now, this is all you need to know). Hence, in practice, difculties of fairness only arise when users do not share the similar (or the same amount of) resources."," In the second case, the cloth is divided proportionally, and the person who claimed twice as much receives twice the amount of the other one. In a network, having users assert different claims would mean that one user is more important than the other, perhaps because she paid more. This kind of per-user prioritization is a bit of a long lost dream in the history of computer networks. It was called Quality of Service (QoS), large amounts of money and effort were put into it and, bluntly put, nothing happened (we will take a much more detailed look at QoS in Chapter 5, but for now, this is all you need to know). Therefore, in practice, difculties of fairness only arise when users do not share the similar (or the same amount of) resources. In other words, all the methods to dene fairness that we will discuss here would equally divide an apple among all users if an apple is all we would care about. Similarly, in Figure 2.4, dening fairness is trivial because there is only one resource. Before we go into details of more complex scenarios, it is perhaps worth mentioning how equal sharing of a single resource can be quantied. This can be done by means of Raj CONGESTION CONTROL PRINCIPLES Flow 2 Flow 2 Flow 1 Flow 1 B (b) Jains fairness index (Jain et al. 1984): if the system allocates rates to n contending users, such that the ith user receives a rate allocation xi , the fairness index f (x) is dened as  n i=1 xi (2.2) f (x) = n n i=1 xi2 This is a good measure for fairness because f (x) is 1 if all allocations xi are perfectly equal and immediately becomes less than 1 upon the slightest deviation. It is therefore often used to evaluate the fairness of congestion control mechanisms"
Explain why the common denition of fairness in the Internet is called TCP CONGESTION CONTROL PRINCIPLES  friendliness; TCP-friendly ows are also called TCP compatible and dened as follows in (Braden et al,"TCP friendliness In the Internet of today, dening fairness follows a more pragmatic approach. Because most of the ows in the network are TCP ows and therefore adhere to the same congestion control rules, and unresponsive ows can cause great harm, it is fair not to push away TCP ows. Hence, the common denition of fairness in the Internet is called TCP CONGESTION CONTROL PRINCIPLES  friendliness; TCP-friendly ows are also called TCP compatible and dened as follows in (Braden et al."," Moreover, it would be more useful to realize a type of fairness that supports heterogeneous utility functions. This, of course, brings about a whole new set of questions. For example, while allocating a rate that is below the limit to a hard real-time application is clearly useless, where should one draw the boundary for a delay-adaptive application? Frameworks that support different utility functions are an active and interesting area of research; examples of related works are (Campbell and Liao 2001) and (Venkitaraman et al. 2002). 2.17.4 TCP friendliness In the Internet of today, dening fairness follows a more pragmatic approach. Because most of the ows in the network are TCP ows and therefore adhere to the same congestion control rules, and unresponsive ows can cause great harm, it is fair not to push away TCP ows. Therefore, the common denition of fairness in the Internet is called TCP CONGESTION CONTROL PRINCIPLES friendliness; TCP-friendly ows are also called TCP compatible and dened as follows in (Braden et al. 1998): A TCP-compatible ow is responsive to congestion notication, and in steady state it uses no more bandwidth than a conforming TCP running under comparable conditions. We will revisit TCP-friendly congestion control and take a look at some of the numerous proposals to realize it in Chapter 4"
Explain why cwnd is not set to ssthresh but to ssthresh + 3  MSS when three DupACKs have arrived," Fast recovery is actually a little more sophisticated: since each DupACK indicates that a segment has left the network, an additional segment can be sent to take its place for every DupACK that arrives at the sender. Hence, cwnd is not set to ssthresh but to ssthresh + 3  MSS when three DupACKs have arrived."," This behaviour is shown by the Reno line in Figure 3.6. While the Tahoe release of the BSD TCP code already contained fast retransmit, fast recovery only made it into a release, which was called Reno. Geographically, Reno is close to Tahoe (albeit in Nevada), but, unlike Tahoe, it is probably not worth visiting. I still remember the face of the man at the Reno Travelodge check-in desk, who raised his eyebrows when I asked him whether he can recommend a jazz club nearby, and replied: In this town, sir? He went on to explain that Reno has nothing but casinos and a group of kayak enthusiasts, and nobody would probably live there if given a choice. While this is, of course, an extremely biased description, it is probably safe to say that Reno, the biggest little city in the world, is a downscaled version of Vegas, which, as we will see in the next chapter, is also a TCP version. Fast recovery is actually a little more sophisticated: since each DupACK indicates that a segment has left the network, an additional segment can be sent to take its place for every DupACK that arrives at the sender. Therefore, cwnd is not set to ssthresh but to ssthresh + 3 MSS when three DupACKs have arrived. Here is how RFC 2581 species the combined implementation of fast retransmit and fast recovery: 1. When the third duplicate ACK is received, set ssthresh to no more than half the amount of outstanding data in the network (i.e. at most cwnd/2), but at least to 2 * MSS. 15 It is probably a little more than 75% because the pipe is overfull (i.e. some packets are stored in queues) when congestion sets in"
Explain why the ECN rule for routers is to follow the traditional model of dropping  packets unless they see a bit that tells them that the source understands ECN,"One major difference between ECN as a theoretical construct and ECN in the Internet is the necessity of a second bit that conveys the information yes, I understand ECN to routers. This is necessary because the Internet can only be upgraded in a gradual manner, and it can of course not be assumed that setting a bit in the IP header will make old end systems act as if a packet had been dropped when they do not even understand what the bit means. Hence, the ECN rule for routers is to follow the traditional model of dropping  packets unless they see a bit that tells them that the source understands ECN."," 3.4.9 Explicit Congestion Notication (ECN) The RED active queue management mechanism has gained reasonable acceptance in the Internet community, and is known to be implemented in some routers. We already discussed active queue management in the previous chapter (Section 2.10.2 on Page 27), and we will take a closer look at RED in Section 3.7. Also, towards the end of the previous chapter, we examined the idea of setting a bit which means behave as if the packet was dropped instead of actually dropping a packet this is Explicit Congestion Notication (ECN) (see Section 2.12.1 on Page 32). Note that ECN requires active queue management to be in place when a queue overows, setting a bit is simply not an option. Also, while ECN can reduce loss, reacting to loss is still necessary because there can always be trafc bursts that are too large to t into the buffer of a router, with or without active queue management. Adding ECN functionality to the Internet was proposed in RFC 2481 (Ramakrishnan and Floyd 1999), which was published in January 1999 and obsoleted by RFC 3168 (Ramakrishnan et al. 2001) in September 2001. For ease of explanation, we will start with a description of ECN as it was originally proposed and then look at the most-important changes that were made. One major difference between ECN as a theoretical construct and ECN in the Internet is the necessity of a second bit that conveys the information yes, I understand ECN to routers. This is necessary because the Internet can only be upgraded in a gradual manner, and it can of course not be assumed that setting a bit in the IP header will make old end systems act as if a packet had been dropped when they do not even understand what the bit means. Therefore, the ECN rule for routers is to follow the traditional model of dropping packets unless they see a bit that tells them that the source understands ECN. There is a second obvious reason for using such a bit not all transport protocols will support ECN, even after the Internet has been upgraded (e.g. UDP). Hence, a router should discard rather than mark such packets, when necessary. Since it is unclear whether an application that uses UDP will react to congestion, the general recommendation is not to make ECN usage available in the UDP socket interface. Introducing ECN to the Internet was a major step; after failed attempts with the ICMP Source Quench message, it is the rst feasible TCP/IP congestion control solution that incorporates explicit feedback. In theory, the benet of ECN is loss reduction in the presence of routers that use active queue management, and there are no disadvantages whatsoever"
"Explain why it can never be guaranteed that sequence numbers stay within a pre-dened range, and a synchronization mechanism is required"," While TCP retransmits packets (and thereby reuses sequence numbers) in the presence of loss, a long lasting loss event causes a DCCP sender to increase the sequence number that it places in packets further and further. Hence, it can never be guaranteed that sequence numbers stay within a pre-dened range, and a synchronization mechanism is required."," This is true for all regular data packet and ACK sequence numbers alike. In order to prevent hijacking attacks, DCCP checks sequence numbers for validity. In TCP, this is a natural property of window-based ow control DCCP denes a Sequence Window to do the same, but unreliability makes this a somewhat difcult task. While TCP retransmits packets (and thereby reuses sequence numbers) in the presence of loss, a long lasting loss event causes a DCCP sender to increase the sequence number that it places in packets further and further. Therefore, it can never be guaranteed that sequence numbers stay within a pre-dened range, and a synchronization mechanism is required. Specically, upon reception of a packet that carries a sequence number that lies outside a so-called Sequence Window, a DCCP entity asks its peer whether the sequence number is really correct and provides it with its own sequence number, which must be acknowledged in the reply. This simple mechanism provides some protection against hijackers that cannot snoop. Signicantly better security could be obtained via encryption, but this would be a duplication of functionality that is already provided by IPSec, and it was therefore not included in the protocol. Finally, some protection against denial-of-service attacks is provided by the aforementioned cookie that is used when setting up a connection"
"Explain why in order to efciently utilize the available resources, it might be necessary for the admission control entity to measure the actual bandwidth usage, thereby adding feedback to the control and deviating from its strictly open character","Things are relatively simple in the telephone network: a call is assumed to have xed bandwidth requirements, and so the link capacity can be divided by a pre-dened value in order to calculate the number of calls that can be admitted. In a multi-service network like the Internet however, where a diverse range of different applications should be supported, neither bandwidth requirements nor application behaviour may be known in advance. Hence, in order to efciently utilize the available resources, it might be necessary for the admission control entity to measure the actual bandwidth usage, thereby adding feedback to the control and deviating from its strictly open character."," Historically speaking, admission control in connection-oriented networks could therefore be regarded as a predecessor of congestion control in packet networks. Things are relatively simple in the telephone network: a call is assumed to have xed bandwidth requirements, and so the link capacity can be divided by a pre-dened value in order to calculate the number of calls that can be admitted. In a multi-service network like the Internet however, where a diverse range of different applications should be supported, neither bandwidth requirements nor application behaviour may be known in advance. Thus, in order to efciently utilize the available resources, it might be necessary for the admission control entity to measure the actual bandwidth usage, thereby adding feedback to the control and deviating from its strictly open character. Open-loop control was called proactive (as opposed to reactive control) in (Keshav 1991a). Keshav also pointed out what we have just seen: that these two control modes are not mutually exclusive. 2.3.2 Congestion control and ow control Since intermediate nodes can act as controllers and measuring points at the same time, a congestion control scheme could theoretically exist where neither the sender nor the receiver is involved. This is, however, not a practical choice as most network technologies are designed to operate in a wide range of environment conditions, including the smallest possible setup: a sender and a receiver, interconnected via a single link. While congestion collapse is less of a problem in this scenario, the receiver should still have some means to slow down the sender if it is busy doing more pressing things than receiving network packets or if it is simply not fast enough. In this case, the function of informing the sender to reduce its rate is normally called ow control "
Explain why the goal is to detect corruption and inform the TCP sender about it so that it can take corrective action instead of decreasing cwnd as if congestion had occurred,"The range of proposals to enhance the performance of TCP in the presence of corruption is sheerly endless. The underlying problem that all these proposals are trying to solve in one way or another is a misunderstanding: when a packet is lost, TCP generally assumes that this is a sign of congestion, whereas in reality, it sometimes is not. When a mobile user passes by a wall, link noise can occur, which leads to bit errors and causes a checksum to fail. Eventually, the packet cannot be delivered, and TCP assumes that it was dropped as a result of congestion in the network (i.e. because a queue overowed or an AQM mechanism decided to drop the packet). Hence, the goal is to detect corruption and inform the TCP sender about it so that it can take corrective action instead of decreasing cwnd as if congestion had occurred."," A large DupThresh value is less harmful for paths where there is hardly any loss. On lossy paths, however, it should be small in order to avoid the adverse effects described above. For events such as timeouts and false fast retransmits, cost functions are derived in (Zhang et al. 2002) on the basis of these calculated costs, the value of the aforementioned X (called the False Fast Retransmit Avoidance (FA) ratio) is decreased. This is perhaps the most-important difference between this work and (Blanton and Allman 2002), where it was already suggested that DupThresh be increased in response to false fast retransmits. The new scheme is called Reordering-Robust TCP (RR-TCP), and it balances DupThresh by increasing it when long reordering events occur and decreasing it when it turns out that the value was too large. RR-TCP includes the complementary updated RTO calculation method that was described in the previous section. Simulations in (Zhang et al. 2002) show promising results, including the ability of RR-TCP to effectively avoid timeouts. 4.1.7 Corruption The range of proposals to enhance the performance of TCP in the presence of corruption is sheerly endless. The underlying problem that all these proposals are trying to solve in one way or another is a misunderstanding: when a packet is lost, TCP generally assumes that this is a sign of congestion, whereas in reality, it sometimes is not. When a mobile user passes by a wall, link noise can occur, which leads to bit errors and causes a checksum to fail. Eventually, the packet cannot be delivered, and TCP assumes that it was dropped as a result of congestion in the network (i.e. because a queue overowed or an AQM mechanism decided to drop the packet). Thus, the goal is to detect corruption and inform the TCP sender about it so that it can take corrective action instead of decreasing cwnd as if congestion had occurred. As with congestion, the type of feedback that can be used to deal with corruption can have two forms: Implicit: Since their feedback loop is typically shorter than the end-to-end feedback loop, it is quite common for link layers to retransmit frames in the presence of corruption. This method, which is called link layer ARQ, is typically carried out with a certain level of persistence (which might even be congurable), which means that the system gives up and decides to drop the whole frame after a timeout or after a certain number of tries (more details can be found in RFC 3366 (Fairhurst and Wood 2002)). Thus, from the perspective of TCP, the segment is either dropped or delayed technologies that detect spurious timeouts (see Section 4.1.5), which generally make TCP more robust in the presence of sudden delay spikes, will therefore lead to better behaviour across such links. Also, while one can never rely on ECN as the only congestion signal because it may be inevitable for a router to drop some packets in the presence of a large amount of trafc, this signal is an explicit congestion notication, which means that it is not prone to the same misinterpretation as dropped packets. If one could assume that all routers support ECN, total absence of such explicit congestion signals could implicitly indicate that loss might stem from corruption instead; while it is questionable whether it can be exploited in such a manner in practice (after all, loss can always indicate congestion), this is still an interesting facet of ECN. Finally, as could be expected, using SACK can make things better because of its ability to cope with multiple packet drops from a single window (which may not be a rare event if a link is noisy)"
"Explain why, one can intend to say something about bribery, but can never be sure that the injunction against bribery is understood in exactly the same way as it was intended"," In terms of the concepts that we employ in business ethics, it is important to take account of this iterability as well. If someone says, Do not bribe!, they Moral decision-making are drawing on some notion of bribery that is operative in any such statement, but the ethical intent which is never fully present in that statement in any selfidentical way can never be said. Hence,  one can intend to say something about bribery, but can never be sure that the injunction against bribery is understood in exactly the same way as it was intended."," This is what Derrida refers to as the iterability of any sign, or word. Things only make sense in relation to other things, and hence any conceptual description of things can only function as such if it is repeated. Though each repetition of the same concept draws on the similarities across various iterations, each iteration is also different. Repetitions are never pure; they always lead to alterations.31 Derrida never argued that an individual who speaks, writes, or makes decisions has no intentions, but he did warn us against thinking about intentionality in simple terms. Derridas work helps us understand that the intentions of an author, speaker, or decision-maker have to be understood as open to many contextual influences, and also to many possible interpretations. This also has implications for how our subjectivity operates, or who we are as agents. In terms of the concepts that we employ in business ethics, it is important to take account of this iterability as well. If someone says, Do not bribe!, they Moral decision-making are drawing on some notion of bribery that is operative in any such statement, but the ethical intent which is never fully present in that statement in any selfidentical way can never be said. Hence, one can intend to say something about bribery, but can never be sure that the injunction against bribery is understood in exactly the same way as it was intended. We should note that Derrida is not saying that there is pure intent, and misinterpretation of that intent, but rather that all language defies the possibility of the existence of such clear-cut content. In his concept of differance, Derrida points out that the meaning of any concept is always deferred (postponed), and always different.32 What are the implications of this for moral decision-makers, i.e. the subjects, or agents, who believe that they consider certain options and make a conscious choice? Though not denying the existence and role of consciousness, Derrida points out that the consciousness of the speaker or agent does not fully determine meaning; instead, it is how the act of speaking, deciding, or acting is performed that determines meaning in ways that we can hardly fathom. In fact, the way that we perform a certain speech act, like Do not bribe!, may in itself contain, or be haunted by, exactly that which that specific imperative opposes. Implications for decision-making in business ethics At this point, it may be helpful to consider a few practical business examples, which may allow us to think through the various implications of what Derrida argues. Let us return, for instance, to the issue of child labour, which has been an important topic for business ethics scholars and civil society activists alike. Many theoretical criticisms of child labour employ both consequentialist and non-consequentialist reasoning to describe child labour as an immoral practice. Yet, in reaching this conclusion too quickly, these well-meaning scholars and activists bracket certain important moral questions in order to place the equally important moral question of avoiding harm to children at the forefront"
"Explain why, the theory seems to assume that women do not or cannot have the gift of Heaven of which Carlyle speaks (which we now call charisma)","Four types of imagery are particularly problematic about the great man: (1) Masculine imagery. The first problem with the great man is that he is a man and not a woman. Hence,  the theory seems to assume that women do not or cannot have the gift of Heaven of which Carlyle speaks (which we now call charisma)."," We will see below that business ethicists generally do not indulge in this kind of imagery, but have by and large offered a more down to earth approach to leadership. But before we enter the field of business ethics, we would like to draw your attention to a few problems relating to what we referred to as the great man theory of leadership. We will treat this theory as just an example of how leadership can be rendered as something beautiful. Four types of imagery are particularly problematic about the great man: (1) Masculine imagery. The first problem with the great man is that he is a man and not a woman. Hence, the theory seems to assume that women do not or cannot have the gift of Heaven of which Carlyle speaks (which we now call charisma). Indeed, much of leadership thinking is very masculine, often excluding women in more and less explicit ways. (2) Adventurous imagery. The great man, in all his various guises, is routinely portrayed as a rule-breaking character, a daredevil, who, entirely aroused by a certain vision, goes where no one else dares to go. This is part and parcel of a tradition which seeks to emphasize that leadership or enterprise are all about breaking through certain more or less deeply ingrained habits"
"Explain why, the simultaneous occurrence of origin and supplement is impossible","On the one hand, the supplement is necessary for the origin to mean anything at all. On the other hand, we cannot have both, origin and supplement (see above), at the same time. Hence,  the simultaneous occurrence of origin and supplement is impossible."," According to Derrida, every apparent origin remains caught up in an aporia. On the one hand, the supplement is necessary for the origin to mean anything at all. On the other hand, we cannot have both, origin and supplement (see above), at the same time. Hence, the simultaneous occurrence of origin and supplement is impossible. That is why Derrida talks about the aporetic supplement.31 For Derrida, any aporia refers to the contradictions we face when trying to justify the pureness of an origin. An aporia describes an undecidable situation in which we cannot turn to or justify one side of the opposition as being pure and self-defining.32 To stick to our example: no rule could ever define its own conditions of application. Hence, the perfect iteration of a rule in different contexts remains impossible. The impasse, which any aporia aims to uncover, is not destructive or paralyzing. Aporia I the rules of corporate responsibility standards Corporate responsibility standards consist of rules. Here, we are interested in regulative rules i.e. rules which regulate the behaviour of corporations.33 For instance, SA 8000 has fixed the following rule: the company shall provide a healthy and safe workplace environment and shall take effective steps to prevent potential accidents and injury to workers health.34 Derrida would question the pureness and easy applicability of such a rule. Of course, strictly speaking, this rule needs to mean the same every time it is applied during a social audit; it not only needs to mean the same throughout multiple applications over time, but also in different social and economic contexts, as SA 8000 claims to be a universal standard applicable to corporations in all parts of the world. Derrida would ask us to consider that rules cannot possess a pure/metaphysical meaning which could be iterated easily. Rather, every rule gains meaning in and through its own application. The supplement (application) is constitutive for the origin (standards rules) to acquire contextualized meaning"
"Explain why, the perfect iteration of a rule in different contexts remains impossible","Derrida, any aporia refers to the contradictions we face when trying to justify the pureness of an origin. An aporia describes an undecidable situation in which we cannot turn to or justify one side of the opposition as being pure and self-defining.32 To stick to our example: no rule could ever define its own conditions of application. Hence,  the perfect iteration of a rule in different contexts remains impossible."," According to Derrida, every apparent origin remains caught up in an aporia. On the one hand, the supplement is necessary for the origin to mean anything at all. On the other hand, we cannot have both, origin and supplement (see above), at the same time. Hence, the simultaneous occurrence of origin and supplement is impossible. That is why Derrida talks about the aporetic supplement.31 For Derrida, any aporia refers to the contradictions we face when trying to justify the pureness of an origin. An aporia describes an undecidable situation in which we cannot turn to or justify one side of the opposition as being pure and self-defining.32 To stick to our example: no rule could ever define its own conditions of application. Hence, the perfect iteration of a rule in different contexts remains impossible. The impasse, which any aporia aims to uncover, is not destructive or paralyzing. Aporia I the rules of corporate responsibility standards Corporate responsibility standards consist of rules. Here, we are interested in regulative rules i.e. rules which regulate the behaviour of corporations.33 For instance, SA 8000 has fixed the following rule: the company shall provide a healthy and safe workplace environment and shall take effective steps to prevent potential accidents and injury to workers health.34 Derrida would question the pureness and easy applicability of such a rule. Of course, strictly speaking, this rule needs to mean the same every time it is applied during a social audit; it not only needs to mean the same throughout multiple applications over time, but also in different social and economic contexts, as SA 8000 claims to be a universal standard applicable to corporations in all parts of the world. Derrida would ask us to consider that rules cannot possess a pure/metaphysical meaning which could be iterated easily. Rather, every rule gains meaning in and through its own application. The supplement (application) is constitutive for the origin (standards rules) to acquire contextualized meaning"
"Explain why, you must take care of all quality attributes from the very beginning of the process","High-speed releasing, together with a high portion of technical debt, leads to degeneration, which has consequences. In particular, degeneration of quality at the beginning of the process negatively affects the entire process, with self-reinforcing results (i.e. degeneration of quality will probably lead to more degeneration of quality) . Hence,  you must take care of all quality attributes from the very beginning of the process."," Degeneration: Faults and Failures Over time, if you dont pay attention to the quality of software, software can degenerate. Degeneration happens, for example, if too many versions are pushed to production, with the application having suboptimal internal quality, and many more upcoming requirements should be implemented in the software. Other worse-case scenarios include automation that is not tested and packing and deploying the software to production when it is error-prone; monitoring is not in place; or nonfunctional requirements were not implemented accordingly (e.g., because the operations team was not consulted early enough). High-speed releasing, together with a high portion of technical debt, leads to degeneration, which has consequences. In particular, degeneration of quality at the beginning of the process negatively affects the entire process, with self-reinforcing results (i.e. degeneration of quality will probably lead to more degeneration of quality) . Hence, you must take care of all quality attributes from the very beginning of the process. This point may be obvious, but many teams fail to adhere to this principle. It will not work to inject quality into the software post mortem. Taking quality as an inherent part will reduce both the amount and the impact of faults"
"Explain why, both sides can experience moral hazards","Moral hazard11 emerges in situations where individual rationality doesnt meet collective rationality. Because teams and roles (e.g., development, operations, or management) can cause conceptual irrationalities when striving for more power, its concepts and requirements may not best t the businesss needs. Combined with an unbalanced need for quick and easily understandable but rarely given IT reporting, a conspiracy between management and development/ operations can be individually rational. Hence,  both sides can experience moral hazards."," Having shared values and goals between the principal and agent. Moral Hazard Moral hazard11 emerges in situations where individual rationality doesnt meet collective rationality. Because teams and roles (e.g., development, operations, or management) can cause conceptual irrationalities when striving for more power, its concepts and requirements may not best t the businesss needs. Combined with an unbalanced need for quick and easily understandable but rarely given IT reporting, a conspiracy between management and development/ operations can be individually rational. Hence, both sides can experience moral hazards. Moral hazard is created as part of a cycle that is repeatedly kicked off when any changes are applied to the system (i.e., new business ideas or new functional or nonfunctional requirements; see costs and the need for new changes. The cost of moral hazard becomes evident if we imagine that the people committing moral hazard use company resources for their own good. The company itself is responsible for generating new opportunities for moral hazard by creating new guidelines or overlooking regulation gaps, both of which can be utilized by a corporate member. Such opportunities create adverse incentives for employees, who often do not share their useful knowledge to gain benet from these gaps"
"Explain why, C-level management (which normally comprises the highest-level executives) is the last place to address these issues because strategic ideas require operational effectiveness and efciency","Because the existence of moral hazard translates into more costs to the company, the best way to minimize the problem is to reduce incentives in corporate conditions. Transparency is the best cure for moral hazard. Most companies are aware of this danger and are interested in minimizing it. Hence,  C-level management (which normally comprises the highest-level executives) is the last place to address these issues because strategic ideas require operational effectiveness and efciency."," dEvoPs foR dEvEloPERs MORAL HAZARD Moral hazard occurs if people use their knowledge to gain individual benets in order to obtain more power or income. Because the existence of moral hazard translates into more costs to the company, the best way to minimize the problem is to reduce incentives in corporate conditions. Transparency is the best cure for moral hazard. Most companies are aware of this danger and are interested in minimizing it. Hence, C-level management (which normally comprises the highest-level executives) is the last place to address these issues because strategic ideas require operational effectiveness and efciency. Monitoring Risk preference Informational asymmetry Adverse selection The cycle starts from any change to the system, which opens opportunities and incentives and addresses the basic precondition of asymmetric information between a principal and an agent Moral hazard players know how to cover their tracks. Thus, one objective of these companies is to reduce the possibilities of nding and tracing problems. This is often done by nding good alibis and excuses. An indicator that points toward this behavior is a disinclination to use any kind of objective statistics, including data historization or central log les for technical and functional error logging. If transparency exists, development and operations could easily trace any problems or misunderstandings to their causes and hence wouldn't suffer a loss of CHAPTER 7 | Unified and Holistic Approach reputation by delving into trustworthy explanations. A good approach for analyzing root causes for issues is the ve whys approach, a technique proposed by Taiichi Ohno12"
Explain why the company incurs heavy losses in sales and reputation,"After going through intensive emotional torture, the company begins an objective examination that nds the root cause of the issue. However, by this point, the users have already been scared away from the new application. Hence, the company incurs heavy losses in sales and reputation."," Escalating the issue leads to nger pointing: development claims it is the fault of the database group, the database team blames the network group, and others speculate that the server group is responsible for the outage. After going through intensive emotional torture, the company begins an objective examination that nds the root cause of the issue. However, by this point, the users have already been scared away from the new application. As a result, the company incurs heavy losses in sales and reputation. Conicts About Performance The blame game can also occur in the following scenario: 1"
Explain why the deployment itself will also only change in smaller batches between different deployments,"Deploying to production frequently will help keep things simple and make the individual changes more focused. The risk of deployments is reduced because you practice the process of deployment. By bringing the pain forward, youll identify problems in your process and tool chains earlier and will be able to optimize accordingly. Hence, the deployment itself will also only change in smaller batches between different deployments."," Big releases Available functionality Small releases shipped late. By using small releases, you will use appropriate batch sizes, and functionality will be available much sooner. Deploying to production frequently will help keep things simple and make the individual changes more focused. The risk of deployments is reduced because you practice the process of deployment. By bringing the pain forward, youll identify problems in your process and tool chains earlier and will be able to optimize accordingly. As a result, the deployment itself will also only change in smaller batches between different deployments. Another big advantage of deploying to production frequently is that the process of xing incidents will become optimized too"
"Explain why the supply is determined according to the actual demand, not according to some theoretical, forecasted, nonrealistic, or even academic target demand","In Kanban, stations receive a pull from the demand. Hence, the supply is determined according to the actual demand, not according to some theoretical, forecasted, nonrealistic, or even academic target demand."," Kanban does not prescribe a specic method to use. Kanban is inuenced by the theory of constraints (TOC).5 The base of TOC is the idiom that a chain is no stronger than its weakest link. This idiom is transported to management and software engineering. The weakest items in the overall chain can cause failure or adversely affect the outcome. Other important inuences of Kanban include Kaizen, which literally means continuous improvement.6 In Kanban, stations receive a pull from the demand. Therefore, the supply is determined according to the actual demand, not according to some theoretical, forecasted, nonrealistic, or even academic target demand. See James E. Tomayko, Computers in Spaceight: The NASA Experience, Chapter 4: Computers in the Space Shuttle Avionics System (Amazon Digital Services, 1988)"
Explain why the conceptual decits originating from complex and dynamic project environments can also be described by the following few typical statements,"Not all individual steps and decisions of frequently applied procedures are consciously executed, and not all reasons for those routines are clear to all persons. Corporate processes are often complex. Thus, the effects of changes on complex and only partially conscious processes arent perfectly predictable. A vivid stakeholder environment magnies this aspect because changes to given routines are conducted more frequently. Hence, the conceptual decits originating from complex and dynamic project environments can also be described by the following few typical statements."," The last aspect in terms of limited rationality is that management sometimes learns a new business aspect while performing it. This situation arises because some management members are new to what they do or the company itself is starting a completely new business. Complex and Dynamic Environments Not all individual steps and decisions of frequently applied procedures are consciously executed, and not all reasons for those routines are clear to all persons. Corporate processes are often complex. Thus, the effects of changes on complex and only partially conscious processes arent perfectly predictable. A vivid stakeholder environment magnies this aspect because changes to given routines are conducted more frequently. Therefore, the conceptual decits originating from complex and dynamic project environments can also be described by the following few typical statements. Management is not always conscious of all dependencies of its actions and wishes. This aspect mainly addresses a situation in which not all relevant aspects of a feature request are conscious to the management"
"Explain why DevOps centers on the concept of sharing: sharing ideas, issues, processes, tools, and goals","Everything starts with how people perceive one another. That is, does the company have an us vs. them culture or a we culture? Hence, DevOps centers on the concept of sharing: sharing ideas, issues, processes, tools, and goals."," Views of DevOps The fundamental bases for successful DevOps are a culture of trust and a feeling of fellowship. Everything starts with how people perceive one another. That is, does the company have an us vs. them culture or a we culture? Thus, DevOps centers on the concept of sharing: sharing ideas, issues, processes, tools, and goals. What Is DevOps? DevOps is a mix of patterns intended to improve collaboration between development and operations. DevOps addresses shared goals and incentives as well as shared processes and tools. Because of the natural conicts among different groups, shared goals and incentives may not always be achievable. However, they should at least be aligned with one another"
Explain why DevOps is also not a new role prole that will supplant current developers and operations experts,"Responsibilities can, and do, shift over time, and as they shift, so do job descriptions. But no matter how you slice it, the same jobs need to be done, and one of those jobs is operations14 and the other is development. Hence, DevOps is also not a new role prole that will supplant current developers and operations experts."," Similarly, the opposite is not true: DevOps does not mean that operations experts take over all development tasks. Responsibilities can, and do, shift over time, and as they shift, so do job descriptions. But no matter how you slice it, the same jobs need to be done, and one of those jobs is operations14 and the other is development. Thus, DevOps is also not a new role prole that will supplant current developers and operations experts. DevOps describes patterns for collaboration, processes, and tools; it is not a new job title (see Figure 1-4). As soon as you understand the DevOps concept, youll see how strange the very idea of hiring a DevOp is. please point the author of the ad to this book Some people may make the following claim: DevOps is a catalog of recipes: implement them all, and you are done. This statement is false because you will focus on nding the best solution for your individual situation by implementing DevOps. There is no one-size-ts-all solution, and no DevOps-by-the-book approach will solve all of your problems"
"Explain why the development department desires a high ow of newly provided functionality, whereas the operations department prefers to avoid putting any new release into production","The comparison of tasks and views of development and operations shows that the two teams have different goals and incentives and that these differences lead to conict. Development strives for change (e.g., new features and bug xes), whereas the operations team strives for stability. Often, those groups are paid precisely for these tasks: development obtains bonus payments if the software is delivered, whereas operations is rewarded if production systems are stable. Hence, the development department desires a high ow of newly provided functionality, whereas the operations department prefers to avoid putting any new release into production."," DevOps One Team Approach: Development and Operations With the DevOps approach, the developer role consists of programmers, testers, QA, and experts from operations. They all develop software and help to bring it to the user. The comparison of tasks and views of development and operations shows that the two teams have different goals and incentives and that these differences lead to conict. Development strives for change (e.g., new features and bug xes), whereas the operations team strives for stability. Often, those groups are paid precisely for these tasks: development obtains bonus payments if the software is delivered, whereas operations is rewarded if production systems are stable. Thus, the development department desires a high ow of newly provided functionality, whereas the operations department prefers to avoid putting any new release into production. Both teams follow their respective goals by focusing on their individual tasks and by fullling their obligations to obtain positive attention from management and visibility for doing a great job. To achieve their respective goals, development and operations often use their own processes and tools. The sets of processes and tools are optimized locally (for each group) to obtain the best local result"
Explain why synergies while administrating applications do not have a high priority,"Development and operations share the same processes, and both groups are focused on delivering application changes to the user at a high frequency and quality. The unied process emphasizes the cycle time and prefers the vertical optimization approach. According to this method, every application is created and executed on the architecture that is perfect for this concrete application (see Figure 2-7). The individual components of the infrastructure are laid out to fulll the requirements for the specic application. Optimization across the borders of individual applications is rare or does not occur at all. Hence, synergies while administrating applications do not have a high priority."," Including operations in Agile frameworks and processes, such as Scrum and Kanban. Development and operations share the same processes, and both groups are focused on delivering application changes to the user at a high frequency and quality. The unied process emphasizes the cycle time and prefers the vertical optimization approach. According to this method, every application is created and executed on the architecture that is perfect for this concrete application (see Figure 2-7). The individual components of the infrastructure are laid out to fulll the requirements for the specic application. Optimization across the borders of individual applications is rare or does not occur at all. Thus, synergies while administrating applications do not have a high priority. Traditionally, the vertical optimization approach is preferred by the development team"
Explain why the test pass/fail ratio is not useful because the team stops and directly xes the regression," Broken Agile Metrics Despite good intentions, metrics are often broken in Agile teams. The following are some examples of broken metrics: Test pass/fail ratios: The Agile team stops the line and immediately xes a broken test. Hence, the test pass/fail ratio is not useful because the team stops and directly xes the regression."," Denition of Done Another well-known and commonly used approach is the Denition of Done (DoD). Before the job is started, the denition of a completed job is specied, and the team commits to this denition. DoD often states that no development job is nished until testing is complete or the software is shipped to target systems, in a dened quality, or that monitoring is available for shipped software. By using DoD, the whole team shares the same understanding of when the task is completed. Additionally, DoD requires new features to add value to the system and to the customer after it has been shipped and made available to him or her. Broken Agile Metrics Despite good intentions, metrics are often broken in Agile teams. The following are some examples of broken metrics: Test pass/fail ratios: The Agile team stops the line and immediately xes a broken test. Thus, the test pass/fail ratio is not useful because the team stops and directly xes the regression. However, the metric is useful for detecting basic aws. For example, if the test coverage is below 20 percent, it is pretty obvious that technical debt has been accumulated. Note Technical debt is a metaphor to describe the eventual consequences of suboptimal software. The debt is open work that needs to be done before a task can be considered completed"
Explain why it is helpful to set up a glossary in your project to ensure a common understanding of key terms,"Some people may use terms in different ways. For example, the term throughput is used to refer to both the quantity of output and the output rate. Hence, it is helpful to set up a glossary in your project to ensure a common understanding of key terms."," Takt time is primarily a manufacturing term that refers to the rhythm of the process. I don't use this term often because I prefer to refer to the cadence of a process, such as the release of code every 24 hours. Usually, manufacturing will refer to a single takt time for a process, such as the production of one car off a line every ve minutes. If the takt time changes this timing, the change is propagated to all subprocesses. I prefer to refer to cadence because I can specify that to a weekly deployment cadence and a daily testing cadence. Some people may use terms in different ways. For example, the term throughput is used to refer to both the quantity of output and the output rate. Thus, it is helpful to set up a glossary in your project to ensure a common understanding of key terms. See Developing Products in Half the Time (Wiley, 1997) and The Principles of Product Development Flow: Second Generation Lean Product Development (Celeritas, 2009)"
"Explain why increasing the change frequency and decreasing the respective change sizes (see Change size less risky, more convenient, and quicker to the market"," DevOps for Developers Hence, increasing the change frequency and decreasing the respective change sizes (see Change size less risky, more convenient, and quicker to the market."," If your deployment pipeline is truly efcient, its often quicker to check in a forward rolling change (e.g., a bug x in the code) instead of working with cumbersome rollbacks. Always applying changes to create new versions of the software (meaning always rolling forward) has the direct advantage that your deployment process is more focused and simpler, because you dont need to set up and maintain an often complex rollback process. DevOps for Developers Thus, increasing the change frequency and decreasing the respective change sizes (see Change size less risky, more convenient, and quicker to the market. Automatic Releasing Many risk reduction practices exist, such as continuous integration and automatic tests. On a higher level, these practices can be summarized as recipes for releasing. There is functional releasing (assigning and tracking work items to target releases) and technical releasing, which forms baselines (known good states), a consistent and versioned set of conguration items"
"Explain how a Kanban board tracks each feature as it ows through the workow, having one column for each step in the workow","Deploy (which is performed by operations) has staging and production states. Kanban cards (i.e., tickets, tasks) ow through these states, from left to right, from the backlog until being done (the live state). Hence, a Kanban board tracks each feature as it ows through the workow, having one column for each step in the workow."," 7 Devops for developers In this example, we have the backlog area on the left that consists of open tasks and four vertical lanes: next, code, test, and deploy, which is followed by the work item being live in production. Both the code column and the test column are split into the doing and done states. Deploy (which is performed by operations) has staging and production states. Kanban cards (i.e., tickets, tasks) ow through these states, from left to right, from the backlog until being done (the live state). Thus, a Kanban board tracks each feature as it ows through the workow, having one column for each step in the workow. All kanban systems are designed to limit work-in-process, because the more work-in-process, the slower the ow.9 In addition to the vertical lanes, in our example we have two horizontal lanes (often called swim lanes) for the two main types of service: standard class and the expedite class of service. Each class of service has its own set of policies, including denitions of how items are prioritized and how they are pulled through the Kanban system.10 WIP values are set as the maximum number of cards that can be in that specic column (of that specic row) at a time. The expedite lane has a WIP of 1 (see the number 1 of row 1, which is the expedite lane, and right of the Live column in Figure 6-2) and enables the team to bring urgent items to production"
Explain why the effects of changes on complex and only partially conscious processes arent perfectly predictable,"Not all individual steps and decisions of frequently applied procedures are consciously executed, and not all reasons for those routines are clear to all persons. Corporate processes are often complex. Hence, the effects of changes on complex and only partially conscious processes arent perfectly predictable."," The last aspect in terms of limited rationality is that management sometimes learns a new business aspect while performing it. This situation arises because some management members are new to what they do or the company itself is starting a completely new business. Complex and Dynamic Environments Not all individual steps and decisions of frequently applied procedures are consciously executed, and not all reasons for those routines are clear to all persons. Corporate processes are often complex. Thus, the effects of changes on complex and only partially conscious processes arent perfectly predictable. A vivid stakeholder environment magnies this aspect because changes to given routines are conducted more frequently. Therefore, the conceptual decits originating from complex and dynamic project environments can also be described by the following few typical statements. Management is not always conscious of all dependencies of its actions and wishes. This aspect mainly addresses a situation in which not all relevant aspects of a feature request are conscious to the management"
Explain why it is individually rational for all parties to use the circumstances and make their own lives easy,"Collaborations between development and management are often stable and protable alliances because the alternative to development is either outlining each conceptual decit or possibly losing reputation by suddenly being known as a griper (in cases where the overall culture is not open and respectful). Additionally, the extra time consumed for doing all of the work in the project may give rise to conicting prior expectations such that development becomes spoilsports and even management suffers a loss of reputation. Hence, it is individually rational for all parties to use the circumstances and make their own lives easy."," Monitoring Risk preference Informational asymmetry Adverse selection The cycle starts from any change to the system, which opens opportunities and incentives and addresses the basic precondition of asymmetric information between a principal and an agent Moral hazard players know how to cover their tracks. Thus, one objective of these companies is to reduce the possibilities of nding and tracing problems. This is often done by nding good alibis and excuses. An indicator that points toward this behavior is a disinclination to use any kind of objective statistics, including data historization or central log les for technical and functional error logging. If transparency exists, development and operations could easily trace any problems or misunderstandings to their causes and hence wouldn't suffer a loss of CHAPTER 7 | Unified and Holistic Approach reputation by delving into trustworthy explanations. A good approach for analyzing root causes for issues is the ve whys approach, a technique proposed by Taiichi Ohno12. Collaborations between development and management are often stable and protable alliances because the alternative to development is either outlining each conceptual decit or possibly losing reputation by suddenly being known as a griper (in cases where the overall culture is not open and respectful). Additionally, the extra time consumed for doing all of the work in the project may give rise to conicting prior expectations such that development becomes spoilsports and even management suffers a loss of reputation. Thus, it is individually rational for all parties to use the circumstances and make their own lives easy. If such behavior is established, returning to normal, constructive work isn't easily possible. Development cannot argue why comparable work will suddenly take longer without the support of management. In turn, management cannot justify increasing costs for equivalent work. However, even if both parties try to end their pointless behavior, other expectations may force the actors to continue to commit moral hazard. Attributes of a Unied Approach A unied approach enables development and operations to create concepts collaboratively"
"Explain why by going into too many arguments without giving immediate and understandable answers, IT authorities often quickly end up in complicated situations","Unfortunately, management denes the requirements and hence can sweep a lot of dirt under the carpet because management also only reports to the C level. Additionally, if there are suspicions, the problem is often declared to be a mere technical problem until it is undeniably proven to be a conceptual decit of other departments. Conversely, IT authorities usually comprise the smallest proportion of management and hence often fear suffering a loss in reputation by providing evidence too often. This problem exists because the causality between effects and conceptual decit is not easy to explain to C-level management and cannot be quickly retrieved. Hence, by going into too many arguments without giving immediate and understandable answers, IT authorities often quickly end up in complicated situations."," Align Goals Sometimes, management is not aligned with the same goals of the company overall, which can result in subversive department behavior or individual activities driven by a hidden agenda. Unfortunately, management denes the requirements and hence can sweep a lot of dirt under the carpet because management also only reports to the C level. Additionally, if there are suspicions, the problem is often declared to be a mere technical problem until it is undeniably proven to be a conceptual decit of other departments. Conversely, IT authorities usually comprise the smallest proportion of management and hence often fear suffering a loss in reputation by providing evidence too often. This problem exists because the causality between effects and conceptual decit is not easy to explain to C-level management and cannot be quickly retrieved. Thus, by going into too many arguments without giving immediate and understandable answers, IT authorities often quickly end up in complicated situations. For more information on how to align with goals, see Chapter 5. Frank Buschmann et al. describe testability as a nonfunctional property of software architecture. See their Pattern-Oriented Software Architecture, Vol. 1 (Wiley, 1996), page 408"
"Explain why before moving out of the requirements phase into the rest of the process, all parties must understand and accept their responsibilities","Creating concepts, particularly writing requirements, is an important area in which the teams of development and operations need to make agreements. Writing requirements documentation is done to guide future actions in developing the solution. Much of the requirements work will be negated if choices, impositions and assumptions are not both understood and accepted by everyone involved. Hence, before moving out of the requirements phase into the rest of the process, all parties must understand and accept their responsibilities."," See Michael T. Nygard, Release It! (Pragmatic Bookshelf, 2007). Devops for developers Creating concepts, particularly writing requirements, is an important area in which the teams of development and operations need to make agreements. Writing requirements documentation is done to guide future actions in developing the solution. Much of the requirements work will be negated if choices, impositions and assumptions are not both understood and accepted by everyone involved. Thus, before moving out of the requirements phase into the rest of the process, all parties must understand and accept their responsibilities. Otherwise, customers will be disappointed when the product is delivered. To ensure understanding and acceptance, you must attempt to convert every choice, imposition, and assumption into explicit, documented agreements.24 Conclusion This chapter has provided background elaboration on concepts, particularly nonfunctional requirements, and conceptual decits. Conceptual decits may have different origins. They may come from limited rationality, complex and dynamic environments, the principal-agent problem, or from moral hazard. Youve learned that its important to minimize conceptual decits and detect them early. For that reason, traceability, aligned goals, and checks are important. This chapter closes the process view of DevOps. The next part will discuss the technical viewpoint of DevOps. See Donald C. Gause and Gerald M. Weinberg, Exploring Requirements: Quality Before Design (Dorset House, 1998), page 274"
Explain how these pipelines streamline the delivery process of software toward production,"From a more technical viewpoint, a pipeline is a way of grouping build jobs and putting them into relation to one another. In other words, pipelines are a way of orchestrating your build process through a series of quality gates, with automated or manually approval processes at specic stages. Hence, these pipelines streamline the delivery process of software toward production."," Delivery pipelines are often also known as staged builds or build pipelines. From a more technical viewpoint, a pipeline is a way of grouping build jobs and putting them into relation to one another. In other words, pipelines are a way of orchestrating your build process through a series of quality gates, with automated or manually approval processes at specic stages. Thus, these pipelines streamline the delivery process of software toward production.6 All code changes (including infrastructure code, see Chapter 9) enter a pipeline into production. The pipeline promotes software and stages artifacts from the development team to the nal release in production. You should take special care in linking all artifact types to consistent releases, and you should realize that the pipeline is not a re-and-forget tube. Instead, See Jez Humble and David Farley, Continuous Delivery (Addison-Wesley, 2011), Chapter 5. CHAPTER 8 | Automatic Releasing development and operations are connected and integrated, and feedback from the user is streamed back, and in most cases, the solution is (hopefully) more of an integrated lifecycle and something similar to a pipeline, where both ends of the tube are connected with each other"
Explain why occasional datagram loss within a connection when a node moves between networks is by no means a catastrophic problem,"A (at which point it can no longer receive datagrams via A) and its attachment to network B (at which point it will register a new COA with its home agent) is small, few datagrams will be lost. Recall from Chapter 3 that end-to-end connections can suffer datagram loss due to network congestion. Hence, occasional datagram loss within a connection when a node moves between networks is by no means a catastrophic problem."," A foreign-agent decapsulation protocol. Extraction of the correspondents original datagram from the encapsulating datagram, and the forwarding of the original datagram to the mobile node. The previous discussion provides all the piecesforeign agents, the home agent, and indirect forwardingneeded for a mobile node to maintain an ongoing connection while moving among networks. As an example of how these pieces fit together, assume the mobile node is attached to foreign network A, has registered a COA in network A with its home agent, and is receiving datagrams that are being indirectly routed through its home agent. The mobile node now moves to foreign network B and registers with the foreign agent in network B, which informs the home agent of the mobile nodes new COA. From this point on, the home agent will reroute datagrams to foreign network B. As far as a correspondent is concerned, mobility is transparentdatagrams are routed via the same home agent both before and after the move. As far as the home agent is concerned, there is no disruption in the flow of datagramsarriving datagrams are first forwarded to foreign network A; after the change in COA, datagrams are forwarded to foreign network B. But will the mobile node see an interrupted flow of datagrams as it moves between networks? As long as the time between the mobile nodes disconnection from network A (at which point it can no longer receive datagrams via A) and its attachment to network B (at which point it will register a new COA with its home agent) is small, few datagrams will be lost. Recall from Chapter 3 that end-to-end connections can suffer datagram loss due to network congestion. Hence occasional datagram loss within a connection when a node moves between networks is by no means a catastrophic problem. If loss-free communication is required, upper-layer mechanisms will recover from datagram loss, whether such loss results from network congestion or from user mobility. An indirect routing approach is used in the mobile IP standard [RFC 5944], as discussed in Section 7.6"
"Explain how Google provides search results and e-mail access almost instantaneously, as if their data centers were running within ones own computer"," As discussed in Section 1.3.3, online service providers, such as Google and Microsoft, have deployed their own extensive private networks, which not only connect together their globally distributed data centers, but are used to bypass the Internet as much as possible by peering directly with lower-tier ISPs. Hence, Google provides search results and e-mail access almost instantaneously, as if their data centers were running within ones own computer."," Online social networkssuch as Facebook, Instagram, Twitter, and WeChat (hugely popular in China)have created massive people networks on top of the Internet. Many of these social networks are extensively used for messaging as well as photo sharing. Many Internet users today live primarily within one or more social networks. Through their APIs, the online social networks create platforms for new networked applications and distributed games. As discussed in Section 1.3.3, online service providers, such as Google and Microsoft, have deployed their own extensive private networks, which not only connect together their globally distributed data centers, but are used to bypass the Internet as much as possible by peering directly with lower-tier ISPs. As a result, Google provides search results and e-mail access almost instantaneously, as if their data centers were running within ones own computer. Many Internet commerce companies are now running their applications in the cloudsuch as in Amazons EC2, in Googles Application Engine, or in Microsofts Azure. Many companies and universities have also migrated their Internet applications (e.g., e-mail and Web hosting) to the cloud. Cloud companies not only provide applications scalable computing and storage environments, but also provide the applications implicit access to their high-performance private networks"
Explain why RSA is often used in practice in combination with symmetric key cryptography," By contrast, DES is at least 100 times faster in software and between 1,000 and 10,000 times faster in hardware [RSA Fast 2012]. Hence, RSA is often used in practice in combination with symmetric key cryptography."," m = c d mod n 12 15 22 Ciphertext c 8.2 PRINCIPLES OF CRYPTOGRAPHY Session Keys We note here that the exponentiation required by RSA is a rather time-consuming process. By contrast, DES is at least 100 times faster in software and between 1,000 and 10,000 times faster in hardware [RSA Fast 2012]. As a result, RSA is often used in practice in combination with symmetric key cryptography. For example, if Alice wants to send Bob a large amount of encrypted data, she could do the following. First Alice chooses a key that will be used to encode the data itself; this key is referred to as a session key, and is denoted by KS. Alice must inform Bob of the session key, since this is the shared symmetric key they will use with a symmetric key cipher (e.g., with DES or AES). Alice encrypts the session key using Bobs public key, that is, computes c = (KS)e mod n. Bob receives the RSA-encrypted session key, c, and decrypts it to obtain the session key, KS. Bob now knows the session key that Alice will use for her encrypted data transfer"
"Explain why the time from when the caravan is stored in front of a tollbooth until the caravan is stored in front of the next tollbooth is the sum of transmission delay and propagation delayin this example, 62 minutes","The time required for the tollbooth to push the entire caravan onto the highway is (10 cars)/(5 cars/minute) = 2 minutes. This time is analogous to the transmission delay in a router. The time required for a car to travel from the exit of one tollbooth to the next tollbooth is 100 km/(100 km/hour) = 1 hour. This time is analogous to propagation delay. Hence, the time from when the caravan is stored in front of a tollbooth until the caravan is stored in front of the next tollbooth is the sum of transmission delay and propagation delayin this example, 62 minutes."," Consider a highway that has a tollbooth every 100 kilometers, as shown in Figure 1.17. You can think of the highway segments between tollbooths as links and the tollbooths as routers. Suppose that cars travel (that is, propagate) on the highway at a rate of 100 km/hour (that is, when a car leaves a tollbooth, it instantaneously accelerates to 100 km/hour and maintains that speed between tollbooths). Suppose next that 10 cars, traveling together as a caravan, follow each other in a fixed order. You can think of each car as a bit and the caravan as a packet. Also suppose that each VideoNote Exploring propagation delay and transmission delay CHAPTER 1 COMPUTER NETWORKS AND THE INTERNET Ten-car caravan Toll booth Toll booth tollbooth services (that is, transmits) a car at a rate of one car per 12 seconds, and that it is late at night so that the caravans cars are the only cars on the highway. Finally, suppose that whenever the first car of the caravan arrives at a tollbooth, it waits at the entrance until the other nine cars have arrived and lined up behind it. (Thus the entire caravan must be stored at the tollbooth before it can begin to be forwarded.) The time required for the tollbooth to push the entire caravan onto the highway is (10 cars)/(5 cars/minute) = 2 minutes. This time is analogous to the transmission delay in a router. The time required for a car to travel from the exit of one tollbooth to the next tollbooth is 100 km/(100 km/hour) = 1 hour. This time is analogous to propagation delay. Therefore, the time from when the caravan is stored in front of a tollbooth until the caravan is stored in front of the next tollbooth is the sum of transmission delay and propagation delayin this example, 62 minutes. Lets explore this analogy a bit more. What would happen if the tollbooth service time for a caravan were greater than the time for a car to travel between tollbooths? For example, suppose now that the cars travel at the rate of 1,000 km/hour and the tollbooth services cars at the rate of one car per minute. Then the traveling delay between two tollbooths is 6 minutes and the time to serve a caravan is 10 minutes. In this case, the first few cars in the caravan will arrive at the second tollbooth before the last cars in the caravan leave the first tollbooth. This situation also arises in packet-switched networksthe first bits in a packet can arrive at a router while many of the remaining bits in the packet are still waiting to be transmitted by the preceding router"
"Explain why when characterizing queuing delay, one typically uses statistical measures, such as average queuing delay, variance of queuing delay, and the probability that the queuing delay exceeds some specified value","PhD thesis on the subject!). Unlike the other three delays (namely, dproc, dtrans, and dprop), the queuing delay can vary from packet to packet. For example, if 10 packets arrive at an empty queue at the same time, the first packet transmitted will suffer no queuing delay, while the last packet transmitted will suffer a relatively large queuing delay (while it waits for the other nine packets to be transmitted). Hence, when characterizing queuing delay, one typically uses statistical measures, such as average queuing delay, variance of queuing delay, and the probability that the queuing delay exceeds some specified value."," If we let dproc, dqueue, dtrans, and dprop denote the processing, queuing, transmission, and propagation delays, then the total nodal delay is given by dnodal = dproc + dqueue + dtrans + dprop The contribution of these delay components can vary significantly. For example, dprop can be negligible (for example, a couple of microseconds) for a link connecting two routers on the same university campus; however, dprop is hundreds of milliseconds for two routers interconnected by a geostationary satellite link, and can be the dominant term in dnodal. Similarly, dtrans can range from negligible to significant. Its contribution is typically negligible for transmission rates of 10 Mbps and higher (for example, for LANs); however, it can be hundreds of milliseconds for large Internet packets sent over low-speed dial-up modem links. The processing delay, dproc, is often negligible; however, it strongly influences a routers maximum throughput, which is the maximum rate at which a router can forward packets. 1.4.2 Queuing Delay and Packet Loss The most complicated and interesting component of nodal delay is the queuing delay, dqueue. In fact, queuing delay is so important and interesting in computer networking that thousands of papers and numerous books have been written about it [Bertsekas 1991; Daigle 1991; Kleinrock 1975, Kleinrock 1976; Ross 1995]. We give only a high-level, intuitive discussion of queuing delay here; the more curious reader may want to browse through some of the books (or even eventually write a PhD thesis on the subject!). Unlike the other three delays (namely, dproc, dtrans, and dprop), the queuing delay can vary from packet to packet. For example, if 10 packets arrive at an empty queue at the same time, the first packet transmitted will suffer no queuing delay, while the last packet transmitted will suffer a relatively large queuing delay (while it waits for the other nine packets to be transmitted). Therefore, when characterizing queuing delay, one typically uses statistical measures, such as average queuing delay, variance of queuing delay, and the probability that the queuing delay exceeds some specified value. When is the queuing delay large and when is it insignificant? The answer to this question depends on the rate at which traffic arrives at the queue, the transmission rate of the link, and the nature of the arriving traffic, that is, whether the traffic arrives periodically or arrives in bursts. To gain some insight here, let a denote the average rate at which packets arrive at the queue (a is in units of packets/sec). Recall that R is the transmission rate; that is, it is the rate (in bits/sec) at which bits are pushed out of the queue. Also suppose, for simplicity, that all packets consist of L bits. Then the average rate at which bits arrive at the queue is La bits/sec. Finally, assume that the queue is very big, so that it can hold essentially an infinite number of bits. The ratio La/R, called the traffic intensity, often plays an important role in estimating the extent of the queuing delay. If La/R > 1, then the average rate at which bits arrive at the queue exceeds the rate at which the bits can be transmitted from the queue. In this unfortunate situation, the queue will tend to increase without bound and the queuing delay will approach infinity! Therefore, one of the golden rules in traffic engineering is: Design your system so that the traffic intensity is no greater than 1"
Explain why one of the golden rules in traffic engineering is: Design your system so that the traffic intensity is no greater than 1,"La/R, called the traffic intensity, often plays an important role in estimating the extent of the queuing delay. If La/R > 1, then the average rate at which bits arrive at the queue exceeds the rate at which the bits can be transmitted from the queue. In this unfortunate situation, the queue will tend to increase without bound and the queuing delay will approach infinity! Hence, one of the golden rules in traffic engineering is: Design your system so that the traffic intensity is no greater than 1."," 1.4.2 Queuing Delay and Packet Loss The most complicated and interesting component of nodal delay is the queuing delay, dqueue. In fact, queuing delay is so important and interesting in computer networking that thousands of papers and numerous books have been written about it [Bertsekas 1991; Daigle 1991; Kleinrock 1975, Kleinrock 1976; Ross 1995]. We give only a high-level, intuitive discussion of queuing delay here; the more curious reader may want to browse through some of the books (or even eventually write a PhD thesis on the subject!). Unlike the other three delays (namely, dproc, dtrans, and dprop), the queuing delay can vary from packet to packet. For example, if 10 packets arrive at an empty queue at the same time, the first packet transmitted will suffer no queuing delay, while the last packet transmitted will suffer a relatively large queuing delay (while it waits for the other nine packets to be transmitted). Therefore, when characterizing queuing delay, one typically uses statistical measures, such as average queuing delay, variance of queuing delay, and the probability that the queuing delay exceeds some specified value. When is the queuing delay large and when is it insignificant? The answer to this question depends on the rate at which traffic arrives at the queue, the transmission rate of the link, and the nature of the arriving traffic, that is, whether the traffic arrives periodically or arrives in bursts. To gain some insight here, let a denote the average rate at which packets arrive at the queue (a is in units of packets/sec). Recall that R is the transmission rate; that is, it is the rate (in bits/sec) at which bits are pushed out of the queue. Also suppose, for simplicity, that all packets consist of L bits. Then the average rate at which bits arrive at the queue is La bits/sec. Finally, assume that the queue is very big, so that it can hold essentially an infinite number of bits. The ratio La/R, called the traffic intensity, often plays an important role in estimating the extent of the queuing delay. If La/R > 1, then the average rate at which bits arrive at the queue exceeds the rate at which the bits can be transmitted from the queue. In this unfortunate situation, the queue will tend to increase without bound and the queuing delay will approach infinity! Therefore, one of the golden rules in traffic engineering is: Design your system so that the traffic intensity is no greater than 1. Now consider the case La/R 1. Here, the nature of the arriving traffic impacts the queuing delay. For example, if packets arrive periodicallythat is, one packet arrives every L/R secondsthen every packet will arrive at an empty queue and CHAPTER 1 COMPUTER NETWORKS AND THE INTERNET there will be no queuing delay. On the other hand, if packets arrive in bursts but periodically, there can be a significant average queuing delay. For example, suppose N packets arrive simultaneously every (L/R)N seconds. Then the first packet transmitted has no queuing delay; the second packet transmitted has a queuing delay of L/R seconds; and more generally, the nth packet transmitted has a queuing delay of (n - 1)L/R seconds. We leave it as an exercise for you to calculate the average queuing delay in this example"
Explain why the constraining factor for throughput in todays Internet is typically the access network,"Rs and Rc. Indeed, today, the core of the Internet is over-provisioned with high speed links that experience little congestion. Also suppose that the only bits being sent in the entire network are those from the server to the client. Because the core of the computer network is like a wide pipe in this example, the rate at which bits can flow from source to destination is again the minimum of Rs and Rc, that is, throughput = min{Rs, Rc}. Hence, the constraining factor for throughput in todays Internet is typically the access network."," Now consider another example motivated by todays Internet. Figure 1.20(a) shows two end systems, a server and a client, connected to a computer network. Consider the throughput for a file transfer from the server to the client. The server is connected to the network with an access link of rate Rs and the client is connected to the network with an access link of rate Rc. Now suppose that all the links in the core of the communication network have very high transmission rates, much higher than Rs and Rc. Indeed, today, the core of the Internet is over-provisioned with high speed links that experience little congestion. Also suppose that the only bits being sent in the entire network are those from the server to the client. Because the core of the computer network is like a wide pipe in this example, the rate at which bits can flow from source to destination is again the minimum of Rs and Rc, that is, throughput = min{Rs, Rc}. Therefore, the constraining factor for throughput in todays Internet is typically the access network. For a final example, consider Figure 1.20(b) in which there are 10 servers and 10 clients connected to the core of the computer network. In this example, there are 10 simultaneous downloads taking place, involving 10 client-server pairs. Suppose that these 10 downloads are the only traffic in the network at the current time. As shown in the figure, there is a link in the core that is traversed by all 10 downloads"
Explain why Host A is never informed that some space has opened up in Host Bs receive buffer Host A is blocked and can transmit no more data!  segments with one data byte when Bs receive window is zero," As the application process at B empties the buffer, TCP does not send new segments with new rwnd values to Host A; indeed, TCP sends a segment to Host A only if it has data to send or if it has an acknowledgment to send. Hence, Host A is never informed that some space has opened up in Host Bs receive buffer Host A is blocked and can transmit no more data! To solve this problem, the TCP specification requires Host A to continue to send segments with one data byte when Bs receive window is zero."," Host A in turn keeps track of two variables, LastByteSent and LastByteAcked, which have obvious meanings. Note that the difference between these two variables, LastByteSent LastByteAcked, is the amount of unacknowledged data that A has sent into the connection. By keeping the amount of unacknowledged data less than the value of rwnd, Host A is assured that it is not RcvBuffer rwnd Data from IP Spare room TCP data in buffer Application process The receive window (rwnd) and the receive buffer (RcvBuffer) overflowing the receive buffer at Host B. Thus, Host A makes sure throughout the connections life that LastByteSent LastByteAcked rwnd There is one minor technical problem with this scheme. To see this, suppose Host Bs receive buffer becomes full so that rwnd = 0. After advertising rwnd = 0 to Host A, also suppose that B has nothing to send to A. Now consider what happens. As the application process at B empties the buffer, TCP does not send new segments with new rwnd values to Host A; indeed, TCP sends a segment to Host A only if it has data to send or if it has an acknowledgment to send. Therefore, Host A is never informed that some space has opened up in Host Bs receive buffer Host A is blocked and can transmit no more data! To solve this problem, the TCP specification requires Host A to continue to send segments with one data byte when Bs receive window is zero. These segments will be acknowledged by the receiver. Eventually the buffer will begin to empty and the acknowledgments will contain a nonzero rwnd value"
Explain why each of the routers needs to know about the existence of your companys /24 prefix (or some aggregate entry),"However, there still remains one other necessary and crucial step to allow outsiders from around the world to access your Web server. Consider what happens when Alice, who knows the IP address of your Web server, sends an IP datagram (e.g., a TCP SYN segment) to that IP address. This datagram will be routed through the Internet, visiting a series of routers in many different ASs, and eventually reach your Web server. When any one of the routers receives the datagram, it is going to look for an entry in its forwarding table to determine on which outgoing port it should forward the datagram. Hence, each of the routers needs to know about the existence of your companys /24 prefix (or some aggregate entry)."," So that people can discover the IP addresses of your Web server, in your DNS server you will need to include entries that map the host name of your Web server (e.g., www.xanadu.com) to its IP address. You will want to have similar entries for other publicly available servers in your company, including your mail server. In this manner, if Alice wants to browse your Web server, the DNS system will contact your DNS server, find the IP address of your Web server, and give it to Alice. Alice can then establish a TCP connection directly with your Web server. However, there still remains one other necessary and crucial step to allow outsiders from around the world to access your Web server. Consider what happens when Alice, who knows the IP address of your Web server, sends an IP datagram (e.g., a TCP SYN segment) to that IP address. This datagram will be routed through the Internet, visiting a series of routers in many different ASs, and eventually reach your Web server. When any one of the routers receives the datagram, it is going to look for an entry in its forwarding table to determine on which outgoing port it should forward the datagram. Therefore, each of the routers needs to know about the existence of your companys /24 prefix (or some aggregate entry). How does a router become aware of your companys prefix? As we have just seen, it becomes aware of it from BGP! Specifically, when your company contracts with a local ISP and gets assigned a prefix (i.e., an address range), your local ISP will use BGP to advertise your prefix to the ISPs to which it connects. Those ISPs will then, in turn, use BGP to propagate the advertisement. Eventually, all Internet routers will know about your prefix (or about some aggregate that includes your prefix) and thus be able to appropriately forward datagrams destined to your Web and mail servers. 5.5 The SDN Control Plane In this section, well dive into the SDN control planethe network-wide logic that controls packet forwarding among a networks SDN-enabled devices, as well as the configuration and management of these devices and their services. Our study here builds on our earlier discussion of generalized SDN forwarding in Section 4.4, so you might want to first review that section, as well as Section 5.1 of this chapter, before continuing on. As in Section 4.4, well again adopt the terminology used in the SDN literature and refer to the networks forwarding devices as packet switches (or just switches, with packet being understood), since forwarding decisions can be made on the basis of network-layer source/destination addresses, link-layer source/destination addresses, as well as many other values in transport-, network-, and link-layer packet-header fields"
Explain why a full client application buffer indirectly imposes a limit on the rate that video can be sent from server to client when streaming over HTTP," Thus, the server send rate can be no higher than the video consumption rate at the client. Hence, a full client application buffer indirectly imposes a limit on the rate that video can be sent from server to client when streaming over HTTP."," In fact, even during regular playback (that is, without pausing), if the client application buffer becomes full, back pressure will cause the TCP buffers to become full, which will force the server to reduce its rate. To determine the resulting rate, note that when the client application removes f bits, it creates room for f bits in the client application buffer, which in turn allows the server to send f additional bits. Thus, the server send rate can be no higher than the video consumption rate at the client. Therefore, a full client application buffer indirectly imposes a limit on the rate that video can be sent from server to client when streaming over HTTP. Analysis of Video Streaming Some simple modeling will provide more insight into initial playout delay and freezing due to application buffer depletion. As shown in Figure 9.3, let B denote the size B Q Depletion rate = r Internet Video server Client application buffer 9.2 STREAMING STORED VIDEO (in bits) of the clients application buffer, and let Q denote the number of bits that must be buffered before the client application begins playout. (Of course, Q 6 B.) Let r denote the video consumption ratethe rate at which the client draws bits out of the client application buffer during playback. So, for example, if the videos frame rate is 30 frames/sec, and each (compressed) frame is 100,000 bits, then r = 3 Mbps"
Explain why it is easy for an instructor to get students excited about basic principles when using the Internet as the guiding focus,"They know that the Internet has been a revolutionary and disruptive technology and can see that it is profoundly changing our world. Given the enormous relevance of the Internet, students are naturally curious about what is under the hood. Hence, it is easy for an instructor to get students excited about basic principles when using the Internet as the guiding focus."," Another benefit of spotlighting the Internet is that most computer science and electrical engineering students are eager to learn about the Internet and its protocols. They know that the Internet has been a revolutionary and disruptive technology and can see that it is profoundly changing our world. Given the enormous relevance of the Internet, students are naturally curious about what is under the hood. Thus, it is easy for an instructor to get students excited about basic principles when using the Internet as the guiding focus. Teaching Networking Principles Two of the unique features of the bookits top-down approach and its focus on the Internethave appeared in the titles of our book. If we could have squeezed a third phrase into the subtitle, it would have contained the word principles. The field of networking is now mature enough that a number of fundamentally important issues can be identified. For example, in the transport layer, the fundamental issues include reliable communication over an unreliable network layer, connection establishment/ teardown and handshaking, congestion and flow control, and multiplexing. Three fundamentally important network-layer issues are determining good paths between two routers, interconnecting a large number of heterogeneous networks, and managing the complexity of a modern network. In the link layer, a fundamental problem is sharing a multiple access channel. In network security, techniques for providing confidentiality, authentication, and message integrity are all based on cryptographic fundamentals"
"Explain how in many ways, packets are analogous to trucks, communication links are analogous to highways and roads, packet switches are analogous to intersections, and end systems are analogous to buildings","Packet-switched networks (which transport packets) are in many ways similar to transportation networks of highways, roads, and intersections (which transport vehicles). Consider, for example, a factory that needs to move a large amount of cargo to some destination warehouse located thousands of kilometers away. At the factory, the cargo is segmented and loaded into a fleet of trucks. Each of the trucks then independently travels through the network of highways, roads, and intersections to the destination warehouse. At the destination warehouse, the cargo is unloaded and grouped with the rest of the cargo arriving from the same shipment. Hence, in many ways, packets are analogous to trucks, communication links are analogous to highways and roads, packet switches are analogous to intersections, and end systems are analogous to buildings."," A packet switch takes a packet arriving on one of its incoming communication links and forwards that packet on one of its outgoing communication links. Packet switches come in many shapes and flavors, but the two most prominent types in todays Internet are routers and link-layer switches. Both types of switches forward packets toward their ultimate destinations. Link-layer switches are typically used in access networks, while routers are typically used in the network core. The sequence of communication links and packet switches traversed by a packet from the sending end system to the receiving end system is known as a route or path through the network. Cisco predicts annual global IP traffic will pass the zettabyte (1021 bytes) threshold by the end of 2016, and will reach 2 zettabytes per year by 2019 [Cisco VNI 2015]. Packet-switched networks (which transport packets) are in many ways similar to transportation networks of highways, roads, and intersections (which transport vehicles). Consider, for example, a factory that needs to move a large amount of cargo to some destination warehouse located thousands of kilometers away. At the factory, the cargo is segmented and loaded into a fleet of trucks. Each of the trucks then independently travels through the network of highways, roads, and intersections to the destination warehouse. At the destination warehouse, the cargo is unloaded and grouped with the rest of the cargo arriving from the same shipment. Thus, in many ways, packets are analogous to trucks, communication links are analogous to highways and roads, packet switches are analogous to intersections, and end systems are analogous to buildings. Just as a truck takes a path through the transportation network, a packet takes a path through a computer network. End systems access the Internet through Internet Service Providers (ISPs), including residential ISPs such as local cable or telephone companies; corporate ISPs; university ISPs; ISPs that provide WiFi access in airports, hotels, coffee shops, and other public places; and cellular data ISPs, providing mobile access to our smartphones and other devices. Each ISP is in itself a network of packet switches and communication links. ISPs provide a variety of types of network access to the end systems, including residential broadband access such as cable modem or DSL, high-speed local area network access, and mobile wireless access. ISPs also provide Internet access to content providers, connecting Web sites and video servers directly to the Internet. The Internet is all about connecting end systems to each other, so the ISPs that provide access to end systems must also be interconnected. These lowertier ISPs are interconnected through national and international upper-tier ISPs such as Level 3 Communications, AT&T, Sprint, and NTT. An upper-tier ISP consists of high-speed routers interconnected with high-speed fiber-optic links. Each ISP network, whether upper-tier or lower-tier, is managed independently, runs the IP protocol (see below), and conforms to certain naming and address conventions. Well examine ISPs and their interconnection more closely in Section 1.3"
"Explain why the postal service has its own postal service interface, or set of rules, that Alice must follow to have the postal service deliver her letter to Bob","Bob using the postal service. Alice, of course, cant just write the letter (the data) and drop the letter out her window. Instead, the postal service requires that Alice put the letter in an envelope; write Bobs full name, address, and zip code in the center of the envelope; seal the envelope; put a stamp in the upper-right-hand corner of the envelope; and finally, drop the envelope into an official postal service mailbox. Hence, the postal service has its own postal service interface, or set of rules, that Alice must follow to have the postal service deliver her letter to Bob."," But we can also describe the Internet from an entirely different anglenamely, as an infrastructure that provides services to applications. In addition to traditional applications such as e-mail and Web surfing, Internet applications include mobile smartphone and tablet applications, including Internet messaging, mapping with real-time road-traffic information, music streaming from the cloud, movie and television streaming, online social networks, video conferencing, multi-person games, and location-based recommendation systems. The applications are said to be distributed applications, since they involve multiple end systems that exchange data with each other. Importantly, Internet applications run on end systemsthey do not run in the packet switches in the network core. Although packet switches facilitate the exchange of data among end systems, they are not concerned with the application that is the source or sink of data. Lets explore a little more what we mean by an infrastructure that provides services to applications. To this end, suppose you have an exciting new idea for a distributed Internet application, one that may greatly benefit humanity or one that may simply make you rich and famous. How might you go about transforming this idea into an actual Internet application? Because applications run on end systems, you are going to need to write programs that run on the end systems. You might, for example, write your programs in Java, C, or Python. Now, because you are developing a distributed Internet application, the programs running on the different end systems will need to send data to each other. And here we get to a central issueone that leads to the alternative way of describing the Internet as a platform for applications. How does one program running on one end system instruct the Internet to deliver data to another program running on another end system? End systems attached to the Internet provide a socket interface that specifies how a program running on one end system asks the Internet infrastructure to deliver data to a specific destination program running on another end system. This Internet socket interface is a set of rules that the sending program must follow so that the Internet can deliver the data to the destination program. Well discuss the Internet socket interface in detail in Chapter 2. For now, lets draw upon a simple analogy, one that we will frequently use in this book. Suppose Alice wants to send a letter to Bob using the postal service. Alice, of course, cant just write the letter (the data) and drop the letter out her window. Instead, the postal service requires that Alice put the letter in an envelope; write Bobs full name, address, and zip code in the center of the envelope; seal the envelope; put a stamp in the upper-right-hand corner of the envelope; and finally, drop the envelope into an official postal service mailbox. Thus, the postal service has its own postal service interface, or set of rules, that Alice must follow to have the postal service deliver her letter to Bob. In a similar manner, the Internet has a socket interface that the program sending data must follow to have the Internet deliver the data to the program that will receive the data. The postal service, of course, provides more than one service to its customers. It provides express delivery, reception confirmation, ordinary use, and many more services. In a similar manner, the Internet provides multiple services to its applications"
"Explain why when DSL is used, a customers telco is also its ISP","Today, the two most prevalent types of broadband residential access are digital subscriber line (DSL) and cable. A residence typically obtains DSL Internet access from the same local telephone company (telco) that provides its wired local phone access. Hence, when DSL is used, a customers telco is also its ISP."," Home Access: DSL, Cable, FTTH, Dial-Up, and Satellite In developed countries as of 2014, more than 78 percent of the households have Internet access, with Korea, Netherlands, Finland, and Sweden leading the way with more than 80 percent of households having Internet access, almost all via a high-speed broadband connection [ITU 2015]. Given this widespread use of home access networks lets begin our overview of access networks by considering how homes connect to the Internet. Today, the two most prevalent types of broadband residential access are digital subscriber line (DSL) and cable. A residence typically obtains DSL Internet access from the same local telephone company (telco) that provides its wired local phone access. Thus, when DSL is used, a customers telco is also its ISP. As shown in office (CO). The homes DSL modem takes digital data and translates it to highfrequency tones for transmission over telephone wires to the CO; the analog signals from many such houses are translated back into digital format at the DSLAM. The residential telephone line carries both data and traditional telephone signals simultaneously, which are encoded at different frequencies: A high-speed downstream channel, in the 50 kHz to 1 MHz band A medium-speed upstream channel, in the 4 kHz to 50 kHz band An ordinary two-way telephone channel, in the 0 to 4 kHz band This approach makes the single DSL link appear as if there were three separate links, so that a telephone call and an Internet connection can share the DSL link at the same time"
"Explain why at time 2L/R, the destination has received the first packet and the router has received the second packet","Now lets calculate the amount of time that elapses from when the source begins to send the first packet until the destination has received all three packets. As before, at time L/R, the router begins to forward the first packet. But also at time L/R the source will begin to send the second packet, since it has just finished sending the entire first packet. Hence, at time 2L/R, the destination has received the first packet and the router has received the second packet."," But, as we will discuss in Section 1.4, routers need to receive, store, and process the entire packet before forwarding. Now lets calculate the amount of time that elapses from when the source begins to send the first packet until the destination has received all three packets. As before, at time L/R, the router begins to forward the first packet. But also at time L/R the source will begin to send the second packet, since it has just finished sending the entire first packet. Thus, at time 2L/R, the destination has received the first packet and the router has received the second packet. Similarly, at time 3L/R, the destination has received the first two packets and the router has received the third packet. Finally, at time 4L/R the destination has received all three packets! Lets now consider the general case of sending one packet from source to destination over a path consisting of N links each of rate R (thus, there are N-1 routers between source and destination). Applying the same logic as above, we see that the end-to-end delay is: dend@to@end = N L R You may now want to try to determine what the delay would be for P packets sent over a series of N links"
"Explain why in addition to the store-and-forward delays, packets suffer output buffer queuing delays","Each packet switch has multiple links attached to it. For each attached link, the packet switch has an output buffer (also called an output queue), which stores packets that the router is about to send into that link. The output buffers play a key role in packet switching. If an arriving packet needs to be transmitted onto a link but finds the link busy with the transmission of another packet, the arriving packet must wait in the output buffer. Hence, in addition to the store-and-forward delays, packets suffer output buffer queuing delays."," Finally, at time 4L/R the destination has received all three packets! Lets now consider the general case of sending one packet from source to destination over a path consisting of N links each of rate R (thus, there are N-1 routers between source and destination). Applying the same logic as above, we see that the end-to-end delay is: dend@to@end = N L R You may now want to try to determine what the delay would be for P packets sent over a series of N links. Queuing Delays and Packet Loss Each packet switch has multiple links attached to it. For each attached link, the packet switch has an output buffer (also called an output queue), which stores packets that the router is about to send into that link. The output buffers play a key role in packet switching. If an arriving packet needs to be transmitted onto a link but finds the link busy with the transmission of another packet, the arriving packet must wait in the output buffer. Thus, in addition to the store-and-forward delays, packets suffer output buffer queuing delays. These delays are variable and depend on the level of congestion in the network. Since the amount of buffer space is finite, an 100 Mbps Ethernet A 15 Mbps Queue of packets waiting for output link B D Packets arriving packet may find that the buffer is completely full with other packets waiting for transmission. In this case, packet loss will occureither the arriving packet or one of the already-queued packets will be dropped. packets are represented by three-dimensional slabs. The width of a slab represents the number of bits in the packet. In this figure, all packets have the same width and hence the same length. Suppose Hosts A and B are sending packets to Host E. Hosts A and B first send their packets along 100 Mbps Ethernet links to the first router"
"Explain why in order for Host A to communicate with Host B, the network must first reserve one circuit on each of two links"," When two hosts want to communicate, the network establishes a dedicated endto-end connection between the two hosts. Hence, in order for Host A to communicate with Host B, the network must first reserve one circuit on each of two links."," circuit switches are interconnected by four links. Each of these links has four circuits, so that each link can support four simultaneous connections. The hosts (for example, PCs and workstations) are each directly connected to one of the switches. When two hosts want to communicate, the network establishes a dedicated endto-end connection between the two hosts. Thus, in order for Host A to communicate with Host B, the network must first reserve one circuit on each of two links. In this example, the dedicated end-to-end connection uses the second circuit in the first link and the fourth circuit in the second link. Because each link has four circuits, for each link used by the end-to-end connection, the connection gets one fourth of the links total transmission capacity for the duration of the connection"
"Explain why when there are 10 or fewer active users, users packets flow through the link essentially  without delay, as is the case with circuit switching","When there are 10 or fewer simultaneously active users (which happens with probability 0.9996), the aggregate arrival rate of data is less than or equal to 1 Mbps, the output rate of the link. Hence, when there are 10 or fewer active users, users packets flow through the link essentially  without delay, as is the case with circuit switching."," Why is packet switching more efficient? Lets look at a simple example. Suppose users share a 1 Mbps link. Also suppose that each user alternates between periods of activity, when a user generates data at a constant rate of 100 kbps, and periods of inactivity, when a user generates no data. Suppose further that a user is active only 10 percent of the time (and is idly drinking coffee during the remaining 90 percent of the time). With circuit switching, 100 kbps must be reserved for each user at all times. For example, with circuit-switched TDM, if a one-second frame is divided into 10 time slots of 100 ms each, then each user would be allocated one time slot per frame. Thus, the circuit-switched link can support only 10 ( = 1 Mbps/100 kbps) simultaneous users. With packet switching, the probability that a specific user is active is 0.1 (that is, 10 percent). If there are 35 users, the probability that there are 11 or more simultaneously active users is approximately 0.0004. (Homework Problem P8 outlines how this probability is obtained.) When there are 10 or fewer simultaneously active users (which happens with probability 0.9996), the aggregate arrival rate of data is less than or equal to 1 Mbps, the output rate of the link. Thus, when there are 10 or fewer active users, users packets flow through the link essentially without delay, as is the case with circuit switching. When there are more than 10 simultaneously active users, then the aggregate arrival rate of packets exceeds the output capacity of the link, and the output queue will begin to grow. (It continues to grow until the aggregate input rate falls back below 1 Mbps, at which point the queue will begin to diminish in length.) Because the probability of having more than 10 simultaneously active users is minuscule in this example, packet switching provides essentially the same performance as circuit switching, but does so while allowing for more than three times the number of users. Lets now consider a second simple example. Suppose there are 10 users and that one user suddenly generates one thousand 1,000-bit packets, while other users remain quiescent and do not generate packets. Under TDM circuit switching with 10 slots per frame and each slot consisting of 1,000 bits, the active user can only use its one time slot per frame to transmit data, while the remaining nine time slots in each frame remain idle. It will be 10 seconds before all of the active users one million bits of data has been transmitted. In the case of packet switching, the active user can continuously send its packets at the full link rate of 1 Mbps, since there are no other users generating packets that need to be multiplexed with the active users packets"
Explain why there is customerprovider relationship at each level of the hierarchy,"Returning to this network of networks, not only are there multiple competing tier-1 ISPs, there may be multiple competing regional ISPs in a region. In such a hierarchy, each access ISP pays the regional ISP to which it connects, and each regional ISP pays the tier-1 ISP to which it connects. (An access ISP can also connect directly to a tier-1 ISP, in which case it pays the tier-1 ISP). Hence, there is customerprovider relationship at each level of the hierarchy."," Network Structure 2, just described, is a two-tier hierarchy with global transit providers residing at the top tier and access ISPs at the bottom tier. This assumes that global transit ISPs are not only capable of getting close to each and every access ISP, but also find it economically desirable to do so. In reality, although some ISPs do have impressive global coverage and do directly connect with many access ISPs, no ISP has presence in each and every city in the world. Instead, in any given region, there may be a regional ISP to which the access ISPs in the region connect. Each regional ISP then connects to tier-1 ISPs. Tier-1 ISPs are similar to our (imaginary) global transit ISP; but tier-1 ISPs, which actually do exist, do not have a presence in every city in the world. There are approximately a dozen tier-1 ISPs, including Level 3 Communications, AT&T, Sprint, and NTT. Interestingly, no group officially sanctions tier-1 status; as the saying goesif you have to ask if youre a member of a group, youre probably not. Returning to this network of networks, not only are there multiple competing tier-1 ISPs, there may be multiple competing regional ISPs in a region. In such a hierarchy, each access ISP pays the regional ISP to which it connects, and each regional ISP pays the tier-1 ISP to which it connects. (An access ISP can also connect directly to a tier-1 ISP, in which case it pays the tier-1 ISP). Thus, there is customerprovider relationship at each level of the hierarchy. Note that the tier-1 ISPs do not pay anyone as they are at the top of the hierarchy. To further complicate matters, in some regions, there may be a larger regional ISP (possibly spanning an entire country) to which the smaller regional ISPs in that region connect; the larger regional ISP then connects to a tier-1 ISP. For example, in China, there are access ISPs in each city, which connect to provincial ISPs, which in turn connect to national ISPs, which finally connect to tier-1 ISPs [Tian 2012]. We refer to this multi-tier hierarchy, which is still only a crude approximation of todays Internet, as Network Structure 3. To build a network that more closely resembles todays Internet, we must add points of presence (PoPs), multi-homing, peering, and Internet exchange points (IXPs) to the hierarchical Network Structure 3. PoPs exist in all levels of the hierarchy, except for the bottom (access ISP) level. A PoP is simply a group of one or more routers (at the same location) in the providers network where customer ISPs can connect into the provider ISP. For a customer network to connect to a providers PoP, it can lease a high-speed link from a third-party telecommunications provider to directly connect one of its routers to a router at the PoP. Any ISP (except for tier-1 ISPs) may choose to multi-home, that is, to connect to two or more provider ISPs. So, for example, an access ISP may multi-home with two regional ISPs, or it may multihome with two regional ISPs and also with a tier-1 ISP. Similarly, a regional ISP may multi-home with multiple tier-1 ISPs. When an ISP multi-homes, it can continue to send and receive packets into the Internet even if one of its providers has a failure"
"Explain why to support these applications, something has to be done to guarantee that the data sent by one end of the application is delivered correctly and completely to the other end of the application","As discussed in Chapter 1, packets can get lost within a computer network. For example, a packet can overflow a buffer in a router, or can be discarded by a host or router after having some of its bits corrupted. For many applicationssuch as electronic mail, file transfer, remote host access, Web document transfers, and financial applicationsdata loss can have devastating consequences (in the latter case, for either the bank or the customer!). Hence, to support these applications, something has to be done to guarantee that the data sent by one end of the application is delivered correctly and completely to the other end of the application."," You have to choose one or the other, and each transportation mode offers different services. (For example, the train offers downtown pickup and drop-off, whereas the plane offers shorter travel time.) What are the services that a transport-layer protocol can offer to applications invoking it? We can broadly classify the possible services along four dimensions: reliable data transfer, throughput, timing, and security. Reliable Data Transfer As discussed in Chapter 1, packets can get lost within a computer network. For example, a packet can overflow a buffer in a router, or can be discarded by a host or router after having some of its bits corrupted. For many applicationssuch as electronic mail, file transfer, remote host access, Web document transfers, and financial applicationsdata loss can have devastating consequences (in the latter case, for either the bank or the customer!). Thus, to support these applications, something has to be done to guarantee that the data sent by one end of the application is delivered correctly and completely to the other end of the application. If a protocol provides such a guaranteed data delivery service, it is said to provide reliable data transfer. One important service that a transport-layer protocol can potentially provide to an application is process-to-process reliable data transfer. When a transport protocol provides this service, the sending process can just pass its data into the socket and know with complete confidence that the data will arrive without errors at the receiving process"
"Explain how HTTP is only one piece (albeit, an important piece) of the Web application"," It is important to distinguish between network applications and application-layer protocols. An application-layer protocol is only one piece of a network application (albeit, a very important piece of the application from our point of view!). Lets look at a couple of examples. The Web is a client-server application that allows users to obtain documents from Web servers on demand. The Web application consists of many components, including a standard for document formats (that is, HTML), Web browsers (for example, Firefox and Microsoft Internet Explorer), Web servers (for example, Apache and Microsoft servers), and an application-layer protocol. The Webs application-layer protocol, HTTP, defines the format and sequence of messages exchanged between browser and Web server. Hence, HTTP is only one piece (albeit, an important piece) of the Web application."," Many other application-layer protocols are proprietary and intentionally not available in the public domain. For example, Skype uses proprietary application-layer protocols. It is important to distinguish between network applications and application-layer protocols. An application-layer protocol is only one piece of a network application (albeit, a very important piece of the application from our point of view!). Lets look at a couple of examples. The Web is a client-server application that allows users to obtain documents from Web servers on demand. The Web application consists of many components, including a standard for document formats (that is, HTML), Web browsers (for example, Firefox and Microsoft Internet Explorer), Web servers (for example, Apache and Microsoft servers), and an application-layer protocol. The Webs application-layer protocol, HTTP, defines the format and sequence of messages exchanged between browser and Web server. Thus, HTTP is only one piece (albeit, an important piece) of the Web application. As another example, an Internet e-mail application also has many components, including mail servers that house user mailboxes; mail clients (such as Microsoft Outlook) that allow users to read and create messages; a standard for defining the structure of an e-mail message; and application-layer protocols that define how messages are passed between servers, how messages are passed between servers and mail clients, and how the contents of message headers are to be interpreted. The principal application-layer protocol for electronic mail is SMTP (Simple Mail Transfer Protocol) [RFC 5321]. Thus, e-mails principal application-layer protocol, SMTP, is only one piece (albeit an important piece) of the e-mail application. 2.1.6 Network Applications Covered in This Book New public domain and proprietary Internet applications are being developed every day. Rather than covering a large number of Internet applications in an encyclopedic manner, we have chosen to focus on a small number of applications that are both pervasive and important. In this chapter we discuss five important applications: the Web, electronic mail, directory service video streaming, and P2P applications. We first discuss the Web, not only because it is an enormously popular application, but also because its application-layer protocol, HTTP, is straightforward and easy to understand. We then discuss electronic mail, the Internets first killer application. E-mail is more complex than the Web in the sense that it makes use of not one but several application-layer protocols. After e-mail, we cover DNS, which provides a directory service for the Internet. Most users do not interact with DNS directly; instead, users invoke DNS indirectly through other applications (including the Web, file transfer, and electronic mail)"
"Explain how e-mails principal application-layer protocol, SMTP, is only one piece (albeit an important piece) of the e-mail application","Webs application-layer protocol, HTTP, defines the format and sequence of messages exchanged between browser and Web server. Thus, HTTP is only one piece (albeit, an important piece) of the Web application. As another example, an Internet e-mail application also has many components, including mail servers that house user mailboxes; mail clients (such as Microsoft Outlook) that allow users to read and create messages; a standard for defining the structure of an e-mail message; and application-layer protocols that define how messages are passed between servers, how messages are passed between servers and mail clients, and how the contents of message headers are to be interpreted. The principal application-layer protocol for electronic mail is SMTP (Simple Mail Transfer Protocol) [RFC 5321]. Hence, e-mails principal application-layer protocol, SMTP, is only one piece (albeit an important piece) of the e-mail application."," Many other application-layer protocols are proprietary and intentionally not available in the public domain. For example, Skype uses proprietary application-layer protocols. It is important to distinguish between network applications and application-layer protocols. An application-layer protocol is only one piece of a network application (albeit, a very important piece of the application from our point of view!). Lets look at a couple of examples. The Web is a client-server application that allows users to obtain documents from Web servers on demand. The Web application consists of many components, including a standard for document formats (that is, HTML), Web browsers (for example, Firefox and Microsoft Internet Explorer), Web servers (for example, Apache and Microsoft servers), and an application-layer protocol. The Webs application-layer protocol, HTTP, defines the format and sequence of messages exchanged between browser and Web server. Thus, HTTP is only one piece (albeit, an important piece) of the Web application. As another example, an Internet e-mail application also has many components, including mail servers that house user mailboxes; mail clients (such as Microsoft Outlook) that allow users to read and create messages; a standard for defining the structure of an e-mail message; and application-layer protocols that define how messages are passed between servers, how messages are passed between servers and mail clients, and how the contents of message headers are to be interpreted. The principal application-layer protocol for electronic mail is SMTP (Simple Mail Transfer Protocol) [RFC 5321]. Thus, e-mails principal application-layer protocol, SMTP, is only one piece (albeit an important piece) of the e-mail application. 2.1.6 Network Applications Covered in This Book New public domain and proprietary Internet applications are being developed every day. Rather than covering a large number of Internet applications in an encyclopedic manner, we have chosen to focus on a small number of applications that are both pervasive and important. In this chapter we discuss five important applications: the Web, electronic mail, directory service video streaming, and P2P applications. We first discuss the Web, not only because it is an enormously popular application, but also because its application-layer protocol, HTTP, is straightforward and easy to understand. We then discuss electronic mail, the Internets first killer application. E-mail is more complex than the Web in the sense that it makes use of not one but several application-layer protocols. After e-mail, we cover DNS, which provides a directory service for the Internet. Most users do not interact with DNS directly; instead, users invoke DNS indirectly through other applications (including the Web, file transfer, and electronic mail)"
"Explain why in this example, when a user requests the Web page, 11 TCP connections are generated","The steps above illustrate the use of non-persistent connections, where each TCP connection is closed after the server sends the objectthe connection does not persist for other objects. Note that each TCP connection transports exactly one request message and one response message. Hence, in this example, when a user requests the Web page, 11 TCP connections are generated."," As the browser receives the Web page, it displays the page to the user. Two different browsers may interpret (that is, display to the user) a Web page in somewhat different ways. HTTP has nothing to do with how a Web page is interpreted by a client. The HTTP specifications ([RFC 1945] and [RFC 2616]) define only the communication protocol between the client HTTP program and the server HTTP program. The steps above illustrate the use of non-persistent connections, where each TCP connection is closed after the server sends the objectthe connection does not persist for other objects. Note that each TCP connection transports exactly one request message and one response message. Thus, in this example, when a user requests the Web page, 11 TCP connections are generated. In the steps described above, we were intentionally vague about whether the client obtains the 10 JPEGs over 10 serial TCP connections, or whether some of the JPEGs are obtained over parallel TCP connections. Indeed, users can configure modern browsers to control the degree of parallelism. In their default modes, most browsers open 5 to 10 parallel TCP connections, and each of these connections handles one requestresponse transaction. If the user prefers, the maximum number of parallel connections can be set to one, in which case the 10 connections are established serially. As well see in the next chapter, the use of parallel connections shortens the response time"
"Explain why roughly, the total response time is two RTTs plus the transmission time at the server of the HTML file","Back-of-the-envelope calculation for the time needed to request and receive an HTML file the server, the server sends the HTML file into the TCP connection. This HTTP request/response eats up another RTT. Hence, roughly, the total response time is two RTTs plus the transmission time at the server of the HTML file."," Before continuing, lets do a back-of-the-envelope calculation to estimate the amount of time that elapses from when a client requests the base HTML file until the entire file is received by the client. To this end, we define the round-trip time (RTT), which is the time it takes for a small packet to travel from client to server and then back to the client. The RTT includes packet-propagation delays, packetqueuing delays in intermediate routers and switches, and packet-processing delays. (These delays were discussed in Section 1.4.) Now consider what happens when a user clicks on a hyperlink. As shown in Figure 2.7, this causes the browser to initiate a TCP connection between the browser and the Web server; this involves a three-way handshakethe client sends a small TCP segment to the server, the server acknowledges and responds with a small TCP segment, and, finally, the client acknowledges back to the server. The first two parts of the three-way handshake take one RTT. After completing the first two parts of the handshake, the client sends the HTTP request message combined with the third part of the three-way handshake (the acknowledgment) into the TCP connection. Once the request message arrives at Initiate TCP connection Request file RTT Time to transmit file Time at client Time at server Back-of-the-envelope calculation for the time needed to request and receive an HTML file the server, the server sends the HTML file into the TCP connection. This HTTP request/response eats up another RTT. Thus, roughly, the total response time is two RTTs plus the transmission time at the server of the HTML file. HTTP with Persistent Connections Non-persistent connections have some shortcomings. First, a brand-new connection must be established and maintained for each requested object. For each of these connections, TCP buffers must be allocated and TCP variables must be kept in both the client and server. This can place a significant burden on the Web server, which may be serving requests from hundreds of different clients simultaneously. Second, as we just described, each object suffers a delivery delay of two RTTsone RTT to establish the TCP connection and one RTT to request and receive an object"
Explain why the two protocols have common characteristics,"Lets now briefly compare SMTP with HTTP. Both protocols are used to transfer files from one host to another: HTTP transfers files (also called objects) from a Web server to a Web client (typically a browser); SMTP transfers files (that is, e-mail messages) from one mail server to another mail server. When transferring the files, both persistent HTTP and SMTP use persistent connections. Hence, the two protocols have common characteristics."," After typing this line, you should immediately receive the 220 reply from the server. Then issue the SMTP commands HELO, MAIL FROM, RCPT TO, DATA, CRLF.CRLF, and QUIT at the appropriate times. It is also highly recommended that you do Programming Assignment 3 at the end of this chapter. In that assignment, youll build a simple user agent that implements the client side of SMTP. It will allow you to send an e-mail message to an arbitrary recipient via a local mail server. 2.3.2 Comparison with HTTP Lets now briefly compare SMTP with HTTP. Both protocols are used to transfer files from one host to another: HTTP transfers files (also called objects) from a Web server to a Web client (typically a browser); SMTP transfers files (that is, e-mail messages) from one mail server to another mail server. When transferring the files, both persistent HTTP and SMTP use persistent connections. Thus, the two protocols have common characteristics. However, there are important differences. First, HTTP is mainly a pull protocolsomeone loads information on a Web server and users use HTTP to pull the information from the server at their convenience. In particular, the TCP connection is initiated by the machine that wants to receive the file. On the other hand, SMTP is primarily a push protocolthe sending mail server pushes the file to the receiving mail server. In particular, the TCP connection is initiated by the machine that wants to send the file. A second difference, which we alluded to earlier, is that SMTP requires each message, including the body of each message, to be in 7-bit ASCII format. If the message contains characters that are not 7-bit ASCII (for example, French characters with accents) or contains binary data (such as an image file), then the message has to be encoded into 7-bit ASCII. HTTP data does not impose this restriction"
"Explain why from the perspective of the invoking application in the users host, DNS is a black box providing a simple, straightforward translation service","Suppose that some application (such as a Web browser or a mail reader) running in a users host needs to translate a hostname to an IP address. The application will invoke the client side of DNS, specifying the hostname that needs to be translated. (On many UNIX-based machines, gethostbyname() is the function call that an application calls in order to perform the translation.) DNS in the users host then takes over, sending a query message into the network. All DNS query and reply messages are sent within UDP datagrams to port 53. After a delay, ranging from milliseconds to seconds, DNS in the users host receives a DNS reply message that provides the desired mapping. This mapping is then passed to the invoking application. Hence, from the perspective of the invoking application in the users host, DNS is a black box providing a simple, straightforward translation service."," Suppose that some application (such as a Web browser or a mail reader) running in a users host needs to translate a hostname to an IP address. The application will invoke the client side of DNS, specifying the hostname that needs to be translated. (On many UNIX-based machines, gethostbyname() is the function call that an application calls in order to perform the translation.) DNS in the users host then takes over, sending a query message into the network. All DNS query and reply messages are sent within UDP datagrams to port 53. After a delay, ranging from milliseconds to seconds, DNS in the users host receives a DNS reply message that provides the desired mapping. This mapping is then passed to the invoking application. Thus, from the perspective of the invoking application in the users host, DNS is a black box providing a simple, straightforward translation service. But in fact, the black box that implements the service is complex, consisting of a large number of DNS servers distributed around the globe, as well as an application-layer protocol that specifies how the DNS servers and querying hosts communicate. A simple design for DNS would have one DNS server that contains all the mappings. In this centralized design, clients simply direct all queries to the single DNS server, and the DNS server responds directly to the querying clients. Although the simplicity of this design is attractive, it is inappropriate for todays Internet, with its vast (and growing) number of hosts. The problems with a centralized design include: A single point of failure. If the DNS server crashes, so does the entire Internet! Traffic volume. A single DNS server would have to handle all DNS queries (for all the HTTP requests and e-mail messages generated from hundreds of millions of hosts)"
Explain how a Type A record provides the standard hostname-to-IP address mapping," A resource record is a four-tuple that contains the following fields: (Name, Value, Type, TTL) TTL is the time to live of the resource record; it determines when a resource should be removed from a cache. In the example records given below, we ignore the TTL field. The meaning of Name and Value depend on Type: If Type=A, then Name is a hostname and Value is the IP address for the hostname. Hence, a Type A record provides the standard hostname-to-IP address mapping."," 2.4.3 DNS Records and Messages The DNS servers that together implement the DNS distributed database store resource records (RRs), including RRs that provide hostname-to-IP address mappings. Each DNS reply message carries one or more resource records. In this and the following subsection, we provide a brief overview of DNS resource records and messages; more details can be found in [Albitz 1993] or in the DNS RFCs [RFC 1034; RFC 1035]. A resource record is a four-tuple that contains the following fields: (Name, Value, Type, TTL) TTL is the time to live of the resource record; it determines when a resource should be removed from a cache. In the example records given below, we ignore the TTL field. The meaning of Name and Value depend on Type: If Type=A, then Name is a hostname and Value is the IP address for the hostname. Thus, a Type A record provides the standard hostname-to-IP address mapping. As an example, (relay1.bar.foo.com, 145.37.93.126, A) is a Type A record. If Type=NS, then Name is a domain (such as foo.com) and Value is the hostname of an authoritative DNS server that knows how to obtain the IP addresses for hosts in the domain. This record is used to route DNS queries further along in the query chain. As an example, (foo.com, dns.foo.com, NS) is a Type NS record"
Explain why the minimum distribution time is at least F/us,"At the beginning of the distribution, only the server has the file. To get this file into the community of peers, the server must send each bit of the file at least once into its access link. Hence, the minimum distribution time is at least F/us."," us dmin This provides a lower bound on the minimum distribution time for the client-server architecture. In the homework problems you will be asked to show that the server can schedule its transmissions so that the lower bound is actually achieved. So lets take this lower bound provided above as the actual distribution time, that is, Dcs = maxb NF F , r us dmin We see from Equation 2.1 that for N large enough, the client-server distribution time is given by NF/us. Thus, the distribution time increases linearly with the number of peers N. So, for example, if the number of peers from one week to the next increases a thousand-fold from a thousand to a million, the time required to distribute the file to all peers increases by 1,000. Lets now go through a similar analysis for the P2P architecture, where each peer can assist the server in distributing the file. In particular, when a peer receives some file data, it can use its own upload capacity to redistribute the data to other peers. Calculating the distribution time for the P2P architecture is somewhat more complicated than for the client-server architecture, since the distribution time depends on how each peer distributes portions of the file to the other peers. Nevertheless, a simple expression for the minimal distribution time can be obtained [Kumar 2006]. To this end, we first make the following observations: At the beginning of the distribution, only the server has the file. To get this file into the community of peers, the server must send each bit of the file at least once into its access link. Thus, the minimum distribution time is at least F/us. (Unlike the client-server scheme, a bit sent once by the server may not have to be sent by the server again, as the peers may redistribute the bit among themselves.) As with the client-server architecture, the peer with the lowest download rate cannot obtain all F bits of the file in less than F/dmin seconds. Thus the minimum distribution time is at least F/dmin. Finally, observe that the total upload capacity of the system as a whole is equal to the upload rate of the server plus the upload rates of each of the individual peers, that is, utotal = us + u1 + g + uN. The system must deliver (upload) F bits to each of the N peers, thus delivering a total of NF bits. This cannot be done at a rate faster than utotal. Thus, the minimum distribution time is also at least NF/(us + u1 + g + uN)"
Explain why the minimum distribution time is also at least NF/(us + u1 + g + uN),"Finally, observe that the total upload capacity of the system as a whole is equal to the upload rate of the server plus the upload rates of each of the individual peers, that is, utotal = us + u1 + g + uN. The system must deliver (upload) F bits to each of the N peers, thus delivering a total of NF bits. This cannot be done at a rate faster than utotal. Hence, the minimum distribution time is also at least NF/(us + u1 + g + uN)."," Lets now go through a similar analysis for the P2P architecture, where each peer can assist the server in distributing the file. In particular, when a peer receives some file data, it can use its own upload capacity to redistribute the data to other peers. Calculating the distribution time for the P2P architecture is somewhat more complicated than for the client-server architecture, since the distribution time depends on how each peer distributes portions of the file to the other peers. Nevertheless, a simple expression for the minimal distribution time can be obtained [Kumar 2006]. To this end, we first make the following observations: At the beginning of the distribution, only the server has the file. To get this file into the community of peers, the server must send each bit of the file at least once into its access link. Thus, the minimum distribution time is at least F/us. (Unlike the client-server scheme, a bit sent once by the server may not have to be sent by the server again, as the peers may redistribute the bit among themselves.) As with the client-server architecture, the peer with the lowest download rate cannot obtain all F bits of the file in less than F/dmin seconds. Thus the minimum distribution time is at least F/dmin. Finally, observe that the total upload capacity of the system as a whole is equal to the upload rate of the server plus the upload rates of each of the individual peers, that is, utotal = us + u1 + g + uN. The system must deliver (upload) F bits to each of the N peers, thus delivering a total of NF bits. This cannot be done at a rate faster than utotal. Thus, the minimum distribution time is also at least NF/(us + u1 + g + uN). Putting these three observations together, we obtain the minimum distribution time for P2P, denoted by DP2P"
Explain how the video streaming application is displaying video as it is receiving and buffering frames corresponding to latter parts of the video,"The server then sends the video file, within an HTTP response message, as quickly as the underlying network protocols and traffic conditions will allow. On the client side, the bytes are collected in a client application buffer. Once the number of bytes in this buffer exceeds a predetermined threshold, the client application begins playbackspecifically, the streaming video application periodically grabs video frames from the client application buffer, decompresses the frames, and displays them on the users screen. Hence, the video streaming application is displaying video as it is receiving and buffering frames corresponding to latter parts of the video."," 2.6.2 HTTP Streaming and DASH In HTTP streaming, the video is simply stored at an HTTP server as an ordinary file with a specific URL. When a user wants to see the video, the client establishes a TCP connection with the server and issues an HTTP GET request for that URL. The server then sends the video file, within an HTTP response message, as quickly as the underlying network protocols and traffic conditions will allow. On the client side, the bytes are collected in a client application buffer. Once the number of bytes in this buffer exceeds a predetermined threshold, the client application begins playbackspecifically, the streaming video application periodically grabs video frames from the client application buffer, decompresses the frames, and displays them on the users screen. Thus, the video streaming application is displaying video as it is receiving and buffering frames corresponding to latter parts of the video. Although HTTP streaming, as described in the previous paragraph, has been extensively deployed in practice (for example, by YouTube since its inception), it has a major shortcoming: All clients receive the same encoding of the video, despite the large variations in the amount of bandwidth available to a client, both across different clients and also over time for the same client. This has led to the development of a new type of HTTP-based streaming, often referred to as Dynamic Adaptive Streaming over HTTP (DASH). In DASH, the video is encoded into several different versions, with each version having a different bit rate and, correspondingly, a different quality level. The client dynamically requests chunks of video segments of a few seconds in length. When the amount of available bandwidth is high, the client naturally selects chunks from a high-rate version; and when the available bandwidth is low, it naturally selects from a low-rate version. The client selects different chunks one at a time with HTTP GET request messages [Akhshabi 2011]"
Explain how each household sends 144 letters to the other household every week,"Consider two houses, one on the East Coast and the other on the West Coast, with each house being home to a dozen kids. The kids in the East Coast household are cousins of the kids in the West Coast household. The kids in the two households love to write to each othereach kid writes each cousin every week, with each letter delivered by the traditional postal service in a separate envelope. Hence, each household sends 144 letters to the other household every week."," 3.1.1 Relationship Between Transport and Network Layers Recall that the transport layer lies just above the network layer in the protocol stack. Whereas a transport-layer protocol provides logical communication between National or Global ISP Mobile Network Network Data link Physical Network Data link Physical Local or Regional ISP Application Transport Data link -to Data link Physical Home Network Log ica l en d Network -en Network Data link Physical spo Application Transport Enterprise Network Network Data link Physical The transport layer provides logical rather than physical communication between application processes CHAPTER 3 TRANSPORT LAYER processes running on different hosts, a network-layer protocol provides logicalcommunication between hosts. This distinction is subtle but important. Lets examine this distinction with the aid of a household analogy. Consider two houses, one on the East Coast and the other on the West Coast, with each house being home to a dozen kids. The kids in the East Coast household are cousins of the kids in the West Coast household. The kids in the two households love to write to each othereach kid writes each cousin every week, with each letter delivered by the traditional postal service in a separate envelope. Thus, each household sends 144 letters to the other household every week. (These kids would save a lot of money if they had e-mail!) In each of the households there is one kidAnn in the West Coast house and Bill in the East Coast houseresponsible for mail collection and mail distribution. Each week Ann visits all her brothers and sisters, collects the mail, and gives the mail to a postal-service mail carrier, who makes daily visits to the house. When letters arrive at the West Coast house, Ann also has the job of distributing the mail to her brothers and sisters. Bill has a similar job on the East Coast. In this example, the postal service provides logical communication between the two housesthe postal service moves mail from house to house, not from person to person. On the other hand, Ann and Bill provide logical communication among the cousinsAnn and Bill pick up mail from, and deliver mail to, their brothers and sisters. Note that from the cousins perspective, Ann and Bill are the mail service, even though Ann and Bill are only a part (the end-system part) of the end-to-end delivery process. This household example serves as a nice analogy for explaining how the transport layer relates to the network layer: application messages = letters in envelopes processes = cousins hosts (also called end systems) = houses transport-layer protocol = Ann and Bill network-layer protocol = postal service (including mail carriers) Continuing with this analogy, note that Ann and Bill do all their work within their respective homes; they are not involved, for example, in sorting mail in any intermediate mail center or in moving mail from one mail center to another"
"Explain why the cousin-pair Susan and Harvey do not provide the same set of services (that is, the same service model) as Ann and Bill","Continuing with our family saga, suppose now that when Ann and Bill go on vacation, another cousin pairsay, Susan and Harveysubstitute for them and provide the household-internal collection and delivery of mail. Unfortunately for the two families, Susan and Harvey do not do the collection and delivery in exactly the same way as Ann and Bill. Being younger kids, Susan and Harvey pick up and drop off the mail less frequently and occasionally lose letters (which are sometimes chewed up by the family dog). Hence, the cousin-pair Susan and Harvey do not provide the same set of services (that is, the same service model) as Ann and Bill."," Similarly, transport-layer protocols live in the end systems. Within an end system, a transport protocol moves messages from application processes to the network edge (that is, the network layer) and vice versa, but it doesnt have any say about how the messages are moved within the network core. In fact, as illustrated in Figure 3.1, intermediate routers neither act on, nor recognize, any information that the transport layer may have added to the application messages. Continuing with our family saga, suppose now that when Ann and Bill go on vacation, another cousin pairsay, Susan and Harveysubstitute for them and provide the household-internal collection and delivery of mail. Unfortunately for the two families, Susan and Harvey do not do the collection and delivery in exactly the same way as Ann and Bill. Being younger kids, Susan and Harvey pick up and drop off the mail less frequently and occasionally lose letters (which are sometimes chewed up by the family dog). Thus, the cousin-pair Susan and Harvey do not provide the same set of services (that is, the same service model) as Ann and Bill. In an analogous manner, a computer network may make available multiple transport protocols, with each protocol offering a different service model to applications. The possible services that Ann and Bill can provide are clearly constrained by the possible services that the postal service provides. For example, if the postal service doesnt provide a maximum bound on how long it can take to deliver mail between the two houses (for example, three days), then there is no way that Ann and Bill can guarantee a maximum delay for mail delivery between any of the cousin pairs. In a similar manner, the services that a transport protocol can provide are often constrained by the service model of the underlying network-layer protocol. If the network-layer protocol cannot provide delay or bandwidth guarantees for transportlayer segments sent between hosts, then the transport-layer protocol cannot provide delay or bandwidth guarantees for application messages sent between processes"
"Explain why if we determine that a port is open on a host, we may be able to map that port to a specific application running on the host","Weve seen that a server process waits patiently on an open port for contact by a remote client. Some ports are reserved for well-known applications (e.g., Web, FTP, DNS, and SMTP servers); other ports are used by convention by popular applications (e.g., the Microsoft 2000 SQL server listens for requests on UDP port 1434). Hence, if we determine that a port is open on a host, we may be able to map that port to a specific application running on the host."," The server host may support many simultaneous TCP connection sockets, with each socket attached to a process, and with each socket identified by its own fourtuple. When a TCP segment arrives at the host, all four fields (source IP address, source port, destination IP address, destination port) are used to direct (demultiplex) the segment to the appropriate socket. FOCUS ON SECURITY PORT SCANNING Weve seen that a server process waits patiently on an open port for contact by a remote client. Some ports are reserved for well-known applications (e.g., Web, FTP, DNS, and SMTP servers); other ports are used by convention by popular applications (e.g., the Microsoft 2000 SQL server listens for requests on UDP port 1434). Thus, if we determine that a port is open on a host, we may be able to map that port to a specific application running on the host. This is very useful for system administrators, who are often interested in knowing which network applications are running on the hosts in their networks. But attackers, in order to case the joint, also want to know which ports are open on target hosts. If a host is found to be running an application with a known security flaw (e.g., a SQL server listening on port 1434 was subject to a buffer overflow, allowing a remote user to execute arbitrary code on the vulnerable host, a flaw exploited by the Slammer worm [CERT 200304]), then that host is ripe for attack. Determining which applications are listening on which ports is a relatively easy task. Indeed there are a number of public domain programs, called port scanners, that do just that. Perhaps the most widely used of these is nmap, freely available at http://nmap.org and included in most Linux distributions. For TCP, nmap sequentially scans ports, looking for ports that are accepting TCP connections. For UDP, nmap again sequentially scans ports, looking for UDP ports that respond to transmitted UDP segments. In both cases, nmap returns a list of open, closed, or unreachable ports"
"Explain why the lack of congestion control in UDP can result in high loss rates between a UDP sender and receiver, and the crowding out of TCP sessionsa potentially serious problem [Floyd 1999]","Although commonly done today, running multimedia applications over UDP is controversial. As we mentioned above, UDP has no congestion control. But congestion control is needed to prevent the network from entering a congested state in which very little useful work is done. If everyone were to start streaming high-bit-rate video without using any congestion control, there would be so much packet overflow at routers that very few UDP packets would successfully traverse the source-to-destination path. Moreover, the high loss rates induced by the uncontrolled UDP senders would cause the TCP senders (which, as well see, do decrease their sending rates in the face of congestion) to dramatically decrease their rates. Hence, the lack of congestion control in UDP can result in high loss rates between a UDP sender and receiver, and the crowding out of TCP sessionsa potentially serious problem [Floyd 1999]."," We just mention now that all of these applications can tolerate a small amount of packet loss, so that reliable data transfer is not absolutely critical for the applications success. Furthermore, real-time applications, like Internet phone and video conferencing, react very poorly to TCPs congestion control. For these reasons, developers of multimedia applications may choose to run their applications over UDP instead of TCP. When packet loss rates are low, and with some organizations blocking UDP traffic for security reasons (see Chapter 8), TCP becomes an increasingly attractive protocol for streaming media transport. Application Application-Layer Protocol Underlying Transport Protocol SMTP Remote terminal access TCP HTTP File transfer TCP NFS Streaming multimedia UDP or TCP typically proprietary Network management Typically UDP DNS Popular Internet applications and their underlying transport protocols Although commonly done today, running multimedia applications over UDP is controversial. As we mentioned above, UDP has no congestion control. But congestion control is needed to prevent the network from entering a congested state in which very little useful work is done. If everyone were to start streaming high-bit-rate video without using any congestion control, there would be so much packet overflow at routers that very few UDP packets would successfully traverse the source-to-destination path. Moreover, the high loss rates induced by the uncontrolled UDP senders would cause the TCP senders (which, as well see, do decrease their sending rates in the face of congestion) to dramatically decrease their rates. Thus, the lack of congestion control in UDP can result in high loss rates between a UDP sender and receiver, and the crowding out of TCP sessionsa potentially serious problem [Floyd 1999]. Many researchers have proposed new mechanisms to force all sources, including UDP sources, to perform adaptive congestion control [Mahdavi 1997; Floyd 2000; Kohler 2006: RFC 4340]"
Explain why there is no need for the receiver to ask the sender to slow down!,"Also, all packet flow is from the sender to receiver; with a perfectly reliable channel there is no need for the receiver side to provide any feedback to the sender since nothing can go wrong! Note that we have also assumed that the receiver is able to receive data as fast as the sender happens to send data. Hence, there is no need for the receiver to ask the sender to slow down!"," In this simple protocol, there is no difference between a unit of data and a packet. Also, all packet flow is from the sender to receiver; with a perfectly reliable channel there is no need for the receiver side to provide any feedback to the sender since nothing can go wrong! Note that we have also assumed that the receiver is able to receive data as fast as the sender happens to send data. Thus, there is no need for the receiver to ask the sender to slow down! Reliable Data Transfer over a Channel with Bit Errors: rdt2.0 A more realistic model of the underlying channel is one in which bits in a packet may be corrupted. Such bit errors typically occur in the physical components of a network as a packet is transmitted, propagates, or is buffered. Well continue to assume for the moment that all transmitted packets are received (although their bits may be corrupted) in the order in which they were sent. Before developing a protocol for reliably communicating over such a channel, first consider how people might deal with such a situation. Consider how you yourself might dictate a long message over the phone. In a typical scenario, the message taker might say OK after each sentence has been heard, understood, and recorded. If the message taker hears a garbled sentence, youre asked to repeat the garbled sentence"
Explain why the use of cumulative acknowledgments is a natural choice for GBN,"The receivers actions in GBN are also simple. If a packet with sequence number n is received correctly and is in order (that is, the data last delivered to the upper layer came from a packet with sequence number n - 1), the receiver sends an ACK for packet n and delivers the data portion of the packet to the upper layer. In all other cases, the receiver discards the packet and resends an ACK for the most recently received in-order packet. Note that since packets are delivered one at a time to the upper layer, if packet k has been received and delivered, then all packets with a sequence number lower than k have also been delivered. Hence, the use of cumulative acknowledgments is a natural choice for GBN."," A timeout event. The protocols name, Go-Back-N, is derived from the senders behavior in the presence of lost or overly delayed packets. As in the stop-and-wait protocol, a timer will again be used to recover from lost data or acknowledgment packets. If a timeout occurs, the sender resends all packets that have been previously sent but that have not yet been acknowledged. Our sender in Figure 3.20 uses only a single timer, which can be thought of as a timer for the oldest transmitted but not yet acknowledged packet. If an ACK is received but there are still additional transmitted but not yet acknowledged packets, the timer is restarted. If there are no outstanding, unacknowledged packets, the timer is stopped. The receivers actions in GBN are also simple. If a packet with sequence number n is received correctly and is in order (that is, the data last delivered to the upper layer came from a packet with sequence number n - 1), the receiver sends an ACK for packet n and delivers the data portion of the packet to the upper layer. In all other cases, the receiver discards the packet and resends an ACK for the most recently received in-order packet. Note that since packets are delivered one at a time to the upper layer, if packet k has been received and delivered, then all packets with a sequence number lower than k have also been delivered. Thus, the use of cumulative acknowledgments is a natural choice for GBN. In our GBN protocol, the receiver discards out-of-order packets. Although it may seem silly and wasteful to discard a correctly received (but out-of-order) packet, there is some justification for doing so. Recall that the receiver must deliver data in order to the upper layer. Suppose now that packet n is expected, but packet n + 1 arrives. Because data must be delivered in order, the receiver could buffer (save) packet n + 1 and then deliver this packet to the upper layer after it had later received and delivered packet n. However, if packet n is lost, both it and packet n + 1 will eventually be retransmitted as a result of the GBN retransmission rule at the sender. Thus, the receiver can simply discard packet n + 1. The advantage of this approach is the simplicity of receiver bufferingthe receiver need not buffer any out-of-order packets. Thus, while the sender must maintain the upper and lower bounds of its window and the position of nextseqnum within this window, the only piece of information the receiver need maintain is the sequence number of the next in-order packet. This value is held in the variable expectedseqnum, shown in the receiver FSM in Figure 3.21. Of course, the disadvantage of throwing away a correctly received packet is that the subsequent retransmission of that packet might be lost or garbled and thus even more retransmissions would be required"
Explain why the receiver can simply discard packet n + 1,"In our GBN protocol, the receiver discards out-of-order packets. Although it may seem silly and wasteful to discard a correctly received (but out-of-order) packet, there is some justification for doing so. Recall that the receiver must deliver data in order to the upper layer. Suppose now that packet n is expected, but packet n + 1 arrives. Because data must be delivered in order, the receiver could buffer (save) packet n + 1 and then deliver this packet to the upper layer after it had later received and delivered packet n. However, if packet n is lost, both it and packet n + 1 will eventually be retransmitted as a result of the GBN retransmission rule at the sender. Hence, the receiver can simply discard packet n + 1."," The receivers actions in GBN are also simple. If a packet with sequence number n is received correctly and is in order (that is, the data last delivered to the upper layer came from a packet with sequence number n - 1), the receiver sends an ACK for packet n and delivers the data portion of the packet to the upper layer. In all other cases, the receiver discards the packet and resends an ACK for the most recently received in-order packet. Note that since packets are delivered one at a time to the upper layer, if packet k has been received and delivered, then all packets with a sequence number lower than k have also been delivered. Thus, the use of cumulative acknowledgments is a natural choice for GBN. In our GBN protocol, the receiver discards out-of-order packets. Although it may seem silly and wasteful to discard a correctly received (but out-of-order) packet, there is some justification for doing so. Recall that the receiver must deliver data in order to the upper layer. Suppose now that packet n is expected, but packet n + 1 arrives. Because data must be delivered in order, the receiver could buffer (save) packet n + 1 and then deliver this packet to the upper layer after it had later received and delivered packet n. However, if packet n is lost, both it and packet n + 1 will eventually be retransmitted as a result of the GBN retransmission rule at the sender. Thus, the receiver can simply discard packet n + 1. The advantage of this approach is the simplicity of receiver bufferingthe receiver need not buffer any out-of-order packets. Thus, while the sender must maintain the upper and lower bounds of its window and the position of nextseqnum within this window, the only piece of information the receiver need maintain is the sequence number of the next in-order packet. This value is held in the variable expectedseqnum, shown in the receiver FSM in Figure 3.21. Of course, the disadvantage of throwing away a correctly received packet is that the subsequent retransmission of that packet might be lost or garbled and thus even more retransmissions would be required. size of four packets. Because of this window size limitation, the sender sends packets 0 through 3 but then must wait for one or more of these packets to be acknowledged before proceeding. As each successive ACK (for example, ACK0 and ACK1) is received, the window slides forward and the sender can transmit one new packet (pkt4 and pkt5, respectively). On the receiver side, packet 2 is lost and thus packets 3, 4, and 5 are found to be out of order and are discarded"
"Explain why while the sender must maintain the upper and lower bounds of its window and the position of nextseqnum within this window, the only piece of information the receiver need maintain is the sequence number of the next in-order packet","In our GBN protocol, the receiver discards out-of-order packets. Although it may seem silly and wasteful to discard a correctly received (but out-of-order) packet, there is some justification for doing so. Recall that the receiver must deliver data in order to the upper layer. Suppose now that packet n is expected, but packet n + 1 arrives. Because data must be delivered in order, the receiver could buffer (save) packet n + 1 and then deliver this packet to the upper layer after it had later received and delivered packet n. However, if packet n is lost, both it and packet n + 1 will eventually be retransmitted as a result of the GBN retransmission rule at the sender. Thus, the receiver can simply discard packet n + 1. The advantage of this approach is the simplicity of receiver bufferingthe receiver need not buffer any out-of-order packets. Hence, while the sender must maintain the upper and lower bounds of its window and the position of nextseqnum within this window, the only piece of information the receiver need maintain is the sequence number of the next in-order packet."," The receivers actions in GBN are also simple. If a packet with sequence number n is received correctly and is in order (that is, the data last delivered to the upper layer came from a packet with sequence number n - 1), the receiver sends an ACK for packet n and delivers the data portion of the packet to the upper layer. In all other cases, the receiver discards the packet and resends an ACK for the most recently received in-order packet. Note that since packets are delivered one at a time to the upper layer, if packet k has been received and delivered, then all packets with a sequence number lower than k have also been delivered. Thus, the use of cumulative acknowledgments is a natural choice for GBN. In our GBN protocol, the receiver discards out-of-order packets. Although it may seem silly and wasteful to discard a correctly received (but out-of-order) packet, there is some justification for doing so. Recall that the receiver must deliver data in order to the upper layer. Suppose now that packet n is expected, but packet n + 1 arrives. Because data must be delivered in order, the receiver could buffer (save) packet n + 1 and then deliver this packet to the upper layer after it had later received and delivered packet n. However, if packet n is lost, both it and packet n + 1 will eventually be retransmitted as a result of the GBN retransmission rule at the sender. Thus, the receiver can simply discard packet n + 1. The advantage of this approach is the simplicity of receiver bufferingthe receiver need not buffer any out-of-order packets. Thus, while the sender must maintain the upper and lower bounds of its window and the position of nextseqnum within this window, the only piece of information the receiver need maintain is the sequence number of the next in-order packet. This value is held in the variable expectedseqnum, shown in the receiver FSM in Figure 3.21. Of course, the disadvantage of throwing away a correctly received packet is that the subsequent retransmission of that packet might be lost or garbled and thus even more retransmissions would be required. size of four packets. Because of this window size limitation, the sender sends packets 0 through 3 but then must wait for one or more of these packets to be acknowledged before proceeding. As each successive ACK (for example, ACK0 and ACK1) is received, the window slides forward and the sender can transmit one new packet (pkt4 and pkt5, respectively). On the receiver side, packet 2 is lost and thus packets 3, 4, and 5 are found to be out of order and are discarded"
"Explain why the recommended TCP timer management   procedures [RFC 6298] use only a single retransmission timer, even if there are multiple transmitted but not yet acknowledged segments","In our earlier development of reliable data transfer techniques, it was conceptually easiest to assume that an individual timer is associated with each transmitted but not yet acknowledged segment. While this is great in theory, timer management can require considerable overhead. Hence, the recommended TCP timer management   procedures [RFC 6298] use only a single retransmission timer, even if there are multiple transmitted but not yet acknowledged segments."," TCP creates a reliable data transfer service on top of IPs unreliable besteffort service. TCPs reliable data transfer service ensures that the data stream that a process reads out of its TCP receive buffer is uncorrupted, without gaps, without duplication, and in sequence; that is, the byte stream is exactly the same byte stream that was sent by the end system on the other side of the connection. How TCP provides a reliable data transfer involves many of the principles that we studied in Section 3.4. In our earlier development of reliable data transfer techniques, it was conceptually easiest to assume that an individual timer is associated with each transmitted but not yet acknowledged segment. While this is great in theory, timer management can require considerable overhead. Thus, the recommended TCP timer management procedures [RFC 6298] use only a single retransmission timer, even if there are multiple transmitted but not yet acknowledged segments. The TCP protocol described in this section follows this single-timer recommendation. We will discuss how TCP provides reliable data transfer in two incremental steps. We first present a highly simplified description of a TCP sender that uses only timeouts to recover from lost segments; we then present a more complete description that uses duplicate acknowledgments in addition to timeouts. In the ensuing discussion, we suppose that data is being sent in only one direction, from Host A to Host B, and that Host A is sending a large file"
Explain why TCP in Host B will discard the bytes in the retransmitted segment,"Suppose that this segment has sequence number 92 and contains 8 bytes of data. After sending this segment, Host A waits for a segment from B with acknowledgment number 100. Although the segment from A is received at B, the acknowledgment from B to A gets lost. In this case, the timeout event occurs, and Host A retransmits the same segment. Of course, when Host B receives the retransmission, it observes from the sequence number that the segment contains data that has already been received. Hence, TCP in Host B will discard the bytes in the retransmitted segment."," A Few Interesting Scenarios We have just described a highly simplified version of how TCP provides reliable data transfer. But even this highly simplified version has many subtleties. To get a good feeling for how this protocol works, lets now walk through a few simple scenarios. Suppose that this segment has sequence number 92 and contains 8 bytes of data. After sending this segment, Host A waits for a segment from B with acknowledgment number 100. Although the segment from A is received at B, the acknowledgment from B to A gets lost. In this case, the timeout event occurs, and Host A retransmits the same segment. Of course, when Host B receives the retransmission, it observes from the sequence number that the segment contains data that has already been received. Thus, TCP in Host B will discard the bytes in the retransmitted segment. In a second scenario, shown in Figure 3.35, Host A sends two segments back to back. The first segment has sequence number 92 and 8 bytes of data, and the second segment has sequence number 100 and 20 bytes of data. Suppose that both segments arrive intact at B, and B sends two separate acknowledgments for each of these segments. The first of these acknowledgments has acknowledgment number 100; the second has acknowledgment number 120. Suppose now that neither of the acknowledgments arrives at Host A before the timeout. When the timeout event occurs, Host Host A Seq=9 2, 8 bytes data ACK=1 X (loss) 2, 8 bytes data Time Time A resends the first segment with sequence number 92 and restarts the timer. As long as the ACK for the second segment arrives before the new timeout, the second segment will not be retransmitted"
Explain why TCPs error-recovery mechanism is probably best categorized as a hybrid of GBN and SR protocols," A proposed modification to TCP, the so-called selective acknowledgment [RFC 2018], allows a TCP receiver to acknowledge out-of-order segments selectively rather than just cumulatively acknowledging the last correctly received, in-order segment. When combined with selective retransmissionskipping the retransmission of segments that have already been selectively acknowledged by the receiver TCP looks a lot like our generic SR protocol. Hence, TCPs error-recovery mechanism is probably best categorized as a hybrid of GBN and SR protocols."," (Note that Table 3.2 allows for the case that the receiver does not discard out-oforder segments.) Because a sender often sends a large number of segments back to back, if one segment is lost, there will likely be many back-to-back duplicate ACKs. If the TCP sender receives three duplicate ACKs for the same data, it takes this as an indication that the segment following the segment that has been ACKed three times has been lost. (In the homework problems, we consider the question of why the sender waits for three duplicate ACKs, rather than just a single duplicate ACK.) In the case that three duplicate ACKs are received, the TCP sender performs a fast retransmit [RFC 5681], retransmitting the missing segment before that segments timer expires. This is shown in Figure 3.37, where the second segment is lost, then retransmitted before its timer expires. For TCP with fast retransmit, the following code snippet replaces the ACK received event in Figure 3.33: event: ACK received, with ACK field value of y if (y > SendBase) { SendBase=y if (there are currently any not yet acknowledged segments) start timer } Host A seq= 92, 8 by seq= tes 100, of d ata 20 b seq= ytes 120, of d 5 ata byte seq= s of 135, d ata byte seq= s of 141, data 16 b ytes of d ata ack=100 ack=100 ack=100 ack=100 0, byt of a Time Fast retransmit: retransmitting the missing segment before the segments timer expires else {/* a duplicate ACK for already ACKed segment */ increment number of duplicate ACKs received for y if (number of duplicate ACKS received for y==3) /* TCP fast retransmit */ resend segment with sequence number y } break; We noted earlier that many subtle issues arise when a timeout/retransmit mechanism is implemented in an actual protocol such as TCP. The procedures above, which have evolved as a result of more than 20 years of experience with TCP timers, should convince you that this is indeed the case! Go-Back-N or Selective Repeat? Let us close our study of TCPs error-recovery mechanism by considering the following question: Is TCP a GBN or an SR protocol? Recall that TCP acknowledgments are cumulative and correctly received but out-of-order segments are not individually ACKed by the receiver. Consequently, as shown in Figure 3.33 (see also Figure 3.19), the TCP sender need only maintain the smallest sequence number of a transmitted but unacknowledged byte (SendBase) and the sequence number of the next byte to be sent (NextSeqNum). In this sense, TCP looks a lot like a GBN-style protocol. But there are some striking differences between TCP and GoBack-N. Many TCP implementations will buffer correctly received but out-of-order segments [Stevens 1994]. Consider also what happens when the sender sends a sequence of segments 1, 2, . . . , N, and all of the segments arrive in order without error at the receiver. Further suppose that the acknowledgment for packet n 6 N gets lost, but the remaining N - 1 acknowledgments arrive at the sender before their respective timeouts. In this example, GBN would retransmit not only packet n, but also all of the subsequent packets n + 1, n + 2, . . . , N. TCP, on the other hand, would retransmit at most one segment, namely, segment n. Moreover, TCP would not even retransmit segment n if the acknowledgment for segment n + 1 arrived before the timeout for segment n. A proposed modification to TCP, the so-called selective acknowledgment [RFC 2018], allows a TCP receiver to acknowledge out-of-order segments selectively rather than just cumulatively acknowledging the last correctly received, in-order segment. When combined with selective retransmissionskipping the retransmission of segments that have already been selectively acknowledged by the receiver TCP looks a lot like our generic SR protocol. Thus, TCPs error-recovery mechanism is probably best categorized as a hybrid of GBN and SR protocols. 3.5.5 Flow Control Recall that the hosts on each side of a TCP connection set aside a receive buffer for the connection. When the TCP connection receives bytes that are correct and in sequence, it places the data in the receive buffer. The associated application process will read data from this buffer, but not necessarily at the instant the data arrives"
"Explain how when a host sends a reset segment, it is telling the source I dont have a socket for that segment","Our discussion above has assumed that both the client and server are prepared to communicate, i.e., that the server is listening on the port to which the client sends its SYN segment. Lets consider what happens when a host receives a TCP segment whose port numbers or source IP address do not match with any of the ongoing sockets in the host. For example, suppose a host receives a TCP SYN packet with destination port 80, but the host is not accepting connections on port 80 (that is, it is not running a Web server on port 80). Then the host will send a special reset segment to the source. This TCP segment has the RST flag bit (see Section 3.5.2) set to 1. Hence, when a host sends a reset segment, it is telling the source I dont have a socket for that segment."," TCP, assuming the client begins connection teardown. The transitions are selfexplanatory. In these two state-transition diagrams, we have only shown how a TCP connection is normally established and shut down. We have not described what happens in certain pathological scenarios, for example, when both sides of a connection want to initiate or shut down at the same time. If you are interested in learning about Receive ACK, send nothing LAST_ACK Server application creates a listen socket Receive SYN send SYN & ACK Receive FIN, send ACK CONNECTION-ORIENTED TRANSPORT: TCP Send FIN ESTABLISHED Receive ACK, send nothing this and other advanced issues concerning TCP, you are encouraged to see Stevens comprehensive book [Stevens 1994]. Our discussion above has assumed that both the client and server are prepared to communicate, i.e., that the server is listening on the port to which the client sends its SYN segment. Lets consider what happens when a host receives a TCP segment whose port numbers or source IP address do not match with any of the ongoing sockets in the host. For example, suppose a host receives a TCP SYN packet with destination port 80, but the host is not accepting connections on port 80 (that is, it is not running a Web server on port 80). Then the host will send a special reset segment to the source. This TCP segment has the RST flag bit (see Section 3.5.2) set to 1. Thus, when a host sends a reset segment, it is telling the source I dont have a socket for that segment. Please do not resend the segment. When a host receives a UDP packet whose destination port number doesnt match with an ongoing UDP socket, the host sends a special ICMP datagram, as discussed in Chapter 5. Now that we have a good understanding of TCP connection management, lets revisit the nmap port-scanning tool and examine more closely how it works. To explore a specific TCP port, say port 6789, on a target host, nmap will send a TCP SYN segment with destination port 6789 to that host. There are three possible outcomes: The source host receives a TCP SYNACK segment from the target host. Since this means that an application is running with TCP port 6789 on the target post, nmap returns open"
"Explain why in the slow-start state, the value of cwnd begins at 1 MSS and increases by 1 MSS every time a transmitted segment is first acknowledged","Since the available bandwidth to the TCP sender may be much larger than MSS/RTT, the TCP sender would like to find the amount of available bandwidth quickly. Hence, in the slow-start state, the value of cwnd begins at 1 MSS and increases by 1 MSS every time a transmitted segment is first acknowledged."," Given this overview of TCP congestion control, were now in a position to consider the details of the celebrated TCP congestion-control algorithm, which was first described in [Jacobson 1988] and is standardized in [RFC 5681]. The algorithm has three major components: (1) slow start, (2) congestion avoidance, and (3) fast recovery. Slow start and congestion avoidance are mandatory components of TCP, differing in how they increase the size of cwnd in response to received ACKs. Well see shortly that slow start increases the size of cwnd more rapidly (despite its name!) than congestion avoidance. Fast recovery is recommended, but not required, for TCP senders. Slow Start When a TCP connection begins, the value of cwnd is typically initialized to a small value of 1 MSS [RFC 3390], resulting in an initial sending rate of roughly MSS/ RTT. For example, if MSS = 500 bytes and RTT = 200 msec, the resulting initial sending rate is only about 20 kbps. Since the available bandwidth to the TCP sender may be much larger than MSS/RTT, the TCP sender would like to find the amount of available bandwidth quickly. Thus, in the slow-start state, the value of cwnd begins at 1 MSS and increases by 1 MSS every time a transmitted segment is first acknowledged. In the example of Figure 3.50, TCP sends the first segment into the network Host A one se gmen RTT two se gmen four s egment Time Time and waits for an acknowledgment. When this acknowledgment arrives, the TCP sender increases the congestion window by one MSS and sends out two maximumsized segments. These segments are then acknowledged, with the sender increasing the congestion window by 1 MSS for each of the acknowledged segments, giving a congestion window of 4 MSS, and so on. This process results in a doubling of the sending rate every RTT. Thus, the TCP send rate starts slow but grows exponentially during the slow start phase. But when should this exponential growth end? Slow start provides several answers to this question. First, if there is a loss event (i.e., congestion) indicated by a timeout, the TCP sender sets the value of cwnd to 1 and begins the slow start process anew. It also sets the value of a second state variable, ssthresh (shorthand for slow start threshold) to cwnd/2half of the value of the congestion window value when congestion was detected. The second way in which slow start may end is directly tied to the value of ssthresh. Since ssthresh is half the value of cwnd when congestion was last detected, it might be a bit reckless to keep doubling cwnd when it reaches or surpasses the value of ssthresh. Thus, when the value of cwnd equals ssthresh, slow start ends and TCP transitions into congestion avoidance mode. As well see, TCP increases cwnd more cautiously when in congestionavoidance mode. The final way in which slow start can end is if three duplicate ACKs are detected, in which case TCP performs a fast retransmit (see Section 3.5.4) and enters the fast recovery state, as discussed below. TCPs behavior in slow start is summarized in the FSM description of TCP congestion control in Figure 3.51. The slow-start algorithm traces it roots to [Jacobson 1988]; an approach similar to slow start was also proposed independently in [Jain 1986]"
"Explain why rather than doubling the value of cwnd every RTT, TCP adopts a more conservative approach and increases the value of cwnd by just a single MSS every RTT [RFC 5681]"," Congestion Avoidance On entry to the congestion-avoidance state, the value of cwnd is approximately half its value when congestion was last encounteredcongestion could be just around the corner! Hence, rather than doubling the value of cwnd every RTT, TCP adopts a more conservative approach and increases the value of cwnd by just a single MSS every RTT [RFC 5681]."," But when should this exponential growth end? Slow start provides several answers to this question. First, if there is a loss event (i.e., congestion) indicated by a timeout, the TCP sender sets the value of cwnd to 1 and begins the slow start process anew. It also sets the value of a second state variable, ssthresh (shorthand for slow start threshold) to cwnd/2half of the value of the congestion window value when congestion was detected. The second way in which slow start may end is directly tied to the value of ssthresh. Since ssthresh is half the value of cwnd when congestion was last detected, it might be a bit reckless to keep doubling cwnd when it reaches or surpasses the value of ssthresh. Thus, when the value of cwnd equals ssthresh, slow start ends and TCP transitions into congestion avoidance mode. As well see, TCP increases cwnd more cautiously when in congestionavoidance mode. The final way in which slow start can end is if three duplicate ACKs are detected, in which case TCP performs a fast retransmit (see Section 3.5.4) and enters the fast recovery state, as discussed below. TCPs behavior in slow start is summarized in the FSM description of TCP congestion control in Figure 3.51. The slow-start algorithm traces it roots to [Jacobson 1988]; an approach similar to slow start was also proposed independently in [Jain 1986]. Congestion Avoidance On entry to the congestion-avoidance state, the value of cwnd is approximately half its value when congestion was last encounteredcongestion could be just around the corner! Thus, rather than doubling the value of cwnd every RTT, TCP adopts a more conservative approach and increases the value of cwnd by just a single MSS every RTT [RFC 5681]. This can be accomplished in several ways. A common approach is for the TCP sender to increase cwnd by MSS bytes (MSS/cwnd) whenever a new acknowledgment arrives. For example, if MSS is 1,460 bytes and cwnd is 14,600 bytes, then 10 segments are being sent within an RTT. Each arriving ACK (assuming one ACK per segment) increases the congestion window size by 1/10 MSS, and thus, the value of the congestion window will have increased by one MSS after ACKs when all 10 segments have been received. But when should congestion avoidances linear increase (of 1 MSS per RTT) end? TCPs congestion-avoidance algorithm behaves the same when a timeout occurs"
Explain why the joint throughput of the two connections proceeds along a 45-degree line (equal increase for both connections) starting from point A,"Suppose that the TCP window sizes are such that at a given point in time, connections 1 and 2 realize throughputs indicated by point A in Figure 3.55. Because the amount of link bandwidth jointly consumed by the two connections is less than R, no loss will occur, and both connections will increase their window by 1 MSS per RTT as a result of TCPs congestion-avoidance algorithm. Hence, the joint throughput of the two connections proceeds along a 45-degree line (equal increase for both connections) starting from point A."," to share the link bandwidth equally between the two connections, then the realized throughput should fall along the 45-degree arrow (equal bandwidth share) emanating from the origin. Ideally, the sum of the two throughputs should equal R. (Certainly, each connection receiving an equal, but zero, share of the link capacity is not a desirable situation!) So the goal should be to have the achieved throughputs fall somewhere near the intersection of the equal bandwidth share line and the full bandwidth utilization line in Figure 3.55. Suppose that the TCP window sizes are such that at a given point in time, connections 1 and 2 realize throughputs indicated by point A in Figure 3.55. Because the amount of link bandwidth jointly consumed by the two connections is less than R, no loss will occur, and both connections will increase their window by 1 MSS per RTT as a result of TCPs congestion-avoidance algorithm. Thus, the joint throughput of the two connections proceeds along a 45-degree line (equal increase for both connections) starting from point A. Eventually, the link bandwidth jointly consumed by the two connections will be greater than R, and eventually packet loss will occur. Suppose that connections 1 and 2 experience packet loss when they realize throughputs indicated by point B. Connections 1 and 2 then decrease their windows by a factor of two. The resulting throughputs realized are thus at point C, halfway along a vector starting at B and ending at the origin. Because the joint bandwidth use is less than R at point C, the two connections again increase their throughputs along a 45-degree line starting from C. Eventually, loss will again occur, for example, at point D, and the two connections again decrease their window sizes by a factor of two, and so on. You should convince yourself that the bandwidth realized by the two connections eventually fluctuates along the equal bandwidth share line. You should also convince R Full bandwidth utilization line Connection 2 throughput Equal bandwidth share D B C A R yourself that the two connections will converge to this behavior regardless of where they are in the two-dimensional space! Although a number of idealized assumptions lie behind this scenario, it still provides an intuitive feel for why TCP results in an equal sharing of bandwidth among connections"
"Explain why not only must lookup be performed in hardware, but techniques beyond a simple linear search through a large table are needed; surveys of fast lookup algorithms can be found in [Gupta 2001, Ruiz-Sanchez 2001]","Given the existence of a forwarding table, lookup is conceptually simple hardware logic just searches through the forwarding table looking for the longest prefix match. But at Gigabit transmission rates, this lookup must be performed in nanoseconds (recall our earlier example of a 10 Gbps link and a 64-byte IP datagram). Hence, not only must lookup be performed in hardware, but techniques beyond a simple linear search through a large table are needed; surveys of fast lookup algorithms can be found in [Gupta 2001, Ruiz-Sanchez 2001]."," Line termination Data link processing (protocol, decapsulation) Lookup, fowarding, queuing Switch fabric As an example of how this issue of scale can be handled, lets suppose that our router has four links, numbered 0 through 3, and that packets are to be forwarded to the link interfaces as follows: Destination Address Range through 11001000 00010111 00011000 00000000 through 11001000 00010111 00011001 00000000 through Otherwise Clearly, for this example, it is not necessary to have 4 billion entries in the routers forwarding table. We could, for example, have the following forwarding table with just four entries: Prefix 11001000 00010111 00011000 Otherwise Link Interface 1 3 With this style of forwarding table, the router matches a prefix of the packets destination address with the entries in the table; if theres a match, the router forwards the packet to a link associated with the match. For example, suppose the packets destination address is 11001000 00010111 00010110 10100001; because the 21-bit prefix of this address matches the first entry in the table, the router forwards the packet to link interface 0. If a prefix doesnt match any of the first three entries, then the router forwards the packet to the default interface 3. Although this sounds simple enough, theres a very important subtlety here. You may have noticed that it is possible for a destination address to match more than one entry. For example, the first 24 bits of the address 11001000 00010111 00011000 10101010 match the second entry in the table, and the first 21 bits of the address match the third entry in the table. When there are multiple matches, the router uses the longest prefix matching rule; that is, it finds the longest matching entry in the table and forwards the packet to the link interface associated with the longest prefix match. Well see exactly why this longest prefix-matching rule is used when we study Internet addressing in more detail in Section 4.3. Given the existence of a forwarding table, lookup is conceptually simple hardware logic just searches through the forwarding table looking for the longest prefix match. But at Gigabit transmission rates, this lookup must be performed in nanoseconds (recall our earlier example of a 10 Gbps link and a 64-byte IP datagram). Thus, not only must lookup be performed in hardware, but techniques beyond a simple linear search through a large table are needed; surveys of fast lookup algorithms can be found in [Gupta 2001, Ruiz-Sanchez 2001]. Special attention must also be paid to memory access times, resulting in designs with embedded on-chip DRAM and faster SRAM (used as a DRAM cache) memories. In practice, Ternary Content Addressable Memories (TCAMs) are also often used for lookup [Yu 2004]. With a TCAM, a 32-bit IP address is presented to the memory, which returns the content of the forwarding table entry for that address in essentially constant time"
"Explain how unlike the previous two switching approaches, crossbar switches are capable of forwarding multiple packets in parallel","Switching via an interconnection network. One way to overcome the bandwidth limitation of a single, shared bus is to use a more sophisticated interconnection network, such as those that have been used in the past to interconnect processors in a multiprocessor computer architecture. A crossbar switch is an interconnection network consisting of 2N buses that connect N input ports to N output ports, as shown in Figure 4.6. Each vertical bus intersects each horizontal bus at a crosspoint, which can be opened or closed at any time by the switch fabric controller (whose logic is part of the switching fabric itself). When a packet arrives from port A and needs to be forwarded to port Y, the switch controller closes the crosspoint at the intersection of busses A and Y, and port A then sends the packet onto its bus, which is picked up (only) by bus Y. Note that a packet from port B can be forwarded to port X at the same time, since the A-to-Y and B-to-X packets use different input and output busses. Hence, unlike the previous two switching approaches, crossbar switches are capable of forwarding multiple packets in parallel."," Switching via a bus. In this approach, an input port transfers a packet directly to the output port over a shared bus, without intervention by the routing processor. This is typically done by having the input port pre-pend a switch-internal label (header) to the packet indicating the local output port to which this packet is being transferred and transmitting the packet onto the bus. All output ports receive the packet, but only the port that matches the label will keep the packet. The label is then removed at the output port, as this label is only used within the switch to cross the bus. If multiple packets arrive to the router at the same time, each at a different input port, all but one must wait since only one packet can cross the bus at a time. Because every packet must cross the single bus, the switching speed of the router is limited to the bus speed; in our roundabout analogy, this is as if the roundabout could only contain one car at a time. Nonetheless, switching via a bus is often sufficient for routers that operate in small local area and enterprise networks. The Cisco 6500 router [Cisco 6500 2016] internally switches packets over a 32-Gbps-backplane bus. Switching via an interconnection network. One way to overcome the bandwidth limitation of a single, shared bus is to use a more sophisticated interconnection network, such as those that have been used in the past to interconnect processors in a multiprocessor computer architecture. A crossbar switch is an interconnection network consisting of 2N buses that connect N input ports to N output ports, as shown in Figure 4.6. Each vertical bus intersects each horizontal bus at a crosspoint, which can be opened or closed at any time by the switch fabric controller (whose logic is part of the switching fabric itself). When a packet arrives from port A and needs to be forwarded to port Y, the switch controller closes the crosspoint at the intersection of busses A and Y, and port A then sends the packet onto its bus, which is picked up (only) by bus Y. Note that a packet from port B can be forwarded to port X at the same time, since the A-to-Y and B-to-X packets use different input and output busses. Thus, unlike the previous two switching approaches, crossbar switches are capable of forwarding multiple packets in parallel. A crossbar switch is non-blockinga packet being forwarded to an output port will not be blocked from reaching that output port as long as no other packet is currently being forwarded to that output port. However, if two packets from two different input ports are destined to that same output port, then one will have to wait at the input, since only one packet can be sent over any given bus at a time. Cisco 12000 series switches [Cisco 12000 2016] use a crossbar switching network; the Cisco 7600 series can be configured to use either a bus or crossbar switch [Cisco 7600 2016]. More sophisticated interconnection networks use multiple stages of switching elements to allow packets from different input ports to proceed towards the same output port at the same time through the multi-stage switching fabric. See [Tobagi 1990] for a survey of switch architectures. The Cisco CRS employs a three-stage non-blocking switching strategy. A routers switching capacity can also be scaled by running multiple switching fabrics in parallel. In this approach, input ports and output ports are connected to N switching fabrics that operate in parallel. An input port breaks a packet into K smaller chunks, and sends (sprays) the chunks through K of these N switching fabrics to the selected output port, which reassembles the K chunks back into the original packet"
Explain how packet queues can form at the output ports even when the switching fabric is N times faster than the port line speeds,"Lets next consider whether queueing can occur at a switchs output ports. Suppose that Rswitch is again N times faster than Rline and that packets arriving at each of the N input ports are destined to the same output port. In this case, in the time it takes to send a single packet onto the outgoing link, N new packets will arrive at this output port (one from each of the N input ports). Since the output port can transmit only a single packet in a unit of time (the packet transmission time), the N arriving packets will have to queue (wait) for transmission over the outgoing link. Then N more packets can possibly arrive in the time it takes to transmit just one of the N packets that had just previously been queued. And so on. Hence, packet queues can form at the output ports even when the switching fabric is N times faster than the port line speeds."," This phenomenon is known as head-of-the-line (HOL) blocking in an input-queued switcha queued packet in an input queue must wait for transfer through the fabric (even though its output port is free) because it is blocked by another packet at the head of the line. [Karol 1987] shows that due to HOL blocking, the input queue will grow to unbounded length (informally, this is equivalent to saying that significant packet loss will occur) under certain assumptions as soon as the packet arrival rate on the input links reaches only 58 percent of their capacity. A number of solutions to HOL blocking are discussed in [McKeown 1997]. Output port contention at time t one dark packet can be transferred Switch fabric Switch fabric Key: destined for upper output port destined for middle output port destined for lower output port Output Queueing Lets next consider whether queueing can occur at a switchs output ports. Suppose that Rswitch is again N times faster than Rline and that packets arriving at each of the N input ports are destined to the same output port. In this case, in the time it takes to send a single packet onto the outgoing link, N new packets will arrive at this output port (one from each of the N input ports). Since the output port can transmit only a single packet in a unit of time (the packet transmission time), the N arriving packets will have to queue (wait) for transmission over the outgoing link. Then N more packets can possibly arrive in the time it takes to transmit just one of the N packets that had just previously been queued. And so on. Thus, packet queues can form at the output ports even when the switching fabric is N times faster than the port line speeds. Eventually, the number of queued packets can grow large enough to exhaust available memory at the output port"
"Explain why an IP address is technically associated with an interface, rather than with the host or router containing that interface","Before discussing IP addressing, however, well need to say a few words about how hosts and routers are connected into the Internet. A host typically has only a single link into the network; when IP in the host wants to send a datagram, it does so over this link. The boundary between the host and the physical link is called an interface. Now consider a router and its interfaces. Because a routers job is to receive a datagram on one link and forward the datagram on some other link, a router necessarily has two or more links to which it is connected. The boundary between the router and any one of its links is also called an interface. A router thus has multiple interfaces, one for each of its links. Because every host and router is capable of sending and receiving IP datagrams, IP requires each host and router interface to have its own IP address. Hence, an IP address is technically associated with an interface, rather than with the host or router containing that interface."," 4.3.3 IPv4 Addressing We now turn our attention to IPv4 addressing. Although you may be thinking that addressing must be a straightforward topic, hopefully by the end of this section youll be convinced that Internet addressing is not only a juicy, subtle, and interesting topic but also one that is of central importance to the Internet. An excellent treatment of IPv4 addressing can be found in the first chapter in [Stewart 1999]. Before discussing IP addressing, however, well need to say a few words about how hosts and routers are connected into the Internet. A host typically has only a single link into the network; when IP in the host wants to send a datagram, it does so over this link. The boundary between the host and the physical link is called an interface. Now consider a router and its interfaces. Because a routers job is to receive a datagram on one link and forward the datagram on some other link, a router necessarily has two or more links to which it is connected. The boundary between the router and any one of its links is also called an interface. A router thus has multiple interfaces, one for each of its links. Because every host and router is capable of sending and receiving IP datagrams, IP requires each host and router interface to have its own IP address. Thus, an IP address is technically associated with an interface, rather than with the host or router containing that interface. Each IP address is 32 bits long (equivalently, 4 bytes), and there are thus a total of 232 (or approximately 4 billion) possible IP addresses. These addresses are typically written in so-called dotted-decimal notation, in which each byte of the address is written in its decimal form and is separated by a period (dot) from other bytes in the address. For example, consider the IP address 193.32.216.9. The 193 is the decimal equivalent of the first 8 bits of the address; the 32 is the decimal equivalent of the second 8 bits of the address, and so on. Thus, the address 193.32.216.9 in binary notation is Each interface on every host and router in the global Internet must have an IP address that is globally unique (except for interfaces behind NATs, as discussed in Section 4.3.4). These addresses cannot be chosen in a willy-nilly manner, however. A portion of an interfaces IP address will be determined by the subnet to which it is connected"
"Explain why the routing algorithms that compute these paths are of fundamental importance, and another candidate for our top-10 list of fundamentally important networking concepts","In this section well study routing algorithms, whose goal is to determine good paths (equivalently, routes), from senders to receivers, through the network of routers. Typically, a good path is one that has the least cost. Well see that in practice, however, real-world concerns such as policy issues (for example, a rule such as router x, belonging to organization Y, should not forward any packets originating from the network owned by organization Z ) also come into play. We note that whether the network control plane adopts a per-router control approach or a logically centralized approach, there must always be a well-defined sequence of routers that a packet will cross in traveling from sending to receiving host. Hence, the routing algorithms that compute these paths are of fundamental importance, and another candidate for our top-10 list of fundamentally important networking concepts."," By logically centralized control [Levin 2012] we mean that the routing control service is accessed as if it were a single central service point, even though the service is likely to be implemented via multiple servers for fault-tolerance, and performance scalability reasons. As we will see in Section 5.5, SDN adopts this notion of a logically centralized controlleran approach that is finding increased use in production deployments. Google uses SDN to control the routers in its internal B4 global wide-area network that interconnects its data centers [Jain 2013]. SWAN [Hong 2013], from Microsoft Research, uses a logically centralized controller to manage routing and forwarding between a wide area network and a data center network. China Telecom and China Unicom are using SDN both within data centers and between data centers [Li 2015]. AT&T has noted [AT&T 2013] that it supports many SDN capabilities and independently defined, proprietary mechanisms that fall under the SDN architectural framework. 5.2 Routing Algorithms In this section well study routing algorithms, whose goal is to determine good paths (equivalently, routes), from senders to receivers, through the network of routers. Typically, a good path is one that has the least cost. Well see that in practice, however, real-world concerns such as policy issues (for example, a rule such as router x, belonging to organization Y, should not forward any packets originating from the network owned by organization Z ) also come into play. We note that whether the network control plane adopts a per-router control approach or a logically centralized approach, there must always be a well-defined sequence of routers that a packet will cross in traveling from sending to receiving host. Thus, the routing algorithms that compute these paths are of fundamental importance, and another candidate for our top-10 list of fundamentally important networking concepts. A graph is used to formulate routing problems. Recall that a graph G = (N, E) is a set N of nodes and a collection E of edges, where each edge is a pair of nodes from N. In the context of network-layer routing, the nodes in the graph represent v 3 w 1 z y routersthe points at which packet-forwarding decisions are madeand the edges connecting these nodes represent the physical links between these routers. Such a graph abstraction of a computer network is shown in Figure 5.3. To view some graphs representing real network maps, see [Dodge 2016, Cheswick 2000]; for a discussion of how well different graph-based models model the Internet, see [Zegura 1997, Faloutsos 1999, Li 2004]"
"Explain why in order to achieve the goal of minimizing the maximum link utilization, the operator must find the set of link weights that achieves this goal","In practice, the cause and effect relationship between link weights and routing paths may be reversed, with network operators configuring link weights in order to obtain routing paths that achieve certain traffic engineering goals [Fortz 2000, Fortz 2002]. For example, suppose a network operator has an estimate of traffic flow entering the network at each ingress point and destined for each egress point. The operator may then want to put in place a specific routing of ingress-to-egress flows that minimizes the maximum utilization over all of the networks links. But with a routing algorithm such as OSPF, the operators main knobs for tuning the routing of flows through the network are the link weights. Hence, in order to achieve the goal of minimizing the maximum link utilization, the operator must find the set of link weights that achieves this goal."," Each router then locally runs Dijkstras shortest-path algorithm to determine a shortest-path tree to all subnets, with itself as the root node. Individual link costs are configured by the network administrator (see sidebar, Principles and Practice: Setting OSPF Weights). The administrator might choose to set all link costs to 1, PRINCIPLES IN PRACTICE SE TTING OSPF LINK WEIGHT S Our discussion of link-state routing has implicitly assumed that link weights are set, a routing algorithm such as OSPF is run, and traffic flows according to the routing tables computed by the LS algorithm. In terms of cause and effect, the link weights are given (i.e., they come first) and result (via Dijkstras algorithm) in routing paths that minimize overall cost. In this viewpoint, link weights reflect the cost of using a link (e.g., if link weights are inversely proportional to capacity, then the use of high-capacity links would have smaller weight and thus be more attractive from a routing standpoint) and Dijsktras algorithm serves to minimize overall cost. In practice, the cause and effect relationship between link weights and routing paths may be reversed, with network operators configuring link weights in order to obtain routing paths that achieve certain traffic engineering goals [Fortz 2000, Fortz 2002]. For example, suppose a network operator has an estimate of traffic flow entering the network at each ingress point and destined for each egress point. The operator may then want to put in place a specific routing of ingress-to-egress flows that minimizes the maximum utilization over all of the networks links. But with a routing algorithm such as OSPF, the operators main knobs for tuning the routing of flows through the network are the link weights. Thus, in order to achieve the goal of minimizing the maximum link utilization, the operator must find the set of link weights that achieves this goal. This is a reversal of the cause and effect relationshipthe desired routing of flows is known, and the OSPF link weights must be found such that the OSPF routing algorithm results in this desired routing of flows. thus achieving minimum-hop routing, or might choose to set the link weights to be inversely proportional to link capacity in order to discourage traffic from using low-bandwidth links. OSPF does not mandate a policy for how link weights are set (that is the job of the network administrator), but instead provides the mechanisms (protocol) for determining least-cost path routing for the given set of link weights"
Explain why the OSPF protocol must itself implement functionality such as reliable message transfer and link-state broadcast,"With OSPF, a router broadcasts routing information to all other routers in the autonomous system, not just to its neighboring routers. A router broadcasts link-state information whenever there is a change in a links state (for example, a change in cost or a change in up/down status). It also broadcasts a links state periodically (at least once every 30 minutes), even if the links state has not changed. RFC 2328 notes that this periodic updating of link state advertisements adds robustness to the link state algorithm. OSPF advertisements are contained in OSPF messages that are carried directly by IP, with an upper-layer protocol of 89 for OSPF. Hence, the OSPF protocol must itself implement functionality such as reliable message transfer and link-state broadcast."," thus achieving minimum-hop routing, or might choose to set the link weights to be inversely proportional to link capacity in order to discourage traffic from using low-bandwidth links. OSPF does not mandate a policy for how link weights are set (that is the job of the network administrator), but instead provides the mechanisms (protocol) for determining least-cost path routing for the given set of link weights. With OSPF, a router broadcasts routing information to all other routers in the autonomous system, not just to its neighboring routers. A router broadcasts link-state information whenever there is a change in a links state (for example, a change in cost or a change in up/down status). It also broadcasts a links state periodically (at least once every 30 minutes), even if the links state has not changed. RFC 2328 notes that this periodic updating of link state advertisements adds robustness to the link state algorithm. OSPF advertisements are contained in OSPF messages that are carried directly by IP, with an upper-layer protocol of 89 for OSPF. Thus, the OSPF protocol must itself implement functionality such as reliable message transfer and link-state broadcast. The OSPF protocol also checks that links are operational (via a HELLO message that is sent to an attached neighbor) and allows an OSPF router to obtain a neighboring routers database of network-wide link state. Some of the advances embodied in OSPF include the following: Security. Exchanges between OSPF routers (for example, link-state updates) can be authenticated. With authentication, only trusted routers can participate in the OSPF protocol within an AS, thus preventing malicious intruders (or networking students taking their newfound knowledge out for a joyride) from injecting incorrect information into router tables. By default, OSPF packets between routers are not authenticated and could be forged. Two types of authentication can be configuredsimple and MD5 (see Chapter 8 for a discussion on MD5 and authentication in general). With simple authentication, the same password is configured on each router. When a router sends an OSPF packet, it includes the password in plaintext. Clearly, simple authentication is not very secure. MD5 authentication is based on shared secret keys that are configured in all the routers. For each OSPF packet that it sends, the router computes the MD5 hash of the content of the OSPF packet appended with the secret key. (See the discussion of message authentication codes in Chapter 8.) Then the router includes the resulting hash value in the OSPF packet. The receiving router, using the preconfigured secret key, will compute an MD5 hash of the packet and compare it with the hash value that the packet carries, thus verifying the packets authenticity. Sequence numbers are also used with MD5 authentication to protect against replay attacks"
"Explain why a routers forwarding table will have entries of the form (x, I), where x is a prefix (such as 138"," In the world of BGP, a destination may take the form 138.16.68/22, which for this example includes 1,024 IP addresses. Hence, a routers forwarding table will have entries of the form (x, I), where x is a prefix (such as 138."," In BGP, packets are not routed to a specific destination address, but instead to CIDRized prefixes, with each prefix representing a subnet or a collection of subnets. In the world of BGP, a destination may take the form 138.16.68/22, which for this example includes 1,024 IP addresses. Thus, a routers forwarding table will have entries of the form (x, I), where x is a prefix (such as 138.16.68/22) and I is an interface number for one of the routers interfaces. As an inter-AS routing protocol, BGP provides each router a means to: 1. Obtain prefix reachability information from neighboring ASs. In particular, BGP allows each subnet to advertise its existence to the rest of the Internet. A subnet screams, I exist and I am here, and BGP makes sure that all the routers in the Internet know about this subnet. If it werent for BGP, each subnet would be an isolated islandalone, unknown and unreachable by the rest of the Internet"
Explain how the request ID field can be used by the managing server to detect lost requests or replies,"SNMP PDUs can be carried via many different transport protocols, the SNMP PDU is typically carried in the payload of a UDP datagram. Indeed, RFC 3417 states that UDP is the preferred transport mapping. However, since UDP is an unreliable transport protocol, there is no guarantee that a request, or its response, will be received at the intended destination. The request ID field of the PDU (see Figure 5.21) is used by the managing server to number its requests to an agent; the agents response takes its request ID from that of the received request. Hence, the request ID field can be used by the managing server to detect lost requests or replies."," RFC 3418 defines well-known trap types that include a cold or warm start by a device, a link going up or down, the loss of a neighbor, or an authentication failure event. A received trap request has no required response from a managing server. Given the request-response nature of SNMP, it is worth noting here that although SNMP PDUs can be carried via many different transport protocols, the SNMP PDU is typically carried in the payload of a UDP datagram. Indeed, RFC 3417 states that UDP is the preferred transport mapping. However, since UDP is an unreliable transport protocol, there is no guarantee that a request, or its response, will be received at the intended destination. The request ID field of the PDU (see Figure 5.21) is used by the managing server to number its requests to an agent; the agents response takes its request ID from that of the received request. Thus, the request ID field can be used by the managing server to detect lost requests or replies. It is up to the managing server to decide whether to retransmit a request if no corresponding response is received after a given amount of time. In particular, the SNMP standard does not mandate any particular procedure for retransmission, or even if retransmission is to be done in the first place. It only requires that the managing server needs to act responsibly in respect to the frequency and duration of retransmissions. This, of course, leads one to wonder how a responsible protocol should act! SNMP has evolved through three versions. The designers of SNMPv3 have said that SNMPv3 can be thought of as SNMPv2 with additional security and administration capabilities [RFC 3410]. Certainly, there are changes in SNMPv3 over SNMPv2, but nowhere are those changes more evident than in the area of administration and security. The central role of security in SNMPv3 was particularly important, since the lack of adequate security resulted in SNMP being used primarily for monitoring rather than control (for example, SetRequest is rarely used in SNMPv1). Once again, we see that securitya topic well cover in detail in Chapter 8 is of critical concern, but once again a concern whose importance had been realized perhaps a bit late and only then added on. 5.7 Summary We have now completed our two-chapter journey into the network corea journey that began with our study of the network layers data plane in Chapter 4 and finished here with our study of the network layers control plane. We learned that the control plane is the network-wide logic that controls not only how a datagram is forwarded among routers along an end-to-end path from the source host to the destination host, but also how network-layer components and services are configured and managed"
"Explain why all the frames involved in the collision are lost, and the broadcast channel is wasted during the collision interval","Because all nodes are capable of transmitting frames, more than two nodes can transmit frames at the same time. When this happens, all of the nodes receive multiple frames at the same time; that is, the transmitted frames collide at all of the receivers. Typically, when there is a collision, none of the receiving nodes can make any sense of any of the frames that were transmitted; in a sense, the signals of the colliding frames become inextricably tangled together. Hence, all the frames involved in the collision are lost, and the broadcast channel is wasted during the collision interval."," Computer networks similarly have protocolsso-called multiple access protocolsby which nodes regulate their transmission into the shared broadcast channel. As shown in Figure 6.8, multiple access protocols are needed in a wide variety of network settings, including both wired and wireless access networks, and satellite networks. Although technically each node accesses the broadcast channel through its adapter, in this section we will refer to the node as the sending and Shared wire (for example, cable access network) Shared wireless (for example, WiFi) Cocktail party Head end 6.3 MULTIPLE ACCESS LINKS AND PROTOCOLS receiving device. In practice, hundreds or even thousands of nodes can directly communicate over a broadcast channel. Because all nodes are capable of transmitting frames, more than two nodes can transmit frames at the same time. When this happens, all of the nodes receive multiple frames at the same time; that is, the transmitted frames collide at all of the receivers. Typically, when there is a collision, none of the receiving nodes can make any sense of any of the frames that were transmitted; in a sense, the signals of the colliding frames become inextricably tangled together. Thus, all the frames involved in the collision are lost, and the broadcast channel is wasted during the collision interval. Clearly, if many nodes want to transmit frames frequently, many transmissions will result in collisions, and much of the bandwidth of the broadcast channel will be wasted. In order to ensure that the broadcast channel performs useful work when multiple nodes are active, it is necessary to somehow coordinate the transmissions of the active nodes. This coordination job is the responsibility of the multiple access protocol. Over the past 40 years, thousands of papers and hundreds of PhD dissertations have been written on multiple access protocols; a comprehensive survey of the first 20 years of this body of work is [Rom 1990]. Furthermore, active research in multiple access protocols continues due to the continued emergence of new types of links, particularly new wireless links"
"Explain why the more collisions experienced by a frame, the larger the interval from which K is chosen","The binary exponential backoff algorithm, used in Ethernet as well as in DOCSIS cable network multiple access protocols [DOCSIS 2011], elegantly solves this problem. Specifically, when transmitting a frame that has already experienced n collisions, a node chooses the value of K at random from {0,1,2, . . . . 2n -1}. Hence, the more collisions experienced by a frame, the larger the interval from which K is chosen."," The need to wait a random (rather than fixed) amount of time is hopefully clearif two nodes transmitted frames at the same time and then both waited the same fixed amount of time, theyd continue colliding forever. But what is a good interval of time from which to choose the random backoff time? If the interval is large and the number of colliding nodes is small, nodes are likely to wait a large amount of time (with the channel remaining idle) before repeating the sense-and-transmit-whenidle step. On the other hand, if the interval is small and the number of colliding nodes is large, its likely that the chosen random values will be nearly the same, and transmitting nodes will again collide. What wed like is an interval that is short when the number of colliding nodes is small, and long when the number of colliding nodes is large. The binary exponential backoff algorithm, used in Ethernet as well as in DOCSIS cable network multiple access protocols [DOCSIS 2011], elegantly solves this problem. Specifically, when transmitting a frame that has already experienced n collisions, a node chooses the value of K at random from {0,1,2, . . . . 2n -1}. Thus, the more collisions experienced by a frame, the larger the interval from which K is chosen. For Ethernet, the actual amount of time a node waits is K # 512 bit times (i.e., K times the amount of time needed to send 512 bits into the Ethernet) and the maximum value that n can take is capped at 10. Lets look at an example. Suppose that a node attempts to transmit a frame for the first time and while transmitting it detects a collision. The node then chooses K = 0 with probability 0.5 or chooses K = 1 with probability 0.5. If the node chooses K = 0, then it immediately begins sensing the channel. If the node chooses K = 1, it waits 512 bit times (e.g., 5.12 microseconds for a 100 Mbps Ethernet) before beginning the sense-and-transmit-when-idle cycle. After a second collision, K is chosen with equal probability from {0,1,2,3}. After three collisions, K is chosen with equal probability from {0,1,2,3,4,5,6,7}. After 10 or more collisions, K is chosen with equal probability from {0,1,2,, 1023}. Thus, the size of the sets from which K is chosen grows exponentially with the number of collisions; for this reason this algorithm is referred to as binary exponential backoff"
Explain why the size of the sets from which K is chosen grows exponentially with the number of collisions; for this reason this algorithm is referred to as binary exponential backoff,"K is chosen with equal probability from {0,1,2,3}. After three collisions, K is chosen with equal probability from {0,1,2,3,4,5,6,7}. After 10 or more collisions, K is chosen with equal probability from {0,1,2,, 1023}. Hence, the size of the sets from which K is chosen grows exponentially with the number of collisions; for this reason this algorithm is referred to as binary exponential backoff."," The binary exponential backoff algorithm, used in Ethernet as well as in DOCSIS cable network multiple access protocols [DOCSIS 2011], elegantly solves this problem. Specifically, when transmitting a frame that has already experienced n collisions, a node chooses the value of K at random from {0,1,2, . . . . 2n -1}. Thus, the more collisions experienced by a frame, the larger the interval from which K is chosen. For Ethernet, the actual amount of time a node waits is K # 512 bit times (i.e., K times the amount of time needed to send 512 bits into the Ethernet) and the maximum value that n can take is capped at 10. Lets look at an example. Suppose that a node attempts to transmit a frame for the first time and while transmitting it detects a collision. The node then chooses K = 0 with probability 0.5 or chooses K = 1 with probability 0.5. If the node chooses K = 0, then it immediately begins sensing the channel. If the node chooses K = 1, it waits 512 bit times (e.g., 5.12 microseconds for a 100 Mbps Ethernet) before beginning the sense-and-transmit-when-idle cycle. After a second collision, K is chosen with equal probability from {0,1,2,3}. After three collisions, K is chosen with equal probability from {0,1,2,3,4,5,6,7}. After 10 or more collisions, K is chosen with equal probability from {0,1,2,, 1023}. Thus, the size of the sets from which K is chosen grows exponentially with the number of collisions; for this reason this algorithm is referred to as binary exponential backoff. We also note here that each time a node prepares a new frame for transmission, it runs the CSMA/CD algorithm, not taking into account any collisions that may have occurred in the recent past. So it is possible that a node with a new frame will immediately be able to sneak in a successful transmission while several other nodes are in the exponential backoff state"
Explain why an adapter may receive a frame that isnt addressed to it,"When an adapter wants to send a frame to some destination adapter, the sending adapter inserts the destination adapters MAC address into the frame and then sends the frame into the LAN. As we will soon see, a switch occasionally broadcasts an incoming frame onto all of its interfaces. Well see in Chapter 7 that 802.11 also broadcasts frames. Hence, an adapter may receive a frame that isnt addressed to it."," An adapters MAC address has a flat structure (as opposed to a hierarchical structure) and doesnt change no matter where the adapter goes. A laptop with an Ethernet interface always has the same MAC address, no matter where the computer goes. A smartphone with an 802.11 interface always has the same MAC address, no matter where the smartphone goes. Recall that, in contrast, IP addresses have a hierarchical structure (that is, a network part and a host part), and a hosts IP addresses needs to be changed when the host moves, i.e., changes the network to which it is attached. An adapters MAC address is analogous to a persons social security number, which also has a flat addressing structure and which doesnt change no matter where the person goes. An IP address is analogous to a persons postal address, which is hierarchical and which must be changed whenever a person moves. Just as a person may find it useful to have both a postal address and a social security number, it is useful for a host and router interfaces to have both a network-layer address and a MAC address. When an adapter wants to send a frame to some destination adapter, the sending adapter inserts the destination adapters MAC address into the frame and then sends the frame into the LAN. As we will soon see, a switch occasionally broadcasts an incoming frame onto all of its interfaces. Well see in Chapter 7 that 802.11 also broadcasts frames. Thus, an adapter may receive a frame that isnt addressed to it. Thus, when an adapter receives a frame, it will check to see whether the destination MAC address in the frame matches its own MAC address. If there is a match, the adapter extracts the enclosed datagram and passes the datagram up the protocol stack. If there isnt a match, the adapter discards the frame, without passing the network-layer datagram up. Thus, the destination only will be interrupted when the frame is received"
"Explain why when an adapter receives a frame, it will check to see whether the destination MAC address  in the frame matches its own MAC address","When an adapter wants to send a frame to some destination adapter, the sending adapter inserts the destination adapters MAC address into the frame and then sends the frame into the LAN. As we will soon see, a switch occasionally broadcasts an incoming frame onto all of its interfaces. Well see in Chapter 7 that 802.11 also broadcasts frames. Thus, an adapter may receive a frame that isnt addressed to it. Hence, when an adapter receives a frame, it will check to see whether the destination MAC address  in the frame matches its own MAC address."," An adapters MAC address has a flat structure (as opposed to a hierarchical structure) and doesnt change no matter where the adapter goes. A laptop with an Ethernet interface always has the same MAC address, no matter where the computer goes. A smartphone with an 802.11 interface always has the same MAC address, no matter where the smartphone goes. Recall that, in contrast, IP addresses have a hierarchical structure (that is, a network part and a host part), and a hosts IP addresses needs to be changed when the host moves, i.e., changes the network to which it is attached. An adapters MAC address is analogous to a persons social security number, which also has a flat addressing structure and which doesnt change no matter where the person goes. An IP address is analogous to a persons postal address, which is hierarchical and which must be changed whenever a person moves. Just as a person may find it useful to have both a postal address and a social security number, it is useful for a host and router interfaces to have both a network-layer address and a MAC address. When an adapter wants to send a frame to some destination adapter, the sending adapter inserts the destination adapters MAC address into the frame and then sends the frame into the LAN. As we will soon see, a switch occasionally broadcasts an incoming frame onto all of its interfaces. Well see in Chapter 7 that 802.11 also broadcasts frames. Thus, an adapter may receive a frame that isnt addressed to it. Thus, when an adapter receives a frame, it will check to see whether the destination MAC address in the frame matches its own MAC address. If there is a match, the adapter extracts the enclosed datagram and passes the datagram up the protocol stack. If there isnt a match, the adapter discards the frame, without passing the network-layer datagram up. Thus, the destination only will be interrupted when the frame is received"
"Explain why Ethernet with a hub-based star topology is also a broadcast LANwhenever a hub receives a bit from one of its interfaces, it sends a copy out on all of its other interfaces"," When a bit, representing a zero or a one, arrives from one interface, the hub simply re-creates the bit, boosts its energy strength, and transmits the bit onto all the other interfaces. Hence, Ethernet with a hub-based star topology is also a broadcast LANwhenever a hub receives a bit from one of its interfaces, it sends a copy out on all of its other interfaces."," A hub is a physical-layer device that acts on individual bits rather than frames. When a bit, representing a zero or a one, arrives from one interface, the hub simply re-creates the bit, boosts its energy strength, and transmits the bit onto all the other interfaces. Thus, Ethernet with a hub-based star topology is also a broadcast LANwhenever a hub receives a bit from one of its interfaces, it sends a copy out on all of its other interfaces. In particular, if a hub receives frames from two different interfaces at the same time, a collision occurs and the nodes that created the frames must retransmit. In the early 2000s Ethernet experienced yet another major evolutionary change"
Explain why switches provide a significant performance improvement over LANs with broadcast links,"In a LAN built from switches (and without hubs), there is no wasted bandwidth due to collisions! The switches buffer frames and never transmit more than one frame on a segment at any one time. As with a router, the maximum aggregate throughput of a switch is the sum of all the switch interface rates. Hence, switches provide a significant performance improvement over LANs with broadcast links."," Switch learns about the location of an adapter with address 01-12-23-34-45-56 Switches are plug-and-play devices because they require no intervention from a network administrator or user. A network administrator wanting to install a switch need do nothing more than connect the LAN segments to the switch interfaces. The administrator need not configure the switch tables at the time of installation or when a host is removed from one of the LAN segments. Switches are also full-duplex, meaning any switch interface can send and receive at the same time. Properties of Link-Layer Switching Having described the basic operation of a link-layer switch, lets now consider their features and properties. We can identify several advantages of using switches, rather than broadcast links such as buses or hub-based star topologies: Elimination of collisions. In a LAN built from switches (and without hubs), there is no wasted bandwidth due to collisions! The switches buffer frames and never transmit more than one frame on a segment at any one time. As with a router, the maximum aggregate throughput of a switch is the sum of all the switch interface rates. Thus, switches provide a significant performance improvement over LANs with broadcast links. Heterogeneous links. Because a switch isolates one link from another, the different links in the LAN can operate at different speeds and can run over different media. For example, the uppermost switch in Figure 6.15 might have three1 Gbps 1000BASE-T copper links, two 100 Mbps 100BASE-FX fiber links, and one 100BASE-T copper link. Thus, a switch is ideal for mixing legacy equipment with new equipment"
Explain why a switch is ideal for mixing legacy equipment with new equipment,"Because a switch isolates one link from another, the different links in the LAN can operate at different speeds and can run over different media. For example, the uppermost switch in Figure 6.15 might have three1 Gbps 1000BASE-T copper links, two 100 Mbps 100BASE-FX fiber links, and one 100BASE-T copper link. Hence, a switch is ideal for mixing legacy equipment with new equipment."," Properties of Link-Layer Switching Having described the basic operation of a link-layer switch, lets now consider their features and properties. We can identify several advantages of using switches, rather than broadcast links such as buses or hub-based star topologies: Elimination of collisions. In a LAN built from switches (and without hubs), there is no wasted bandwidth due to collisions! The switches buffer frames and never transmit more than one frame on a segment at any one time. As with a router, the maximum aggregate throughput of a switch is the sum of all the switch interface rates. Thus, switches provide a significant performance improvement over LANs with broadcast links. Heterogeneous links. Because a switch isolates one link from another, the different links in the LAN can operate at different speeds and can run over different media. For example, the uppermost switch in Figure 6.15 might have three1 Gbps 1000BASE-T copper links, two 100 Mbps 100BASE-FX fiber links, and one 100BASE-T copper link. Thus, a switch is ideal for mixing legacy equipment with new equipment. Management. In addition to providing enhanced security (see sidebar on Focus on Security), a switch also eases network management. For example, if an adapter malfunctions and continually sends Ethernet frames (called a jabbering adapter), a switch can detect the problem and internally disconnect the malfunctioning adapter. With this feature, the network administrator need not get out of bed and drive back to work in order to correct the problem. Similarly, a cable cut disconnects only that host that was using the cut cable to connect to the switch. In the days of coaxial cable, many a network manager spent hours walking the line (or more accurately, crawling the floor) to find the cable break that brought down the entire network. Switches also gather statistics on bandwidth usage, collision rates, and traffic types, and make this information available to the network manager. This information can be used to debug and correct problems, and to plan how the LAN should evolve in the future. Researchers are exploring adding yet more management functionality into Ethernet LANs in prototype deployments [Casado 2007; Koponen 2011]"
Explain why the MPLS-capable router need not extract the destination IP address and perform a lookup of the longest prefix match in the forwarding table,"Its immediately evident from Figure 6.28 that an MPLS-enhanced frame can only be sent between routers that are both MPLS capable (since a non-MPLS-capable router would be quite confused when it found an MPLS header where it had expected to find the IP header!). An MPLS-capable router is often referred to as a labelswitched router, since it forwards an MPLS frame by looking up the MPLS label in its forwarding table and then immediately passing the datagram to the appropriate output interface. Hence, the MPLS-capable router need not extract the destination IP address and perform a lookup of the longest prefix match in the forwarding table."," Lets begin our study of MPLS by considering the format of a link-layer frame that is handled by an MPLS-capable router. Figure 6.28 shows that a link-layer frame transmitted between MPLS-capable devices has a small MPLS header added between the layer-2 (e.g., Ethernet) header and layer-3 (i.e., IP) header. RFC 3032 defines the format of the MPLS header for such links; headers are defined for ATM and frame-relayed networks as well in other RFCs. Among the fields in the MPLS PPP or Ethernet header Label IP header S TTL MPLS header: Located between link- and network-layer headers header are the label, 3 bits reserved for experimental use, a single S bit, which is used to indicate the end of a series of stacked MPLS headers (an advanced topic that well not cover here), and a time-to-live field. Its immediately evident from Figure 6.28 that an MPLS-enhanced frame can only be sent between routers that are both MPLS capable (since a non-MPLS-capable router would be quite confused when it found an MPLS header where it had expected to find the IP header!). An MPLS-capable router is often referred to as a labelswitched router, since it forwards an MPLS frame by looking up the MPLS label in its forwarding table and then immediately passing the datagram to the appropriate output interface. Thus, the MPLS-capable router need not extract the destination IP address and perform a lookup of the longest prefix match in the forwarding table. But how does a router know if its neighbor is indeed MPLS capable, and how does a router know what label to associate with the given IP destination? To answer these questions, well need to take a look at the interaction among a group of MPLS-capable routers. In the example in Figure 6.29, routers R1 through R4 are MPLS capable. R5 and R6 are standard IP routers. R1 has advertised to R2 and R3 that it (R1) can route to destination A, and that a received frame with MPLS label 6 will be forwarded to destination A. Router R3 has advertised to router R4 that it can route to destinations A and D, and that incoming frames with MPLS labels 10 and 12, respectively, will be switched toward those destinations. Router R2 has also advertised to router R4 that it (R2) can reach destination A, and that a received frame with MPLS label 8 will be switched toward A. Note that router R4 is now in the interesting position of having in label out label out interface in label out label out interface 12 A D A 0 12 9 A D 0 R4 D A in label out label out interface in label out label out interface 0 CHAPTER 6 THE LINK LAYER AND LANS two MPLS paths to reach A: via interface 0 with outbound MPLS label 10, and via interface 1 with an MPLS label of 8. The broad picture painted in Figure 6.29 is that IP devices R5, R6, A, and D are connected together via an MPLS infrastructure (MPLS-capable routers R1, R2, R3, and R4) in much the same way that a switched LAN or an ATM network can connect together IP devices. And like a switched LAN or ATM network, the MPLS-capable routers R1 through R4 do so without ever touching the IP header of a packet"
Explain how MPLS provides the ability to forward packets along routes that would not be possible using standard IP routing protocols,"MPLS performs switching based on labels, without needing to consider the IP address of a packet. The true advantages of MPLS and the reason for current interest in MPLS, however, lie not in the potential increases in switching speeds, but rather in the new traffic management capabilities that MPLS enables. As noted above, R4 has two MPLS paths to A. If forwarding were performed up at the IP layer on the basis of IP address, the IP routing protocols we studied in Chapter 5 would specify only a single, least-cost path to A. Hence, MPLS provides the ability to forward packets along routes that would not be possible using standard IP routing protocols."," Weve also not discussed how MPLS actually computes the paths for packets among MPLS capable routers, nor how it gathers link-state information (e.g., amount of link bandwidth unreserved by MPLS) to use in these path computations. Existing linkstate routing algorithms (e.g., OSPF) have been extended to flood this information to MPLS-capable routers. Interestingly, the actual path computation algorithms are not standardized, and are currently vendor-specific. Thus far, the emphasis of our discussion of MPLS has been on the fact that MPLS performs switching based on labels, without needing to consider the IP address of a packet. The true advantages of MPLS and the reason for current interest in MPLS, however, lie not in the potential increases in switching speeds, but rather in the new traffic management capabilities that MPLS enables. As noted above, R4 has two MPLS paths to A. If forwarding were performed up at the IP layer on the basis of IP address, the IP routing protocols we studied in Chapter 5 would specify only a single, least-cost path to A. Thus, MPLS provides the ability to forward packets along routes that would not be possible using standard IP routing protocols. This is one simple form of traffic engineering using MPLS [RFC 3346; RFC 3272; RFC 2702; Xiao 2000], in which a network operator can override normal IP routing and force some of the traffic headed toward a given destination along one path, and other traffic destined toward the same destination along another path (whether for policy, performance, or some other reason). It is also possible to use MPLS for many other purposes as well. It can be used to perform fast restoration of MPLS forwarding paths, e.g., to reroute traffic over a precomputed failover path in response to link failure [Kar 2000; Huang 2002; RFC 3469]. Finally, we note that MPLS can, and has, been used to implement so-called virtual private networks (VPNs). In implementing a VPN for a customer, an ISP uses its MPLS-enabled network to connect together the customers various networks. MPLS can be used to isolate both the resources and addressing used by the customers VPN from that of other users crossing the ISPs network; see [DeClercq 2002] for details"
Explain why any packet network can connect to the Internet,"In the past, the simplicity of the Internets IP protocol was its greatest strength in vanquishing competition and becoming the de facto standard for internetworking. Unlike competitors, such as X.25 in the 1980s and ATM in the 1990s, IP can run on top of any link-layer networking technology, because it offers only a best-effort datagram service. Hence, any packet network can connect to the Internet."," Back then, all of these progressive developments in networking were viewed (I believe) as logical rather than magical. No one could have envisioned the scale of the Internet and power of personal computers today. It was a decade before appearance of the first PCs. To put things in perspective, most students submitted their computer programs as decks of punched cards for batch processing. Only some students had direct access to computers, which were typically housed in a restricted area. Modems were slow and still a rarity. As a graduate student, I had only a phone on my desk, and I used pencil and paper to do most of my work. Where do you see the field of networking and the Internet heading in the future? In the past, the simplicity of the Internets IP protocol was its greatest strength in vanquishing competition and becoming the de facto standard for internetworking. Unlike competitors, such as X.25 in the 1980s and ATM in the 1990s, IP can run on top of any link-layer networking technology, because it offers only a best-effort datagram service. Thus, any packet network can connect to the Internet. Today, IPs greatest strength is actually a shortcoming. IP is like a straitjacket that confines the Internets development to specific directions. In recent years, many researchers have redirected their efforts to the application layer only. There is also a great deal of research on wireless ad hoc networks, sensor networks, and satellite networks. These networks can be viewed either as stand-alone systems or link-layer systems, which can flourish because they are outside of the IP straitjacket"
"Explain why each of the wireless stations is hidden from the other, although neither is hidden from the AP"," However, due to fading, the signal ranges of wireless stations are limited to the interiors of the shaded circles shown in Figure 7.11. Hence, each of the wireless stations is hidden from the other, although neither is hidden from the AP."," Dealing with Hidden Terminals: RTS and CTS The 802.11 MAC protocol also includes a nifty (but optional) reservation scheme that helps avoid collisions even in the presence of hidden terminals. Lets investigate this scheme in the context of Figure 7.11, which shows two wireless stations and one access point. Both of the wireless stations are within range of the AP (whose coverage is shown as a shaded circle) and both have associated with the AP. However, due to fading, the signal ranges of wireless stations are limited to the interiors of the shaded circles shown in Figure 7.11. Thus, each of the wireless stations is hidden from the other, although neither is hidden from the AP. Lets now consider why hidden terminals can be problematic. Suppose Station H1 is transmitting a frame and halfway through H1s transmission, Station H2 wants to send a frame to the AP. H2, not hearing the transmission from H1, will first wait a DIFS interval and then transmit the frame, resulting in a collision. The channel will therefore be wasted during the entire period of H1s transmission as well as during H2s transmission"
Explain how address 3 allows the AP to determine the appropriate destination MAC address when constructing the Ethernet frame," The source address field for this frame is H1s MAC address, and the destination address field is R1s MAC address. Hence, address 3 allows the AP to determine the appropriate destination MAC address when constructing the Ethernet frame."," When the AP receives the 802.11 frame, it converts the frame to an Ethernet frame. The source address field for this frame is H1s MAC address, and the destination address field is R1s MAC address. Thus, address 3 allows the AP to determine the appropriate destination MAC address when constructing the Ethernet frame. In summary, address 3 plays a crucial role for internetworking the BSS with a wired LAN"
"Explain why when H1 moves from BSS1 to BSS2, it may keep its IP address and all of its ongoing TCP connections"," Lets now look at a specific example of mobility between BSSs in the same subnet. Figure 7.15 shows two interconnected BSSs with a host, H1, moving from BSS1 to BSS2. Because in this example the interconnection device that connects the two BSSs is not a router, all of the stations in the two BSSs, including the APs, belong to the same IP subnet. Hence, when H1 moves from BSS1 to BSS2, it may keep its IP address and all of its ongoing TCP connections."," 7.3.4 Mobility in the Same IP Subnet In order to increase the physical range of a wireless LAN, companies and universities will often deploy multiple BSSs within the same IP subnet. This naturally raises the issue of mobility among the BSSshow do wireless stations seamlessly move from one BSS to another while maintaining ongoing TCP sessions? As well see in this subsection, mobility can be handled in a relatively straightforward manner when the BSSs are part of the subnet. When stations move between subnets, more sophisticated mobility management protocols will be needed, such as those well study in Sections 7.5 and 7.6. Lets now look at a specific example of mobility between BSSs in the same subnet. Figure 7.15 shows two interconnected BSSs with a host, H1, moving from BSS1 to BSS2. Because in this example the interconnection device that connects the two BSSs is not a router, all of the stations in the two BSSs, including the APs, belong to the same IP subnet. Thus, when H1 moves from BSS1 to BSS2, it may keep its IP address and all of its ongoing TCP connections. If the interconnection device were a router, then H1 would have to obtain a new IP address in the subnet in which it was moving. This address change would disrupt (and eventually terminate) any on-going TCP connections at H1. In Section 7.6, well see how a network-layer mobility protocol, such as mobile IP, can be used to avoid this problem. But what specifically happens when H1 moves from BSS1 to BSS2? As H1 wanders away from AP1, H1 detects a weakening signal from AP1 and starts to scan for a stronger signal. H1 receives beacon frames from AP2 (which in many corporate and university settings will have the same SSID as AP1). H1 then disassociates with AP1 and associates with AP2, while keeping its IP address and maintaining its ongoing TCP sessions"
"Explain why for a combined FDM/TDM system, if the channel is partitioned into F sub-bands and time is partitioned into T slots, then the channel will be able to support F","The GSM standard for 2G cellular systems uses combined FDM/TDM (radio) for the air interface. Recall from Chapter 1 that, with pure FDM, the channel is partitioned into a number of frequency bands with each band devoted to a call. Also recall from Chapter 1 that, with pure TDM, time is partitioned into frames with each frame further partitioned into slots and each call being assigned the use of a particular slot in the revolving frame. In combined FDM/TDM systems, the channel is partitioned into a number of frequency sub-bands; within each sub-band, time is partitioned into frames and slots. Hence, for a combined FDM/TDM system, if the channel is partitioned into F sub-bands and time is partitioned into T slots, then the channel will be able to support F."," Although Figure 7.18 shows each cell containing one base transceiver station residing in the middle of the cell, many systems today place the BTS at corners where three cells intersect, so that a single BTS with directional antennas can service three cells. The GSM standard for 2G cellular systems uses combined FDM/TDM (radio) for the air interface. Recall from Chapter 1 that, with pure FDM, the channel is partitioned into a number of frequency bands with each band devoted to a call. Also recall from Chapter 1 that, with pure TDM, time is partitioned into frames with each frame further partitioned into slots and each call being assigned the use of a particular slot in the revolving frame. In combined FDM/TDM systems, the channel is partitioned into a number of frequency sub-bands; within each sub-band, time is partitioned into frames and slots. Thus, for a combined FDM/TDM system, if the channel is partitioned into F sub-bands and time is partitioned into T slots, then the channel will be able to support F.T simultaneous calls. Recall that we saw in Section 6.3.4 that cable access networks also use a combined FDM/TDM approach. GSM systems consist of 200-kHz frequency bands with each band supporting eight TDM calls. GSM encodes speech at 13 kbps and 12.2 kbps. A GSM networks base station controller (BSC) will typically service several tens of base transceiver stations. The role of the BSC is to allocate BTS radio channels to mobile subscribers, perform paging (finding the cell in which a mobile user is resident), and perform handoff of mobile usersa topic well cover shortly in Section 7.7.2. The base station controller and its controlled base transceiver stations collectively constitute a GSM base station subsystem (BSS)"
"Explain how while 3G cellular voice and cellular data services use different core networks, they share a common first/last-hop radio access network","The 3G radio access network is the wireless first-hop network that we see as a 3G user. The Radio Network Controller (RNC) typically controls several cell base transceiver stations similar to the base stations that we encountered in 2G systems (but officially known in 3G UMTS parlance as a Node Bsa rather non-descriptive name!). Each cells wireless link operates between the mobile nodes and a base transceiver station, just as in 2G networks. The RNC connects to both the circuitswitched cellular voice network via an MSC, and to the packet-switched Internet via an SGSN. Hence, while 3G cellular voice and cellular data services use different core networks, they share a common first/last-hop radio access network."," There are two types of nodes in the 3G core network: Serving GPRS Support Nodes (SGSNs) and Gateway GPRS Support Nodes (GGSNs). (GPRS stands for Generalized Packet Radio Service, an early cellular data service in 2G networks; here we discuss the evolved version of GPRS in 3G networks). An SGSN is responsible for delivering datagrams to/from the mobile nodes in the radio access network to which the SGSN is attached. The SGSN interacts with the cellular voice networks MSC for that area, providing user authorization and handoff, maintaining location (cell) information about active mobile nodes, and performing datagram forwarding between mobile nodes in the radio access network and a GGSN. The GGSN acts as a gateway, connecting multiple SGSNs into the larger Internet. A GGSN is thus the last piece of 3G infrastructure that a datagram originating at a mobile node encounters before entering the larger Internet. To the outside world, the GGSN looks like any other gateway router; the mobility of the 3G nodes within the GGSNs network is hidden from the outside world behind the GGSN. 3G Radio Access Network: The Wireless Edge The 3G radio access network is the wireless first-hop network that we see as a 3G user. The Radio Network Controller (RNC) typically controls several cell base transceiver stations similar to the base stations that we encountered in 2G systems (but officially known in 3G UMTS parlance as a Node Bsa rather non-descriptive name!). Each cells wireless link operates between the mobile nodes and a base transceiver station, just as in 2G networks. The RNC connects to both the circuitswitched cellular voice network via an MSC, and to the packet-switched Internet via an SGSN. Thus, while 3G cellular voice and cellular data services use different core networks, they share a common first/last-hop radio access network. A significant change in 3G UMTS over 2G networks is that rather than using GSMs FDMA/TDMA scheme, UMTS uses a CDMA technique known as Direct Sequence Wideband CDMA (DS-WCDMA) [Dahlman 1998] within TDMA slots; TDMA slots, in turn, are available on multiple frequenciesan interesting use of all three dedicated channel-sharing approaches that we earlier identified in Chapter 6 and similar to the approach taken in wired cable access networks (see Section 6.3.4). This change requires a new 3G cellular wireless-access network operating in parallel with the 2G BSS radio network shown in Figure 7.19. The data service associated with the WCDMA specification is known as HSPA (High Speed Packet Access) and promises downlink data rates of up to 14 Mbps. Details regarding 3G networks can be found at the 3rd Generation Partnership Project (3GPP) Web site [3GPP 2016]"
"Explain why updating the COA at the home agent, while necessary, will not be enough to solve the problem of routing data to the mobile nodes new foreign network"," However, with direct routing, the home agent is queried for the COA by the correspondent agent only once, at the beginning of the session. Hence, updating the COA at the home agent, while necessary, will not be enough to solve the problem of routing data to the mobile nodes new foreign network."," When the mobile node moves from one foreign network to another, how will data now be forwarded to the new foreign network? In the case of indirect routing, this problem was easily solved by updating the COA maintained by the home agent. However, with direct routing, the home agent is queried for the COA by the correspondent agent only once, at the beginning of the session. Thus, updating the COA at the home agent, while necessary, will not be enough to solve the problem of routing data to the mobile nodes new foreign network. One solution would be to create a new protocol to notify the correspondent of the changing COA. An alternate solution, and one that well see adopted in practice Home network: 128.119.40/24 Visited network: 79.129.13/24 Mobile node Permanent address: Permanent address: Home agent Wide area network Foreign agent Care-of address: Correspondent agent Correspondent Key: Control messages Data flow in GSM networks, works as follows. Suppose data is currently being forwarded to the mobile node in the foreign network where the mobile node was located when the session first started (step 1 in Figure 7.27). Well identify the foreign agent in that foreign network where the mobile node was first found as the anchor foreign agent. When the mobile node moves to a new foreign network (step 2 in new foreign agent provides the anchor foreign agent with the mobile nodes new COA (step 4). When the anchor foreign agent receives an encapsulated datagram for a departed mobile node, it can then re-encapsulate the datagram and forward it to the mobile node (step 5) using the new COA. If the mobile node later moves yet again to a new foreign network, the foreign agent in that new visited network would then contact the anchor foreign agent in order to set up forwarding to this new foreign network"
"Explain why at all times there are at most three MSCs (the home MSC, the anchor MSC, and the visited MSC) between the correspondent and the mobile","MSC, and then from the anchor MSC to the visited MSC where the mobile is currently located. When a mobile moves from the coverage area of one MSC to another, the ongoing call is rerouted from the anchor MSC to the new visited MSC containing the new base station. Hence, at all times there are at most three MSCs (the home MSC, the anchor MSC, and the visited MSC) between the correspondent and the mobile."," Mobile station roaming number (MSRN) or simply roaming number Routable address for telephone call segment between home MSC and visited MSC, visible to neither the mobile nor the correspondent. Table 7.2 Commonalities between mobile IP and GSM mobility transfers performed by the mobile, the call is routed from the home MSC to the anchor MSC, and then from the anchor MSC to the visited MSC where the mobile is currently located. When a mobile moves from the coverage area of one MSC to another, the ongoing call is rerouted from the anchor MSC to the new visited MSC containing the new base station. Thus, at all times there are at most three MSCs (the home MSC, the anchor MSC, and the visited MSC) between the correspondent and the mobile. Rather than maintaining a single MSC hop from the anchor MSC to the current MSC, an alternative approach would have been to simply chain the MSCs visited by the mobile, having an old MSC forward the ongoing call to the new MSC each time the mobile moves to a new MSC. Such MSC chaining can in fact occur in IS-41 cellular networks, with an optional path minimization step to remove MSCs between the anchor MSC and the current visited MSC [Lin 2001]"
"Explain why the same letter, appearing in different positions in the plaintext message, might be encoded differently"," Five hundred years ago, techniques improving on monoalphabetic encryption, known as polyalphabetic encryption, were invented. The idea behind polyalphabetic encryption is to use multiple monoalphabetic ciphers, with a specific Plaintext letter: C1(k = 5): C2(k = 19): a b c d e f g h i j k l m n o p q r s t u v w x y z f g h i j k l m n o p q r s t u v w x y z a b c d e t u v w x y z a b c d e f g h i j k l m n o p q r s monoalphabetic cipher to encode a letter in a specific position in the plaintext message. Hence, the same letter, appearing in different positions in the plaintext message, might be encoded differently."," Chosen-plaintext attack. In a chosen-plaintext attack, the intruder is able to choose the plaintext message and obtain its corresponding ciphertext form. For the simple encryption algorithms weve seen so far, if Trudy could get Alice to send the message, The quick brown fox jumps over the lazy dog, she could completely break the encryption scheme. Well see shortly that for more sophisticated encryption techniques, a chosen-plaintext attack does not necessarily mean that the encryption technique can be broken. Five hundred years ago, techniques improving on monoalphabetic encryption, known as polyalphabetic encryption, were invented. The idea behind polyalphabetic encryption is to use multiple monoalphabetic ciphers, with a specific Plaintext letter: C1(k = 5): C2(k = 19): a b c d e f g h i j k l m n o p q r s t u v w x y z f g h i j k l m n o p q r s t u v w x y z a b c d e t u v w x y z a b c d e f g h i j k l m n o p q r s monoalphabetic cipher to encode a letter in a specific position in the plaintext message. Thus, the same letter, appearing in different positions in the plaintext message, might be encoded differently. An example of a polyalphabetic encryption scheme is shown in Figure 8.4. It has two Caesar ciphers (with k = 5 and k = 19), shown as rows. We might choose to use these two Caesar ciphers, C1 and C2, in the repeating pattern C1, C2, C2, C1, C2. That is, the first letter of plaintext is to be encoded using C1, the second and third using C2, the fourth using C1, and the fifth using C2. The pattern then repeats, with the sixth letter being encoded using C1, the seventh with C2, and so on. The plaintext message bob, i love you. is thus encrypted ghu, n etox dhz. Note that the first b in the plaintext message is encrypted using C1, while the second b is encrypted using C2. In this example, the encryption and decryption key is the knowledge of the two Caesar keys (k = 5, k = 19) and the pattern C1, C2, C2, C1, C2. Block Ciphers Let us now move forward to modern times and examine how symmetric key encryption is done today. There are two broad classes of symmetric encryption techniques: stream ciphers and block ciphers. Well briefly examine stream ciphers in Section 8.7 when we investigate security for wireless LANs. In this section, we focus on block ciphers, which are used in many secure Internet protocols, including PGP (for secure e-mail), SSL (for securing TCP connections), and IPsec (for securing the network-layer transport)"
"Explain why a full-table block cipher, providing predetermined mappings between all inputs and outputs (as in the example above), is simply out of the question","Although full-table block ciphers, as just described, with moderate values of k can produce robust symmetric key encryption schemes, they are unfortunately difficult to implement. For k = 64 and for a given mapping, Alice and Bob would need to maintain a table with 264 input values, which is an infeasible task. Moreover, if Alice and Bob were to change keys, they would have to each regenerate the table. Hence, a full-table block cipher, providing predetermined mappings between all inputs and outputs (as in the example above), is simply out of the question."," The brute-force attack for this cipher is to try to decrypt ciphtertext by using all mappings. With only 40,320 mappings (when k = 3), this can quickly be accomplished on a desktop PC. To thwart brute-force attacks, block ciphers typically use much larger blocks, consisting of k = 64 bits or even larger. Note that the number of possible mappings for a general k-block cipher is 2k!, which is astronomical for even moderate values of k (such as k = 64). Although full-table block ciphers, as just described, with moderate values of k can produce robust symmetric key encryption schemes, they are unfortunately difficult to implement. For k = 64 and for a given mapping, Alice and Bob would need to maintain a table with 264 input values, which is an infeasible task. Moreover, if Alice and Bob were to change keys, they would have to each regenerate the table. Thus, a full-table block cipher, providing predetermined mappings between all inputs and outputs (as in the example above), is simply out of the question. Instead, block ciphers typically use functions that simulate randomly permuted tables. An example (adapted from [Kaufman 1995]) of such a function for k = 64 bits is shown in Figure 8.5. The function first breaks a 64-bit block into 8 chunks, with each chunk consisting of 8 bits. Each 8-bit chunk is processed by an 8-bit to 8-bit table, which is of manageable size. For example, the first chunk is processed by the table denoted by T1. Next, the 8 output chunks are reassembled into a 64-bit block. The positions of the 64 bits in the block are then scrambled (permuted) to produce a 64-bit output. This output is fed back to the 64-bit input, where another cycle begins. After n such cycles, the function provides a 64-bit block of ciphertext"
"Explain why when encrypting a message with RSA, it is equivalent to encrypting the unique integer number that represents the message","Now suppose that Alice wants to send to Bob an RSA-encrypted message, as shown in Figure 8.6. In our discussion of RSA, lets always keep in mind that a message is nothing but a bit pattern, and every bit pattern can be uniquely represented by an integer number (along with the length of the bit pattern). For example, suppose a message is the bit pattern 1001; this message can be represented by the decimal integer 9. Hence, when encrypting a message with RSA, it is equivalent to encrypting the unique integer number that represents the message."," So lets briefly review modular arithmetic. Recall that x mod n simply means the remainder of x when divided by n; so, for example, 19 mod 5 = 4. In modular arithmetic, one performs the usual operations of addition, multiplication, and exponentiation. However, the result of each operation is replaced by the integer remainder that is left when the result is divided by n. Adding and multiplying with modular arithmetic is facilitated with the following handy facts: [(a mod n) + (b mod n)] mod n = (a + b) mod n [(a mod n) - (b mod n)] mod n = (a - b) mod n [(a mod n) # (b mod n)] mod n = (a # b) mod n It follows from the third fact that (a mod n)d mod n = ad mod n, which is an identity that we will soon find very useful. Now suppose that Alice wants to send to Bob an RSA-encrypted message, as shown in Figure 8.6. In our discussion of RSA, lets always keep in mind that a message is nothing but a bit pattern, and every bit pattern can be uniquely represented by an integer number (along with the length of the bit pattern). For example, suppose a message is the bit pattern 1001; this message can be represented by the decimal integer 9. Thus, when encrypting a message with RSA, it is equivalent to encrypting the unique integer number that represents the message. There are two interrelated components of RSA: The choice of the public key and the private key The encryption and decryption algorithm To generate the public and private RSA keys, Bob performs the following steps: 1. Choose two large prime numbers, p and q. How large should p and q be? The larger the values, the more difficult it is to break RSA, but the longer it takes to perform the encoding and decoding. RSA Laboratories recommends that the product of p and q be on the order of 1,024 bits. For a discussion of how to find large prime numbers, see [Caldwell 2012]"
"Explain why a digital signature is a heavier technique, since it requires an underlying Public Key Infrastructure (PKI) with certification authorities as described below"," To create a digital signature, we first take the hash of the message and then encrypt the message with our private key (using public key cryptography). Hence, a digital signature is a heavier technique, since it requires an underlying Public Key Infrastructure (PKI) with certification authorities as described below."," Fixed-length hash Many-to-one hash function Opgmdvboijrtnsd gghPPdogm;lcvkb Package to send to Alice Signed hash Fgkopdgoo69cmxw 54psdterma[asofmz Encryption algorithm MACs start with a message (or a document). To create a MAC out of the message, we append an authentication key to the message, and then take the hash of the result. Note that neither public key nor symmetric key encryption is involved in creating the MAC. To create a digital signature, we first take the hash of the message and then encrypt the message with our private key (using public key cryptography). Thus, a digital signature is a heavier technique, since it requires an underlying Public Key Infrastructure (PKI) with certification authorities as described below. Well see in Section 8.4 that PGPa popular secure e-mail systemuses digital signatures for message integrity. Weve seen already that OSPF uses MACs for message integrity. Well see in Sections 8.5 and 8.6 that MACs are also used for popular transport-layer and network-layer security protocols"
Explain why there is a need for security functionality at higher layers as well as blanket coverage at lower layers,"You might be wondering why security functionality is being provided at more than one layer in the Internet. Wouldnt it suffice simply to provide the security functionality at the network layer and be done with it? There are two answers to this question. First, although security at the network layer can offer blanket coverage by encrypting all the data in the datagrams (that is, all the transport-layer segments) and by authenticating all the source IP addresses, it cant provide user-level security. For example, a commerce site cannot rely on IP-layer security to authenticate a customer who is purchasing goods at the commerce site. Hence, there is a need for security functionality at higher layers as well as blanket coverage at lower layers."," In Sections 8.5 through 8.8, we examine how security tools are being used in the application, transport, network, and link layers. Being consistent with the general structure of this book, we begin at the top of the protocol stack and discuss security at the application layer. Our approach is to use a specific application, e-mail, as a case study for application-layer security. We then move down the protocol stack. Well examine the SSL protocol (which provides security at the transport layer), IPsec (which provides security at the network layer), and the security of the IEEE 802.11 wireless LAN protocol. You might be wondering why security functionality is being provided at more than one layer in the Internet. Wouldnt it suffice simply to provide the security functionality at the network layer and be done with it? There are two answers to this question. First, although security at the network layer can offer blanket coverage by encrypting all the data in the datagrams (that is, all the transport-layer segments) and by authenticating all the source IP addresses, it cant provide user-level security. For example, a commerce site cannot rely on IP-layer security to authenticate a customer who is purchasing goods at the commerce site. Thus, there is a need for security functionality at higher layers as well as blanket coverage at lower layers. Second, it is generally easier to deploy new Internet services, including security services, at the higher layers of the protocol stack. While waiting for security to be broadly deployed at the network layer, which is probably still many years in the future, many application developers just do it and introduce security functionality into their favorite applications. A classic example is Pretty Good Privacy (PGP), which provides secure e-mail (discussed later in this section). Requiring only client and server application code, PGP was one of the first security technologies to be broadly used in the Internet. 8.5.1 Secure E-Mail We now use the cryptographic principles of Sections 8.2 through 8.3 to create a secure e-mail system. We create this high-level design in an incremental manner, at each step introducing new security services. When designing a secure e-mail system, let us keep in mind the racy example introduced in Section 8.1the love affair between Alice and Bob. Imagine that Alice wants to send an e-mail message to Bob, and Trudy wants to intrude"
Explain why the gateway router (and the laptops) will emit into the Internet both vanilla IPv4 datagrams and secured IPsec datagrams,"To answer this question, note that there are two SAs between the headquarters gateway router and the branch-office gateway router (one in each direction); for each salespersons laptop, there are two SAs between the headquarters gateway router and the laptop (again, one in each direction). So, in total, there are (2 + 2n) SAs. Keep in mind, however, that not all traffic sent into the Internet by the gateway routers or by the laptops will be IPsec secured. For example, a host in headquarters may want to access a Web server (such as Amazon or Google) in the public Internet. Hence, the gateway router (and the laptops) will emit into the Internet both vanilla IPv4 datagrams and secured IPsec datagrams."," 8.7.3 Security Associations IPsec datagrams are sent between pairs of network entities, such as between two hosts, between two routers, or between a host and router. Before sending IPsec datagrams from source entity to destination entity, the source and destination entities create a network-layer logical connection. This logical connection is called a security association (SA). An SA is a simplex logical connection; that is, it is unidirectional from source to destination. If both entities want to send secure datagrams to each other, then two SAs (that is, two logical connections) need to be established, one in each direction. For example, consider once again the institutional VPN in Figure 8.27. This institution consists of a headquarters office, a branch office and, say, n traveling salespersons. For the sake of example, lets suppose that there is bi-directional IPsec traffic between headquarters and the branch office and bi-directional IPsec traffic between headquarters and the salespersons. In this VPN, how many SAs are there? To answer this question, note that there are two SAs between the headquarters gateway router and the branch-office gateway router (one in each direction); for each salespersons laptop, there are two SAs between the headquarters gateway router and the laptop (again, one in each direction). So, in total, there are (2 + 2n) SAs. Keep in mind, however, that not all traffic sent into the Internet by the gateway routers or by the laptops will be IPsec secured. For example, a host in headquarters may want to access a Web server (such as Amazon or Google) in the public Internet. Thus, the gateway router (and the laptops) will emit into the Internet both vanilla IPv4 datagrams and secured IPsec datagrams. Headquarters Branch Office Internet SA 172.16.2/24 Lets now take a look inside an SA. To make the discussion tangible and concrete, lets do this in the context of an SA from router R1 to router R2 in Figure 8.28. (You can think of Router R1 as the headquarters gateway router and Router R2 as the branch office gateway router from Figure 8.27.) Router R1 will maintain state information about this SA, which will include: A 32-bit identifier for the SA, called the Security Parameter Index (SPI) The origin interface of the SA (in this case 200.168.1.100) and the destination interface of the SA (in this case 193.68.2.23) The type of encryption to be used (for example, 3DES with CBC) The encryption key The type of integrity check (for example, HMAC with MD5) The authentication key Whenever router R1 needs to construct an IPsec datagram for forwarding over this SA, it accesses this state information to determine how it should authenticate and encrypt the datagram. Similarly, router R2 will maintain the same state information for this SA and will use this information to authenticate and decrypt any IPsec datagram that arrives from the SA"
"Explain how the Telnet application gateway not only performs user authorization but also acts as a Telnet server and a Telnet client, relaying information between the user and the remote Telnet server","Consider now an internal user who wants to Telnet to the outside world. The user must first set up a Telnet session with the application gateway. An application running in the gateway, which listens for incoming Telnet sessions, prompts the user for a user ID and password. When the user supplies this information, the application gateway checks to see if the user has permission to Telnet to the outside world. If not, the Telnet connection from the internal user to the gateway is terminated by the gateway. If the user has permission, then the gateway (1) prompts the user for the host name of the external host to which the user wants to connect, (2) sets up a Telnet session between the gateway and the external host, and (3) relays to the external host all data arriving from the user, and relays to the user all data arriving from the external host. Hence, the Telnet application gateway not only performs user authorization but also acts as a Telnet server and a Telnet client, relaying information between the user and the remote Telnet server."," To get some insight into application gateways, lets design a firewall that allows only a restricted set of internal users to Telnet outside and prevents all external clients from Telneting inside. Such a policy can be accomplished by implementing Host-to-gateway Telnet session Gateway-to-remote host Telnet session Application gateway Router and filter a combination of a packet filter (in a router) and a Telnet application gateway, as shown in Figure 8.34. The routers filter is configured to block all Telnet connections except those that originate from the IP address of the application gateway. Such a filter configuration forces all outbound Telnet connections to pass through the application gateway. Consider now an internal user who wants to Telnet to the outside world. The user must first set up a Telnet session with the application gateway. An application running in the gateway, which listens for incoming Telnet sessions, prompts the user for a user ID and password. When the user supplies this information, the application gateway checks to see if the user has permission to Telnet to the outside world. If not, the Telnet connection from the internal user to the gateway is terminated by the gateway. If the user has permission, then the gateway (1) prompts the user for the host name of the external host to which the user wants to connect, (2) sets up a Telnet session between the gateway and the external host, and (3) relays to the external host all data arriving from the user, and relays to the user all data arriving from the external host. Thus, the Telnet application gateway not only performs user authorization but also acts as a Telnet server and a Telnet client, relaying information between the user and the remote Telnet server. Note that the filter will permit step 2 because the gateway initiates the Telnet connection to the outside world. CASE HISTORY ANONYMIT Y AND P R I VA C Y Suppose you want to visit a controversial Web site (for example, a political activist site) and you (1) dont want to reveal your IP address to the Web site, (2) dont want your local ISP (which may be your home or office ISP) to know that you are visiting the site, and (3) dont want your local ISP to see the data you are exchanging with the site. If you use the traditional approach of connecting directly to the Web site without any encryption, you fail on all three counts. Even if you use SSL, you fail on the first two counts: Your source IP address is presented to the Web site in every datagram you send; and the destination address of every packet you send can easily be sniffed by your local ISP"
"Explain why it is critical for BitTorrent to have a mechanism that allows a peer to verify the integrity of a block, so that it doesnt redistribute bogus blocks","Without any protection, an attacker can easily wreak havoc in a torrent by masquerading as a benevolent peer and sending bogus blocks to a small subset of peers in the torrent. These unsuspecting peers then redistribute the bogus blocks to other peers, which in turn redistribute the bogus blocks to even more peers. Hence, it is critical for BitTorrent to have a mechanism that allows a peer to verify the integrity of a block, so that it doesnt redistribute bogus blocks."," Is there any way around this? In the BitTorrent P2P file distribution protocol (see Chapter 2), the seed breaks the file into blocks, and the peers redistribute the blocks to each other. Without any protection, an attacker can easily wreak havoc in a torrent by masquerading as a benevolent peer and sending bogus blocks to a small subset of peers in the torrent. These unsuspecting peers then redistribute the bogus blocks to other peers, which in turn redistribute the bogus blocks to even more peers. Thus, it is critical for BitTorrent to have a mechanism that allows a peer to verify the integrity of a block, so that it doesnt redistribute bogus blocks. Assume that when a peer joins a torrent, it initially gets a .torrent file from a fully trusted source. Describe a simple scheme that allows peers to verify the integrity of blocks. Solving factorization in polynomial time implies breaking the RSA cryptosystem"
"Explain why when the available rate in the network is less than the video rate, playout will alternate between periods of continuous playout and periods of freezing"," Now lets determine tf , the point in time when the client application buffer becomes full. We first observe that if x 6 r (that is, if the server send rate is less than the video consumption rate), then the client buffer will never become full! Indeed, starting at time tp, the buffer will be depleted at rate r and will only be filled at rate x 6 r. Eventually the client buffer will empty out entirely, at which time the video will freeze on the screen while the client buffer waits another tp seconds to build up Q bits of video. Hence, when the available rate in the network is less than the video rate, playout will alternate between periods of continuous playout and periods of freezing."," Lets assume that the server sends bits at a constant rate x whenever the client buffer is not full. (This is a gross simplification, since TCPs send rate varies due to congestion control; well examine more realistic time-dependent rates x (t) in the problems at the end of this chapter.) Suppose at time t = 0, the application buffer is empty and video begins arriving to the client application buffer. We now ask at what time t = tp does playout begin? And while we are at it, at what time t = tf does the client application buffer become full? First, lets determine tp, the time when Q bits have entered the application buffer and playout begins. Recall that bits arrive to the client application buffer at rate x and no bits are removed from this buffer before playout begins. Thus, the amount of time required to build up Q bits (the initial buffering delay) is tp = Q/x. Now lets determine tf , the point in time when the client application buffer becomes full. We first observe that if x 6 r (that is, if the server send rate is less than the video consumption rate), then the client buffer will never become full! Indeed, starting at time tp, the buffer will be depleted at rate r and will only be filled at rate x 6 r. Eventually the client buffer will empty out entirely, at which time the video will freeze on the screen while the client buffer waits another tp seconds to build up Q bits of video. Thus, when the available rate in the network is less than the video rate, playout will alternate between periods of continuous playout and periods of freezing. In a homework problem, you will be asked to determine the length of each continuous playout and freezing period as a function of Q, r, and x. Now lets determine tf for when x 7 r. In this case, starting at time tp, the buffer increases from Q to B at rate x - r since bits are being depleted at rate r but are arriving at rate x, as shown in Figure 9.3. Given these hints, you will be asked in a homework problem to determine tf , the time the client buffer becomes full. Note that when the available rate in the network is more than the video rate, after the initial buffering delay, the user will enjoy continuous playout until the video ends. Early Termination and Repositioning the Video HTTP streaming systems often make use of the HTTP byte-range header in the HTTP GET request message, which specifies the specific range of bytes the client currently wants to retrieve from the desired video. This is particularly useful when the user wants to reposition (that is, jump) to a future point in time in the video. When the user repositions to a new position, the client sends a new HTTP request, indicating with the byte-range header from which byte in the file should the server send data. When the server receives the new HTTP request, it can forget about any earlier request and instead send bytes beginning with the byte indicated in the byte-range request"
Explain how packets that are delayed by more than the threshold are effectively lost,"For real-time conversational applications, such as VoIP, end-to-end delays smaller than 150 msecs are not perceived by a human listener; delays between 150 and 400 msecs can be acceptable but are not ideal; and delays exceeding 400 msecs can seriously hinder the interactivity in voice conversations. The receiving side of a VoIP application will typically disregard any packets that are delayed more than a certain threshold, for example, more than 400 msecs. Hence, packets that are delayed by more than the threshold are effectively lost."," End-to-End Delay End-to-end delay is the accumulation of transmission, processing, and queuing delays in routers; propagation delays in links; and end-system processing delays. For real-time conversational applications, such as VoIP, end-to-end delays smaller than 150 msecs are not perceived by a human listener; delays between 150 and 400 msecs can be acceptable but are not ideal; and delays exceeding 400 msecs can seriously hinder the interactivity in voice conversations. The receiving side of a VoIP application will typically disregard any packets that are delayed more than a certain threshold, for example, more than 400 msecs. Thus, packets that are delayed by more than the threshold are effectively lost. Packet Jitter A crucial component of end-to-end delay is the varying queuing delays that a packet experiences in the networks routers. Because of these varying delays, the time from when a packet is generated at the source until it is received at the receiver can fluctuate from packet to packet, as shown in Figure 9.1. This phenomenon is called jitter. As an example, consider two consecutive packets in our VoIP application"
"Explain why the token-generation rate, r, serves to limit the long-term average rate at which packets can enter the network","Because there can be at most b tokens in the bucket, the maximum burst size for a leaky-bucket-policed flow is b packets. Furthermore, because the token generation rate is r, the maximum number of packets that can enter the network of any interval of time of length t is rt + b. Hence, the token-generation rate, r, serves to limit the long-term average rate at which packets can enter the network."," Suppose that before a packet is transmitted into the network, it must first remove a token from the token bucket. If the token bucket is empty, the packet must wait for r tokens/sec Bucket holds up to b tokens Token wait area To network Remove token a token. (An alternative is for the packet to be dropped, although we will not consider that option here.) Let us now consider how this behavior polices a traffic flow. Because there can be at most b tokens in the bucket, the maximum burst size for a leaky-bucket-policed flow is b packets. Furthermore, because the token generation rate is r, the maximum number of packets that can enter the network of any interval of time of length t is rt + b. Thus, the token-generation rate, r, serves to limit the long-term average rate at which packets can enter the network. It is also possible to use leaky buckets (specifically, two leaky buckets in series) to police a flows peak rate in addition to the long-term average rate; see the homework problems at the end of this chapter. Leaky Bucket Weighted Fair Queuing Provable Maximum Delay in a Queue Lets close our discussion on policing by showing how the leaky bucket and WFQ can be combined to provide a bound on the delay through a routers queue. (Readers who have forgotten about WFQ are encouraged to review WFQ, which is covered in Section 4.2.) Lets consider a routers output link that multiplexes n flows, each policed by a leaky bucket with parameters bi and ri, i = 1, . . . , n, using WFQ scheduling. We use the term flow here loosely to refer to the set of packets that are not distinguished from each other by the scheduler. In practice, a flow might be comprised of traffic from a single end-to-end connection or a collection of many such connections, see Figure 9.15"
Explain why networking researchers have to be familiar with subjects well beyond protocols and routing algorithms,"Networking bridges disciplines. It draws from electrical engineering, all aspects of computer science, operations research, statistics, economics, and other disciplines. Hence, networking researchers have to be familiar with subjects well beyond protocols and routing algorithms."," Beyond multimedia signaling, SIP offers a new service that has been missing in the Internet: event notification. We have approximated such services with HTTP kludges and e-mail, but this was never very satisfactory. Since events are a common abstraction for distributed systems, this may simplify the construction of new services. Do you have any advice for students entering the networking field? Networking bridges disciplines. It draws from electrical engineering, all aspects of computer science, operations research, statistics, economics, and other disciplines. Thus, networking researchers have to be familiar with subjects well beyond protocols and routing algorithms. Given that networks are becoming such an important part of everyday life, students wanting to make a difference in the field should think of the new resource constraints in networks: human time and effort, rather than just bandwidth or storage"
"Explain why, the proper management of disk storage is of central importance to a computer system, as we discuss in Chapter 10","The most common secondary-storage device is a magnetic disk, which provides storage for both programs and data. Most programs (system and application) are stored on a disk until they are loaded into memory. Many programs then use the disk as both the source and the destination of their processing. Hence,  the proper management of disk storage is of central importance to a computer system, as we discuss in Chapter 10."," Thus, most computer systems provide secondary storage as an extension of main memory. The main requirement for secondary storage is that it be able to hold large quantities of data permanently. The most common secondary-storage device is a magnetic disk, which provides storage for both programs and data. Most programs (system and application) are stored on a disk until they are loaded into memory. Many programs then use the disk as both the source and the destination of their processing. Hence, the proper management of disk storage is of central importance to a computer system, as we discuss in Chapter 10. In a larger sense, however, the storage structure that we have described consisting of registers, main memory, and magnetic disksis only one of many possible storage systems. Others include cache memory, CD-ROM, magnetic tapes, and so on. Each storage system provides the basic functions of storing a datum and holding that datum until it is retrieved at a later time. The main differences among the various storage systems lie in speed, cost, size, and volatility"
"Explain why, the proper management of disk storage is of central importance to a computer system","As we have already seen, because main memory is too small to accommodate all data and programs, and because the data that it holds are lost when power is lost, the computer system must provide secondary storage to back up main memory. Most modern computer systems use disks as the principal on-line storage medium for both programs and data. Most programsincluding compilers, assemblers, word processors, editors, and formattersare stored on a disk until loaded into memory. They then use the disk as both the source and destination of their processing. Hence,  the proper management of disk storage is of central importance to a computer system."," The operating system is responsible for the following activities in connection with le management: 1.8 Storage Management Creating and deleting directories to organize les Supporting primitives for manipulating les and directories Mapping les onto secondary storage Backing up les on stable (nonvolatile) storage media File-management techniques are discussed in Chapters 11 and 12. As we have already seen, because main memory is too small to accommodate all data and programs, and because the data that it holds are lost when power is lost, the computer system must provide secondary storage to back up main memory. Most modern computer systems use disks as the principal on-line storage medium for both programs and data. Most programsincluding compilers, assemblers, word processors, editors, and formattersare stored on a disk until loaded into memory. They then use the disk as both the source and destination of their processing. Hence, the proper management of disk storage is of central importance to a computer system. The operating system is responsible for the following activities in connection with disk management: Free-space management Storage allocation Disk scheduling Because secondary storage is used frequently, it must be used efciently. The entire speed of operation of a computer may hinge on the speeds of the disk subsystem and the algorithms that manipulate that subsystem. There are, however, many uses for storage that is slower and lower in cost (and sometimes of higher capacity) than secondary storage. Backups of disk data, storage of seldom-used data, and long-term archival storage are some examples. Magnetic tape drives and their tapes and CD and DVD drives and platters are typical tertiary storage devices. The media (tapes and optical platters) vary between WORM (write-once, read-many-times) and RW (read write) formats"
"Explain how, each layer hides the existence of certain data structures, operations, and hardware from higher-level layers","Each layer is implemented only with operations provided by lower-level layers. A layer does not need to know how these operations are implemented; it needs to know only what these operations do. Hence,  each layer hides the existence of certain data structures, operations, and hardware from higher-level layers."," The main advantage of the layered approach is simplicity of construction and debugging. The layers are selected so that each uses functions (operations) and services of only lower-level layers. This approach simplies debugging and system verication. The rst layer can be debugged without any concern for the rest of the system, because, by denition, it uses only the basic hardware (which is assumed correct) to implement its functions. Once the rst layer is debugged, its correct functioning can be assumed while the second layer is debugged, and so on. If an error is found during the debugging of a particular layer, the error must be on that layer, because the layers below it are already debugged. Thus, the design and implementation of the system are simplied. Each layer is implemented only with operations provided by lower-level layers. A layer does not need to know how these operations are implemented; it needs to know only what these operations do. Hence, each layer hides the existence of certain data structures, operations, and hardware from higher-level layers. The major difculty with the layered approach involves appropriately dening the various layers. Because a layer can use only lower-level layers, careful planning is necessary. For example, the device driver for the backing store (disk space used by virtual-memory algorithms) must be at a lower level than the memory-management routines, because memory management requires the ability to use the backing store"
"Explain why, the I in RAID, which once stood for inexpensive, now stands for independent","RAID storage can be structured in a variety of ways. For example, a system can have disks directly attached to its buses. In this case, the operating system or system software can implement RAID functionality. Alternatively, an intelligent host controller can control multiple attached disks and can implement RAID on those disks in hardware. Finally, a storage array, or RAID array, can be used. A RAID array is a standalone unit with its own controller, cache (usually), and disks. It is attached to the host via one or more standard controllers (for example, FC). This common setup allows an operating system or software without RAID functionality to have RAID-protected disks. It is even used on systems that do have RAID software layers because of its simplicity and exibility. their higher reliability and higher data-transfer rate, rather than for economic reasons. Hence,  the I in RAID, which once stood for inexpensive, now stands for independent."," In the past, RAIDs composed of small, cheap disks were viewed as a cost-effective alternative to large, expensive disks. Today, RAIDs are used for STRUCTURING RAID RAID storage can be structured in a variety of ways. For example, a system can have disks directly attached to its buses. In this case, the operating system or system software can implement RAID functionality. Alternatively, an intelligent host controller can control multiple attached disks and can implement RAID on those disks in hardware. Finally, a storage array, or RAID array, can be used. A RAID array is a standalone unit with its own controller, cache (usually), and disks. It is attached to the host via one or more standard controllers (for example, FC). This common setup allows an operating system or software without RAID functionality to have RAID-protected disks. It is even used on systems that do have RAID software layers because of its simplicity and exibility. their higher reliability and higher data-transfer rate, rather than for economic reasons. Hence, the I in RAID, which once stood for inexpensive, now stands for independent. Lets rst consider the reliability of RAIDs. The chance that some disk out of a set of N disks will fail is much higher than the chance that a specic single disk will fail. Suppose that the mean time to failure of a single disk is 100,000 hours. Then the mean time to failure of some disk in an array of 100 disks will be 100,000/100 = 1,000 hours, or 41.66 days, which is not long at all! If we store only one copy of the data, then each disk failure will result in loss of a signicant amount of dataand such a high rate of data loss is unacceptable"
"Explain why, replicating the mapping makes a simple yet consistent update of this information impossible","To enhance the availability of the crucial mapping information, we can use replication, local caching, or both. As we noted, location independence means that the mapping changes over time. Hence,  replicating the mapping makes a simple yet consistent update of this information impossible."," 17.9.1.3 Implementation Techniques Implementation of transparent naming requires a provision for the mapping of a le name to the associated location. To keep this mapping manageable, we must aggregate sets of les into component units and provide the mapping on a component-unit basis rather than on a single-le basis. This aggregation serves administrative purposes as well. UNIX-like systems use the hierarchical directory tree to provide name-to-location mapping and to aggregate les recursively into directories. To enhance the availability of the crucial mapping information, we can use replication, local caching, or both. As we noted, location independence means that the mapping changes over time. Hence, replicating the mapping makes a simple yet consistent update of this information impossible. To overcome this obstacle, we can introduce low-level, location-independent le identiers. (OpenAFS uses this approach.) Textual le names are mapped to lower-level le identiers that indicate to which component unit the le belongs. These identiers are still location independent. They can be replicated and cached freely without being invalidated by migration of component units. The inevitable price is the need for a second level of mapping, which maps component units to locations and needs a simple yet consistent update mechanism. Implementing UNIX-like directory trees using these low-level, location-independent identiers makes the whole hierarchy invariant under component-unit migration. The only aspect that does change is the componentunit location mapping. A common way to implement low-level identiers is to use structured names. These names are bit strings that usually have two parts. The rst part identies the component unit to which the le belongs; the second part identies the particular le within the unit. Variants with more parts are possible. The invariant of structured names, however, is that individual parts of the name are unique at all times only within the context of the rest of the parts"
Explain why Microsoft was found guilty of using its operating-system monopoly to limit competition,"The matter of what constitutes an operating system became increasingly important as personal computers became more widespread and operating systems grew increasingly sophisticated. In 1998, the United States Department of Justice led suit against Microsoft, in essence claiming that Microsoft included too much functionality in its operating systems and thus prevented application vendors from competing. (For example, a Web browser was an integral part of the operating systems.) Hence, Microsoft was found guilty of using its operating-system monopoly to limit competition."," The common functions of controlling and allocating resources are then brought together into one piece of software: the operating system. In addition, we have no universally accepted denition of what is part of the operating system. A simple viewpoint is that it includes everything a vendor ships when you order the operating system. The features included, however, vary greatly across systems. Some systems take up less than a megabyte of space and lack even a full-screen editor, whereas others require gigabytes of space and are based entirely on graphical windowing systems. A more common denition, and the one that we usually follow, is that the operating system is the one program running at all times on the computerusually called the kernel. (Along with the kernel, there are two other types of programs: system programs, which are associated with the operating system but are not necessarily part of the kernel, and application programs, which include all programs not associated with the operation of the system.) The matter of what constitutes an operating system became increasingly important as personal computers became more widespread and operating systems grew increasingly sophisticated. In 1998, the United States Department of Justice led suit against Microsoft, in essence claiming that Microsoft included too much functionality in its operating systems and thus prevented application vendors from competing. (For example, a Web browser was an integral part of the operating systems.) As a result, Microsoft was found guilty of using its operating-system monopoly to limit competition. Today, however, if we look at operating systems for mobile devices, we see that once again the number of features constituting the operating system is increasing. Mobile operating systems often include not only a core kernel but also middlewarea set of software frameworks that provide additional services to application developers. For example, each of the two most prominent mobile operating systemsApples iOS and Googles Android features a core kernel along with middleware that supports databases, multimedia, and graphics (to name a only few)"
"Explain why ordinary pipes are unidirectional, allowing only one-way communication","Ordinary Pipes Ordinary pipes allow two processes to communicate in standard producer consumer fashion: the producer writes to one end of the pipe (the write-end) and the consumer reads from the other end (the read-end). Hence, ordinary pipes are unidirectional, allowing only one-way communication."," A pipe acts as a conduit allowing two processes to communicate. Pipes were one of the rst IPC mechanisms in early UNIX systems. They typically provide one of the simpler ways for processes to communicate with one another, although they also have some limitations. In implementing a pipe, four issues must be considered: 1. Does the pipe allow bidirectional communication, or is communication unidirectional? 2. If two-way communication is allowed, is it half duplex (data can travel only one way at a time) or full duplex (data can travel in both directions at the same time)? 3. Must a relationship (such as parentchild) exist between the communicating processes? 4. Can the pipes communicate over a network, or must the communicating processes reside on the same machine? In the following sections, we explore two common types of pipes used on both UNIX and Windows systems: ordinary pipes and named pipes. 3.6.3.1 Ordinary Pipes Ordinary pipes allow two processes to communicate in standard producer consumer fashion: the producer writes to one end of the pipe (the write-end) and the consumer reads from the other end (the read-end). As a result, ordinary pipes are unidirectional, allowing only one-way communication. If two-way communication is required, two pipes must be used, with each pipe sending data in a different direction. We next illustrate constructing ordinary pipes on both UNIX and Windows systems. In both program examples, one process writes the message Greetings to the pipe, while the other process reads this message from the pipe. On UNIX systems, ordinary pipes are constructed using the function pipe(int fd[]) This function creates a pipe that is accessed through the int fd[] le descriptors: fd[0] is the read-end of the pipe, and fd[1] is the write-end"
Explain how the code implementing an operating system (kernel code) is subject to several possible race conditions," At a given point in time, many kernel-mode processes may be active in the operating system. Hence, the code implementing an operating system (kernel code) is subject to several possible race conditions."," We assume that each process is executing at a nonzero speed. However, we can make no assumption concerning the relative speed of the n processes. At a given point in time, many kernel-mode processes may be active in the operating system. As a result, the code implementing an operating system (kernel code) is subject to several possible race conditions. Consider as an example a kernel data structure that maintains a list of all open les in the system. This list must be modied when a new le is opened or closed (adding the le to the list or removing it from the list). If two processes were to open les simultaneously, the separate updates to this list could result in a race condition. Other kernel data structures that are prone to possible race conditions include structures for maintaining memory allocation, for maintaining process lists, and for interrupt handling. It is up to kernel developers to ensure that the operating system is free from such race conditions"
Explain why successive memory accesses by the process are often satised in cache memory,"Consider what happens to cache memory when a process has been running on a specic processor. The data most recently accessed by the process populate the cache for the processor. Hence, successive memory accesses by the process are often satised in cache memory."," #include <pthread.h> #include <stdio.h> #define NUM THREADS 5 int main(int argc, char *argv[]) { int i, scope; pthread t tid[NUM THREADS]; pthread attr t attr; /* get the default attributes */ pthread attr init(&attr); /* first inquire on the current scope */ if (pthread attr getscope(&attr, &scope) != 0) fprintf(stderr, ""Unable to get scheduling scope\n""); else { if (scope == PTHREAD SCOPE PROCESS) printf(""PTHREAD SCOPE PROCESS""); else if (scope == PTHREAD SCOPE SYSTEM) printf(""PTHREAD SCOPE SYSTEM""); else fprintf(stderr, ""Illegal scope value.\n""); } /* set the scheduling algorithm to PCS or SCS */ pthread attr setscope(&attr, PTHREAD SCOPE SYSTEM); /* create the threads */ for (i = 0; i < NUM THREADS; i++) pthread create(&tid[i],&attr,runner,NULL); /* now join on each thread */ for (i = 0; i < NUM THREADS; i++) pthread join(tid[i], NULL); /* Each thread will begin control in this function */ void *runner(void *param) { /* do some work ... */ } pthread exit(0); A second approach uses symmetric multiprocessing (SMP), where each processor is self-scheduling. All processes may be in a common ready queue, or each processor may have its own private queue of ready processes. Regardless, scheduling proceeds by having the scheduler for each processor examine the ready queue and select a process to execute. As we saw in Chapter 5, if we have multiple processors trying to access and update a common data structure, the scheduler must be programmed carefully. We must ensure that two separate processors do not choose to schedule the same process and that processes are not lost from the queue. Virtually all modern operating systems support SMP, including Windows, Linux, and Mac OS X. In the remainder of this section, we discuss issues concerning SMP systems. Consider what happens to cache memory when a process has been running on a specic processor. The data most recently accessed by the process populate the cache for the processor. As a result, successive memory accesses by the process are often satised in cache memory. Now consider what happens if the process migrates to another processor. The contents of cache memory must be invalidated for the rst processor, and the cache for the second processor must be repopulated. Because of the high cost of invalidating and repopulating caches, most SMP systems try to avoid migration of processes from one processor to another and instead attempt to keep a process running on the same processor. This is known as processor afnitythat is, a process has an afnity for the processor on which it is currently running. Processor afnity takes several forms. When an operating system has a policy of attempting to keep a process running on the same processorbut not guaranteeing that it will do sowe have a situation known as soft afnity"
Explain why the cost of switching between threads is small,"Once this new thread begins execution, it begins lling the pipeline with its instructions. Fine-grained (or interleaved) multithreading switches between threads at a much ner level of granularitytypically at the boundary of an instruction cycle. However, the architectural design of ne-grained systems includes logic for thread switching. Hence, the cost of switching between threads is small."," In general, there are two ways to multithread a processing core: coarsegrained and ne-grained multithreading. With coarse-grained multithreading, a thread executes on a processor until a long-latency event such as a memory stall occurs. Because of the delay caused by the long-latency event, the processor must switch to another thread to begin execution. However, the cost of switching between threads is high, since the instruction pipeline must thread1 C M M M C C C 6.6 Real-Time CPU Scheduling be ushed before the other thread can begin execution on the processor core. Once this new thread begins execution, it begins lling the pipeline with its instructions. Fine-grained (or interleaved) multithreading switches between threads at a much ner level of granularitytypically at the boundary of an instruction cycle. However, the architectural design of ne-grained systems includes logic for thread switching. As a result, the cost of switching between threads is small. Notice that a multithreaded multicore processor actually requires two different levels of scheduling. On one level are the scheduling decisions that must be made by the operating system as it chooses which software thread to run on each hardware thread (logical processor). For this level of scheduling, the operating system may choose any scheduling algorithm, such as those described in Section 6.3. A second level of scheduling species how each core decides which hardware thread to run. There are several strategies to adopt in this situation. The UltraSPARC T3, mentioned earlier, uses a simple roundrobin algorithm to schedule the eight hardware threads to each core. Another example, the Intel Itanium, is a dual-core processor with two hardwaremanaged threads per core. Assigned to each hardware thread is a dynamic urgency value ranging from 0 to 7, with 0 representing the lowest urgency and 7 the highest. The Itanium identies ve different events that may trigger a thread switch. When one of these events occurs, the thread-switching logic compares the urgency of the two threads and selects the thread with the highest urgency value to execute on the processor core"
Explain why the optimal algorithm is used mainly for comparison studies," Unfortunately, the optimal page-replacement algorithm is difcult to implement, because it requires future knowledge of the reference string. (We encountered a similar situation with the SJF CPU-scheduling algorithm in Section 6.3.2.) Hence, the optimal algorithm is used mainly for comparison studies."," 14 10 6 2 3 number of frames For example, on our sample reference string, the optimal page-replacement algorithm would yield nine page faults, as shown in Figure 9.14. The rst three references cause faults that ll the three empty frames. The reference to page 2 replaces page 7, because page 7 will not be used until reference 18, whereas page 0 will be used at 5, and page 1 at 14. The reference to page 3 replaces page 1, as page 1 will be the last of the three pages in memory to be referenced again. With only nine page faults, optimal replacement is much better than a FIFO algorithm, which results in fteen faults. (If we ignore the rst three, which all algorithms must suffer, then optimal replacement is twice as good as FIFO replacement.) In fact, no replacement algorithm can process this reference string in three frames with fewer than nine faults. Unfortunately, the optimal page-replacement algorithm is difcult to implement, because it requires future knowledge of the reference string. (We encountered a similar situation with the SJF CPU-scheduling algorithm in Section 6.3.2.) As a result, the optimal algorithm is used mainly for comparison studies. For instance, it may be useful to know that, although a new algorithm is not optimal, it is within 12.3 percent of optimal at worst and within 4.7 percent on average. reference string 7 3 3 1 7 2 2 0 0 3 1 page frames If the optimal algorithm is not feasible, perhaps an approximation of the optimal algorithm is possible. The key distinction between the FIFO and OPT algorithms (other than looking backward versus forward in time) is that the FIFO algorithm uses the time when a page was brought into memory, whereas the OPT algorithm uses the time when a page is to be used. If we use the recent past as an approximation of the near future, then we can replace the page that has not been used for the longest period of time. This approach is the least recently used (LRU) algorithm"
"Explain why CPU utilization drops even further, and the CPU scheduler tries to increase the degree of multiprogramming even more","The CPU scheduler sees the decreasing CPU utilization and increases the degree of multiprogramming as a result. The new process tries to get started by taking frames from running processes, causing more page faults and a longer queue for the paging device. Hence, CPU utilization drops even further, and the CPU scheduler tries to increase the degree of multiprogramming even more."," The operating system monitors CPU utilization. If CPU utilization is too low, we increase the degree of multiprogramming by introducing a new process to the system. A global page-replacement algorithm is used; it replaces pages without regard to the process to which they belong. Now suppose that a process enters a new phase in its execution and needs more frames. It starts faulting and taking frames away from other processes. These processes need those pages, however, and so they also fault, taking frames from other processes. These faulting processes must use the paging device to swap pages in and out. As they queue up for the paging device, the ready queue empties. As processes wait for the paging device, CPU utilization decreases. The CPU scheduler sees the decreasing CPU utilization and increases the degree of multiprogramming as a result. The new process tries to get started by taking frames from running processes, causing more page faults and a longer queue for the paging device. As a result, CPU utilization drops even further, and the CPU scheduler tries to increase the degree of multiprogramming even more. Thrashing has occurred, and system throughput plunges. The pagefault rate increases tremendously. As a result, the effective memory-access time increases. No work is getting done, because the processes are spending all their time paging. This phenomenon is illustrated in Figure 9.18, in which CPU utilization is plotted against the degree of multiprogramming. As the degree of multiprogramming increases, CPU utilization also increases, although more slowly, until a maximum is reached. If the degree of multiprogramming is increased even further, thrashing sets in, and CPU utilization drops sharply. At this point, to increase CPU utilization and stop thrashing, we must decrease the degree of multiprogramming"
Explain why the effective memory-access time increases,"The CPU scheduler sees the decreasing CPU utilization and increases the degree of multiprogramming as a result. The new process tries to get started by taking frames from running processes, causing more page faults and a longer queue for the paging device. As a result, CPU utilization drops even further, and the CPU scheduler tries to increase the degree of multiprogramming even more. Thrashing has occurred, and system throughput plunges. The pagefault rate increases tremendously. Hence, the effective memory-access time increases."," The operating system monitors CPU utilization. If CPU utilization is too low, we increase the degree of multiprogramming by introducing a new process to the system. A global page-replacement algorithm is used; it replaces pages without regard to the process to which they belong. Now suppose that a process enters a new phase in its execution and needs more frames. It starts faulting and taking frames away from other processes. These processes need those pages, however, and so they also fault, taking frames from other processes. These faulting processes must use the paging device to swap pages in and out. As they queue up for the paging device, the ready queue empties. As processes wait for the paging device, CPU utilization decreases. The CPU scheduler sees the decreasing CPU utilization and increases the degree of multiprogramming as a result. The new process tries to get started by taking frames from running processes, causing more page faults and a longer queue for the paging device. As a result, CPU utilization drops even further, and the CPU scheduler tries to increase the degree of multiprogramming even more. Thrashing has occurred, and system throughput plunges. The pagefault rate increases tremendously. As a result, the effective memory-access time increases. No work is getting done, because the processes are spending all their time paging. This phenomenon is illustrated in Figure 9.18, in which CPU utilization is plotted against the degree of multiprogramming. As the degree of multiprogramming increases, CPU utilization also increases, although more slowly, until a maximum is reached. If the degree of multiprogramming is increased even further, thrashing sets in, and CPU utilization drops sharply. At this point, to increase CPU utilization and stop thrashing, we must decrease the degree of multiprogramming"
Explain why the kernel must use memory conservatively and attempt to minimize waste due to fragmentation,"Kernel memory is often allocated from a free-memory pool different from the list used to satisfy ordinary user-mode processes. There are two primary reasons for this: 1. The kernel requests memory for data structures of varying sizes, some of which are less than a page in size. Hence, the kernel must use memory conservatively and attempt to minimize waste due to fragmentation."," This list is typically populated using a page-replacement algorithm such as those discussed in Section 9.4 and most likely contains free pages scattered throughout physical memory, as explained earlier. Remember, too, that if a user process requests a single byte of memory, internal fragmentation will result, as the process will be granted an entire page frame. Kernel memory is often allocated from a free-memory pool different from the list used to satisfy ordinary user-mode processes. There are two primary reasons for this: 1. The kernel requests memory for data structures of varying sizes, some of which are less than a page in size. As a result, the kernel must use memory conservatively and attempt to minimize waste due to fragmentation. This is especially important because many operating systems do not subject kernel code or data to the paging system. 2. Pages allocated to user-mode processes do not necessarily have to be in contiguous physical memory. However, certain hardware devices interact directly with physical memorywithout the benet of a virtual memory interfaceand consequently may require memory residing in physically contiguous pages"
Explain why there are no articial limits on storage use and no need to relocate le systems between volumes or resize volumes,"ZFS combines le-system management and volume management into a unit providing greater functionality than the traditional separation of those functions allows. Disks, or partitions of disks, are gathered together via RAID sets into pools of storage. A pool can hold one or more ZFS le systems. The entire pools free space is available to all le systems within that pool. ZFS uses the memory model of malloc() and free() to allocate and release storage for each le system as blocks are used and freed within the le system. Hence, there are no articial limits on storage use and no need to relocate le systems between volumes or resize volumes."," Even if the storage array allowed the entire set of twenty disks to be created as one large RAID set, other issues could arise. Several volumes of various sizes could be built on the set. But some volume managers do not allow us to change a volumes size. In that case, we would be left with the same issue described above mismatched le-system sizes. Some volume managers allow size changes, but some le systems do not allow for le-system growth or shrinkage. The volumes could change sizes, but the le systems would need to be recreated to take advantage of those changes. ZFS combines le-system management and volume management into a unit providing greater functionality than the traditional separation of those functions allows. Disks, or partitions of disks, are gathered together via RAID sets into pools of storage. A pool can hold one or more ZFS le systems. The entire pools free space is available to all le systems within that pool. ZFS uses the memory model of malloc() and free() to allocate and release storage for each le system as blocks are used and freed within the le system. As a result, there are no articial limits on storage use and no need to relocate le systems between volumes or resize volumes. ZFS provides quotas to limit the size of a le system and reservations to assure that a le system can grow by a specied amount, but those variables can be changed by the le-system owner at any time. Figure 10.14(a) depicts traditional volumes and le systems, and Figure 10.14(b) shows the ZFS model. 10.8 Stable-Storage Implementation In Chapter 5, we introduced the write-ahead log, which requires the availability of stable storage. By denition, information residing in stable storage is never lost. To implement such storage, we need to replicate the required information 10.8 Stable-Storage Implementation FS FS volume ZFS ZFS (b) ZFS and pooled storage"
Explain why a system performing many I/O operations used most of the available memory for caching pages,"Regardless of whether we are caching disk blocks or pages (or both), LRU (Section 9.4.4) seems a reasonable general-purpose algorithm for block or page replacement. However, the evolution of the Solaris page-caching algorithms reveals the difculty in choosing an algorithm. Solaris allows processes and the page cache to share unused memory. Versions earlier than Solaris 2.5.1 made no distinction between allocating pages to a process and allocating them to the page cache. Hence, a system performing many I/O operations used most of the available memory for caching pages."," The unied buffer cache is shown in Figure 12.12. Regardless of whether we are caching disk blocks or pages (or both), LRU (Section 9.4.4) seems a reasonable general-purpose algorithm for block or page replacement. However, the evolution of the Solaris page-caching algorithms reveals the difculty in choosing an algorithm. Solaris allows processes and the page cache to share unused memory. Versions earlier than Solaris 2.5.1 made no distinction between allocating pages to a process and allocating them to the page cache. As a result, a system performing many I/O operations used most of the available memory for caching pages. Because of the high rates of I/O, the page scanner (Section 9.10.2) reclaimed pages from processesrather than from the page cachewhen free memory ran low. Solaris 2.6 and Solaris 7 optionally implemented priority paging, in which the page scanner gives memory-mapped I/O I/O using read( ) and write( ) file system priority to process pages over the page cache. Solaris 8 applied a xed limit to process pages and the le-system page cache, preventing either from forcing the other out of memory. Solaris 9 and 10 again changed the algorithms to maximize memory use and minimize thrashing. Another issue that can affect the performance of I/O is whether writes to the le system occur synchronously or asynchronously. Synchronous writes occur in the order in which the disk subsystem receives them, and the writes are not buffered. Thus, the calling routine must wait for the data to reach the disk drive before it can proceed. In an asynchronous write, the data are stored in the cache, and control returns to the caller. Most writes are asynchronous"
Explain why enforcing protection at the granularity of the JVM process is insufcient,"Because of these capabilities, protection is a paramount concern. Classes running in the same JVM may be from different sources and may not be equally trusted. Hence, enforcing protection at the granularity of the JVM process is insufcient."," Because Java was designed to run in a distributed environment, the Java virtual machine or JVM has many built-in protection mechanisms. Java programs are composed of classes, each of which is a collection of data elds and functions (called methods) that operate on those elds. The JVM loads a class in response to a request to create instances (or objects) of that class. One of the most novel and useful features of Java is its support for dynamically loading untrusted classes over a network and for executing mutually distrusting classes within the same JVM. Because of these capabilities, protection is a paramount concern. Classes running in the same JVM may be from different sources and may not be equally trusted. As a result, enforcing protection at the granularity of the JVM process is insufcient. Intuitively, whether a request to open a le should be allowed will generally depend on which class has requested the open. The operating system lacks this knowledge. Thus, such protection decisions are handled within the JVM. When the JVM loads a class, it assigns the class to a protection domain that gives the permissions of that class. The protection domain to which the class is assigned depends on the URL from which the class was loaded and any digital signatures on the class le. (Digital signatures are covered in Section 15.4.1.3.) A congurable policy le determines the permissions granted to the domain (and its classes). For example, classes loaded from a trusted server might be placed in a protection domain that allows them to access les in the users home directory, whereas classes loaded from an untrusted server might have no le access permissions at all"
"Explain how an object cannot manipulate its runtime stack, because it cannot get a reference to the stack or other components of the protection system","References cannot be forged, and manipulations are made only through welldened interfaces. Compliance is enforced through a sophisticated collection of load-time and run-time checks. Hence, an object cannot manipulate its runtime stack, because it cannot get a reference to the stack or other components of the protection system."," Summary protection domain: untrusted applet networking socket permission: *.lucent.com:80, connect class: get(URL u): get(url); open(addr); doPrivileged { open(proxy.lucent.com:80); } request u from proxy checkPermission (a, connect); connect (a); Of course, for stack inspection to work, a program must be unable to modify the annotations on its own stack frame or to otherwise manipulate stack inspection. This is one of the most important differences between Java and many other languages (including C++). A Java program cannot directly access memory; it can manipulate only an object for which it has a reference. References cannot be forged, and manipulations are made only through welldened interfaces. Compliance is enforced through a sophisticated collection of load-time and run-time checks. As a result, an object cannot manipulate its runtime stack, because it cannot get a reference to the stack or other components of the protection system. More generally, Javas load-time and run-time checks enforce type safety of Java classes. Type safety ensures that classes cannot treat integers as pointers, write past the end of an array, or otherwise access memory in arbitrary ways"
Explain why it no longer requires modied guests and essentially does not need the paravirtualization method,"Xen allowed virtualization of x86 CPUs without the use of binary translation, instead requiring modications in the guest operating systems like the one described above. Over time, Xen has taken advantage of hardware features supporting virtualization. Hence, it no longer requires modied guests and essentially does not need the paravirtualization method."," Rather, each guest has its own set of page tables, set to read-only. Xen requires the guest to use a specic mechanism, a hypercall from the guest to the hypervisor VMM, when a page-table change is needed. This means that the guest operating systems kernel code must be changed from the default code to these Xen-specic methods. To optimize performance, Xen allows the guest to queue up multiple page-table changes asynchronously via hypercalls and then check to ensure that the changes are complete before continuing operation. Xen allowed virtualization of x86 CPUs without the use of binary translation, instead requiring modications in the guest operating systems like the one described above. Over time, Xen has taken advantage of hardware features supporting virtualization. As a result, it no longer requires modied guests and essentially does not need the paravirtualization method. Paravirtualization is still used in other solutions, however, such as type 0 hypervisors. Another kind of virtualization, based on a different execution model, is the virtualization of programming environments. Here, a programming language is designed to run within a custom-built virtualized environment. For example, Oracles Java has many features that depend on its running in the Java virtual machine (JVM), including specic methods for security and memory management"
"Explain why wireless networks are popular in homes and businesses, as well as public areas such as libraries, Internet cafes, sports arenas, and even buses and airplanes"," The 802.11g standard can theoretically run at 54 megabits per second, but in practice, data rates are often less than half that. The recent 802.11n standard provides theoretically much higher data rates. In actual practice, though, these Router Firewall WAN networks have typical data rates of around 75 megabits per second. Data rates of wireless networks are heavily inuenced by the distance between the wireless router and the host, as well as interference in the wireless spectrum. On the positive side, wireless networks often have a physical advantage over wired Ethernet networks because they require no cabling to connect communicating hosts. Hence, wireless networks are popular in homes and businesses, as well as public areas such as libraries, Internet cafes, sports arenas, and even buses and airplanes."," A disadvantage of wireless networks concerns their speed. Whereas Ethernet systems often run at 1 gigabit per second, WiFi networks typically run considerably slower. There are several IEEE standards for wireless networks. The 802.11g standard can theoretically run at 54 megabits per second, but in practice, data rates are often less than half that. The recent 802.11n standard provides theoretically much higher data rates. In actual practice, though, these Router Firewall WAN networks have typical data rates of around 75 megabits per second. Data rates of wireless networks are heavily inuenced by the distance between the wireless router and the host, as well as interference in the wireless spectrum. On the positive side, wireless networks often have a physical advantage over wired Ethernet networks because they require no cabling to connect communicating hosts. As a result, wireless networks are popular in homes and businesses, as well as public areas such as libraries, Internet cafes, sports arenas, and even buses and airplanes. Wide-area networks emerged in the late 1960s, mainly as an academic research project to provide efcient communication among sites, allowing hardware and software to be shared conveniently and economically by a wide community of users. The rst WAN to be designed and developed was the Arpanet. Begun in 1968, the Arpanet has grown from a four-site experimental network to a worldwide network of networks, the Internet, comprising millions of computer systems"
Explain why the 64-bit version of Windows 7 is now commonly installed on larger client systems,"Windows XP was the rst version of Windows to ship a 64-bit version (for the IA64 in 2001 and the AMD64 in 2005). Internally, the native NT le system (NTFS) and many of the Win32 APIs have always used 64-bit integers where appropriate so the major extension to 64-bit in Windows XP was support for large virtual addresses. However, 64-bit editions of Windows also support much larger physical memories. By the time Windows 7 shipped, the AMD64 ISA had become available on almost all CPUs from both Intel and AMD. In addition, by that time, physical memories on client systems frequently exceeded the 4-GB limit of the IA-32. Hence, the 64-bit version of Windows 7 is now commonly installed on larger client systems."," Windows 7 made substantial changes to the DWM, signicantly reducing its memory footprint and improving its performance. Windows XP was the rst version of Windows to ship a 64-bit version (for the IA64 in 2001 and the AMD64 in 2005). Internally, the native NT le system (NTFS) and many of the Win32 APIs have always used 64-bit integers where appropriate so the major extension to 64-bit in Windows XP was support for large virtual addresses. However, 64-bit editions of Windows also support much larger physical memories. By the time Windows 7 shipped, the AMD64 ISA had become available on almost all CPUs from both Intel and AMD. In addition, by that time, physical memories on client systems frequently exceeded the 4-GB limit of the IA-32. As a result, the 64-bit version of Windows 7 is now commonly installed on larger client systems. Because the AMD64 architecture supports high-delity IA-32 compatibility at the level of individual processes, 32- and 64-bit applications can be freely mixed in a single system. In the rest of our description of Windows 7, we will not distinguish between the client editions of Windows 7 and the corresponding server editions. They are based on the same core components and run the same binary les for the kernel and most drivers. Similarly, although Microsoft ships a variety of different editions of each release to address different market price points, few of the differences between editions are reected in the core of the system. In this chapter, we focus primarily on the core components of Windows 7"
Explain why no site would switch from its temporary system to TSS/360,"When TSS/360 was eventually delivered, it was a failure. It was too large and too slow. Hence, no site would switch from its temporary system to TSS/360."," TSS/360 was delayed, however, so other time-sharing systems were developed as temporary systems until TSS/360 was available. A time-sharing option (TSO) was added to OS/360. IBMs Cambridge Scientic Center developed CMS as a single-user system and CP/67 to provide a virtual machine to run it on. When TSS/360 was eventually delivered, it was a failure. It was too large and too slow. As a result, no site would switch from its temporary system to TSS/360. Today, time sharing on IBM systems is largely provided either by TSO under MVS or by CMS under CP/67 (renamed VM). Neither TSS/360 nor MULTICS achieved commercial success. What went wrong? Part of the problem was that these advanced systems were too large and too complex to be understood. Another problem was the assumption that computing power would be available from a large, remote source"
"Explain why if another process also on host X wished to establish another connection with the same web server, it would be assigned a port number greater than 1024 and not equal to 1625"," All connections must be unique. Hence, if another process also on host X wished to establish another connection with the same web server, it would be assigned a port number greater than 1024 and not equal to 1625."," When a client process initiates a request for a connection, it is assigned a port by its host computer. This port has some arbitrary number greater than 1024. For example, if a client on host X with IP address 146.86.5.20 wishes to establish a connection with a web server (which is listening on port 80) at address 161.25.19.8, host X may be assigned port 1625. The connection will consist of a pair of sockets: (146.86.5.20:1625) on host X and (161.25.19.8:80) on the web server. This situation is illustrated in Figure 3.20. The packets traveling between the hosts are delivered to the appropriate process based on the destination port number. All connections must be unique. Therefore, if another process also on host X wished to establish another connection with the same web server, it would be assigned a port number greater than 1024 and not equal to 1625. This ensures that all connections consist of a unique pair of sockets. host X (146.86.5.20) socket (146.86.5.20:1625) web server (161.25.19.8) socket (161.25.19.8:80) Although most program examples in this text use C, we will illustrate sockets using Java, as it provides a much easier interface to sockets and has a rich library for networking utilities. Those interested in socket programming in C or C++ should consult the bibliographical notes at the end of the chapter"
"Explain why in some cases, an asynchronous signal may be delivered only to those threads that are not blocking it","This function species the process (pid) to which a particular signal (signal) is to be delivered. Most multithreaded versions of UNIX allow a thread to specify which signals it will accept and which it will block. Hence, in some cases, an asynchronous signal may be delivered only to those threads that are not blocking it."," For example, synchronous signals need to be delivered to the thread causing the signal and not to other threads in the process. However, the situation with asynchronous signals is not as clear. Some asynchronous signalssuch as a signal that terminates a process (<control><C>, for example)should be sent to all threads. kill(pid t pid, int signal) This function species the process (pid) to which a particular signal (signal) is to be delivered. Most multithreaded versions of UNIX allow a thread to specify which signals it will accept and which it will block. Therefore, in some cases, an asynchronous signal may be delivered only to those threads that are not blocking it. However, because signals need to be handled only once, a signal is typically delivered only to the rst thread found that is not blocking it. POSIX Pthreads provides the following function, which allows a signal to be delivered to a specied thread (tid): pthread kill(pthread t tid, int signal) Although Windows does not explicitly provide support for signals, it allows us to emulate them using asynchronous procedure calls (APCs). The APC facility enables a user thread to specify a function that is to be called when the user thread receives notication of a particular event. As indicated by its name, an APC is roughly equivalent to an asynchronous signal in UNIX. However, whereas UNIX must contend with how to deal with signals in a multithreaded environment, the APC facility is more straightforward, since an APC is delivered to a particular thread rather than a process"
Explain why canceling a thread asynchronously may not free a necessary system-wide resource,"The difculty with cancellation occurs in situations where resources have been allocated to a canceled thread or where a thread is canceled while in the midst of updating data it is sharing with other threads. This becomes especially troublesome with asynchronous cancellation. Often, the operating system will reclaim system resources from a canceled thread but will not reclaim all resources. Hence, canceling a thread asynchronously may not free a necessary system-wide resource."," 2. Deferred cancellation. The target thread periodically checks whether it should terminate, allowing it an opportunity to terminate itself in an orderly fashion. The difculty with cancellation occurs in situations where resources have been allocated to a canceled thread or where a thread is canceled while in the midst of updating data it is sharing with other threads. This becomes especially troublesome with asynchronous cancellation. Often, the operating system will reclaim system resources from a canceled thread but will not reclaim all resources. Therefore, canceling a thread asynchronously may not free a necessary system-wide resource. With deferred cancellation, in contrast, one thread indicates that a target thread is to be canceled, but cancellation occurs only after the target thread has checked a ag to determine whether or not it should be canceled. The thread can perform this check at a point at which it can be canceled safely"
Explain why SMP systems must provide alternative locking techniquessuch as compare and swap() or spinlocksto ensure that wait() and signal() are performed atomically,"In a multiprocessor environment, interrupts must be disabled on every processor. Otherwise, instructions from different processes (running on different processors) may be interleaved in some arbitrary way. Disabling interrupts on every processor can be a difcult task and furthermore can seriously diminish performance. Hence, SMP systems must provide alternative locking techniquessuch as compare and swap() or spinlocksto ensure that wait() and signal() are performed atomically."," This scheme works in a single-processor environment because, once interrupts are inhibited, instructions from different processes cannot be interleaved. Only the currently running process executes until interrupts are reenabled and the scheduler can regain control. In a multiprocessor environment, interrupts must be disabled on every processor. Otherwise, instructions from different processes (running on different processors) may be interleaved in some arbitrary way. Disabling interrupts on every processor can be a difcult task and furthermore can seriously diminish performance. Therefore, SMP systems must provide alternative locking techniquessuch as compare and swap() or spinlocksto ensure that wait() and signal() are performed atomically. It is important to admit that we have not completely eliminated busy waiting with this denition of the wait() and signal() operations. Rather, we have moved busy waiting from the entry section to the critical sections of application programs. Furthermore, we have limited busy waiting to the critical sections of the wait() and signal() operations, and these sections are short (if properly coded, they should be no more than about ten instructions)"
Explain why the highest-priority real-time thread at the front of the FIFO queue will be granted the CPU until it terminates or blocks,"SCHED FIFO schedules threads according to a rst-come, rst-served policy using a FIFO queue as outlined in Section 6.3.1. However, there is no time slicing among threads of equal priority. Hence, the highest-priority real-time thread at the front of the FIFO queue will be granted the CPU until it terminates or blocks."," Proportional share schedulers must work in conjunction with an admission-control policy to guarantee that an application receives its allocated shares of time. An admission-control policy will admit a client requesting a particular number of shares only if sufcient shares are available. In our current example, we have allocated 50 + 15 + 20 = 85 shares of the total of 100 shares. If a new process D requested 30 shares, the admission controller would deny D entry into the system. The POSIX standard also provides extensions for real-time computing POSIX.1b. Here, we cover some of the POSIX API related to scheduling real-time threads. POSIX denes two scheduling classes for real-time threads: SCHED FIFO SCHED RR SCHED FIFO schedules threads according to a rst-come, rst-served policy using a FIFO queue as outlined in Section 6.3.1. However, there is no time slicing among threads of equal priority. Therefore, the highest-priority real-time thread at the front of the FIFO queue will be granted the CPU until it terminates or blocks. SCHED RR uses a round-robin policy. It is similar to SCHED FIFO except that it provides time slicing among threads of equal priority. POSIX provides an additional scheduling class SCHED OTHER but its implementation is undened and system specic; it may behave differently on different systems. The POSIX API species the following two functions for getting and setting the scheduling policy: pthread attr getsched policy(pthread attr t *attr, int *policy) pthread attr setsched policy(pthread attr t *attr, int policy) The rst parameter to both functions is a pointer to the set of attributes for the thread. The second parameter is either (1) a pointer to an integer that is set to the current scheduling policy (for pthread attr getsched policy()) or (2) an integer value (SCHED FIFO, SCHED RR, or SCHED OTHER) for the pthread attr setsched policy() function. Both functions return nonzero values if an error occurs"
"Explain why the value of vruntime will eventually be lower for the I/O-bound task than for the CPU-bound task, giving the I/O-bound task higher priority than the CPU-bound task"," Lets examine the CFS scheduler in action: Assume that two tasks have the same nice values. One task is I/O-bound and the other is CPU-bound. Typically, the I/O-bound task will run only for short periods before blocking for additional I/O, and the CPU-bound task will exhaust its time period whenever it has an opportunity to run on a processor. Hence, the value of vruntime will eventually be lower for the I/O-bound task than for the CPU-bound task, giving the I/O-bound task higher priority than the CPU-bound task."," CFS PERFORMANCE The Linux CFS scheduler provides an efcient algorithm for selecting which task to run next. Each runnable task is placed in a red-black tree a balanced binary search tree whose key is based on the value of vruntime. This tree is shown below: Task with the smallest value of vruntime T7 T5 T8 T9 When a task becomes runnable, it is added to the tree. If a task on the tree is not runnable (for example, if it is blocked while waiting for I/O), it is removed. Generally speaking, tasks that have been given less processing time (smaller values of vruntime) are toward the left side of the tree, and tasks that have been given more processing time are on the right side. According to the properties of a binary search tree, the leftmost node has the smallest key value, which for the sake of the CFS scheduler means that it is the task with the highest priority. Because the red-black tree is balanced, navigating it to discover the leftmost node will require O(lg N) operations (where N is the number of nodes in the tree). However, for efciency reasons, the Linux scheduler caches this value in the variable rb leftmost, and thus determining which task to run next requires only retrieving the cached value. Lets examine the CFS scheduler in action: Assume that two tasks have the same nice values. One task is I/O-bound and the other is CPU-bound. Typically, the I/O-bound task will run only for short periods before blocking for additional I/O, and the CPU-bound task will exhaust its time period whenever it has an opportunity to run on a processor. Therefore, the value of vruntime will eventually be lower for the I/O-bound task than for the CPU-bound task, giving the I/O-bound task higher priority than the CPU-bound task. At that point, if the CPU-bound task is executing when the I/O-bound task becomes eligible to run (for example, when I/O the task is waiting for becomes available), the I/O-bound task will preempt the CPU-bound task. Linux also implements real-time scheduling using the POSIX standard as described in Section 6.6.6. Any task scheduled using either the SCHED FIFO or the SCHED RR real-time policy runs at a higher priority than normal (non-real- Real-Time Higher Lower Priority time) tasks. Linux uses two separate priority ranges, one for real-time tasks and a second for normal tasks. Real-time tasks are assigned static priorities within the range of 0 to 99, and normal (i.e. non real-time) tasks are assigned priorities from 100 to 139. These two ranges map into a global priority scheme wherein numerically lower values indicate higher relative priorities. Normal tasks are assigned a priority based on their nice values, where a value of 20 maps to priority 100 and a nice value of +19 maps to 139. This scheme is shown in Figure 6.21"
Explain why access must be controlled through mutex locks to prevent race conditions,"Multiple threads (customers) will concurrently access shared data through these two functions. Hence, access must be controlled through mutex locks to prevent race conditions."," The Banker The banker will consider requests from n customers for m resources types. as outlined in Section 7.5.3. The banker will keep track of the resources using the following data structures: /* these may be any values >= 0 */ #define NUMBER OF CUSTOMERS 5 #define NUMBER OF RESOURCES 3 /* the available amount of each resource */ int available[NUMBER OF RESOURCES]; /*the maximum demand of each customer */ int maximum[NUMBER OF CUSTOMERS][NUMBER OF RESOURCES]; /* the amount currently allocated to each customer */ int allocation[NUMBER OF CUSTOMERS][NUMBER OF RESOURCES]; /* the remaining need of each customer */ int need[NUMBER OF CUSTOMERS][NUMBER OF RESOURCES]; The Customers Create n customer threads that request and release resources from the bank. The customers will continually loop, requesting and then releasing random numbers of resources. The customers requests for resources will be bounded by their respective values in the need array. The banker will grant a request if it satises the safety algorithm outlined in Section 7.5.3.1. If a request does not leave the system in a safe state, the banker will deny it. Function prototypes for requesting and releasing resources are as follows: int request resources(int customer num, int request[]); int release resources(int customer num, int release[]); These two functions should return 0 if successful (the request has been granted) and 1 if unsuccessful. Multiple threads (customers) will concurrently access shared data through these two functions. Therefore, access must be controlled through mutex locks to prevent race conditions. Both the Pthreads and Windows APIs provide mutex locks. The use of Pthreads mutex locks is covered in Section 5.9.4; mutex locks for Windows systems are described in the project entitled ProducerConsumer Problem at the end of Chapter 5. Implementation You should invoke your program by passing the number of resources of each type on the command line. For example, if there were three resource types, with ten instances of the rst type, ve of the second type, and seven of the third type, you would invoke your program follows: ./a.out 10 5 7 The available array would be initialized to these values. You may initialize the maximum array (which holds the maximum demand of each customer) using any method you nd convenient"
"Explain why any instructions in execution, and any data being used by the instructions, must be in one of these direct-access storage devices","Main memory and the registers built into the processor itself are the only general-purpose storage that the CPU can access directly. There are machine instructions that take memory addresses as arguments, but none that take disk addresses. Hence, any instructions in execution, and any data being used by the instructions, must be in one of these direct-access storage devices."," We begin our discussion by covering several issues that are pertinent to managing memory: basic hardware, the binding of symbolic memory addresses to actual physical addresses, and the distinction between logical and physical addresses. We conclude the section with a discussion of dynamic linking and shared libraries. Main memory and the registers built into the processor itself are the only general-purpose storage that the CPU can access directly. There are machine instructions that take memory addresses as arguments, but none that take disk addresses. Therefore, any instructions in execution, and any data being used by the instructions, must be in one of these direct-access storage devices. If the data are not in memory, they must be moved there before the CPU can operate on them. Registers that are built into the CPU are generally accessible within one cycle of the CPU clock. Most CPUs can decode instructions and perform simple operations on register contents at the rate of one or more operations per clock tick. The same cannot be said of main memory, which is accessed via a transaction on the memory bus. Completing a memory access may take many cycles of the CPU clock. In such cases, the processor normally needs to stall, since it does not have the data required to complete the instruction that it is executing. This situation is intolerable because of the frequency of memory accesses. The remedy is to add fast memory between the CPU and main memory, typically on the CPU chip for fast access. Such a cache was described in Section 1.8.3. To manage a cache built into the CPU, the hardware automatically speeds up memory access without any operating-system control"
Explain why paging lets us use physical memory that is larger than what can be addressed by the CPUs address pointer length," If frame size is 4 KB (212 ), then a system with 4-byte entries can address 244 bytes (or 16 TB) of physical memory. We should note here that the size of physical memory in a paged memory system is different from the maximum logical size of a process. As we further explore paging, we introduce other information that must be kept in the page-table entries. That information reduces the number 8.5 Paging free-frame list 13 20 page 0 page 1 page 2 page 3 new process 15 17 19 21 (a) free-frame list page 0 page 1 page 2 page 3 new process 0 14 1 13 2 18 3 20 new-process page table 18 page 2 20 page 3 of bits available to address page frames. Thus, a system with 32-bit page-table entries may address less physical memory than the possible maximum. A 32-bit CPU uses 32-bit addresses, meaning that a given process space can only be 232 bytes (4 TB). Hence, paging lets us use physical memory that is larger than what can be addressed by the CPUs address pointer length."," Frequently, on a 32-bit CPU, each page-table entry is 4 bytes long, but that size can vary as well. A 32-bit entry can point to one of 232 physical page frames. If frame size is 4 KB (212 ), then a system with 4-byte entries can address 244 bytes (or 16 TB) of physical memory. We should note here that the size of physical memory in a paged memory system is different from the maximum logical size of a process. As we further explore paging, we introduce other information that must be kept in the page-table entries. That information reduces the number 8.5 Paging free-frame list 13 20 page 0 page 1 page 2 page 3 new process 15 17 19 21 (a) free-frame list page 0 page 1 page 2 page 3 new process 0 14 1 13 2 18 3 20 new-process page table 18 page 2 20 page 3 of bits available to address page frames. Thus, a system with 32-bit page-table entries may address less physical memory than the possible maximum. A 32-bit CPU uses 32-bit addresses, meaning that a given process space can only be 232 bytes (4 TB). Therefore, paging lets us use physical memory that is larger than what can be addressed by the CPUs address pointer length. When a process arrives in the system to be executed, its size, expressed in pages, is examined. Each page of the process needs one frame. Thus, if the process requires n pages, at least n frames must be available in memory. If n frames are available, they are allocated to this arriving process. The rst page of the process is loaded into one of the allocated frames, and the frame number is put in the page table for this process. The next page is loaded into another frame, its frame number is put into the page table, and so on (Figure 8.13)"
Explain why a single page-table entry can store the mappings for multiple physical-page frames,"A variation of this scheme that is useful for 64-bit address spaces has been proposed. This variation uses clustered page tables, which are similar to physical address logical address p d hash function s d physical memory hashed page tables except that each entry in the hash table refers to several pages (such as 16) rather than a single page. Hence, a single page-table entry can store the mappings for multiple physical-page frames."," If there is no match, subsequent entries in the linked list are searched for a matching virtual page number. This scheme is shown in Figure 8.19. A variation of this scheme that is useful for 64-bit address spaces has been proposed. This variation uses clustered page tables, which are similar to 8.6 Structure of the Page Table physical address logical address p d hash function s d physical memory hashed page tables except that each entry in the hash table refers to several pages (such as 16) rather than a single page. Therefore, a single page-table entry can store the mappings for multiple physical-page frames. Clustered page tables are particularly useful for sparse address spaces, where memory references are noncontiguous and scattered throughout the address space. Usually, each process has an associated page table. The page table has one entry for each page that the process is using (or one slot for each virtual address, regardless of the latters validity). This table representation is a natural one, since processes reference pages through the pages virtual addresses. The operating system must then translate this reference into a physical memory address. Since the table is sorted by virtual address, the operating system is able to calculate where in the table the associated physical address entry is located and to use that value directly. One of the drawbacks of this method is that each page table may consist of millions of entries. These tables may consume large amounts of physical memory just to keep track of how other physical memory is being used"
Explain why two base  limit register pairs are provided: one for instructions and one for data,"Consider a system in which a program can be separated into two parts: code and data. The CPU knows whether it wants an instruction (instruction fetch) or data (data fetch or store). Hence, two base  limit register pairs are provided: one for instructions and one for data."," Protection. If paging or segmentation is provided, different sections of a user program can be declared execute-only, read-only, or readwrite. This restriction is necessary with shared code or data and is generally useful in any case to provide simple run-time checks for common programming errors. Practice Exercises 8.2 Consider a system in which a program can be separated into two parts: code and data. The CPU knows whether it wants an instruction (instruction fetch) or data (data fetch or store). Therefore, two base limit register pairs are provided: one for instructions and one for data. The instruction base limit register pair is automatically read-only, so programs can be shared among different users. Discuss the advantages and disadvantages of this scheme"
Explain why vfork() must be used with caution to ensure that the child process does not modify the address space of the parent," Because vfork() does not use copy-on-write, if the child process changes any pages of the parents address space, the altered pages will be visible to the parent once it resumes. Hence, vfork() must be used with caution to ensure that the child process does not modify the address space of the parent."," Several versions of UNIX (including Solaris and Linux) provide a variation of the fork() system call vfork() (for virtual memory fork)that operates differently from fork() with copy-on-write. With vfork(), the parent process is suspended, and the child process uses the address space of the parent. Because vfork() does not use copy-on-write, if the child process changes any pages of the parents address space, the altered pages will be visible to the parent once it resumes. Therefore, vfork() must be used with caution to ensure that the child process does not modify the address space of the parent. vfork() is intended to be used when the child process calls exec() immediately after creation. Because no copying of pages takes place, vfork() is an extremely efcient method of process creation and is sometimes used to implement UNIX command-line shell interfaces. Page Replacement In our earlier discussion of the page-fault rate, we assumed that each page faults at most once, when it is rst referenced. This representation is not strictly accurate, however. If a process of ten pages actually uses only half of them, then demand paging saves the I/O necessary to load the ve pages that are never used. We could also increase our degree of multiprogramming by running twice as many processes. Thus, if we had forty frames, we could run eight processes, rather than the four that could run if each required ten frames (ve of which were never used)"
Explain why it is imperative that the kernel keep a sufcient amount of free memory available," Virtual memory is discussed in great detail in the Windows case study in In Solaris, when a thread incurs a page fault, the kernel assigns a page to the faulting thread from the list of free pages it maintains. Hence, it is imperative that the kernel keep a sufcient amount of free memory available."," A process that is at its working-set minimum may be allocated pages from the free-page-frame list once sufcient free memory is available. Windows performs working-set trimming on both user mode and system processes. Virtual memory is discussed in great detail in the Windows case study in In Solaris, when a thread incurs a page fault, the kernel assigns a page to the faulting thread from the list of free pages it maintains. Therefore, it is imperative that the kernel keep a sufcient amount of free memory available. Associated with this list of free pages is a parameter lotsfree that represents a threshold to begin paging. The lotsfree parameter is typically set to 1/64 the size of the physical memory. Four times per second, the kernel checks whether the amount of free memory is less than lotsfree. If the number of free pages falls below lotsfree, a process known as a pageout starts up. The pageout process is similar to the second-chance algorithm described in Section 9.4.5.2, except that it uses two hands while scanning pages, rather than one. The pageout process works as follows: The front hand of the clock scans all pages in memory, setting the reference bit to 0. Later, the back hand of the clock examines the reference bit for the pages in memory, appending each page whose reference bit is still set to 0 to the free list and writing to disk its contents if modied. Solaris maintains a cache list of pages that have been freed but have not yet been overwritten. The free list contains frames that have invalid contents. Pages can be reclaimed from the cache list if they are accessed before being moved to the free list"
"Explain why writing a program that creates a breach of security, or causing a normal process to change its behavior and create a breach, is a common goal of crackers","Program Threats Processes, along with the kernel, are the only means of accomplishing work on a computer. Hence, writing a program that creates a breach of security, or causing a normal process to change its behavior and create a breach, is a common goal of crackers."," In the remainder of this chapter, we address security at the network and operating-system levels. Security at the physical and human levels, although important, is for the most part beyond the scope of this text. Security within the operating system and between operating systems is implemented in several ways, ranging from passwords for authentication through guarding against viruses to detecting intrusions. We start with an exploration of security threats. 15.2 Program Threats Processes, along with the kernel, are the only means of accomplishing work on a computer. Therefore, writing a program that creates a breach of security, or causing a normal process to change its behavior and create a breach, is a common goal of crackers. In fact, even most nonprogram security events have as their goal causing a program threat. For example, while it is useful to log in to a system without authorization, it is quite a lot more useful to leave behind a back-door daemon that provides information or allows easy access even if the original exploit is blocked. In this section, we describe common methods by which programs cause security breaches. Note that there is considerable variation in the naming conventions for security holes and that we use the most common or descriptive terms. Many systems have mechanisms for allowing programs written by users to be executed by other users. If these programs are executed in a domain that provides the access rights of the executing user, the other users may misuse these rights. A text-editor program, for example, may include code to search the le to be edited for certain keywords. If any are found, the entire le may be copied to a special area accessible to the creator of the text editor"
Explain why the secrecy of k must be protected,"In a symmetric encryption algorithm, the same key is used to encrypt and to decrypt. Hence, the secrecy of k must be protected."," There are two main types of encryption algorithms: symmetric and asymmetric. We discuss both types in the following sections. 15.4.1.1 Symmetric Encryption In a symmetric encryption algorithm, the same key is used to encrypt and to decrypt. Therefore, the secrecy of k must be protected. Figure 15.7 shows an example of two users communicating securely via symmetric encryption over an insecure channel. Note that the key exchange can take place directly between the two parties or via a trusted third party (that is, a certicate authority), as discussed in Section 15.4.1.4. For the past several decades, the most commonly used symmetric encryption algorithm in the United States for civilian applications has been the data-encryption standard (DES) cipher adopted by the National Institute of Standards and Technology (NIST). DES works by taking a 64-bit value and a 56-bit key and performing a series of transformations that are based on substitution and permutation operations. Because DES works on a block of bits at a time, is known as a block cipher, and its transformations are typical of block ciphers. With block ciphers, if the same key is used for encrypting an extended amount of data, it becomes vulnerable to attack"
"Explain why a server running a VMM may have dozens of addresses, and the VMM acts as a virtual switch to route the network packets to the addressed guest","In the area of networking, VMMs also have work to do. General-purpose operating systems typically have one Internet protocol (IP) address, although they sometimes have more than onefor example, to connect to a management network, backup network, and production network. With virtualization, each guest needs at least one IP address, because that is the guests main mode of communication. Hence, a server running a VMM may have dozens of addresses, and the VMM acts as a virtual switch to route the network packets to the addressed guest."," Consider a disk drive to which multiple guests have access. The VMM must provide protection while sharing the device, assuring that a guest can access only the blocks specied in the guests conguration. In such instances, the VMM must be part of every I/O, checking it for correctness as well as routing the data to and from the appropriate devices and guests. In the area of networking, VMMs also have work to do. General-purpose operating systems typically have one Internet protocol (IP) address, although they sometimes have more than onefor example, to connect to a management network, backup network, and production network. With virtualization, each guest needs at least one IP address, because that is the guests main mode of communication. Therefore, a server running a VMM may have dozens of addresses, and the VMM acts as a virtual switch to route the network packets to the addressed guest. The guests can be directly connected to the network by an IP address that is seen by the broader network (this is known as bridging). Alternatively, the VMM can provide a network address translation (NAT) address. The NAT address is local to the server on which the guest is running, and the VMM provides routing between the broader network and the guest. The VMM also provides rewalling, moderating connections between guests within the system and between guests and external systems"
Explain why location independence is a stronger property than is location transparency,"Both denitions relate to the level of naming discussed previously, since les have different names at different levels (that is, user-level textual names and system-level numerical identiers). A location-independent naming scheme is a dynamic mapping, since it can map the same le name to different locations at two different times. Hence, location independence is a stronger property than is location transparency."," 2. Location independence. The name of a le does not need to be changed when the les physical storage location changes. Both denitions relate to the level of naming discussed previously, since les have different names at different levels (that is, user-level textual names and system-level numerical identiers). A location-independent naming scheme is a dynamic mapping, since it can map the same le name to different locations at two different times. Therefore, location independence is a stronger property than is location transparency. In practice, most of the current DFSs provide a static, location-transparent mapping for user-level names. Some support le migrationthat is, changing the location of a le automatically, providing location independence. OpenAFS supports location independence and le mobility, for example. The Hadoop distributed le system (HDFS)a special le system written for the Hadoop frameworkis a more recent creation. It includes le migration but does so without following POSIX standards, providing more exibility in implementation and interface. HDFS keeps track of the location of data but hides this information from clients. This dynamic location transparency allows the underlying mechanism to self-tune. In another example, Amazons 3 cloud storage facility provides blocks of storage on demand via APIs, placing the storage where it sees t and moving the data as necessary to meet performance, reliability, and capacity requirements"
"Explain why in the case of program error, a dump of memory and registers was taken, and the programmer had to debug from the dump","The programmer provided whatever cards or tapes were needed, as well as a short description of how the job was to be run. Of course, the operator could not debug an incorrect program at the console, since the operator would not understand the program. Hence, in the case of program error, a dump of memory and registers was taken, and the programmer had to debug from the dump."," The programmer no longer operated the machine. As soon as one job was nished, the operator could start the next. Since the operator had more experience with mounting tapes than a programmer, setup time was reduced. The programmer provided whatever cards or tapes were needed, as well as a short description of how the job was to be run. Of course, the operator could not debug an incorrect program at the console, since the operator would not understand the program. Therefore, in the case of program error, a dump of memory and registers was taken, and the programmer had to debug from the dump. Dumping the memory and registers allowed the operator to continue immediately with the next job but left the programmer with the more difcult debugging problem. Second, jobs with similar needs were batched together and run through the computer as a group to reduce setup time. For instance, suppose the operator received one FORTRAN job, one COBOL job, and another FORTRAN job. If she ran them in that order, she would have to set up for FORTRAN (load the compiler tapes and so on), then set up for COBOL, and then set up for FORTRAN again. If she ran the two FORTRAN programs as a batch, however, she could setup only once for FORTRAN, saving operator time"
Explain why there are a variety of approaches to constructing a Beowulf cluster,"A Beowulf cluster consists of commodity hardware such as personal computers connected via a simple local-area network. No single specic software package is required to construct a cluster. Rather, the nodes use a set of open-source software libraries to communicate with one another. Hence, there are a variety of approaches to constructing a Beowulf cluster."," Other forms of clusters include parallel clusters and clustering over a wide-area network (WAN) (as described in Chapter 17). Parallel clusters allow multiple hosts to access the same data on shared storage. Because most operating systems lack support for simultaneous data access by multiple hosts, parallel clusters usually require the use of special versions of software and special releases of applications. For example, Oracle Real Application Cluster is a version of Oracles database that has been designed to run on a parallel cluster. Each machine runs Oracle, and a layer of software tracks access to the shared disk. Each machine has full access to all data in the database. To provide this shared access, the system must also supply access control and locking to BEOWULF CLUSTERS Beowulf clusters are designed to solve high-performance computing tasks. A Beowulf cluster consists of commodity hardware such as personal computers connected via a simple local-area network. No single specic software package is required to construct a cluster. Rather, the nodes use a set of open-source software libraries to communicate with one another. Thus, there are a variety of approaches to constructing a Beowulf cluster. Typically, though, Beowulf computing nodes run the Linux operating system. Since Beowulf clusters require no special hardware and operate using open-source software that is available free, they offer a low-cost strategy for building a high-performance computing cluster. In fact, some Beowulf clusters built from discarded personal computers are using hundreds of nodes to solve computationally expensive scientic computing problems. ensure that no conicting operations occur. This function, commonly known as a distributed lock manager (DLM), is included in some cluster technology"
"Explain how whenever the operating system gains control of the computer, it is in kernel mode","At system boot time, the hardware starts in kernel mode. The operating system is then loaded and starts user applications in user mode. Whenever a trap or interrupt occurs, the hardware switches from user mode to kernel mode (that is, changes the state of the mode bit to 0). Hence, whenever the operating system gains control of the computer, it is in kernel mode."," user process user process executing calls system call trap mode bit = 0 return mode bit = 1 user mode (mode bit = 1) kernel mode (mode bit = 0) At the very least, we need two separate modes of operation: user mode and kernel mode (also called supervisor mode, system mode, or privileged mode). A bit, called the mode bit, is added to the hardware of the computer to indicate the current mode: kernel (0) or user (1). With the mode bit, we can distinguish between a task that is executed on behalf of the operating system and one that is executed on behalf of the user. When the computer system is executing on behalf of a user application, the system is in user mode. However, when a user application requests a service from the operating system (via a system call), the system must transition from user to kernel mode to fulll the request. This is shown in Figure 1.10. As we shall see, this architectural enhancement is useful for many other aspects of system operation as well. At system boot time, the hardware starts in kernel mode. The operating system is then loaded and starts user applications in user mode. Whenever a trap or interrupt occurs, the hardware switches from user mode to kernel mode (that is, changes the state of the mode bit to 0). Thus, whenever the operating system gains control of the computer, it is in kernel mode. The system always switches to user mode (by setting the mode bit to 1) before passing control to a user program. The dual mode of operation provides us with the means for protecting the operating system from errant usersand errant users from one another. We accomplish this protection by designating some of the machine instructions that may cause harm as privileged instructions. The hardware allows privileged instructions to be executed only in kernel mode. If an attempt is made to execute a privileged instruction in user mode, the hardware does not execute the instruction but rather treats it as illegal and traps it to the operating system"
"Explain why although two processes may be associated with the same program, they are nevertheless considered two separate execution sequences","We emphasize that a program by itself is not a process. A program is a passive entity, like the contents of a le stored on disk, whereas a process is an active entity. A single-threaded process has one program counter specifying the next instruction to execute. (Threads are covered in Chapter 4.) The execution of such a process must be sequential. The CPU executes one instruction of the process after another, until the process completes. Further, at any time, one instruction at most is executed on behalf of the process. Hence, although two processes may be associated with the same program, they are nevertheless considered two separate execution sequences."," A process needs certain resourcesincluding CPU time, memory, les, and I/O devicesto accomplish its task. These resources are either given to the process when it is created or allocated to it while it is running. In addition to the various physical and logical resources that a process obtains when it is created, various initialization data (input) may be passed along. For example, consider a process whose function is to display the status of a le on the screen of a terminal. The process will be given the name of the le as an input and will execute the appropriate instructions and system calls to obtain and display the desired information on the terminal. When the process terminates, the operating system will reclaim any reusable resources. We emphasize that a program by itself is not a process. A program is a passive entity, like the contents of a le stored on disk, whereas a process is an active entity. A single-threaded process has one program counter specifying the next instruction to execute. (Threads are covered in Chapter 4.) The execution of such a process must be sequential. The CPU executes one instruction of the process after another, until the process completes. Further, at any time, one instruction at most is executed on behalf of the process. Thus, although two processes may be associated with the same program, they are nevertheless considered two separate execution sequences. A multithreaded process has multiple program counters, each pointing to the next instruction to execute for a given thread. A process is the unit of work in a system. A system consists of a collection of processes, some of which are operating-system processes (those that execute system code) and the rest of which are user processes (those that execute user code). All these processes can potentially execute concurrentlyby multiplexing on a single CPU, for example"
Explain why bitmaps are commonly used when there is a need to represent the availability of a large number of resources,"The power of bitmaps becomes apparent when we consider their space efciency. If we were to use an eight-bit Boolean value instead of a single bit, the resulting data structure would be eight times larger. Hence, bitmaps are commonly used when there is a need to represent the availability of a large number of resources."," A bitmap is a string of n binary digits that can be used to represent the status of n items. For example, suppose we have several resources, and the availability of each resource is indicated by the value of a binary digit: 0 means that the resource is available, while 1 indicates that it is unavailable (or vice-versa). The value of the i th position in the bitmap is associated with the i th resource. As an example, consider the bitmap shown below: Resources 2, 4, 5, 6, and 8 are unavailable; resources 0, 1, 3, and 7 are available. The power of bitmaps becomes apparent when we consider their space efciency. If we were to use an eight-bit Boolean value instead of a single bit, the resulting data structure would be eight times larger. Thus, bitmaps are commonly used when there is a need to represent the availability of a large number of resources. Disk drives provide a nice illustration. A medium-sized disk drive might be divided into several thousand individual units, called disk blocks. A bitmap can be used to indicate the availability of each disk block. Data structures are pervasive in operating system implementations. Thus, we will see the structures discussed here, along with others, throughout this text as we explore kernel algorithms and their implementations"
"Explain why we can run Java programs on Java virtual machines, but technically those virtual machines are Java emulators","Some languages, such as BASIC, can be either compiled or interpreted. Java, in contrast, is always interpreted. Interpretation is a form of emulation in that the high-level language code is translated to native CPU instructions, emulating not another CPU but a theoretical virtual machine on which that language could run natively. Hence, we can run Java programs on Java virtual machines, but technically those virtual machines are Java emulators."," A common example of emulation occurs when a computer language is not compiled to native code but instead is either executed in its high-level form or translated to an intermediate form. This is known as interpretation. Some languages, such as BASIC, can be either compiled or interpreted. Java, in contrast, is always interpreted. Interpretation is a form of emulation in that the high-level language code is translated to native CPU instructions, emulating not another CPU but a theoretical virtual machine on which that language could run natively. Thus, we can run Java programs on Java virtual machines, but technically those virtual machines are Java emulators. With virtualization, in contrast, an operating system that is natively compiled for a particular CPU architecture runs within another operating system also native to that CPU. Virtualization rst came about on IBM mainframes as a method for multiple users to run tasks concurrently. Running multiple virtual machines allowed (and still allows) many users to run tasks on a system designed for a single user. Later, in response to problems with running multiple Microsoft Windows XP applications on the Intel x86 CPU, VMware created a new virtualization technology in the form of an application that ran on XP"
Explain why most of the details of the operating-system interface are hidden from the programmer by the API and are managed by the run-time support library,"The caller need know nothing about how the system call is implemented or what it does during execution. Rather, the caller need only obey the API and understand what the operating system will do as a result of the execution of that system call. Hence, most of the details of the operating-system interface are hidden from the programmer by the API and are managed by the run-time support library."," For most programming languages, the run-time support system (a set of functions built into libraries included with a compiler) provides a systemcall interface that serves as the link to system calls made available by the operating system. The system-call interface intercepts function calls in the API and invokes the necessary system calls within the operating system. Typically, a number is associated with each system call, and the system-call interface maintains a table indexed according to these numbers. The system call interface user application open ( ) user mode system call interface kernel mode open ( ) i Implementation of open ( ) system call return then invokes the intended system call in the operating-system kernel and returns the status of the system call and any return values. The caller need know nothing about how the system call is implemented or what it does during execution. Rather, the caller need only obey the API and understand what the operating system will do as a result of the execution of that system call. Thus, most of the details of the operating-system interface are hidden from the programmer by the API and are managed by the run-time support library. The relationship between an API, the system-call interface, and the operating system is shown in Figure 2.6, which illustrates how the operating system handles a user application invoking the open() system call. System calls occur in different ways, depending on the computer in use"
Explain how the design and implementation of the system are simplied,"The main advantage of the layered approach is simplicity of construction and debugging. The layers are selected so that each uses functions (operations) and services of only lower-level layers. This approach simplies debugging and system verication. The rst layer can be debugged without any concern for the rest of the system, because, by denition, it uses only the basic hardware (which is assumed correct) to implement its functions. Once the rst layer is debugged, its correct functioning can be assumed while the second layer is debugged, and so on. If an error is found during the debugging of a particular layer, the error must be on that layer, because the layers below it are already debugged. Hence, the design and implementation of the system are simplied."," layer N user interface layer 1 layer 0 hardware An operating-system layer is an implementation of an abstract object made up of data and the operations that can manipulate those data. A typical operating-system layersay, layer M consists of data structures and a set of routines that can be invoked by higher-level layers. Layer M, in turn, can invoke operations on lower-level layers. The main advantage of the layered approach is simplicity of construction and debugging. The layers are selected so that each uses functions (operations) and services of only lower-level layers. This approach simplies debugging and system verication. The rst layer can be debugged without any concern for the rest of the system, because, by denition, it uses only the basic hardware (which is assumed correct) to implement its functions. Once the rst layer is debugged, its correct functioning can be assumed while the second layer is debugged, and so on. If an error is found during the debugging of a particular layer, the error must be on that layer, because the layers below it are already debugged. Thus, the design and implementation of the system are simplied. Each layer is implemented only with operations provided by lower-level layers. A layer does not need to know how these operations are implemented; it needs to know only what these operations do. Hence, each layer hides the existence of certain data structures, operations, and hardware from higher-level layers"
Explain why the operating system must have some means of computing and displaying measures of system behavior,"We mentioned earlier that performance tuning seeks to improve performance by removing processing bottlenecks. To identify bottlenecks, we must be able to monitor system performance. Hence, the operating system must have some means of computing and displaying measures of system behavior."," dump le within a le system for analysis. Obviously, such strategies would be unnecessary for debugging ordinary user-level processes. We mentioned earlier that performance tuning seeks to improve performance by removing processing bottlenecks. To identify bottlenecks, we must be able to monitor system performance. Thus, the operating system must have some means of computing and displaying measures of system behavior. In a number of systems, the operating system does this by producing trace listings of system behavior. All interesting events are logged with their time and important parameters and are written to a le. Later, an analysis program can process the log le to determine system performance and to identify bottlenecks and inefciencies. These same traces can be run as input for a simulation of a suggested improved system. Traces also can help people to nd errors in operating-system behavior. Another approach to performance tuning uses single-purpose, interactive tools that allow users and administrators to question the state of various system components to look for bottlenecks. One such tool employs the UNIX command top to display the resources used on the system, as well as a sorted list of the top resource-using processes. Other tools display the state of disk I/O, memory allocation, and network trafc"
Explain why the long-term scheduler may need to be invoked only when a process leaves the system,"The long-term scheduler executes much less frequently; minutes may separate the creation of one new process and the next. The long-term scheduler controls the degree of multiprogramming (the number of processes in memory). If the degree of multiprogramming is stable, then the average rate of process creation must be equal to the average departure rate of processes leaving the system. Hence, the long-term scheduler may need to be invoked only when a process leaves the system."," The primary distinction between these two schedulers lies in frequency of execution. The short-term scheduler must select a new process for the CPU frequently. A process may execute for only a few milliseconds before waiting for an I/O request. Often, the short-term scheduler executes at least once every 100 milliseconds. Because of the short time between executions, the short-term scheduler must be fast. If it takes 10 milliseconds to decide to execute a process for 100 milliseconds, then 10/(100 + 10) = 9 percent of the CPU is being used (wasted) simply for scheduling the work. The long-term scheduler executes much less frequently; minutes may separate the creation of one new process and the next. The long-term scheduler controls the degree of multiprogramming (the number of processes in memory). If the degree of multiprogramming is stable, then the average rate of process creation must be equal to the average departure rate of processes leaving the system. Thus, the long-term scheduler may need to be invoked only when a process leaves the system. Because of the longer interval between executions, the long-term scheduler can afford to take more time to decide which process should be selected for execution. It is important that the long-term scheduler make a careful selection. In general, most processes can be described as either I/O bound or CPU bound"
"Explain why when one process creates a new process, the identity of the newly created process is passed to the parent","Note that a parent needs to know the identities of its children if it is to terminate them. Hence, when one process creates a new process, the identity of the newly created process is passed to the parent."," #include <stdio.h> #include <windows.h> int main(VOID) { STARTUPINFO si; PROCESS INFORMATION pi; /* allocate memory */ ZeroMemory(&si, sizeof(si)); si.cb = sizeof(si); ZeroMemory(&pi, sizeof(pi)); /* create child process */ if (!CreateProcess(NULL, /* use command line */ ""C:\\WINDOWS\\system32\\mspaint.exe"", /* command */ NULL, /* dont inherit process handle */ NULL, /* dont inherit thread handle */ FALSE, /* disable handle inheritance */ 0, /* no creation flags */ NULL, /* use parents environment block */ NULL, /* use parents existing directory */ &si, &pi)) { fprintf(stderr, ""Create Process Failed""); return -1; } /* parent will wait for the child to complete */ WaitForSingleObject(pi.hProcess, INFINITE); printf(""Child Complete""); /* close handles */ CloseHandle(pi.hProcess); CloseHandle(pi.hThread); Process Termination A process terminates when it nishes executing its nal statement and asks the operating system to delete it by using the exit() system call. At that point, the process may return a status value (typically an integer) to its parent process (via the wait() system call). All the resources of the processincluding physical and virtual memory, open les, and I/O buffersare deallocated by the operating system. Termination can occur in other circumstances as well. A process can cause the termination of another process via an appropriate system call (for example, TerminateProcess() in Windows). Usually, such a system call can be invoked only by the parent of the process that is to be terminated. Otherwise, users could arbitrarily kill each others jobs. Note that a parent needs to know the identities of its children if it is to terminate them. Thus, when one process creates a new process, the identity of the newly created process is passed to the parent. A parent may terminate the execution of one of its children for a variety of reasons, such as these: The child has exceeded its usage of some of the resources that it has been allocated. (To determine whether this has occurred, the parent must have a mechanism to inspect the state of its children.) The task assigned to the child is no longer required"
Explain why application programs can be considered clients of a subsystem server,"The Windows operating system is an example of modern design that employs modularity to increase functionality and decrease the time needed to implement new features. Windows provides support for multiple operating environments, or subsystems. Application programs communicate with these subsystems via a message-passing mechanism. Hence, application programs can be considered clients of a subsystem server."," The Mach message system attempts to avoid double-copy operations by using virtual-memory-management techniques (Chapter 9). Essentially, Mach maps the address space containing the senders message into the receivers address space. The message itself is never actually copied. This message-management technique provides a large performance boost but works for only intrasystem messages. The Mach operating system is discussed in more detail in the online Appendix B. 3.5 Examples of IPC Systems An Example: Windows The Windows operating system is an example of modern design that employs modularity to increase functionality and decrease the time needed to implement new features. Windows provides support for multiple operating environments, or subsystems. Application programs communicate with these subsystems via a message-passing mechanism. Thus, application programs can be considered clients of a subsystem server. The message-passing facility in Windows is called the advanced local procedure call (ALPC) facility. It is used for communication between two processes on the same machine. It is similar to the standard remote procedure call (RPC) mechanism that is widely used, but it is optimized for and specic to Windows. (Remote procedure calls are covered in detail in Section 3.6.2.) Like Mach, Windows uses a port object to establish and maintain a connection between two processes. Windows uses two types of ports: connection ports and communication ports"
Explain why your echo server must use an object that extends java,"However, the echo server cannot guarantee that it will read characters from clients; it may receive binary data as well. The class java.io.InputStream deals with data at the byte level rather than the character level. Hence, your echo server must use an object that extends java."," The date server shown in Figure 3.21 uses the java.io.BufferedReader class. BufferedReader extends the java.io.Reader class, which is used for reading character streams. However, the echo server cannot guarantee that it will read characters from clients; it may receive binary data as well. The class java.io.InputStream deals with data at the byte level rather than the character level. Thus, your echo server must use an object that extends java.io.InputStream. The read() method in the java.io.InputStream class returns 1 when the client has closed its end of the socket connection. 3.26 Design a program using ordinary pipes in which one process sends a string message to a second process, and the second process reverses the case of each character in the message and sends it back to the rst process. For example, if the rst process sends the message Hi There, the second process will return hI tHERE. This will require using two pipes, one for sending the original message from the rst to the second process and the other for sending the modied message from the second to the rst process. You can write this program using either UNIX or Windows pipes"
Explain why it is possible to have concurrency without parallelism," In contrast, a concurrent system supports more than one task by allowing all the tasks to make progress. Hence, it is possible to have concurrency without parallelism."," Notice the distinction between parallelism and concurrency in this discussion. A system is parallel if it can perform more than one task simultaneously. In contrast, a concurrent system supports more than one task by allowing all the tasks to make progress. Thus, it is possible to have concurrency without parallelism. Before the advent of SMP and multicore architectures, most computer systems had only a single processor. CPU schedulers were designed to provide the illusion of parallelism by rapidly switching between processes in T1 core 2 T4 AMDAHLS LAW Amdahls Law is a formula that identies potential performance gains from adding additional computing cores to an application that has both serial (nonparallel) and parallel components. If S is the portion of the application that must be performed serially on a system with N processing cores, the formula appears as follows: speedup S+ (1S) N As an example, assume we have an application that is 75 percent parallel and 25 percent serial. If we run this application on a system with two processing cores, we can get a speedup of 1.6 times. If we add two additional cores (for a total of four), the speedup is 2.28 times. One interesting fact about Amdahls Law is that as N approaches innity, the speedup converges to 1/S. For example, if 40 percent of an application is performed serially, the maximum speedup is 2.5 times, regardless of the number of processing cores we add. This is the fundamental principle behind Amdahls Law: the serial portion of an application can have a disproportionate effect on the performance we gain by adding additional computing cores"
"Explain why when locks are expected to be held for short times, spinlocks are useful"," Spinlocks do have an advantage, however, in that no context switch is required when a process must wait on a lock, and a context switch may take considerable time. Hence, when locks are expected to be held for short times, spinlocks are useful."," In fact, this type of mutex lock is also called a spinlock because the process spins while waiting for the lock to become available. (We see the same issue with the code examples illustrating the test and set() instruction and the compare and swap() instruction.) This continual looping is clearly a problem in a real multiprogramming system, where a single CPU is shared among many processes. Busy waiting wastes CPU cycles that some other process might be able to use productively. Spinlocks do have an advantage, however, in that no context switch is required when a process must wait on a lock, and a context switch may take considerable time. Thus, when locks are expected to be held for short times, spinlocks are useful. They are often employed on multiprocessor systems where one thread can spin on one processor while another thread performs its critical section on another processor. Later in this chapter (Section 5.7), we examine how mutex locks can be used to solve classical synchronization problems. We also discuss how these locks are used in several operating systems, as well as in Pthreads"
Explain why binary semaphores behave similarly to mutex locks," The value of a binary semaphore can range only between 0 and 1. Hence, binary semaphores behave similarly to mutex locks."," The value of a counting semaphore can range over an unrestricted domain. The value of a binary semaphore can range only between 0 and 1. Thus, binary semaphores behave similarly to mutex locks. In fact, on systems that do not provide mutex locks, binary semaphores can be used instead for providing mutual exclusion. Counting semaphores can be used to control access to a given resource consisting of a nite number of instances. The semaphore is initialized to the number of resources available. Each process that wishes to use a resource performs a wait() operation on the semaphore (thereby decrementing the count). When a process releases a resource, it performs a signal() operation (incrementing the count). When the count for the semaphore goes to 0, all resources are being used. After that, processes that wish to use a resource will block until the count becomes greater than 0"
Explain why a function dened within a monitor can access only those variables declared locally within the monitor and its formal parameters," The representation of a monitor type cannot be used directly by the various processes. Hence, a function dened within a monitor can access only those variables declared locally within the monitor and its formal parameters."," The monitor type also declares the variables whose values dene the state of an instance of that type, along with the bodies of functions that operate on those variables. The syntax of a monitor type is shown in Figure 5.15. The representation of a monitor type cannot be used directly by the various processes. Thus, a function dened within a monitor can access only those variables declared locally within the monitor and its formal parameters. Similarly, the local variables of a monitor can be accessed by only the local functions"
"Explain why on a dual-threaded, dual-core system, four logical processors are presented to the operating system","A dual-threaded processor core on which the execution of thread 0 and the execution of thread 1 are interleaved. From an operating-system perspective, each hardware thread appears as a logical processor that is available to run a software thread. Hence, on a dual-threaded, dual-core system, four logical processors are presented to the operating system."," up to 50 percent of its time waiting for data to become available from memory. To remedy this situation, many recent hardware designs have implemented multithreaded processor cores in which two (or more) hardware threads are assigned to each core. That way, if one thread stalls while waiting for memory, the core can switch to another thread. Figure 6.11 illustrates a dual-threaded processor core on which the execution of thread 0 and the execution of thread 1 are interleaved. From an operating-system perspective, each hardware thread appears as a logical processor that is available to run a software thread. Thus, on a dual-threaded, dual-core system, four logical processors are presented to the operating system. The UltraSPARC T3 CPU has sixteen cores per chip and eight hardware threads per core. From the perspective of the operating system, there appear to be 128 logical processors. In general, there are two ways to multithread a processing core: coarsegrained and ne-grained multithreading. With coarse-grained multithreading, a thread executes on a processor until a long-latency event such as a memory stall occurs. Because of the delay caused by the long-latency event, the processor must switch to another thread to begin execution. However, the cost of switching between threads is high, since the instruction pipeline must thread1 C M M M C C C 6.6 Real-Time CPU Scheduling be ushed before the other thread can begin execution on the processor core"
"Explain why if a task with default priority runs for 200 milliseconds, its vruntime will also be 200 milliseconds","The CFS scheduler doesnt directly assign priorities. Rather, it records how long each task has run by maintaining the virtual run time of each task using the per-task variable vruntime. The virtual run time is associated with a decay factor based on the priority of a task: lower-priority tasks have higher rates of decay than higher-priority tasks. For tasks at normal priority (nice values of 0), virtual run time is identical to actual physical run time. Hence, if a task with default priority runs for 200 milliseconds, its vruntime will also be 200 milliseconds."," Rather than using strict rules that associate a relative priority value with the length of a time quantum, the CFS scheduler assigns a proportion of CPU processing time to each task. This proportion is calculated based on the nice value assigned to each task. Nice values range from 20 to +19, where a numerically lower nice value indicates a higher relative priority. Tasks with lower nice values receive a higher proportion of CPU processing time than tasks with higher nice values. The default nice value is 0. (The term nice comes from the idea that if a task increases its nice value from, say, 0 to +10, it is being nice to other tasks in the system by lowering its relative priority.) CFS doesnt use discrete values of time slices and instead identies a targeted latency, which is an interval of time during which every runnable task should run at least once. Proportions of CPU time are allocated from the value of targeted latency. In addition to having default and minimum values, targeted latency can increase if the number of active tasks in the system grows beyond a certain threshold. The CFS scheduler doesnt directly assign priorities. Rather, it records how long each task has run by maintaining the virtual run time of each task using the per-task variable vruntime. The virtual run time is associated with a decay factor based on the priority of a task: lower-priority tasks have higher rates of decay than higher-priority tasks. For tasks at normal priority (nice values of 0), virtual run time is identical to actual physical run time. Thus, if a task with default priority runs for 200 milliseconds, its vruntime will also be 200 milliseconds. However, if a lower-priority task runs for 200 milliseconds, its vruntime will be higher than 200 milliseconds. Similarly, if a higher-priority task runs for 200 milliseconds, its vruntime will be less than 200 milliseconds. To decide which task to run next, the scheduler simply selects the task that has the smallest vruntime value. In addition, a higher-priority task that becomes available to run can preempt a lower-priority task"
Explain how an application can create and schedule multiple threads without involving the Windows kernel scheduler," Windows 7 introduced user-mode scheduling (UMS), which allows applications to create and manage threads independently of the kernel. Hence, an application can create and schedule multiple threads without involving the Windows kernel scheduler."," When a user is running an interactive program, the system needs to provide especially good performance. For this reason, Windows has a special scheduling rule for processes in the NORMAL PRIORITY CLASS. Windows distinguishes between the foreground process that is currently selected on the screen and the background processes that are not currently selected. When a process moves into the foreground, Windows increases the scheduling quantum by some factortypically by 3. This increase gives the foreground process three times longer to run before a time-sharing preemption occurs. Windows 7 introduced user-mode scheduling (UMS), which allows applications to create and manage threads independently of the kernel. Thus, an application can create and schedule multiple threads without involving the Windows kernel scheduler. For applications that create a large number of threads, scheduling threads in user mode is much more efcient than kernel-mode thread scheduling, as no kernel intervention is necessary. Earlier versions of Windows provided a similar feature known as bers, which allowed several user-mode threads (bers) to be mapped to a single kernel thread. However, bers were of limited practical use. A ber was unable to make calls to the Windows API because all bers had to share the thread environment block (TEB) of the thread on which they were running. This presented a problem if a Windows API function placed state information into the TEB for one ber, only to have the information overwritten by a different ber. UMS overcomes this obstacle by providing each user-mode thread with its own thread context"
Explain why arrival and service distributions are often dened in mathematically tractable but unrealisticways,"Queueing analysis can be useful in comparing scheduling algorithms, but it also has limitations. At the moment, the classes of algorithms and distributions that can be handled are fairly limited. The mathematics of complicated algorithms and distributions can be difcult to work with. Hence, arrival and service distributions are often dened in mathematically tractable but unrealisticways."," We can use Littles formula to compute one of the three variables if we know the other two. For example, if we know that 7 processes arrive every second (on average) and that there are normally 14 processes in the queue, then we can compute the average waiting time per process as 2 seconds. Queueing analysis can be useful in comparing scheduling algorithms, but it also has limitations. At the moment, the classes of algorithms and distributions that can be handled are fairly limited. The mathematics of complicated algorithms and distributions can be difcult to work with. Thus, arrival and service distributions are often dened in mathematically tractable but unrealisticways. It is also generally necessary to make a number of independent assumptions, which may not be accurate. As a result of these difculties, queueing models are often only approximations of real systems, and the accuracy of the computed results may be questionable. To get a more accurate evaluation of scheduling algorithms, we can use simulations. Running simulations involves programming a model of the computer system. Software data structures represent the major components of the system. The simulator has a variable representing a clock. As this variables value is increased, the simulator modies the system state to reect the activities of the devices, the processes, and the scheduler. As the simulation executes, statistics that indicate algorithm performance are gathered and printed"
Explain why resource utilization may be lower than it would otherwise be," In this scheme, if a process requests a resource that is currently available, it may still have to wait. Hence, resource utilization may be lower than it would otherwise be."," Whenever a process requests a resource that is currently available, the system must decide whether the resource can be allocated immediately or whether the process must wait. The request is granted only if the allocation leaves the system in a safe state. In this scheme, if a process requests a resource that is currently available, it may still have to wait. Thus, resource utilization may be lower than it would otherwise be. If we have a resource-allocation system with only one instance of each resource type, we can use a variant of the resource-allocation graph dened in Section 7.2.2 for deadlock avoidance. In addition to the request and assignment edges already described, we introduce a new type of edge, called a claim edge"
"Explain why although the address space of the computer may start at 00000, the rst address of the user process need not be 00000"," Most systems allow a user process to reside in any part of the physical memory. Hence, although the address space of the computer may start at 00000, the rst address of the user process need not be 00000."," The normal single-tasking procedure is to select one of the processes in the input queue and to load that process into memory. As the process is executed, it accesses instructions and data from memory. Eventually, the process terminates, and its memory space is declared available. Most systems allow a user process to reside in any part of the physical memory. Thus, although the address space of the computer may start at 00000, the rst address of the user process need not be 00000. You will see later how a user program actually places a process in physical memory. In most cases, a user program goes through several stepssome of which may be optionalbefore being executed (Figure 8.3). Addresses may be represented in different ways during these steps. Addresses in the source program are generally symbolic (such as the variable count). A compiler typically binds these symbolic addresses to relocatable addresses (such as 14 bytes from the beginning of this module). The linkage editor or loader in turn binds the relocatable addresses to absolute addresses (such as 74014)"
"Explain why in the execution-time address-binding scheme, the logical and physical address spaces differ","CPU physical address memory binding scheme results in differing logical and physical addresses. In this case, we usually refer to the logical address as a virtual address. We use logical address and virtual address interchangeably in this text. The set of all logical addresses generated by a program is a logical address space. The set of all physical addresses corresponding to these logical addresses is a physical address space. Hence, in the execution-time address-binding scheme, the logical and physical address spaces differ."," An address generated by the CPU is commonly referred to as a logical address, whereas an address seen by the memory unitthat is, the one loaded into the memory-address register of the memoryis commonly referred to as a physical address. The compile-time and load-time address-binding methods generate identical logical and physical addresses. However, the execution-time address- relocation register logical address CPU  physical address memory binding scheme results in differing logical and physical addresses. In this case, we usually refer to the logical address as a virtual address. We use logical address and virtual address interchangeably in this text. The set of all logical addresses generated by a program is a logical address space. The set of all physical addresses corresponding to these logical addresses is a physical address space. Thus, in the execution-time address-binding scheme, the logical and physical address spaces differ. The run-time mapping from virtual to physical addresses is done by a hardware device called the memory-management unit (MMU). We can choose from many different methods to accomplish such mapping, as we discuss in Section 8.3 through Section 8.5. For the time being, we illustrate this mapping with a simple MMU scheme that is a generalization of the base-register scheme described in Section 8.1.1. The base register is now called a relocation register"
"Explain why the next time that particular code segment is reached, the library routine is executed directly, incurring no cost for dynamic linking","With dynamic linking, a stub is included in the image for each libraryroutine reference. The stub is a small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present. When the stub is executed, it checks to see whether the needed routine is already in memory. If it is not, the program loads the routine into memory. Either way, the stub replaces itself with the address of the routine and executes the routine. Hence, the next time that particular code segment is reached, the library routine is executed directly, incurring no cost for dynamic linking."," This feature is usually used with system libraries, such as language subroutine libraries. Without this facility, each program on a system must include a copy of its language library (or at least the routines referenced by the program) in the executable image. This requirement wastes both disk space and main memory. With dynamic linking, a stub is included in the image for each libraryroutine reference. The stub is a small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present. When the stub is executed, it checks to see whether the needed routine is already in memory. If it is not, the program loads the routine into memory. Either way, the stub replaces itself with the address of the routine and executes the routine. Thus, the next time that particular code segment is reached, the library routine is executed directly, incurring no cost for dynamic linking. Under this scheme, all processes that use a language library execute only one copy of the library code. This feature can be extended to library updates (such as bug xes). A library may be replaced by a new version, and all programs that reference the library will automatically use the new version. Without dynamic linking, all such programs would need to be relinked to gain access to the new library. So that programs will not accidentally execute new, incompatible versions of libraries, version information is included in both the program and the library. More than one version of a library may be loaded into memory, and each program uses its version information to decide which copy of the library to use. Versions with minor changes retain the same version number, whereas versions with major changes increment the number. Thus, only programs that are compiled with the new library version are affected by any incompatible changes incorporated in it. Other programs linked before the new library was installed will continue using the older library. This system is also known as shared libraries"
Explain why a process with dynamic memory requirements will need to issue system calls (request memory() and release memory()) to inform the operating system of its changing memory needs,"Clearly, it would be useful to know exactly how much memory a user process is using, not simply how much it might be using. Then we would need to swap only what is actually used, reducing swap time. For this method to be effective, the user must keep the system informed of any changes in memory requirements. Hence, a process with dynamic memory requirements will need to issue system calls (request memory() and release memory()) to inform the operating system of its changing memory needs."," The context-switch time in such a swapping system is fairly high. To get an idea of the context-switch time, lets assume that the user process is 100 MB in size and the backing store is a standard hard disk with a transfer rate of 50 MB per second. The actual transfer of the 100-MB process to or from main memory takes 100 MB/50 MB per second = 2 seconds The swap time is 200 milliseconds. Since we must swap both out and in, the total swap time is about 4,000 milliseconds. (Here, we are ignoring other disk performance aspects, which we cover in Chapter 10.) Notice that the major part of the swap time is transfer time. The total transfer time is directly proportional to the amount of memory swapped. If we have a computer system with 4 GB of main memory and a resident operating system taking 1 GB, the maximum size of the user process is 3 GB. However, many user processes may be much smaller than thissay, 100 MB. A 100-MB process could be swapped out in 2 seconds, compared with the 60 seconds required for swapping 3 GB. Clearly, it would be useful to know exactly how much memory a user process is using, not simply how much it might be using. Then we would need to swap only what is actually used, reducing swap time. For this method to be effective, the user must keep the system informed of any changes in memory requirements. Thus, a process with dynamic memory requirements will need to issue system calls (request memory() and release memory()) to inform the operating system of its changing memory needs. Swapping is constrained by other factors as well. If we want to swap a process, we must be sure that it is completely idle. Of particular concern is any pending I/O. A process may be waiting for an I/O operation when we want to swap that process to free up memory. However, if the I/O is asynchronously accessing the user memory for I/O buffers, then the process cannot be swapped. Assume that the I/O operation is queued because the device is busy. If we were to swap out process P1 and swap in process P2 , the I/O operation might then attempt to use memory that now belongs to process P2 . There are two main solutions to this problem: never swap a process with pending I/O, or execute I/O operations only into operating-system buffers"
Explain why the degree of multiprogramming is bound by the number of partitions,"One of the simplest methods for allocating memory is to divide memory into several xed-sized partitions. Each partition may contain exactly one process. Hence, the degree of multiprogramming is bound by the number of partitions."," The relocation-register scheme provides an effective way to allow the operating systems size to change dynamically. This exibility is desirable in many situations. For example, the operating system contains code and buffer space for device drivers. If a device driver (or other operating-system service) is not commonly used, we do not want to keep the code and data in memory, as we might be able to use that space for other purposes. Such code is sometimes called transient operating-system code; it comes and goes as needed. Thus, using this code changes the size of the operating system during program execution. relocation register limit register logical address CPU physical address memory trap: addressing error Now we are ready to turn to memory allocation. One of the simplest methods for allocating memory is to divide memory into several xed-sized partitions. Each partition may contain exactly one process. Thus, the degree of multiprogramming is bound by the number of partitions. In this multiplepartition method, when a partition is free, a process is selected from the input queue and is loaded into the free partition. When the process terminates, the partition becomes available for another process. This method was originally used by the IBM OS/360 operating system (called MFT)but is no longer in use. The method described next is a generalization of the xed-partition scheme (called MVT); it is used primarily in batch environments. Many of the ideas presented here are also applicable to a time-sharing environment in which pure segmentation is used for memory management (Section 8.4)"
"Explain why a logical address consists of a two tuple: <segment-number, offset>"," For simplicity of implementation, segments are numbered and are referred to by a segment number, rather than by a segment name. Hence, a logical address consists of a two tuple: <segment-number, offset>."," subroutine stack symbol table main program Each segment has a name and a length. The addresses specify both the segment name and the offset within the segment. The programmer therefore species each address by two quantities: a segment name and an offset. For simplicity of implementation, segments are numbered and are referred to by a segment number, rather than by a segment name. Thus, a logical address consists of a two tuple: <segment-number, offset>. Normally, when a program is compiled, the compiler automatically constructs segments reecting the input program"
"Explain why if the process requires n pages, at least n frames must be available in memory","When a process arrives in the system to be executed, its size, expressed in pages, is examined. Each page of the process needs one frame. Hence, if the process requires n pages, at least n frames must be available in memory."," If frame size is 4 KB (212 ), then a system with 4-byte entries can address 244 bytes (or 16 TB) of physical memory. We should note here that the size of physical memory in a paged memory system is different from the maximum logical size of a process. As we further explore paging, we introduce other information that must be kept in the page-table entries. That information reduces the number 8.5 Paging free-frame list 13 20 page 0 page 1 page 2 page 3 new process 15 17 19 21 (a) free-frame list page 0 page 1 page 2 page 3 new process 0 14 1 13 2 18 3 20 new-process page table 18 page 2 20 page 3 of bits available to address page frames. Thus, a system with 32-bit page-table entries may address less physical memory than the possible maximum. A 32-bit CPU uses 32-bit addresses, meaning that a given process space can only be 232 bytes (4 TB). Therefore, paging lets us use physical memory that is larger than what can be addressed by the CPUs address pointer length. When a process arrives in the system to be executed, its size, expressed in pages, is examined. Each page of the process needs one frame. Thus, if the process requires n pages, at least n frames must be available in memory. If n frames are available, they are allocated to this arriving process. The rst page of the process is loaded into one of the allocated frames, and the frame number is put in the page table for this process. The next page is loaded into another frame, its frame number is put into the page table, and so on (Figure 8.13). An important aspect of paging is the clear separation between the programmers view of memory and the actual physical memory. The programmer views memory as one single space, containing only this one program. In fact, the user program is scattered throughout physical memory, which also holds other programs. The difference between the programmers view of memory and the actual physical memory is reconciled by the address-translation hardware"
Explain why memory access is slowed by a factor of 2,"The problem with this approach is the time required to access a user memory location. If we want to access location i, we must rst index into the page table, using the value in the PTBR offset by the page number for i. This task requires a memory access. It provides us with the frame number, which is combined with the page offset to produce the actual address. We can then access the desired place in memory. With this scheme, two memory accesses are needed to access a byte (one for the page-table entry, one for the byte). Hence, memory access is slowed by a factor of 2."," For these machines, the use of fast registers to implement the page table is not feasible. Rather, the page table is kept in main memory, and a page-table base register (PTBR) points to the page table. Changing page tables requires changing only this one register, substantially reducing context-switch time. The problem with this approach is the time required to access a user memory location. If we want to access location i, we must rst index into the page table, using the value in the PTBR offset by the page number for i. This task requires a memory access. It provides us with the frame number, which is combined with the page offset to produce the actual address. We can then access the desired place in memory. With this scheme, two memory accesses are needed to access a byte (one for the page-table entry, one for the byte). Thus, memory access is slowed by a factor of 2. This delay would be intolerable under most circumstances. We might as well resort to swapping! The standard solution to this problem is to use a special, small, fastlookup hardware cache called a translation look-aside buffer (TLB). The TLB is associative, high-speed memory. Each entry in the TLB consists of two parts: a key (or tag) and a value. When the associative memory is presented with an item, the item is compared with all keys simultaneously. If the item is found, the corresponding value eld is returned. The search is fast; a TLB lookup in modern hardware is part of the instruction pipeline, essentially adding no performance penalty. To be able to execute the search within a pipeline step, however, the TLB must be kept small. It is typically between 32 and 1,024 entries in size. Some CPUs implement separate instruction and data address TLBs. That can double the number of TLB entries available, because those lookups occur in different pipeline steps. We can see in this development an example of the evolution of CPU technology: systems have evolved from having no TLBs to having multiple levels of TLBs, just as they have multiple levels of caches. The TLB is used with page tables in the following way. The TLB contains only a few of the page-table entries. When a logical address is generated by the CPU, its page number is presented to the TLB. If the page number is found, its frame number is immediately available and is used to access memory. As just mentioned, these steps are executed as part of the instruction pipeline within the CPU, adding no performance penalty compared with a system that does not implement paging"
Explain why two or more processes can execute the same code at the same time," Reentrant code is non-self-modifying code: it never changes during execution. Hence, two or more processes can execute the same code at the same time."," An advantage of paging is the possibility of sharing common code. This consideration is particularly important in a time-sharing environment. Consider a system that supports 40 users, each of whom executes a text editor. If the text editor consists of 150 KB of code and 50 KB of data space, we need 8,000 KB to support the 40 users. If the code is reentrant code (or pure code), however, it can be shared, as shown in Figure 8.16. Here, we see three processes sharing a three-page editoreach page 50 KB in size (the large page size is used to simplify the gure). Each process has its own data page. Reentrant code is non-self-modifying code: it never changes during execution. Thus, two or more processes can execute the same code at the same time. 8.5 Paging ed 1 3 ed 2 ed 3 6 data 1 data 3 ed 1 ed 2 data 1 process P1 page table for P1 ed 1 ed 2 ed 3 data 2 ed 1 ed 2 ed 3 7 ed 3 page table for P2 data 2 6 process P2 page table for P3 11 Each process has its own copy of registers and data storage to hold the data for the processs execution. The data for two different processes will, of course, be different"
Explain how the segmentation and paging units form the equivalent of the memory-management unit (MMU),"Memory management in IA-32 systems is divided into two components segmentation and pagingand works as follows: The CPU generates logical addresses, which are given to the segmentation unit. The segmentation unit produces a linear address for each logical address. The linear address is then given to the paging unit, which in turn generates the physical address in main memory. Hence, the segmentation and paging units form the equivalent of the memory-management unit (MMU)."," logical address segmentation unit linear address paging unit physical address physical memory In this section, we examine address translation for both IA-32 and x86-64 architectures. Before we proceed, however, it is important to note that because Intel has released several versionsas well as variationsof its architectures over the years, we cannot provide a complete description of the memorymanagement structure of all its chips. Nor can we provide all of the CPU details, as that information is best left to books on computer architecture. Rather, we present the major memory-management concepts of these Intel CPUs. 8.7.1 IA-32 Architecture Memory management in IA-32 systems is divided into two components segmentation and pagingand works as follows: The CPU generates logical addresses, which are given to the segmentation unit. The segmentation unit produces a linear address for each logical address. The linear address is then given to the paging unit, which in turn generates the physical address in main memory. Thus, the segmentation and paging units form the equivalent of the memory-management unit (MMU). This scheme is shown in Figure 8.21. 8.7.1.1 IA-32 Segmentation The IA-32 architecture allows a segment to be as large as 4 GB, and the maximum number of segments per process is 16 K. The logical address space of a process is divided into two partitions. The rst partition consists of up to 8 K segments that are private to that process. The second partition consists of up to 8 K segments that are shared among all the processes. Information about the rst partition is kept in the local descriptor table (LDT); information about the second partition is kept in the global descriptor table (GDT). Each entry in the LDT and GDT consists of an 8-byte segment descriptor with detailed information about a particular segment, including the base location and limit of that segment"
Explain why a bad replacement choice increases the page-fault rate and slows process execution,"Notice that, even if we select for replacement a page that is in active use, everything still works correctly. After we replace an active page with a new one, a fault occurs almost immediately to retrieve the active page. Some other page must be replaced to bring the active page back into memory. Hence, a bad replacement choice increases the page-fault rate and slows process execution."," However, its performance is not always good. On the one hand, the page replaced may be an initialization module that was used a long time ago and is no longer needed. On the other hand, it could contain a heavily used variable that was initialized early and is in constant use. Notice that, even if we select for replacement a page that is in active use, everything still works correctly. After we replace an active page with a new one, a fault occurs almost immediately to retrieve the active page. Some other page must be replaced to bring the active page back into memory. Thus, a bad replacement choice increases the page-fault rate and slows process execution. It does not, however, cause incorrect execution"
Explain why a page that is given a second chance will not be replaced until all other pages have been replaced (or given  9," When a page gets a second chance, its reference bit is cleared, and its arrival time is reset to the current time. Hence, a page that is given a second chance will not be replaced until all other pages have been replaced (or given  9."," The number of bits of history included in the shift register can be varied, of course, and is selected (depending on the hardware available) to make the updating as fast as possible. In the extreme case, the number can be reduced to zero, leaving only the reference bit itself. This algorithm is called the second-chance page-replacement algorithm. 9.4.5.2 Second-Chance Algorithm The basic algorithm of second-chance replacement is a FIFO replacement algorithm. When a page has been selected, however, we inspect its reference bit. If the value is 0, we proceed to replace this page; but if the reference bit is set to 1, we give the page a second chance and move on to select the next FIFO page. When a page gets a second chance, its reference bit is cleared, and its arrival time is reset to the current time. Thus, a page that is given a second chance will not be replaced until all other pages have been replaced (or given 9.4 Page Replacement reference bits 0 0 0 next victim 1 pages reference bits circular queue of pages (a) second chances). In addition, if a page is used often enough to keep its reference bit set, it will never be replaced. One way to implement the second-chance algorithm (sometimes referred to as the clock algorithm) is as a circular queue. A pointer (that is, a hand on the clock) indicates which page is to be replaced next. When a frame is needed, the pointer advances until it nds a page with a 0 reference bit. As it advances, it clears the reference bits (Figure 9.17). Once a victim page is found, the page is replaced, and the new page is inserted in the circular queue in that position"
Explain why global replacement generally results in greater system throughput and is therefore the more commonly used method,"One problem with a global replacement algorithm is that a process cannot control its own page-fault rate. The set of pages in memory for a process depends not only on the paging behavior of that process but also on the paging behavior of other processes. Therefore, the same process may perform quite differently (for example, taking 0.5 seconds for one execution and 10.3 seconds for the next execution) because of totally external circumstances. Such is not the case with a local replacement algorithm. Under local replacement, the set of pages in memory for a process is affected by the paging behavior of only that process. Local replacement might hinder a process, however, by not making available to it other, less used pages of memory. Hence, global replacement generally results in greater system throughput and is therefore the more commonly used method."," For example, consider an allocation scheme wherein we allow high-priority processes to select frames from low-priority processes for replacement. A process can select a replacement from among its own frames or the frames of any lower-priority process. This approach allows a high-priority process to increase its frame allocation at the expense of a low-priority process. With a local replacement strategy, the number of frames allocated to a process does not change. With global replacement, a process may happen to select only frames allocated to other processes, thus increasing the number of frames allocated to it (assuming that other processes do not choose its frames for replacement). One problem with a global replacement algorithm is that a process cannot control its own page-fault rate. The set of pages in memory for a process depends not only on the paging behavior of that process but also on the paging behavior of other processes. Therefore, the same process may perform quite differently (for example, taking 0.5 seconds for one execution and 10.3 seconds for the next execution) because of totally external circumstances. Such is not the case with a local replacement algorithm. Under local replacement, the set of pages in memory for a process is affected by the paging behavior of only that process. Local replacement might hinder a process, however, by not making available to it other, less used pages of memory. Thus, global replacement generally results in greater system throughput and is therefore the more commonly used method. Thus far in our coverage of virtual memory, we have assumed that all main memory is created equalor at least that it is accessed equally. On many computer systems, that is not the case. Often, in systems with multiple CPUs (Section 1.3.2), a given CPU can access some sections of main memory faster than it can access others. These performance differences are caused by how CPUs and memory are interconnected in the system. Frequently, such a system is made up of several system boards, each containing multiple CPUs and some memory. The system boards are interconnected in various ways, ranging from system buses to high-speed network connections like InniBand. As you might expect, the CPUs on a particular board can access the memory on that board with less delay than they can access memory on other boards in the system. Systems in which memory access times vary signicantly are known collectively as non-uniform memory access (NUMA) systems, and without exception, they are slower than systems in which memory and CPUs are located on the same motherboard"
Explain why the effective access time will increase even for a process that is not thrashing,"With local replacement, if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash as well. However, the problem is not entirely solved. If processes are thrashing, they will be in the queue for the paging device most of the time. The average service time for a page fault will increase because of the longer average queue for the paging device. Hence, the effective access time will increase even for a process that is not thrashing."," This phenomenon is illustrated in Figure 9.18, in which CPU utilization is plotted against the degree of multiprogramming. As the degree of multiprogramming increases, CPU utilization also increases, although more slowly, until a maximum is reached. If the degree of multiprogramming is increased even further, thrashing sets in, and CPU utilization drops sharply. At this point, to increase CPU utilization and stop thrashing, we must decrease the degree of multiprogramming. CPU utilization degree of multiprogramming 9.6 Thrashing We can limit the effects of thrashing by using a local replacement algorithm (or priority replacement algorithm). With local replacement, if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash as well. However, the problem is not entirely solved. If processes are thrashing, they will be in the queue for the paging device most of the time. The average service time for a page fault will increase because of the longer average queue for the paging device. Thus, the effective access time will increase even for a process that is not thrashing. To prevent thrashing, we must provide a process with as many frames as it needs. But how do we know how many frames it needs? There are several techniques. The working-set strategy (Section 9.6.2) starts by looking at how many frames a process is actually using. This approach denes the locality model of process execution"
Explain how working set optimizes CPU utilization," This working-set strategy prevents thrashing while keeping the degree of multiprogramming as high as possible. Hence, it optimizes CPU utilization."," WS(t 1) = {1,2,5,6,7} WS(t 2 ) = {3,4} that working set enough frames to provide it with its working-set size. If there are enough extra frames, another process can be initiated. If the sum of the working-set sizes increases, exceeding the total number of available frames, the operating system selects a process to suspend. The processs pages are written out (swapped), and its frames are reallocated to other processes. The suspended process can be restarted later. This working-set strategy prevents thrashing while keeping the degree of multiprogramming as high as possible. Thus, it optimizes CPU utilization. The difculty with the working-set model is keeping track of the working set. The working-set window is a moving window. At each memory reference, a new reference appears at one end, and the oldest reference drops off the other end. A page is in the working set if it is referenced anywhere in the working-set window"
Explain how we can directly measure and control the page-fault rate to prevent thrashing,"The specic problem is how to prevent thrashing. Thrashing has a high page-fault rate. Thus, we want to control the page-fault rate. When it is too high, we know that the process needs more frames. Conversely, if the page-fault rate is too low, then the process may have too many frames. We can establish upper and lower bounds on the desired page-fault rate (Figure 9.21). If the actual page-fault rate exceeds the upper limit, we allocate the process another page-fault rate increase number of frames upper bound lower bound decrease number of frames number of frames frame. If the page-fault rate falls below the lower limit, we remove a frame from the process. Hence, we can directly measure and control the page-fault rate to prevent thrashing."," The working-set model is successful, and knowledge of the working set can be useful for prepaging (Section 9.9.1), but it seems a clumsy way to control thrashing. A strategy that uses the page-fault frequency (PFF) takes a more direct approach. The specic problem is how to prevent thrashing. Thrashing has a high page-fault rate. Thus, we want to control the page-fault rate. When it is too high, we know that the process needs more frames. Conversely, if the page-fault rate is too low, then the process may have too many frames. We can establish upper and lower bounds on the desired page-fault rate (Figure 9.21). If the actual page-fault rate exceeds the upper limit, we allocate the process another page-fault rate increase number of frames upper bound lower bound decrease number of frames number of frames frame. If the page-fault rate falls below the lower limit, we remove a frame from the process. Thus, we can directly measure and control the page-fault rate to prevent thrashing. As with the working-set strategy, we may have to swap out a process. If the page-fault rate increases and no free frames are available, we must select some process and swap it out to backing store. The freed frames are then distributed to processes with high page-fault rates"
"Explain how when the kernel requests memory for an object, the slab allocator returns the exact amount of memory required to represent the object","The slab allocator provides two main benets: 1. No memory is wasted due to fragmentation. Fragmentation is not an issue because each unique kernel data structure has an associated cache, and each cache is made up of one or more slabs that are divided into chunks the size of the objects being represented. Hence, when the kernel requests memory for an object, the slab allocator returns the exact amount of memory required to represent the object."," The slab allocator rst attempts to satisfy the request with a free object in a partial slab. If none exists, a free object is assigned from an empty slab. If no empty slabs are available, a new slab is allocated from contiguous physical pages and assigned to a cache; memory for the object is allocated from this slab. The slab allocator provides two main benets: 1. No memory is wasted due to fragmentation. Fragmentation is not an issue because each unique kernel data structure has an associated cache, and each cache is made up of one or more slabs that are divided into chunks the size of the objects being represented. Thus, when the kernel requests memory for an object, the slab allocator returns the exact amount of memory required to represent the object. 2. Memory requests can be satised quickly. The slab allocation scheme is thus particularly effective for managing memory when objects are frequently allocated and deallocated, as is often the case with requests from the kernel. The act of allocatingand releasingmemory can be a time-consuming process. However, objects are created in advance and thus can be quickly allocated from the cache. Furthermore, when the kernel has nished with an object and releases it, it is marked as free and returned to its cache, thus making it immediately available for subsequent requests from the kernel"
Explain why SLUB provides better performance as the number of processors on a system increases,"SLAB allocator maintains for objects in each cache. For systems with a large number of processors, the amount of memory allocated to these queues was not insignicant. Hence, SLUB provides better performance as the number of processors on a system increases."," Recent distributions of Linux now include two other kernel memory allocatorsthe SLOB and SLUB allocators. (Linux refers to its slab implementation as SLAB.) The SLOB allocator is designed for systems with a limited amount of memory, such as embedded systems. SLOB (which stands for Simple List of Blocks) works by maintaining three lists of objects: small (for objects less than 256 bytes), medium (for objects less than 1,024 bytes), and large (for objects less than 1,024 bytes). Memory requests are allocated from an object on an appropriately sized list using a rst-t policy. Beginning with Version 2.6.24, the SLUB allocator replaced SLAB as the default allocator for the Linux kernel. SLUB addresses performance issues with slab allocation by reducing much of the overhead required by the SLAB allocator. One change is to move the metadata that is stored with each slab under SLAB allocation to the page structure the Linux kernel uses for each page. Additionally, SLUB removes the per-CPU queues that the SLAB allocator maintains for objects in each cache. For systems with a large number of processors, the amount of memory allocated to these queues was not insignicant. Thus, SLUB provides better performance as the number of processors on a system increases. Other Considerations The major decisions that we make for a paging system are the selections of a replacement algorithm and an allocation policy, which we discussed earlier in this chapter. There are many other considerations as well, and we discuss several of them here"
Explain why a part of the nal page must be allocated (because pages are the units of allocation) but will be unused (creating internal fragmentation),"Memory is better utilized with smaller pages, however. If a process is allocated memory starting at location 00000 and continuing until it has as much as it needs, it probably will not end exactly on a page boundary. Hence, a part of the nal page must be allocated (because pages are the units of allocation) but will be unused (creating internal fragmentation)."," How do we select a page size? One concern is the size of the page table. For a given virtual memory space, decreasing the page size increases the number of pages and hence the size of the page table. For a virtual memory of 4 MB (222 ), for example, there would be 4,096 pages of 1,024 bytes but only 512 pages of 8,192 bytes. Because each active process must have its own copy of the page table, a large page size is desirable. Memory is better utilized with smaller pages, however. If a process is allocated memory starting at location 00000 and continuing until it has as much as it needs, it probably will not end exactly on a page boundary. Thus, a part of the nal page must be allocated (because pages are the units of allocation) but will be unused (creating internal fragmentation). Assuming independence of process size and page size, we can expect that, on the average, half of the nal page of each process will be wasted. This loss is only 256 bytes for a page of 512 bytes but is 4,096 bytes for a page of 8,192 bytes. To minimize internal fragmentation, then, we need a small page size. Another problem is the time required to read or write a page. I/O time is composed of seek, latency, and transfer times. Transfer time is proportional to the amount transferred (that is, the page size)a fact that would seem to argue for a small page size. However, as we shall see in Section 10.1.1, latency and seek time normally dwarf transfer time. At a transfer rate of 2 MB per second, it takes only 0.2 milliseconds to transfer 512 bytes. Latency time, though, is perhaps 8 milliseconds, and seek time 20 milliseconds. Of the total I/O time (28.2 milliseconds), therefore, only 1 percent is attributable to the actual transfer"
Explain why a desire to minimize I/O time argues for a larger page size,"Doubling the page size increases I/O time to only 28.4 milliseconds. It takes 28.4 milliseconds to read a single page of 1,024 bytes but 56.4 milliseconds to read the same amount as two pages of 512 bytes each. Hence, a desire to minimize I/O time argues for a larger page size."," Another problem is the time required to read or write a page. I/O time is composed of seek, latency, and transfer times. Transfer time is proportional to the amount transferred (that is, the page size)a fact that would seem to argue for a small page size. However, as we shall see in Section 10.1.1, latency and seek time normally dwarf transfer time. At a transfer rate of 2 MB per second, it takes only 0.2 milliseconds to transfer 512 bytes. Latency time, though, is perhaps 8 milliseconds, and seek time 20 milliseconds. Of the total I/O time (28.2 milliseconds), therefore, only 1 percent is attributable to the actual transfer. Doubling the page size increases I/O time to only 28.4 milliseconds. It takes 28.4 milliseconds to read a single page of 1,024 bytes but 56.4 milliseconds to read the same amount as two pages of 512 bytes each. Thus, a desire to minimize I/O time argues for a larger page size. With a smaller page size, though, total I/O should be reduced, since locality will be improved. A smaller page size allows each page to match program locality more accurately. For example, consider a process 200 KB in size, of which only half (100 KB) is actually used in an execution. If we have only one large page, we must bring in the entire page, a total of 200 KB transferred and allocated. If instead we had pages of only 1 byte, then we could bring in only the 100 KB that are actually used, resulting in only 100 KB transferred and allocated. With a smaller page size, then, we have better resolution, allowing us to isolate only the memory that is actually needed. With a larger page size, we must allocate and transfer not only what is needed but also anything else that happens to be in the page, whether it is needed or not. Thus, a smaller page size should result in less I/O and less total allocated memory"
Explain why a smaller page size should result in less I/O and less total allocated memory,"With a smaller page size, though, total I/O should be reduced, since locality will be improved. A smaller page size allows each page to match program locality more accurately. For example, consider a process 200 KB in size, of which only half (100 KB) is actually used in an execution. If we have only one large page, we must bring in the entire page, a total of 200 KB transferred and allocated. If instead we had pages of only 1 byte, then we could bring in only the 100 KB that are actually used, resulting in only 100 KB transferred and allocated. With a smaller page size, then, we have better resolution, allowing us to isolate only the memory that is actually needed. With a larger page size, we must allocate and transfer not only what is needed but also anything else that happens to be in the page, whether it is needed or not. Hence, a smaller page size should result in less I/O and less total allocated memory."," Doubling the page size increases I/O time to only 28.4 milliseconds. It takes 28.4 milliseconds to read a single page of 1,024 bytes but 56.4 milliseconds to read the same amount as two pages of 512 bytes each. Thus, a desire to minimize I/O time argues for a larger page size. With a smaller page size, though, total I/O should be reduced, since locality will be improved. A smaller page size allows each page to match program locality more accurately. For example, consider a process 200 KB in size, of which only half (100 KB) is actually used in an execution. If we have only one large page, we must bring in the entire page, a total of 200 KB transferred and allocated. If instead we had pages of only 1 byte, then we could bring in only the 100 KB that are actually used, resulting in only 100 KB transferred and allocated. With a smaller page size, then, we have better resolution, allowing us to isolate only the memory that is actually needed. With a larger page size, we must allocate and transfer not only what is needed but also anything else that happens to be in the page, whether it is needed or not. Thus, a smaller page size should result in less I/O and less total allocated memory. But did you notice that with a page size of 1 byte, we would have a page fault for each byte? A process of 200 KB that used only half of that memory would generate only one page fault with a page size of 200 KB but 102,400 page faults with a page size of 1 byte. Each page fault generates the large amount of overhead needed for processing the interrupt, saving registers, replacing a page, queueing for the paging device, and updating tables. To minimize the number of page faults, we need to have a large page size"
Explain why each memory reference through the page table takes two accesses,"Consider a demand-paging system with a paging disk that has an average access and transfer time of 20 milliseconds. Addresses are translated through a page table in main memory, with an access time of 1 microsecond per memory access. Hence, each memory reference through the page table takes two accesses."," What is the initial value of the counters? When are counters increased? When are counters decreased? How is the page to be replaced selected? How many page faults occur for your algorithm for the following reference string with four page frames? 1, 2, 3, 4, 5, 3, 4, 1, 6, 7, 8, 7, 8, 9, 7, 8, 9, 5, 4, 5, 4, 2. What is the minimum number of page faults for an optimal pagereplacement strategy for the reference string in part b with four page frames? Consider a demand-paging system with a paging disk that has an average access and transfer time of 20 milliseconds. Addresses are translated through a page table in main memory, with an access time of 1 microsecond per memory access. Thus, each memory reference through the page table takes two accesses. To improve this time, we have added an associative memory that reduces access time to one memory reference if the page-table entry is in the associative memory. Assume that 80 percent of the accesses are in the associative memory and that, of those remaining, 10 percent (or 2 percent of the total) cause page faults. What is the effective memory access time? What is the cause of thrashing? How does the system detect thrashing? Once it detects thrashing, what can the system do to eliminate this problem? Is it possible for a process to have two working sets, one representing data and another representing code? Explain"
Explain how it may be easiest to think of NAS as simply another storage-access protocol,"A network-attached storage (NAS) device is a special-purpose storage system that is accessed remotely over a data network (Figure 10.2). Clients access network-attached storage via a remote-procedure-call interface such as NFS for UNIX systems or CIFS for Windows machines. The remote procedure calls (RPCs) are carried via TCP or UDP over an IP networkusually the same localarea network (LAN) that carries all data trafc to the clients. Hence, it may be easiest to think of NAS as simply another storage-access protocol."," A wide variety of storage devices are suitable for use as host-attached storage. Among these are hard disk drives, RAID arrays, and CD, DVD, and tape drives. The I/O commands that initiate data transfers to a host-attached storage device are reads and writes of logical data blocks directed to specically identied storage units (such as bus ID or target logical unit). A network-attached storage (NAS) device is a special-purpose storage system that is accessed remotely over a data network (Figure 10.2). Clients access network-attached storage via a remote-procedure-call interface such as NFS for UNIX systems or CIFS for Windows machines. The remote procedure calls (RPCs) are carried via TCP or UDP over an IP networkusually the same localarea network (LAN) that carries all data trafc to the clients. Thus, it may be easiest to think of NAS as simply another storage-access protocol. The networkattached storage unit is usually implemented as a RAID array with software that implements the RPC interface. client NAS LAN/WAN NAS client Network-attached storage provides a convenient way for all the computers on a LAN to share a pool of storage with the same ease of naming and access enjoyed with local host-attached storage. However, it tends to be less efcient and have lower performance than some direct-attached storage options"
Explain why networksrather than SCSI cablescan be used as the interconnects between hosts and their storage," iSCSI is the latest network-attached storage protocol. In essence, it uses the IP network protocol to carry the SCSI protocol. Hence, networksrather than SCSI cablescan be used as the interconnects between hosts and their storage."," client NAS LAN/WAN NAS client Network-attached storage provides a convenient way for all the computers on a LAN to share a pool of storage with the same ease of naming and access enjoyed with local host-attached storage. However, it tends to be less efcient and have lower performance than some direct-attached storage options. iSCSI is the latest network-attached storage protocol. In essence, it uses the IP network protocol to carry the SCSI protocol. Thus, networksrather than SCSI cablescan be used as the interconnects between hosts and their storage. As a result, hosts can treat their storage as if it were directly attached, even if the storage is distant from the host"
"Explain why even if a disk fails, data are not lost"," The solution to the problem of reliability is to introduce redundancy; we store extra information that is not normally needed but that can be used in the event of failure of a disk to rebuild the lost information. Hence, even if a disk fails, data are not lost."," Lets rst consider the reliability of RAIDs. The chance that some disk out of a set of N disks will fail is much higher than the chance that a specic single disk will fail. Suppose that the mean time to failure of a single disk is 100,000 hours. Then the mean time to failure of some disk in an array of 100 disks will be 100,000/100 = 1,000 hours, or 41.66 days, which is not long at all! If we store only one copy of the data, then each disk failure will result in loss of a signicant amount of dataand such a high rate of data loss is unacceptable. The solution to the problem of reliability is to introduce redundancy; we store extra information that is not normally needed but that can be used in the event of failure of a disk to rebuild the lost information. Thus, even if a disk fails, data are not lost. The simplest (but most expensive) approach to introducing redundancy is to duplicate every disk. This technique is called mirroring. With mirroring, a logical disk consists of two physical disks, and every write is carried out on both disks. The result is called a mirrored volume. If one of the disks in the volume fails, the data can be read from the other. Data will be lost only if the second disk fails before the rst failed disk is replaced"
Explain how all single-bit errors are detected by the memory system,"RAID level 2 is also known as memory-style error-correctingcode (ECC) organization. Memory systems have long detected certain errors by using parity bits. Each byte in a memory system may have a parity bit associated with it that records whether the number of bits in the byte set to 1 is even (parity = 0) or odd (parity = 1). If one of the bits in the byte is damaged (either a 1 becomes a 0, or a 0 becomes a 1), the parity of the byte changes and thus does not match the stored parity. Similarly, if the stored parity bit is damaged, it does not match the computed parity. Hence, all single-bit errors are detected by the memory system."," RAID level 1. RAID level 1 refers to disk mirroring. Figure 10.11(b) shows a mirrored organization. RAID level 2. RAID level 2 is also known as memory-style error-correctingcode (ECC) organization. Memory systems have long detected certain errors by using parity bits. Each byte in a memory system may have a parity bit associated with it that records whether the number of bits in the byte set to 1 is even (parity = 0) or odd (parity = 1). If one of the bits in the byte is damaged (either a 1 becomes a 0, or a 0 becomes a 1), the parity of the byte changes and thus does not match the stored parity. Similarly, if the stored parity bit is damaged, it does not match the computed parity. Thus, all single-bit errors are detected by the memory system. Error-correcting schemes store two or more extra bits and can reconstruct the data if a single bit is damaged. The idea of ECC can be used directly in disk arrays via striping of bytes across disks. For example, the rst bit of each byte can be stored in disk 1, the second bit in disk 2, and so on until the eighth bit is stored in disk 8; the error-correction bits are stored in further disks. This scheme is shown in Figure 10.11(c), where the disks labeled P store the errorcorrection bits. If one of the disks fails, the remaining bits of the byte and the associated error-correction bits can be read from other disks and used to reconstruct the damaged data. Note that RAID level 2 requires only three disks overhead for four disks of data, unlike RAID level 1, which requires four disks overhead"
"Explain why the data-transfer rate for each access is slower, but multiple read accesses can proceed in parallel, leading to a higher overall I/O rate"," A block read accesses only one disk, allowing other requests to be processed by the other disks. Hence, the data-transfer rate for each access is slower, but multiple read accesses can proceed in parallel, leading to a higher overall I/O rate."," RAID level 4. RAID level 4, or block-interleaved parity organization, uses block-level striping, as in RAID 0, and in addition keeps a parity block on a separate disk for corresponding blocks from N other disks. This scheme is diagrammed in Figure 10.11(e). If one of the disks fails, the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk. A block read accesses only one disk, allowing other requests to be processed by the other disks. Thus, the data-transfer rate for each access is slower, but multiple read accesses can proceed in parallel, leading to a higher overall I/O rate. The transfer rates for large reads are high, since all the disks can be read in parallel. Large writes also have high transfer rates, since the data and parity can be written in parallel. Small independent writes cannot be performed in parallel. An operatingsystem write of data smaller than a block requires that the block be read, modied with the new data, and written back. The parity block has to be updated as well. This is known as the read-modify-write cycle. Thus, a single write requires four disk accesses: two to read the two old blocks and two to write the two new blocks"
Explain why a single write requires four disk accesses: two to read the two old blocks and two to write the two new blocks,"Small independent writes cannot be performed in parallel. An operatingsystem write of data smaller than a block requires that the block be read, modied with the new data, and written back. The parity block has to be updated as well. This is known as the read-modify-write cycle. Hence, a single write requires four disk accesses: two to read the two old blocks and two to write the two new blocks."," A block read accesses only one disk, allowing other requests to be processed by the other disks. Thus, the data-transfer rate for each access is slower, but multiple read accesses can proceed in parallel, leading to a higher overall I/O rate. The transfer rates for large reads are high, since all the disks can be read in parallel. Large writes also have high transfer rates, since the data and parity can be written in parallel. Small independent writes cannot be performed in parallel. An operatingsystem write of data smaller than a block requires that the block be read, modied with the new data, and written back. The parity block has to be updated as well. This is known as the read-modify-write cycle. Thus, a single write requires four disk accesses: two to read the two old blocks and two to write the two new blocks. WAFL (which we cover in Chapter 12) uses RAID level 4 because this RAID level allows disks to be added to a RAID set seamlessly. If the added disks are initialized with blocks containing only zeros, then the parity value does not change, and the RAID set is still correct"
"Explain why most DFS protocols either enforce or allow delaying of le-system operations to remote hosts, with the hope that the remote host will become available again","This scenario is rather common, so it would not be appropriate for the client system to act as it would if a local le system were lost. Rather, the system can either terminate all operations to the lost server or delay operations until the server is again reachable. These failure semantics are dened and implemented as part of the remote-le-system protocol. Termination of all operations can result in users losing dataand patience. Hence, most DFS protocols either enforce or allow delaying of le-system operations to remote hosts, with the hope that the remote host will become available again."," Consider a client in the midst of using a remote le system. It has les open from the remote host; among other activities, it may be performing directory lookups to open les, reading or writing data to les, and closing les. Now consider a partitioning of the network, a crash of the server, or even a scheduled shutdown of the server. Suddenly, the remote le system is no longer reachable. This scenario is rather common, so it would not be appropriate for the client system to act as it would if a local le system were lost. Rather, the system can either terminate all operations to the lost server or delay operations until the server is again reachable. These failure semantics are dened and implemented as part of the remote-le-system protocol. Termination of all operations can result in users losing dataand patience. Thus, most DFS protocols either enforce or allow delaying of le-system operations to remote hosts, with the hope that the remote host will become available again. To implement this kind of recovery from failure, some kind of state information may be maintained on both the client and the server. If both server and client maintain knowledge of their current activities and open les, then they can seamlessly recover from a failure. In the situation where the server crashes but must recognize that it has remotely mounted exported le systems and opened les, NFS takes a simple approach, implementing a stateless DFS"
Explain why the advancing of the pointer by one user affects all sharing users," One mode of sharing allows users to share the pointer of current location into the le. Hence, the advancing of the pointer by one user affects all sharing users."," 11.5.3.1 UNIX Semantics The UNIX le system (Chapter 17) uses the following consistency semantics: Writes to an open le by a user are visible immediately to other users who have this le open. One mode of sharing allows users to share the pointer of current location into the le. Thus, the advancing of the pointer by one user affects all sharing users. Here, a le has a single image that interleaves all accesses, regardless of their origin. In the UNIX semantics, a le is associated with a single physical image that is accessed as an exclusive resource. Contention for this single image causes delays in user processes"
Explain why a user can list the content of a subdirectory only if the r bit is set in the appropriate eld,"In the UNIX system, directory protection and le protection are handled similarly. Associated with each subdirectory are three elds owner, group, and universe each consisting of the three bits rwx. Hence, a user can list the content of a subdirectory only if the r bit is set in the appropriate eld."," All other users should be able to read, but not write, the le. (Sara is interested in letting as many people as possible read the text so that she can obtain feedback.) To achieve such protection, we must create a new groupsay, text with members Jim, Dawn, and Jill. The name of the group, text, must then be associated with the le book.tex, and the access rights must be set in accordance with the policy we have outlined. Now consider a visitor to whom Sara would like to grant temporary access to Chapter 1. The visitor cannot be added to the text group because that would give him access to all chapters. Because a le can be in only one group, Sara cannot add another group to Chapter 1. With the addition of access-control-list functionality, though, the visitor can be added to the access control list of Chapter 11 File-System Interface PERMISSIONS IN A UNIX SYSTEM In the UNIX system, directory protection and le protection are handled similarly. Associated with each subdirectory are three elds owner, group, and universe each consisting of the three bits rwx. Thus, a user can list the content of a subdirectory only if the r bit is set in the appropriate eld. Similarly, a user can change his current directory to another current directory (say, foo) only if the x bit associated with the foo subdirectory is set in the appropriate eld"
Explain how control is achieved through human interaction,"For this scheme to work properly, permissions and access lists must be controlled tightly. This control can be accomplished in several ways. For example, in the UNIX system, groups can be created and modied only by the manager of the facility (or by any superuser). Hence, control is achieved through human interaction."," A sample directory listing from a UNIX environment is shown in below: -rw-rw-r-drwx-----drwxrwxr-x drwxrwx---rw-r--r--rwxr-xr-x drwx--x--x drwx-----drwxrwxrwx 1 pbg 5 pbg 2 pbg 2 jwg 1 pbg 1 pbg 4 tag 3 pbg 3 pbg 31200 Sep 3 08:30 staff 512 Jul 8 09.33 staff 512 Jul 8 09:35 staff 512 Aug 3 14:13 student 9423 Feb 24 2012 staff 20471 Feb 24 2012 staff 512 Jul 31 10:31 faculty 1024 Aug 29 06:52 staff 512 Jul 8 09:35 staff intro.ps private/ doc/ student-proj/ program.c program lib/ mail/ test/ The rst eld describes the protection of the le or directory. A d as the rst character indicates a subdirectory. Also shown are the number of links to the le, the owners name, the groups name, the size of the le in bytes, the date of last modication, and nally the les name (with optional extension). For this scheme to work properly, permissions and access lists must be controlled tightly. This control can be accomplished in several ways. For example, in the UNIX system, groups can be created and modied only by the manager of the facility (or by any superuser). Thus, control is achieved through human interaction. Access lists are discussed further in Section 14.5.2. With the more limited protection classication, only three elds are needed to dene protection. Often, each eld is a collection of bits, and each bit either allows or prevents the access associated with it. For example, the UNIX system denes three elds of 3 bits each rwx, where r controls read access, w controls write access, and x controls execution. A separate eld is kept for the le owner, for the les group, and for all other users. In this scheme, 9 bits per le are needed to record protection information. Thus, for our example, the protection elds for the le book.tex are as follows: for the owner Sara, all bits are set; for the group text, the r and w bits are set; and for the universe, only the r bit is set"
"Explain why it is simple to access any le either sequentially or randomly, and switching from one le to another requires only moving the read write heads and waiting for the disk to rotate","Disks provide most of the secondary storage on which le systems are maintained. Two characteristics make them convenient for this purpose: 1. A disk can be rewritten in place; it is possible to read a block from the disk, modify the block, and write it back into the same place. 2. A disk can access directly any block of information it contains. Hence, it is simple to access any le either sequentially or randomly, and switching from one le to another requires only moving the read write heads and waiting for the disk to rotate."," 12.1 File-System Structure Disks provide most of the secondary storage on which le systems are maintained. Two characteristics make them convenient for this purpose: 1. A disk can be rewritten in place; it is possible to read a block from the disk, modify the block, and write it back into the same place. 2. A disk can access directly any block of information it contains. Thus, it is simple to access any le either sequentially or randomly, and switching from one le to another requires only moving the read write heads and waiting for the disk to rotate. We discuss disk structure in great detail in Chapter 10"
"Explain why the number of disk seeks required for accessing contiguously allocated les is minimal, as is seek time when a seek is nally needed","Contiguous allocation requires that each le occupy a set of contiguous blocks on the disk. Disk addresses dene a linear ordering on the disk. With this ordering, assuming that only one job is accessing the disk, accessing block b + 1 after block b normally requires no head movement. When head movement is needed (from the last sector of one cylinder to the rst sector of the next cylinder), the head need only move from one track to the next. Hence, the number of disk seeks required for accessing contiguously allocated les is minimal, as is seek time when a seek is nally needed."," 12.4 Allocation Methods The direct-access nature of disks gives us exibility in the implementation of les. In almost every case, many les are stored on the same disk. The main problem is how to allocate space to these les so that disk space is utilized effectively and les can be accessed quickly. Three major methods of allocating disk space are in wide use: contiguous, linked, and indexed. Each method has advantages and disadvantages. Although some systems support all three, it is more common for a system to use one method for all les within a le-system type. Contiguous allocation requires that each le occupy a set of contiguous blocks on the disk. Disk addresses dene a linear ordering on the disk. With this ordering, assuming that only one job is accessing the disk, accessing block b + 1 after block b normally requires no head movement. When head movement is needed (from the last sector of one cylinder to the rst sector of the next cylinder), the head need only move from one track to the next. Thus, the number of disk seeks required for accessing contiguously allocated les is minimal, as is seek time when a seek is nally needed. Contiguous allocation of a le is dened by the disk address and length (in block units) of the rst block. If the le is n blocks long and starts at location b, then it occupies blocks b, b + 1, b + 2, ..., b + n 1. The directory entry for each le indicates the address of the starting block and the length of the area allocated for this le (Figure 12.5)"
Explain how both sequential and direct access can be supported by contiguous allocation,"Accessing a le that has been allocated contiguously is easy. For sequential access, the le system remembers the disk address of the last block referenced and, when necessary, reads the next block. For direct access to block i of a directory count 1 file f 7 14 tr 17 mail 21 22 28 list 29 30 count tr mail list f start length 19 3 4 le that starts at block b, we can immediately access block b + i. Hence, both sequential and direct access can be supported by contiguous allocation."," Contiguous allocation of a le is dened by the disk address and length (in block units) of the rst block. If the le is n blocks long and starts at location b, then it occupies blocks b, b + 1, b + 2, ..., b + n 1. The directory entry for each le indicates the address of the starting block and the length of the area allocated for this le (Figure 12.5). Accessing a le that has been allocated contiguously is easy. For sequential access, the le system remembers the disk address of the last block referenced and, when necessary, reads the next block. For direct access to block i of a directory count 1 file f 7 14 tr 17 mail 21 22 28 list 29 30 count tr mail list f start length 19 3 4 le that starts at block b, we can immediately access block b + i. Thus, both sequential and direct access can be supported by contiguous allocation. Contiguous allocation has some problems, however. One difculty is nding space for a new le. The system chosen to manage free space determines how this task is accomplished; these management systems are discussed in Section 12.5. Any management system can be used, but some are slower than others"
"Explain why the performance of indexed allocation depends on the index structure, on the size of the le, and on the position of the block desired","Indexed allocation is more complex. If the index block is already in memory, then the access can be made directly. However, keeping the index block in memory requires considerable space. If this memory space is not available, then we may have to read rst the index block and then the desired data block. For a two-level index, two index-block reads might be necessary. For an extremely large le, accessing a block near the end of the le would require reading in all the index blocks before the needed data block nally could be read. Hence, the performance of indexed allocation depends on the index structure, on the size of the le, and on the position of the block desired."," A le created for sequential access will be linked and cannot be used for direct access. A le created for direct access will be contiguous and can support both direct access and sequential access, but its maximum length must be declared when it is created. In this case, the operating system must have appropriate data structures and algorithms to support both allocation methods. Files can be converted from one type to another by the creation of a new le of the desired type, into which the contents of the old le are copied. The old le may then be deleted and the new le renamed. Indexed allocation is more complex. If the index block is already in memory, then the access can be made directly. However, keeping the index block in memory requires considerable space. If this memory space is not available, then we may have to read rst the index block and then the desired data block. For a two-level index, two index-block reads might be necessary. For an extremely large le, accessing a block near the end of the le would require reading in all the index blocks before the needed data block nally could be read. Thus, the performance of indexed allocation depends on the index structure, on the size of the le, and on the position of the block desired. Some systems combine contiguous allocation with indexed allocation by using contiguous allocation for small les (up to three or four blocks) and automatically switching to an indexed allocation if the le grows large. Since most les are small, and contiguous allocation is efcient for small les, average performance can be quite good"
Explain why the calling routine must wait for the data to reach the disk drive before it can proceed,"Another issue that can affect the performance of I/O is whether writes to the le system occur synchronously or asynchronously. Synchronous writes occur in the order in which the disk subsystem receives them, and the writes are not buffered. Hence, the calling routine must wait for the data to reach the disk drive before it can proceed."," Regardless of whether we are caching disk blocks or pages (or both), LRU (Section 9.4.4) seems a reasonable general-purpose algorithm for block or page replacement. However, the evolution of the Solaris page-caching algorithms reveals the difculty in choosing an algorithm. Solaris allows processes and the page cache to share unused memory. Versions earlier than Solaris 2.5.1 made no distinction between allocating pages to a process and allocating them to the page cache. As a result, a system performing many I/O operations used most of the available memory for caching pages. Because of the high rates of I/O, the page scanner (Section 9.10.2) reclaimed pages from processesrather than from the page cachewhen free memory ran low. Solaris 2.6 and Solaris 7 optionally implemented priority paging, in which the page scanner gives memory-mapped I/O I/O using read( ) and write( ) file system priority to process pages over the page cache. Solaris 8 applied a xed limit to process pages and the le-system page cache, preventing either from forcing the other out of memory. Solaris 9 and 10 again changed the algorithms to maximize memory use and minimize thrashing. Another issue that can affect the performance of I/O is whether writes to the le system occur synchronously or asynchronously. Synchronous writes occur in the order in which the disk subsystem receives them, and the writes are not buffered. Thus, the calling routine must wait for the data to reach the disk drive before it can proceed. In an asynchronous write, the data are stored in the cache, and control returns to the caller. Most writes are asynchronous. However, metadata writes, among others, can be synchronous. Operating systems frequently include a ag in the open system call to allow a process to request that writes be performed synchronously. For example, databases use this feature for atomic transactions, to assure that data reach stable storage in the required order"
"Explain why output to the disk through the le system is often faster than is input for large transfers, counter to intuition","Unless synchronous writes are required, a process writing to disk simply writes into the cache, and the system asynchronously writes the data to disk when convenient. The user process sees very fast writes. When data are read from a disk le, the block I/O system does some read-ahead; however, writes are much more nearly asynchronous than are reads. Hence, output to the disk through the le system is often faster than is input for large transfers, counter to intuition."," The page cache, the le system, and the disk drivers have some interesting interactions. When data are written to a disk le, the pages are buffered in the cache, and the disk driver sorts its output queue according to disk address. These two actions allow the disk driver to minimize disk-head seeks and to write data at times optimized for disk rotation. Unless synchronous writes are required, a process writing to disk simply writes into the cache, and the system asynchronously writes the data to disk when convenient. The user process sees very fast writes. When data are read from a disk le, the block I/O system does some read-ahead; however, writes are much more nearly asynchronous than are reads. Thus, output to the disk through the le system is often faster than is input for large transfers, counter to intuition. 12.7 Recovery Files and directories are kept both in main memory and on disk, and care must be taken to ensure that a system failure does not result in loss of data or in data inconsistency. We deal with these issues in this section. We also consider how a system can recover from such a failure"
Explain why the mount mechanism does not exhibit a transitivity property,"Diskless workstations can even mount their own roots from servers. Cascading mounts are also permitted in some NFS implementations. That is, a le system can be mounted over another le system that is remotely mounted, not local. A machine is affected by only those mounts that it has itself invoked. Mounting a remote le system does not give the client access to other le systems that were, by chance, mounted over the former le system. Hence, the mount mechanism does not exhibit a transitivity property."," Subject to access-rights accreditation, any le system, or any directory within a le system, can be mounted remotely on top of any local directory. Diskless workstations can even mount their own roots from servers. Cascading mounts are also permitted in some NFS implementations. That is, a le system can be mounted over another le system that is remotely mounted, not local. A machine is affected by only those mounts that it has itself invoked. Mounting a remote le system does not give the client access to other le systems that were, by chance, mounted over the former le system. Thus, the mount mechanism does not exhibit a transitivity property. In Figure 12.14(b), we illustrate cascading mounts. The gure shows the result of mounting S2:/usr/dir2 over U:/usr/local/dir1, which is already remotely mounted from S1. Users can access les within dir2 on U using the prex /usr/local/dir1. If a shared le system is mounted over a users home directories on all machines in a network, the user can log into any workstation and get their home environment. This property permits user mobility"
Explain how a server crash and recovery will be invisible to a client; all blocks that the server is managing for the client will be intact,"A further implication of the stateless-server philosophy and a result of the synchrony of an RPC is that modied data (including indirection and status blocks) must be committed to the servers disk before results are returned to the client. That is, a client can cache write blocks, but when it ushes them to the server, it assumes that they have reached the servers disks. The server must write all NFS data synchronously. Hence, a server crash and recovery will be invisible to a client; all blocks that the server is managing for the client will be intact."," Maintaining the list of clients that we mentioned seems to violate the statelessness of the server. However, this list is not essential for the correct operation of the client or the server, and hence it does not need to be restored after a server crash. Consequently, it may include inconsistent data and is treated as only a hint. A further implication of the stateless-server philosophy and a result of the synchrony of an RPC is that modied data (including indirection and status blocks) must be committed to the servers disk before results are returned to the client. That is, a client can cache write blocks, but when it ushes them to the server, it assumes that they have reached the servers disks. The server must write all NFS data synchronously. Thus, a server crash and recovery will be invisible to a client; all blocks that the server is managing for the client will be intact. The resulting performance penalty can be large, because the advantages of caching are lost. Performance can be increased by using storage with its own nonvolatile cache (usually battery-backed-up memory). The disk controller acknowledges the disk write when the write is stored in the nonvolatile cache. In essence, the host sees a very fast synchronous write. These blocks remain intact even after a system crash and are written from this stable storage to disk periodically"
Explain how a remote le operation can be translated directly to the corresponding RPC,"The WAFL File System Remote Operations With the exception of opening and closing les, there is an almost one-to-one correspondence between the regular UNIX system calls for le operations and the NFS protocol RPCs. Hence, a remote le operation can be translated directly to the corresponding RPC."," Recall that some implementations of NFS allow mounting a remote le system on top of another already-mounted remote le system (a cascading mount). When a client has a cascading mount, more than one server can be involved in a path-name traversal. However, when a client does a lookup on a directory on which the server has mounted a le system, the client sees the underlying directory instead of the mounted directory. 12.9 Example: The WAFL File System Remote Operations With the exception of opening and closing les, there is an almost one-to-one correspondence between the regular UNIX system calls for le operations and the NFS protocol RPCs. Thus, a remote le operation can be translated directly to the corresponding RPC. Conceptually, NFS adheres to the remote-service paradigm; but in practice, buffering and caching techniques are employed for the sake of performance. No direct correspondence exists between a remote operation and an RPC. Instead, le blocks and le attributes are fetched by the RPCs and are cached locally. Future remote operations use the cached data, subject to consistency constraints. There are two caches: the le-attribute (inode-information) cache and the le-blocks cache. When a le is opened, the kernel checks with the remote server to determine whether to fetch or revalidate the cached attributes. The cached le blocks are used only if the corresponding cached attributes are up to date. The attribute cache is updated whenever new attributes arrive from the server. Cached attributes are, by default, discarded after 60 seconds. Both read-ahead and delayed-write techniques are used between the server and the client. Clients do not free delayed-write blocks until the server conrms that the data have been written to disk. Delayed-write is retained even when a le is opened concurrently, in conicting modes. Hence, UNIX semantics (Section 11.5.3.1) are not preserved"
Explain why NFS provides neither strict emulation of UNIX semantics nor the session semantics of Andrew,"Tuning the system for performance makes it difcult to characterize the consistency semantics of NFS. New les created on a machine may not be visible elsewhere for 30 seconds. Furthermore, writes to a le at one site may or may not be visible at other sites that have this le open for reading. New opens of a le observe only the changes that have already been ushed to the server. Hence, NFS provides neither strict emulation of UNIX semantics nor the session semantics of Andrew"," There are two caches: the le-attribute (inode-information) cache and the le-blocks cache. When a le is opened, the kernel checks with the remote server to determine whether to fetch or revalidate the cached attributes. The cached le blocks are used only if the corresponding cached attributes are up to date. The attribute cache is updated whenever new attributes arrive from the server. Cached attributes are, by default, discarded after 60 seconds. Both read-ahead and delayed-write techniques are used between the server and the client. Clients do not free delayed-write blocks until the server conrms that the data have been written to disk. Delayed-write is retained even when a le is opened concurrently, in conicting modes. Hence, UNIX semantics (Section 11.5.3.1) are not preserved. Tuning the system for performance makes it difcult to characterize the consistency semantics of NFS. New les created on a machine may not be visible elsewhere for 30 seconds. Furthermore, writes to a le at one site may or may not be visible at other sites that have this le open for reading. New opens of a le observe only the changes that have already been ushed to the server. Thus, NFS provides neither strict emulation of UNIX semantics nor the session semantics of Andrew (Section 11.5.3.2). In spite of these drawbacks, the utility and good performance of the mechanism make it the most widely used multi-vendor-distributed system in operation. 12.9 Example: The WAFL File System Because disk I/O has such a huge impact on system performance, le-system design and implementation command quite a lot of attention from system designers. Some le systems are general purpose, in that they can provide reasonable performance and functionality for a wide variety of le sizes, le types, and I/O loads. Others are optimized for specic tasks in an attempt to provide better performance in those areas than general-purpose le systems"
Explain how we can attach new peripherals to a computer without waiting for the operating-system vendor to develop support code,"SCSI devices mouse independent of the hardware simplies the job of the operating-system developer. It also benets the hardware manufacturers. They either design new devices to be compatible with an existing host controller interface (such as SATA), or they write device drivers to interface the new hardware to popular operating systems. Hence, we can attach new peripherals to a computer without waiting for the operating-system vendor to develop support code."," Like other complex software-engineering problems, the approach here involves abstraction, encapsulation, and software layering. Specically, we can abstract away the detailed differences in I/O devices by identifying a few general kinds. Each general kind is accessed through a standardized set of functionsan interface. The differences are encapsulated in kernel modules called device drivers that internally are custom-tailored to specic devices but that export one of the standard interfaces. Figure 13.6 illustrates how the I/O-related portions of the kernel are structured in software layers. The purpose of the device-driver layer is to hide the differences among device controllers from the I/O subsystem of the kernel, much as the I/O system calls encapsulate the behavior of devices in a few generic classes that hide hardware differences from applications. Making the I/O subsystem software hardware kernel I/O subsystem SCSI device driver keyboard device driver mouse device driver PCI bus device driver floppy device driver ATAPI device driver SCSI device controller keyboard device controller mouse device controller PCI bus device controller floppy device controller ATAPI device controller floppydisk drives ATAPI devices (disks, tapes, drives) SCSI devices mouse independent of the hardware simplies the job of the operating-system developer. It also benets the hardware manufacturers. They either design new devices to be compatible with an existing host controller interface (such as SATA), or they write device drivers to interface the new hardware to popular operating systems. Thus, we can attach new peripherals to a computer without waiting for the operating-system vendor to develop support code. Unfortunately for device-hardware manufacturers, each type of operating system has its own standards for the device-driver interface. A given device may ship with multiple device driversfor instance, drivers for Windows, Linux, AIX, and Mac OS X. Devices vary on many dimensions, as illustrated in Character-stream or block. A character-stream device transfers bytes one by one, whereas a block device transfers a block of bytes as a unit"
Explain how users cannot issue I/O instructions directly; they must do it through the operating system," To prevent users from performing illegal I/O, we dene all I/O instructions to be privileged instructions. Hence, users cannot issue I/O instructions directly; they must do it through the operating system."," Errors are closely related to the issue of protection. A user process may accidentally or purposely attempt to disrupt the normal operation of a system by attempting to issue illegal I/O instructions. We can use various mechanisms to ensure that such disruptions cannot take place in the system. To prevent users from performing illegal I/O, we dene all I/O instructions to be privileged instructions. Thus, users cannot issue I/O instructions directly; they must do it through the operating system. To do I/O, a user program executes a system call to request that the operating system perform I/O on its behalf (Figure 13.11). The operating system, executing in monitor mode, checks that the request is valid and, if it is, does the I/O requested. The operating system then returns to the user. In addition, any memory-mapped and I/O port memory locations must be protected from user access by the memory-protection system. Note that a kernel cannot simply deny all user access. Most graphics games and video editing and playback software need direct access to memory-mapped graphics controller memory to speed the performance of the graphics, for example. The kernel might in this case provide a locking mechanism to allow a section of graphics memory (representing a window on screen) to be allocated to one process at a time"
Explain why we can introduce new devices and drivers into a computer without recompiling the kernel,"Modern operating systems gain signicant exibility from the multiple stages of lookup tables in the path between a request and a physical device controller. The mechanisms that pass requests between applications and drivers are general. Hence, we can introduce new devices and drivers into a computer without recompiling the kernel."," UNIX represents device names in the regular le-system name space. Unlike an MS-DOS le name, which has a colon separator, a UNIX path name has no clear separation of the device portion. In fact, no part of the path name is the name of a device. UNIX has a mount table that associates prexes of path names with specic device names. To resolve a path name, UNIX looks up the name in the mount table to nd the longest matching prex; the corresponding entry in the mount table gives the device name. This device name also has the form of a name in the le-system name space. When UNIX looks up this name in the le-system directory structures, it nds not an inode number but a <major, minor> device number. The major device number identies a device driver that should be called to handle I/O to this device. The minor device number is passed to the device driver to index into a device table. The corresponding device-table entry gives the port address or the memory-mapped address of the device controller. Modern operating systems gain signicant exibility from the multiple stages of lookup tables in the path between a request and a physical device controller. The mechanisms that pass requests between applications and drivers are general. Thus, we can introduce new devices and drivers into a computer without recompiling the kernel. In fact, some operating systems have the ability to load device drivers on demand. At boot time, the system user process system call can already satisfy request? I/O completed, input data available, or output completed return from system call kernel I/O subsystem yes transfer data (if appropriate) to process, return completion or error code no send request to device driver, block process if appropriate kernel I/O subsystem process request, issue commands to controller, configure controller to block until interrupted device driver determine which I/O completed, indicate state change to I/O subsystem interrupt handler receive interrupt, store data in device-driver buffer if input, signal to unblock device driver monitor device, interrupt when I/O completed device controller I/O completed, generate interrupt rst probes the hardware buses to determine what devices are present. It then loads in the necessary drivers, either immediately or when rst required by an I/O request. We next describe the typical life cycle of a blocking read request, as depicted in Figure 13.13. The gure suggests that an I/O operation requires a great many steps that together consume a tremendous number of CPU cycles"
"Explain how the hardware can distinguish integers, oating-point numbers, pointers, Booleans, characters, instructions, capabilities, and uninitialized values by their tags","Each object has a tag to denote whether it is a capability or accessible data. The tags themselves must not be directly accessible by an application program. Hardware or rmware support may be used to enforce this restriction. Although only one bit is necessary to distinguish between capabilities and other objects, more bits are often used. This extension allows all objects to be tagged with their types by the hardware. Hence, the hardware can distinguish integers, oating-point numbers, pointers, Booleans, characters, instructions, capabilities, and uninitialized values by their tags."," Capabilities were originally proposed as a kind of secure pointer, to meet the need for resource protection that was foreseen as multiprogrammed computer systems came of age. The idea of an inherently protected pointer provides a foundation for protection that can be extended up to the application level. To provide inherent protection, we must distinguish capabilities from other kinds of objects, and they must be interpreted by an abstract machine on which higher-level programs run. Capabilities are usually distinguished from other data in one of two ways: Each object has a tag to denote whether it is a capability or accessible data. The tags themselves must not be directly accessible by an application program. Hardware or rmware support may be used to enforce this restriction. Although only one bit is necessary to distinguish between capabilities and other objects, more bits are often used. This extension allows all objects to be tagged with their types by the hardware. Thus, the hardware can distinguish integers, oating-point numbers, pointers, Booleans, characters, instructions, capabilities, and uninitialized values by their tags. Alternatively, the address space associated with a program can be split into two parts. One part is accessible to the program and contains the programs normal data and instructions. The other part, containing the capability list, is accessible only by the operating system. A segmented memory space (Section 8.4) is useful to support this approach"
Explain how the user can access only those les that have been opened,"Consider a le system in which each le has an associated access list. When a process opens a le, the directory structure is searched to nd the le, access permission is checked, and buffers are allocated. All this information is recorded in a new entry in a le table associated with the process. The operation returns an index into this table for the newly opened le. All operations on the le are made by specication of the index into the le table. The entry in the le table then points to the le and its buffers. When the le is closed, the le-table entry is deleted. Since the le table is maintained by the operating system, the user cannot accidentally corrupt it. Hence, the user can access only those les that have been opened."," This strategy is used in the MULTICS system and in the CAL system. As an example of how such a strategy works, consider a le system in which each le has an associated access list. When a process opens a le, the directory structure is searched to nd the le, access permission is checked, and buffers are allocated. All this information is recorded in a new entry in a le table associated with the process. The operation returns an index into this table for the newly opened le. All operations on the le are made by specication of the index into the le table. The entry in the le table then points to the le and its buffers. When the le is closed, the le-table entry is deleted. Since the le table is maintained by the operating system, the user cannot accidentally corrupt it. Thus, the user can access only those les that have been opened. Since access is checked when the le is opened, protection is ensured. This strategy is used in the UNIX system"
Explain why the user-protection requirement can be circumvented,"When a user passes an object as an argument to a procedure, we may need to ensure that the procedure cannot modify the object. We can implement this restriction readily by passing an access right that does not have the modication (write) right. However, if amplication may occur, the right to modify may be reinstated. Hence, the user-protection requirement can be circumvented."," On return from P, the capability for A is restored to its original, unamplied state. This case is a typical one in which the rights held by a process for access to a protected segment must change dynamically, depending on the task to be performed. The dynamic adjustment of rights is performed to guarantee consistency of a programmer-dened abstraction. Amplication of rights can be stated explicitly in the declaration of an abstract type to the Hydra operating system. When a user passes an object as an argument to a procedure, we may need to ensure that the procedure cannot modify the object. We can implement this restriction readily by passing an access right that does not have the modication (write) right. However, if amplication may occur, the right to modify may be reinstated. Thus, the user-protection requirement can be circumvented. In general, of course, a user may trust that a procedure performs its task correctly. This assumption is not always correct, however, because of hardware or software errors. Hydra solves this problem by restricting amplications"
Explain why the old adage that a chain is only as strong as its weakest link is especially true of system security,"Security at the rst two levels must be maintained if operating-system security is to be ensured. A weakness at a high level of security (physical or human) allows circumvention of strict low-level (operating-system) security measures. Hence, the old adage that a chain is only as strong as its weakest link is especially true of system security."," 4. Network. Much computer data in modern systems travels over private leased lines, shared lines like the Internet, wireless connections, or dial-up lines. Intercepting these data could be just as harmful as breaking into a computer, and interruption of communications could constitute a remote denial-of-service attack, diminishing users use of and trust in the system. Security at the rst two levels must be maintained if operating-system security is to be ensured. A weakness at a high level of security (physical or human) allows circumvention of strict low-level (operating-system) security measures. Thus, the old adage that a chain is only as strong as its weakest link is especially true of system security. All of these aspects must be addressed for security to be maintained. Furthermore, the system must provide protection (Chapter 14) to allow the implementation of security features. Without the ability to authorize users and processes, to control their access, and to log their activities, it would be impossible for an operating system to implement security measures or to run securely. Hardware protection features are needed to support an overall protection scheme. For example, a system without memory protection cannot be secure. New hardware features are allowing systems to be made more secure, as we shall discuss"
"Explain why a computer holding k can decrypt ciphertexts to the plaintexts used to produce them, but a computer not holding k cannot decrypt ciphertexts"," An encryption algorithm must provide this essential property: given a ciphertext c C, a computer can compute m such that E k (m) = c only if it possesses k. Hence, a computer holding k can decrypt ciphertexts to the plaintexts used to produce them, but a computer not holding k cannot decrypt ciphertexts."," A decrypting function D : K (C M). That is, for each k K , Dk is a function for generating messages from ciphertexts. Both D and Dk for any k should be efciently computable functions. An encryption algorithm must provide this essential property: given a ciphertext c C, a computer can compute m such that E k (m) = c only if it possesses k. Thus, a computer holding k can decrypt ciphertexts to the plaintexts used to produce them, but a computer not holding k cannot decrypt ciphertexts. Since ciphertexts are generally exposed (for example, sent on a network), it is important that it be infeasible to derive k from the ciphertexts. There are two main types of encryption algorithms: symmetric and asymmetric. We discuss both types in the following sections"
Explain how a computer holding k can  generate authenticators on messages so that any computer possessing k can verify them," The critical property that an authentication algorithm must possess is this: for a message m, a computer can generate an authenticator a A such that Vk (m, a ) = true only if it possesses k. Hence, a computer holding k can  generate authenticators on messages so that any computer possessing k can verify them."," A function V : K (M A {true, false}). That is, for each k K , Vk is a function for verifying authenticators on messages. Both V and Vk for any k should be efciently computable functions. The critical property that an authentication algorithm must possess is this: for a message m, a computer can generate an authenticator a A such that Vk (m, a ) = true only if it possesses k. Thus, a computer holding k can generate authenticators on messages so that any computer possessing k can verify them. However, a computer not holding k cannot generate authenticators on messages that can be veried using Vk . Since authenticators are generally exposed (for example, sent on a network with the messages themselves), it must not be feasible to derive k from the authenticators. Practically, if Vk (m, a ) = true, then we know that m has not been modied, and that the sender of the message has k. If we share k with only one entity, then we know that the message originated from k. Just as there are two types of encryption algorithms, there are two main varieties of authentication algorithms. The rst step in understanding these algorithms is to explore hash functions. A hash function H(m) creates a small, xed-sized block of data, known as a message digest or hash value, from a message m. Hash functions work by taking a message, splitting it into blocks, and processing the blocks to produce an n-bit hash. H must be collision resistant that is, it must be infeasible to nd an m = m such that H(m) = H(m ). Now, if H(m) = H(m ), we know that m = m that is, we know that the message has not been modied. Common message-digest functions include MD5, now considered insecure, which produces a 128-bit hash, and SHA-1, which outputs a 160-bit hash. Message digests are useful for detecting changed messages but are not useful as authenticators. For example, H(m) can be sent along with a message; but if H is known, then someone could modify m to m and recompute H(m ), and the message modication would not be detected. Therefore, we must authenticate H(m)"
"Explain why kv is the public key, and ks is the private key","Digital signatures are very useful in that they enable anyone to verify the authenticity of the message. In a digital-signature algorithm, it is computationally infeasible to derive ks from kv . Hence, kv is the public key, and ks is the private key."," The second main type of authentication algorithm is a digital-signature algorithm, and the authenticators thus produced are called digital signatures. Digital signatures are very useful in that they enable anyone to verify the authenticity of the message. In a digital-signature algorithm, it is computationally infeasible to derive ks from kv . Thus, kv is the public key, and ks is the private key. Consider as an example the RSA digital-signature algorithm. It is similar to the RSA encryption algorithm, but the key use is reversed. The digital signature of a message is derived by computing Sks (m) = H(m)ks mod N"
Explain why a major security problem for operating systems is user authentication,"Our earlier discussion of authentication involves messages and sessions. But what about users? If a system cannot authenticate a user, then authenticating that a message came from that user is pointless. Hence, a major security problem for operating systems is user authentication."," In addition to its use on the Internet, SSL is being used for a wide variety of tasks. For example, IPSec VPNs now have a competitor in SSL VPNs. IPSec is good for point-to-point encryption of trafcsay, between two company ofces. SSL VPNs are more exible but not as efcient, so they might be used between an individual employee working remotely and the corporate ofce. 15.5 User Authentication Our earlier discussion of authentication involves messages and sessions. But what about users? If a system cannot authenticate a user, then authenticating that a message came from that user is pointless. Thus, a major security problem for operating systems is user authentication. The protection system depends on the ability to identify the programs and processes currently executing, which in turn depends on the ability to identify each user of the system. Users normally identify themselves. How do we determine whether a users identity is authentic? Generally, user authentication is based on one or more of three things: the users possession of something (a key or card), the users knowledge of something (a user identier and password), or an attribute of the user (ngerprint, retina pattern, or signature)"
Explain why the password le does not need to be kept secret,"Only encoded passwords are stored. When a user presents a password, it is hashed and compared against the stored encoded password. Even if the stored encoded password is seen, it cannot be decoded, so the password cannot be determined. Hence, the password le does not need to be kept secret."," Heres how this system works. Each user has a password. The system contains a function that is extremely difcultthe designers hope impossible to invert but is simple to compute. That is, given a value x, it is easy to compute the hash function value f (x). Given a function value f (x), however, it is impossible to compute x. This function is used to encode all passwords. Only encoded passwords are stored. When a user presents a password, it is hashed and compared against the stored encoded password. Even if the stored encoded password is seen, it cannot be decoded, so the password cannot be determined. Thus, the password le does not need to be kept secret. The aw in this method is that the system no longer has control over the passwords. Although the passwords are hashed, anyone with a copy of the password le can run fast hash routines against ithashing each word in a dictionary, for instance, and comparing the results against the passwords"
Explain why new attacks that were not contemplated when the signatures were generated will evade signature-based detection,"Signature-based detection, in contrast, will identify only known attacks that can be codied in a recognizable pattern. Hence, new attacks that were not contemplated when the signatures were generated will evade signature-based detection."," Signature-based detection and anomaly detection can be viewed as two sides of the same coin. Signature-based detection attempts to characterize dangerous behaviors and to detect when one of these behaviors occurs, whereas anomaly detection attempts to characterize normal (or nondangerous) behaviors and to detect when something other than these behaviors occurs. These different approaches yield IDSs and IDPs with very different properties, however. In particular, anomaly detection can nd previously unknown methods of intrusion (so-called zero-day attacks). Signature-based detection, in contrast, will identify only known attacks that can be codied in a recognizable pattern. Thus, new attacks that were not contemplated when the signatures were generated will evade signature-based detection. This problem is well known to vendors of virus-detection software, who must release new signatures with great frequency as new viruses are detected manually. Anomaly detection is not necessarily superior to signature-based detection, however. Indeed, a signicant challenge for systems that attempt anomaly detection is to benchmark normal system behavior accurately. If the system has already been penetrated when it is benchmarked, then the intrusive activity may be included in the normal benchmark. Even if the system is benchmarked cleanly, without inuence from intrusive behavior, the benchmark must give a fairly complete picture of normal behavior. Otherwise, the number of false positives (false alarms) or, worse, false negatives (missed intrusions) will be excessive"
Explain why it is necessary to test all changes to the operating system carefully,"Operating systems are large and complex programs, and a change in one part may cause obscure bugs to appear in some other part. The power of the operating system makes changing it particularly dangerous. Because the operating system executes in kernel mode, a wrong change in a pointer could cause an error that would destroy the entire le system. Hence, it is necessary to test all changes to the operating system carefully."," A virtual machine system is a perfect vehicle for operating-system research and development. Normally, changing an operating system is a difcult task. Operating systems are large and complex programs, and a change in one part may cause obscure bugs to appear in some other part. The power of the operating system makes changing it particularly dangerous. Because the operating system executes in kernel mode, a wrong change in a pointer could cause an error that would destroy the entire le system. Thus, it is necessary to test all changes to the operating system carefully. Furthermore, the operating system runs on and controls the entire machine, meaning that the system must be stopped and taken out of use while changes are made and tested. This period is commonly called system-development time. Since it makes the system unavailable to users, system-development time on shared systems is often scheduled late at night or on weekends, when system load is low"
"Explain why they need not be expensive, high-performance components","In the area of desktop computing, virtualization is enabling desktop and laptop computer users to connect remotely to virtual machines located in remote data centers and access their applications as if they were local. This practice can increase security, because no data are stored on local disks at the users site. The cost of the users computing resource may also decrease. The user must have networking, CPU, and some memory, but all that these system components need to do is display an image of the guest as its runs remotely (via a protocol such as RDP). Hence, they need not be expensive, high-performance components."," Virtualization has laid the foundation for many other advances in computer facility implementation, management, and monitoring. Cloud computing, for example, is made possible by virtualization in which resources such as CPU, memory, and I/O are provided as services to customers using Internet technologies. By using APIs, a program can tell a cloud computing facility to create thousands of VMs, all running a specic guest operating system and application, which others can access via the Internet. Many multiuser games, photo-sharing sites, and other web services use this functionality. In the area of desktop computing, virtualization is enabling desktop and laptop computer users to connect remotely to virtual machines located in remote data centers and access their applications as if they were local. This practice can increase security, because no data are stored on local disks at the users site. The cost of the users computing resource may also decrease. The user must have networking, CPU, and some memory, but all that these system components need to do is display an image of the guest as its runs remotely (via a protocol such as RDP). Thus, they need not be expensive, high-performance components. Other uses of virtualization are sure to follow as it becomes more prevalent and hardware support continues to improve. 16.4 Building Blocks Although the virtual machine concept is useful, it is difcult to implement"
"Explain why unless the new machine is ten times faster than the old, the program running on the new machine will run more slowly than it did on its native hardware"," Instruction-set emulation can run an order of magnitude slower than native instructions, because it may take ten instructions on the new system to read, parse, and simulate an instruction from the old system. Hence, unless the new machine is ten times faster than the old, the program running on the new machine will run more slowly than it did on its native hardware."," As may be expected, the major challenge of emulation is performance. Instruction-set emulation can run an order of magnitude slower than native instructions, because it may take ten instructions on the new system to read, parse, and simulate an instruction from the old system. Thus, unless the new machine is ten times faster than the old, the program running on the new machine will run more slowly than it did on its native hardware. Another challenge for emulator writers is that it is difcult to create a correct emulator because, in essence, this task involves writing an entire CPU in software. In spite of these challenges, emulation is very popular, particularly in gaming circles. Many popular video games were written for platforms that are no longer in production. Users who want to run those games frequently can nd an emulator of such a platform and then run the game unmodied within the emulator. Modern systems are so much faster than old game consoles that even the Apple iPhone has game emulators and games available to run within them"
"Explain why several copies of the same le may exist, resulting in a waste of space","In this scheme, the le location is not transparent to the user; users must know exactly where each le is. Moreover, there is no real le sharing, because a user can only copy a le from one site to another. Hence, several copies of the same le may exist, resulting in a waste of space."," 17.2.1.2 Remote File Transfer Another major function of a network operating system is to provide a mechanism for remote le transfer from one machine to another. In such an environment, each computer maintains its own local le system. If a user at one site (say, cs.uvm.edu) wants to access a le located on another computer (say, cs.yale.edu), then the le must be copied explicitly from the computer at Yale to the computer at the University of Vermont. The Internet provides a mechanism for such a transfer with the le transfer protocol (FTP) program and the more private secure le transfer protocol (SFTP) program. Suppose that a user on cs.uvm.edu wants to copy a Java program Server.java that resides on cs.yale.edu. The user must rst invoke the sftp program by executing sftp cs.yale.edu The program then asks the user for a login name and a password. Once the correct information has been received, the user must connect to the subdirectory where the le Server.java resides and then copy the le by executing get Server.java In this scheme, the le location is not transparent to the user; users must know exactly where each le is. Moreover, there is no real le sharing, because a user can only copy a le from one site to another. Thus, several copies of the same le may exist, resulting in a waste of space. In addition, if these copies are modied, the various copies will be inconsistent. Notice that, in our example, the user at the University of Vermont must have login permission on cs.yale.edu. FTP also provides a way to allow a user who does not have an account on the Yale computer to copy les remotely. This remote copying is accomplished through the anonymous FTP method, which works as follows. The le to be copied (that is, Server.java) must be placed in a special subdirectory (say, ftp) with the protection set to allow the public to read the le. A user who wishes to copy the le uses the ftp command. When the user is asked for the login name, the user supplies the name anonymous and an arbitrary password"
"Explain why the multiple resources in a distributed system represent an inherent advantage, giving the system a greater potential for fault tolerance and scalability","Scalability is related to fault tolerance, discussed earlier. A heavily loaded component can become paralyzed and behave like a faulty component. In addition, shifting the load from a faulty component to that components backup can saturate the latter. Generally, having spare resources is essential for ensuring reliability as well as for handling peak loads gracefully. Hence, the multiple resources in a distributed system represent an inherent advantage, giving the system a greater potential for fault tolerance and scalability."," Still another issue is scalabilitythe capability of a system to adapt to increased service load. Systems have bounded resources and can become completely saturated under increased load. For example, with respect to a le system, saturation occurs either when a servers CPU runs at a high utilization rate or when disks I/O requests overwhelm the I/O subsystem. Scalability is a relative property, but it can be measured accurately. A scalable system reacts more gracefully to increased load than does a nonscalable one. First, its performance degrades more moderately; and second, its resources reach a saturated state later. Even perfect design cannot accommodate an ever-growing load. Adding new resources might solve the problem, but it might generate additional indirect load on other resources (for example, adding machines to a distributed system can clog the network and increase service loads). Even worse, expanding the system can call for expensive design modications. A scalable system should have the potential to grow without these problems. In a distributed system, the ability to scale up gracefully is of special importance, since expanding the network by adding new machines or interconnecting two networks is commonplace. In short, a scalable design should withstand high service load, accommodate growth of the user community, and allow simple integration of added resources. Scalability is related to fault tolerance, discussed earlier. A heavily loaded component can become paralyzed and behave like a faulty component. In addition, shifting the load from a faulty component to that components backup can saturate the latter. Generally, having spare resources is essential for ensuring reliability as well as for handling peak loads gracefully. Thus, the multiple resources in a distributed system represent an inherent advantage, giving the system a greater potential for fault tolerance and scalability. However, inappropriate design can obscure this potential. Fault-tolerance and scalability considerations call for a design demonstrating distribution of control and data"
Explain why le-structure loss and directory-structure corruption are avoided when a client or the server crashes,"NFS treats metadata (directory data and le-attribute data) differently. Any metadata changes are issued synchronously to the server. Hence, le-structure loss and directory-structure corruption are avoided when a client or the server crashes."," Variations of the delayed-write policy differ in when modied data blocks are ushed to the server. One alternative is to ush a block when it is about to be ejected from the clients cache. This option can result in good performance, but some blocks can reside in the clients cache a long time before they are written back to the server. A compromise between this alternative and the write-through policy is to scan the cache at regular intervals and to ush blocks that have been modied since the most recent scan, just as UNIX scans its local cache. Sprite uses this policy with a 30-second interval. NFS uses the policy for le data, but once a write is issued to the server during a cache ush, the write must reach the servers disk before it is considered complete. NFS treats metadata (directory data and le-attribute data) differently. Any metadata changes are issued synchronously to the server. Thus, le-structure loss and directory-structure corruption are avoided when a client or the server crashes. Yet another variation on delayed write is to write data back to the server when the le is closed. This write-on-close policy is used in OpenAFS. In the case of les that are open for short periods or are modied rarely, this policy does not signicantly reduce network trafc. In addition, the write-on-close policy requires the closing process to delay while the le is written through, which reduces the performance advantages of delayed writes. For les that are open for long periods and are modied frequently, however, the performance advantages of this policy over delayed write with more frequent ushing are apparent"
Explain why processes that do not use oating-point arithmetic do not incur the overhead of saving that state,"The most important part of the process context is its scheduling context the information that the scheduler needs to suspend and restart the process. This information includes saved copies of all the processs registers. Floating-point registers are stored separately and are restored only when needed. Hence, processes that do not use oating-point arithmetic do not incur the overhead of saving that state."," The environment-variable mechanism custom-tailors the operating system on a per-process basis. Users can choose their own languages or select their own editors independently of one another. 18.4.1.3 Process Context The process identity and environment properties are usually set up when a process is created and not changed until that process exits. A process may choose to change some aspects of its identity if it needs to do so, or it may alter its environment. In contrast, process context is the state of the running program at any one time; it changes constantly. Process context includes the following parts: Scheduling context. The most important part of the process context is its scheduling contextthe information that the scheduler needs to suspend and restart the process. This information includes saved copies of all the processs registers. Floating-point registers are stored separately and are restored only when needed. Thus, processes that do not use oating-point arithmetic do not incur the overhead of saving that state. The scheduling context also includes information about scheduling priority and about any outstanding signals waiting to be delivered to the process. A key part of the scheduling context is the processs kernel stack, a separate area of kernel memory reserved for use by kernel-mode code. Both system calls and interrupts that occur while the process is executing will use this stack. Accounting. The kernel maintains accounting information about the resources currently being consumed by each process and the total resources consumed by the process in its entire lifetime so far"
"Explain why by increasing the nice value, you are decreasing your priority and being nice to the rest of the system","The Linux scheduler is a preemptive, priority-based algorithm with two separate priority ranges: a real-time range from 0 to 99 and a nice value ranging from 20 to 19. Smaller nice values indicate higher priorities. Hence, by increasing the nice value, you are decreasing your priority and being nice to the rest of the system."," Consequently, the process scheduler was overhauled a second time, with Linux kernel version 2.6. This version ushered in the Completely Fair Scheduler (CFS). The Linux scheduler is a preemptive, priority-based algorithm with two separate priority ranges: a real-time range from 0 to 99 and a nice value ranging from 20 to 19. Smaller nice values indicate higher priorities. Thus, by increasing the nice value, you are decreasing your priority and being nice to the rest of the system. CFS is a signicant departure from the traditional UNIX process scheduler"
"Explain how after the fork, the parent and child share the same physical pages of memory in their address spaces","The second case, creating a new process with fork(), involves creating a complete copy of the existing processs virtual address space. The kernel copies the parent processs vm area struct descriptors, then creates a new set of page tables for the child. The parents page tables are copied directly into the childs, and the reference count of each page covered is incremented. Hence, after the fork, the parent and child share the same physical pages of memory in their address spaces."," It is up to the routines for loading the program to populate the address space with virtual memory regions. The second case, creating a new process with fork(), involves creating a complete copy of the existing processs virtual address space. The kernel copies the parent processs vm area struct descriptors, then creates a new set of page tables for the child. The parents page tables are copied directly into the childs, and the reference count of each page covered is incremented. Thus, after the fork, the parent and child share the same physical pages of memory in their address spaces. A special case occurs when the copying operation reaches a virtual memory region that is mapped privately. Any pages to which the parent process has written within such a region are private, and subsequent changes to these pages by either the parent or the child must not update the page in the other processs address space. When the page-table entries for such regions are copied, they are set to be read only and are marked for copy-on-write. As long as neither process modies these pages, the two processes share the same page of physical memory. However, if either process tries to modify a copy-on-write page, the reference count on the page is checked. If the page is still shared, then the process copies the pages contents to a brand-new page of physical memory and uses its copy instead. This mechanism ensures that private data pages are shared between processes whenever possible and copies are made only when absolutely necessary"
"Explain how the owner of an object might have full read, write, and execute access to a le; other users in a certain group might be given read access but denied write access; and everybody else might be given no access at all"," Linux performs access control by assigning objects a protection mask that species which access modesread, write, or execute are to be granted to processes with owner, group, or world access. Hence, the owner of an object might have full read, write, and execute access to a le; other users in a certain group might be given read access but denied write access; and everybody else might be given no access at all."," Security If the UIDs do not match but any GID of the process matches the objects GID, then group rights are conferred; otherwise, the process has world rights to the object. Linux performs access control by assigning objects a protection mask that species which access modesread, write, or execute are to be granted to processes with owner, group, or world access. Thus, the owner of an object might have full read, write, and execute access to a le; other users in a certain group might be given read access but denied write access; and everybody else might be given no access at all. The only exception is the privileged root UID. A process with this special UID is granted automatic access to any object in the system, bypassing normal access checks. Such processes are also granted permission to perform privileged operations, such as reading any physical memory or opening reserved network sockets. This mechanism allows the kernel to prevent normal users from accessing these resources: most of the kernels key internal resources are implicitly owned by the root UID"
Explain why programs written for the Win32 APIs and POSIX all run on Windows in the appropriate environment,"Extensibility refers to the capacity of an operating system to keep up with advances in computing technology. To facilitate change over time, the developers implemented Windows using a layered architecture. The Windows executive runs in kernel mode and provides the basic system services and abstractions that support shared use of the system. On top of the executive, several server subsystems operate in user mode. Among them are environmental subsystems that emulate different operating systems. Hence, programs written for the Win32 APIs and POSIX all run on Windows in the appropriate environment."," The advent of multiple CPUs on the smallest computers is only part of the shift taking place to parallel computing. Graphics processing units (GPUs) accelerate the computational algorithms needed for graphics by using SIMD architectures to execute a single instruction for multiple data at the same time. This has given rise to the use of GPUs for general computing, not just graphics. Operating-system support for software like OpenCL and CUDA is allowing programs to take advantage of the GPUs. Windows supports use of GPUs through software in its DirectX graphics support. This software, called DirectCompute, allows programs to specify computational kernels using the same HLSL (high-level shader language) programming model used to program the SIMD hardware for graphics shaders. The computational kernels run very quickly on the GPU and return their results to the main computation running on the CPU. Extensibility refers to the capacity of an operating system to keep up with advances in computing technology. To facilitate change over time, the developers implemented Windows using a layered architecture. The Windows executive runs in kernel mode and provides the basic system services and abstractions that support shared use of the system. On top of the executive, several server subsystems operate in user mode. Among them are environmental subsystems that emulate different operating systems. Thus, programs written for the Win32 APIs and POSIX all run on Windows in the appropriate environment. Because of the modular structure, additional environmental subsystems can be added without affecting the executive. In addition, Windows uses loadable drivers in the I/O system, so new le systems, new kinds of I/O devices, and new kinds of networking can be added while the system is running. Windows uses a clientserver model like the Mach operating system and supports distributed processing by remote procedure calls (RPCs) as dened by the Open Software Foundation. An operating system is portable if it can be moved from one CPU architecture to another with relatively few changes. Windows was designed to be portable"
Explain why DPCs do not block other device ISRs,"DPCs are used to postpone interrupt processing. After handling all urgent device-interrupt processing, the ISR schedules the remaining processing by queuing a DPC. The associated software interrupt will not occur until the CPU is next at a priority lower than the priority of all I/O device interrupts but higher than the priority at which threads run. Hence, DPCs do not block other device ISRs."," User-mode execution of an APC cannot occur at arbitrary times, but only when the thread is waiting in the kernel and marked alertable. DPCsare used to postpone interrupt processing. After handling all urgent device-interrupt processing, the ISR schedules the remaining processing by queuing a DPC. The associated software interrupt will not occur until the CPU is next at a priority lower than the priority of all I/O device interrupts but higher than the priority at which threads run. Thus, DPCs do not block other device ISRs. In addition to deferring device-interrupt processing, the dispatcher uses DPCs to process timer expirations and to preempt thread execution at the end of the scheduling quantum. Execution of DPCs prevents threads from being scheduled on the current processor and also keeps APCs from signaling the completion of I/O. This is done so that completion of DPC routines does not take an extended amount of time. As an alternative, the dispatcher maintains a pool of worker threads"
Explain how the number of PDEs or PTEs that t in a page  determine how many virtual addresses are translated by that page,"IA-32 and AMD64 processors, each process has a page directory that contains 512 page-directory entries (PDEs) 8 bytes in size. Each PDE points to a PTE table that contains 512 page-table entries (PTEs) 8 bytes in size. Each PTE points to a 4-KB page frame in physical memory. For a variety of reasons, the hardware requires that the page directories or PTE tables at each level of a multilevel page table occupy a single page. Hence, the number of PDEs or PTEs that t in a page  determine how many virtual addresses are translated by that page."," VM manager to use physical memory more efciently. When two processes want independent copies of data from the same section object, the VM manager places a single shared copy into virtual memory and activates the copy-on-write property for that region of memory. If one of the processes tries to modify data in a copy-on-write page, the VM manager makes a private copy of the page for the process. The virtual address translation in Windows uses a multilevel page table. For IA-32 and AMD64 processors, each process has a page directory that contains 512 page-directory entries (PDEs) 8 bytes in size. Each PDE points to a PTE table that contains 512 page-table entries (PTEs) 8 bytes in size. Each PTE points to a 4-KB page frame in physical memory. For a variety of reasons, the hardware requires that the page directories or PTE tables at each level of a multilevel page table occupy a single page. Thus, the number of PDEs or PTEs that t in a page determine how many virtual addresses are translated by that page. See Figure 19.3 for a diagram of this structure. The structure described so far can be used to represent only 1 GB of virtual address translation. For IA-32, a second page-directory level is needed, Page directory pointer table Pointer 0 Pointer 2 Page directory entry 0 Page directory Page directory entry 511 Page directory entry 0 Page directory Page directory entry 511 Page table entry 0 Page table 0 Page table entry 511 Page table entry 0 Page table 511 Page table entry 511 4 KB page 4 KB page 4 KB page 4 KB page containing only four entries, as shown in the diagram. On 64-bit processors, more levels are needed. For AMD64, Windows uses a total of four full levels"
Explain how a 12-bit value can select any byte within a 4-KB page of memory,"We next consider how virtual addresses are translated into physical addresses on IA-32-compatible processors. A 2-bit value can represent the values 0, 1, 2, 3. A 9-bit value can represent values from 0 to 511; a 12-bit value, values from 0 to 4,095. Hence, a 12-bit value can select any byte within a 4-KB page of memory."," The page-table pages are faulted back into memory when referenced. We next consider how virtual addresses are translated into physical addresses on IA-32-compatible processors. A 2-bit value can represent the values 0, 1, 2, 3. A 9-bit value can represent values from 0 to 511; a 12-bit value, values from 0 to 4,095. Thus, a 12-bit value can select any byte within a 4-KB page of memory. A 9-bit value can represent any of the 512 PDEs or PTEs in a page directory or PTE-table page. As shown in Figure 19.4, translating a virtual address pointer to a byte address in physical memory involves breaking the 32-bit pointer into four values, starting from the most signicant bits: Two bits are used to index into the four PDEs at the top level of the page table. The selected PDE will contain the physical page number for each of the four page-directory pages that map 1 GB of the address space. PTR PDE index page offset 19.3 System Components Nine bits are used to select another PDE, this time from a second-level page directory. This PDE will contain the physical page numbers of up to 512 PTE-table pages"
Explain how the resident monitor would automatically sequence from one program to another and from one job to another,"When the computer was turned on, the resident monitor was invoked, and it would transfer control to a program. When the program terminated, it would return control to the resident monitor, which would then go on to the next program. Hence, the resident monitor would automatically sequence from one program to another and from one job to another."," A small program, called a resident monitor, was created to transfer control automatically from one job to the next (Figure 20.2). The resident monitor is always in memory (or resident). When the computer was turned on, the resident monitor was invoked, and it would transfer control to a program. When the program terminated, it would return control to the resident monitor, which would then go on to the next program. Thus, the resident monitor would automatically sequence from one program to another and from one job to another. But how would the resident monitor know which program to execute? Previously, the operator had been given a short description of what programs were to be run on what data. Control cards were introduced to provide this information directly to the monitor. The idea is simple. In addition to the program or data for a job, the programmer supplied control cards, which contained directives to the resident monitor indicating what program to run"
Explain why the difference in speed between the CPU and its I/O devices may be three orders of magnitude or more,"the CPU is often idle. The problem is the speed of the mechanical I/O devices, which are intrinsically slower than electronic devices. Even a slow CPU works in the microsecond range, with thousands of instructions executed per second. A fast card reader, in contrast, might read 1,200 cards per minute (or 20 cards per second). Hence, the difference in speed between the CPU and its I/O devices may be three orders of magnitude or more."," The switch to batch systems with automatic job sequencing was made to improve performance. The problem, quite simply, is that humans are considerably slower than computers. Consequently, it is desirable to replace human operation with operating-system software. Automatic job sequencing eliminates the need for human setup time and job sequencing. Even with this arrangement, however, the CPU is often idle. The problem is the speed of the mechanical I/O devices, which are intrinsically slower than electronic devices. Even a slow CPU works in the microsecond range, with thousands of instructions executed per second. A fast card reader, in contrast, might read 1,200 cards per minute (or 20 cards per second). Thus, the difference in speed between the CPU and its I/O devices may be three orders of magnitude or more. Over time, of course, improvements in technology resulted in faster I/O devices. Unfortunately, CPU speeds increased even faster, so that the problem was not only unresolved but also exacerbated. One common solution to the I/O problem was to replace slow card readers (input devices) and line printers (output devices) with magnetic-tape units"
Explain why spooling can keep both the CPU and the I/O devices working at much higher rates," For the cost of some disk space and a few tables, the computation of one job and the I/O of other jobs can take place at the same time. Hence, spooling can keep both the CPU and the I/O devices working at much higher rates."," Spooling has a direct benecial effect on the performance of the system. For the cost of some disk space and a few tables, the computation of one job and the I/O of other jobs can take place at the same time. Thus, spooling can keep both the CPU and the I/O devices working at much higher rates. Spooling leads naturally to multiprogramming, which is the foundation of all modern operating systems. 20.3 Atlas The Atlas operating system was designed at the University of Manchester in England in the late 1950s and early 1960s. Many of its basic features that were novel at the time have become standard parts of modern operating systems"
"Explain why the system structure was layered, and only the lower levelscomprising the kernelwere provided"," Rather, the goal was to create an operating-system nucleus, or kernel, on which a complete operating system could be built. Hence, the system structure was layered, and only the lower levelscomprising the kernelwere provided."," 20.6 RC 4000 The RC 4000 system, like the THE system, was notable primarily for its design concepts. It was designed in the late 1960s for the Danish 4000 computer by Regnecentralen, particularly by Brinch-Hansen. The objective was not to design a batch system, or a time-sharing system, or any other specic system. Rather, the goal was to create an operating-system nucleus, or kernel, on which a complete operating system could be built. Thus, the system structure was layered, and only the lower levelscomprising the kernelwere provided. The kernel supported a collection of concurrent processes. A round-robin CPU scheduler was used. Although processes could share memory, the primary communication and synchronization mechanism was the message system provided by the kernel. Processes could communicate with each other by exchanging xed-sized messages of eight words in length. All messages were stored in buffers from a common buffer pool. When a message buffer was no longer required, it was returned to the common pool"
Explain why a process would write to a terminal by sending that terminal a message,"I/O devices were also treated as processes. The device drivers were code that converted the device interrupts and registers into messages. Hence, a process would write to a terminal by sending that terminal a message."," Messages were removed from the queue in FIFO order. The system supported four primitive operations, which were executed atomically: send-message (in receiver, in message, out buffer) wait-message (out sender, out message, out buffer) send-answer (out result, in message, in buffer) wait-answer (out result, out message, in buffer) The last two operations allowed processes to exchange several messages at a time. These primitives required that a process service its message queue in FIFO order and that it block itself while other processes were handling its messages. To remove these restrictions, the developers provided two additional communication primitives that allowed a process to wait for the arrival of the next message or to answer and service its queue in any order: wait-event (in previous-buffer, out next-buffer, out result) get-event (out buffer) I/O devices were also treated as processes. The device drivers were code that converted the device interrupts and registers into messages. Thus, a process would write to a terminal by sending that terminal a message. The device driver would receive the message and output the character to the terminal. An input character would interrupt the system and transfer to a device driver. The device driver would create a message from the input character and send it to a waiting process. 20.7 CTSS The Compatible Time-Sharing System (CTSS) was designed at MIT as an experimental time-sharing system and rst appeared in 1961. It was implemented on an IBM 7090 and eventually supported up to 32 interactive users. The users were provided with a set of interactive commands that allowed them to manipulate les and to compile and run programs through a terminal"
"Explain how, LEGO blocks have the characteristic that they are easily composable and are generic","LEGO boxes, can be combined. Moreover, they can be combined into constructions other than the construction suggested by the manufacturer. Hence,  LEGO blocks have the characteristic that they are easily composable and are generic."," The idea of separating the manufacturing of parts and the assemblage of those parts into a product was pioneered around 1800 in the production of rifles. This idea has fundamentally changed the hardware-manufacturing business. In this chapter, we discuss how to apply the same idea to the production of software. The production of software parts - the components - is separated from the assemblage of those parts into applications. As a consequence, there will be two separate development processes, one for components and one for applications. These processes are not independent. Applications need certain components and components had better be used in applications. So there are dependencies between the two development processes. These development processes and their dependencies are discussed in Section 18.3. LEGO is often taken as an example of a component-based approach. LEGO is a set of building blocks in a large variety of shapes and colors. LEGO is sold in boxes that contain a number of blocks that can be composed to make up toys such as cars, trains or airplanes. In order to enable their composition, all LEGO blocks provide small cylindrical stubs on the top and complementary holes on the bottom of the shape. The conformance of the blocks to this convention ensures that blocks, possibly from different LEGO boxes, can be combined. Moreover, they can be combined into constructions other than the construction suggested by the manufacturer. Hence, LEGO blocks have the characteristic that they are easily composable and are generic. These are characteristics that we also look for in software components. There are however, many more toys that consist of building blocks that can be assembled. Take, for example, Meccano and traditional jigsaw puzzles. While each of these can be combined with blocks of the same type (Meccano - Meccano, LEGO-LEGO, etc.), they cannot be combined with blocks of another type (e.g. Meccano - LEGO). This illustrates that composability of building blocks is related to the conformance to a set of conventions. This set of conventions is called a component model. Component models and components are discussed in Section 18.2"
"Explain why, the effort needed to change a component-based system is likely to be less than the effort needed when the same changes are made to a system that is not modularized according to a component-based design discipline","Developing software in a component-based way requires a discipline of strict modularization. The strict modularization limits the domino effect of changes in the system. Hence,  the effort needed to change a component-based system is likely to be less than the effort needed when the same changes are made to a system that is not modularized according to a component-based design discipline."," Components must be we 11-documented, otherwise parties other than the creator of the component have great difficulty in using the component. Developing software in a component-based way requires a discipline of strict modularization. The strict modularization limits the domino effect of changes in the system. Hence, the effort needed to change a component-based system is likely to be less than the effort needed when the same changes are made to a system that is not modularized according to a component-based design discipline. This property improves maintainability. When subsystems are strictly modularized, it becomes easier to change parts to support new features. This improves the ability of the system to evolve. CBSE increases productivity through the reuse of building blocks that have been developed in earlier efforts or by other parties. In order for this to enhance productivity, the acquisition and integration of a component should of course be cheaper than developing that component from scratch. This general characteristic of software reuse has been discussed in Chapter 17"
"Explain why, in CBSE, components are designed and specified in such a way that they can be constructed independently of, and concurrently with, other components","Concurrent development of components is easier if components have minimal mutual dependencies, i.e. when components are loosely coupled. Any dependencies that do exist between components must be specified explicitly. Hence,  in CBSE, components are designed and specified in such a way that they can be constructed independently of, and concurrently with, other components."," Concurrent development of components is easier if components have minimal mutual dependencies, i.e. when components are loosely coupled. Any dependencies that do exist between components must be specified explicitly. Hence, in CBSE, components are designed and specified in such a way that they can be constructed independently of, and concurrently with, other components. 18.2. COMPONENT MODELS AND COMPONENTS In the introduction to this chapter, we made an analogy with LEGO and Meccano and noticed that composing building blocks is feasible only if they conform to the same set of conventions"
Explain why that bridge collapsed shortly after it was finished,"Narrows Bridge failure in 1940. The designers of that bridge extrapolated beyond their experience to create more flexible stiffening girders for suspension bridges. They did not think about aerodynamics and the response of the bridge to wind. Hence, that bridge collapsed shortly after it was finished."," Even in a mature engineering discipline, say bridge design, accidents do happen. Bridges collapse once in a while. Most problems in bridge design occur when designers extrapolate beyond their models and expertise. A famous example is the Tacoma Narrows Bridge failure in 1940. The designers of that bridge extrapolated beyond their experience to create more flexible stiffening girders for suspension bridges. They did not think about aerodynamics and the response of the bridge to wind. As a result, that bridge collapsed shortly after it was finished. This type of extrapolation seems to be the rule rather than the exception in software development. We regularly embark on software development projects that go far beyond our expertise. There are additional reasons for considering the construction of software as something quite different from the construction of physical products. The cost of constructing software is incurred during development and not during production"
"Explain how abstractions, such as QuickSort, embodied in procedures and abstract data types, such as Stack and BinaryTree, have become part of our vocabulary and are routinely used in our daily work"," At the level of algorithms and abstract data types, such a body of knowledge has been accumulated over the years, and has been codified in text books and libraries of reusable components. Hence, abstractions, such as QuickSort, embodied in procedures and abstract data types, such as Stack and BinaryTree, have become part of our vocabulary and are routinely used in our daily work."," An expert programmer has at his disposal a much larger number of knowledge chunks than a novice programmer. This concerns both programming knowledge and knowledge about the application domain. Both during the search for a solution and during program comprehension, the programmer tries to link up with knowledge already present. As a corollary, part of our education as programmer or software engineer should consist of acquiring a set of useful knowledge chunks. At the level of algorithms and abstract data types, such a body of knowledge has been accumulated over the years, and has been codified in text books and libraries of reusable components. As a result, abstractions, such as QuickSort, embodied in procedures and abstract data types, such as Stack and BinaryTree, have become part of our vocabulary and are routinely used in our daily work. SOFTWARE ARCHITECTURE The concepts embodied in these abstractions are useful during the design, implementation and maintenance of software for the following reasons:    They can be used in a variety of settings and can be given unique names"
Explain why massive renovation projects have started and many a high-rise apartment building has been demolished,"Shaw (1996) characterizes a number of well-known software architectural styles in a framework that resembles a popular way of describing design patterns. Both the characterization and the framework are shaped after Alexanders way of describing 2 Here, we may note another similarity between classical architecture and software architecture. In the 1950s and 1960s, housing was a major problem in Western Europe and beyond. There were far too few houses available, while those available were mostly of a bad quality (damp, no bathroom, too small). In the post-war economic boom, many suburbs were constructed, with lots of spacious apartments, each one a container made of steel and concrete. These new suburbs solved one problem -- the housing of a large number of people -- but at the same time created other problems which only showed themselves much later, e.g. lack of community feeling and social ties, high crime rates. Hence, massive renovation projects have started and many a high-rise apartment building has been demolished."," An Alexandrian pattern is not a cookbook, black-box recipe for architects, any more than a dictionary is a toolkit for a novelist. Rather, a pattern is a flexible generic scheme providing a solution to a problem in a given context. In a narrative form, its application looks like this: IF you find yourself in <context>, for example <examples>, with <problem>, THEN for some <reasons>, apply <pattern> to construct a solution leading to a <new context> and <other patterns>. The above Four-Story Limit pattern may for example be applied in a context where one has to design a suburb. The citation gives some of the reasons for applying this pattern. If it is followed, it will give rise to the application of other patterns, such as those for planning parking lots, the layout of roads, or the design of individual houses.2 Shaw (1996) characterizes a number of well-known software architectural styles in a framework that resembles a popular way of describing design patterns. Both the characterization and the framework are shaped after Alexanders way of describing 2 Here, we may note another similarity between classical architecture and software architecture. In the 1950s and 1960s, housing was a major problem in Western Europe and beyond. There were far too few houses available, while those available were mostly of a bad quality (damp, no bathroom, too small). In the post-war economic boom, many suburbs were constructed, with lots of spacious apartments, each one a container made of steel and concrete. These new suburbs solved one problem -- the housing of a large number of people -- but at the same time created other problems which only showed themselves much later, e.g. lack of community feeling and social ties, high crime rates. As a result, massive renovation projects have started and many a high-rise apartment building has been demolished. In software development, we developed company-wide systems in the 1970s and 1980s, with an emphasis on performance, uniformity, and standardized ways of working. Many of these systems are unable to cope satisfactorily with todays requirements of flexibility and adaptability, and are therefore being renovated. 11.4. ARCHITECTURAL STYLES Type computational The component performs a computation of some sort"
Explain why there are many levels of visibility between local and global,"Nowadays, programming languages have much more flexible means of passing information from one module to another, and this requires a more detailed set of coupling levels. For example, modules may pass control data through records (as opposed to scalars only). Modules may allow some modules access to their data and deny it to others. Hence, there are many levels of visibility between local and global."," The various types of coupling emerged in the 1970s and reflect the data type concepts of programming languages in use at that time. For example, programming languages of that time had simple scalar data types such as real and integer. They allowed arrays of scalar values and records were used to store values of different types. Modules were considered data-coupled if they passed scalars or arrays. They were considered stamp-coupled if they passed record data. When two modules are control-coupled, the assumption is that the control is passed through a scalar value. Nowadays, programming languages have much more flexible means of passing information from one module to another, and this requires a more detailed set of coupling levels. For example, modules may pass control data through records (as opposed to scalars only). Modules may allow some modules access to their data and deny it to others. As a result, there are many levels of visibility between local and global. Finally, the coupling between modules need not be commutative. When module A passes a scalar value to B and B returns a value which is used to control the further execution of A, then A is data-coupled to B, while B is control-coupled to A. As a result, people have extended and refined the definitions of cohesion and coupling levels. Coupling and cohesion are dual characteristics. If the various modules exhibit strong internal cohesion, the intermodule coupling tends to be minimal, and vice versa"
Explain how people have extended and refined the definitions of cohesion and coupling levels,"Nowadays, programming languages have much more flexible means of passing information from one module to another, and this requires a more detailed set of coupling levels. For example, modules may pass control data through records (as opposed to scalars only). Modules may allow some modules access to their data and deny it to others. As a result, there are many levels of visibility between local and global. Finally, the coupling between modules need not be commutative. When module A passes a scalar value to B and B returns a value which is used to control the further execution of A, then A is data-coupled to B, while B is control-coupled to A. Hence, people have extended and refined the definitions of cohesion and coupling levels."," The various types of coupling emerged in the 1970s and reflect the data type concepts of programming languages in use at that time. For example, programming languages of that time had simple scalar data types such as real and integer. They allowed arrays of scalar values and records were used to store values of different types. Modules were considered data-coupled if they passed scalars or arrays. They were considered stamp-coupled if they passed record data. When two modules are control-coupled, the assumption is that the control is passed through a scalar value. Nowadays, programming languages have much more flexible means of passing information from one module to another, and this requires a more detailed set of coupling levels. For example, modules may pass control data through records (as opposed to scalars only). Modules may allow some modules access to their data and deny it to others. As a result, there are many levels of visibility between local and global. Finally, the coupling between modules need not be commutative. When module A passes a scalar value to B and B returns a value which is used to control the further execution of A, then A is data-coupled to B, while B is control-coupled to A. As a result, people have extended and refined the definitions of cohesion and coupling levels. Coupling and cohesion are dual characteristics. If the various modules exhibit strong internal cohesion, the intermodule coupling tends to be minimal, and vice versa"
Explain why the review process does not pay off sufficiently,"A potential danger of this type of review is that it remains too shallow. The people involved become overwhelmed with information, they may have insufficient knowledge of the problem domain, their responsibilities may not have been clearly delineated. Hence, the review process does not pay off sufficiently."," Thus, these techniques also serve as a vehicle for process improvement. Under the general umbrella of peer reviews, they are part of the CMM level 3 key process area Verification (see section 6.6). A potential danger of this type of review is that it remains too shallow. The people involved become overwhelmed with information, they may have insufficient knowledge of the problem domain, their responsibilities may not have been clearly delineated. As a result, the review process does not pay off sufficiently. Parnas and Weiss (1987) describe a type of review process in which the people involved have to play a more active role. Parnas distinguishes between different types of specialized design review. Each of these reviews concentrates on certain desirable properties of the design. As a consequence, the responsibilities of the people involved are clear. The reviewers have to answer a list of questions (under which conditions may this function be called, what is the effect of this function on the behavior of other functions, and the like). In this way, the reviewers are forced to study carefully the design information received. Problems with the questionnaire and documentation can be posed to the designers, and the completed questionnaires are discussed by the designers and reviewers. Experiments suggest that inspections with specialized review roles are more effective than inspections in which review roles are not specialized"
Explain why the picture might well get distorted,"The faults found by the first group can then be considered seeded faults for the second group. In using this technique, though, we must realize that there is a chance that both groups will detect (the same type of) simple faults. Hence, the picture might well get distorted."," ) SOFTWARE TESTING Another technique is to have the program independently tested by two groups. The faults found by the first group can then be considered seeded faults for the second group. In using this technique, though, we must realize that there is a chance that both groups will detect (the same type of) simple faults. As a result, the picture might well get distorted. A useful rule of thumb for this technique is the following: if we find many seeded faults and relatively few others, the result can be trusted. The opposite is not true. This phenomenon is more generally applicable: if, during testing of a certain component, many faults are found, it should not be taken as a positive sign. Quite the contrary, it is an indication that the component is probably of low quality. As Myers observed: The probability of the existence of more errors in a section of a program is proportional to the number of errors already found in that section. (Myers, 1979)"
Explain how the notion of component differs between different types of domain,"A warning about terminology is warranted. In CBSE, the word component has a more specific meaning than its meaning in Standard English. Whereas component in general means part, in CBSE a component is a piece of software that conforms to the rules of a particular software component model. The component model used in one system may be very different from that used in another system. Hence, the notion of component differs between different types of domain."," Components are to become parts of bigger systems. As such, they have to fit the architecture of those systems. One possibility is to first design the architecture and then search for components that fit the architecture. A second possibility is to design an architecture of a family of (similar) systems and develop components that fit a variety of products within that family. A third possibility is to use a bottom-up approach in which the architecture is made to fit the available components. These different architectural approaches are discussed in Section 18.4. A warning about terminology is warranted. In CBSE, the word component has a more specific meaning than its meaning in Standard English. Whereas component in general means part, in CBSE a component is a piece of software that conforms to the rules of a particular software component model. The component model used in one system may be very different from that used in another system. As a result, the notion of component differs between different types of domain. In the literature, the component model assumed is often implicit. Furthermore, in earlier chapters of this book we used the term component in its general sense of part or constituent. CBSE is not a goal in itself, but a means to an end. CBSE is aimed at contributing to the realization of the business goals of an organization. The main motivations for applying CBSE are the topic of Section 18. 1"
Explain why large teams PEOPLE MANAGEMENT AND TEAM ORGANIZATION  are usually split into smaller teams in a way that confines most of the coordination and communication within the sub-team,"Software development involves teamwork. The members of the team have to coordinate their work, communicate their decisions, etc. For a small project, the team will consist of up to a few individuals. As the size of the project increases, so will the team. Large teams are difficult to manage, though. Coordinating the work of a large team is difficult. Communication between team members tends to increase exponentially with the size of the team (see also chapter 7). Hence, large teams PEOPLE MANAGEMENT AND TEAM ORGANIZATION  are usually split into smaller teams in a way that confines most of the coordination and communication within the sub-team."," Section 5.1 further elaborates the various internal and external factors that affect the way projects are managed, and emphasizes the need to pay ample attention to the human element in project management. Software development involves teamwork. The members of the team have to coordinate their work, communicate their decisions, etc. For a small project, the team will consist of up to a few individuals. As the size of the project increases, so will the team. Large teams are difficult to manage, though. Coordinating the work of a large team is difficult. Communication between team members tends to increase exponentially with the size of the team (see also chapter 7). Therefore, large teams PEOPLE MANAGEMENT AND TEAM ORGANIZATION are usually split into smaller teams in a way that confines most of the coordination and communication within the sub-team. Section 5.2 discusses several ways to organize a software development team"
Explain why we may want  to go from a linear development model to an incremental one," In these situations, we will need frequently to measure progress towards the projects goals in order to allow for timely adjustments. Hence, we may want  to go from a linear development model to an incremental one."," In order to make a project of this kind manageable, one needs overcapacity. As far as the process is concerned, this necessitates margins in development time and budget. Keeping extra personnel is not feasible, in general. In these situations, we will need frequently to measure progress towards the projects goals in order to allow for timely adjustments. Therefore, we may want to go from a linear development model to an incremental one. This preference will increase as the uncertainty increases. Cost estimation will have to rely on past experience. We will usually not have enough data to use one of the more formalized cost estimation models. In this situation too, we will need sensitivity analyses. This need will be more pressing than in the previous situation, since the uncertainty is greater. The project manager will be interested in the sensitivity of cost estimates to certain cost drivers. He might be interested in such questions as: what will happen to the development schedule if two extra analysts are assigned to this project, or: what will the effect be on the total cost if we shorten the development time by x days? By viewing cost estimation in this way, the manager will gain insight to, and increase his feeling for, possible solution strategies"
Explain why d ia lo g s should not contain information that i s irrelev an t or ra re ly needed,"Use a simple and natural d ialo g Humans can only pay atten tio n to a few things at a time. Hence, d ia lo g s should not contain information that i s irrelev an t or ra re ly needed."," H euristic evaluation i s based on some d e fin itio n o f u s a b ility . U sab ility i s a complex o f asp ects such as ease o f use, ease of learning, ease o f r e c a ll a f te r a period o f not using a system (known as re - le a rn a b ility ), affe ctio n , help, lik e a b ility , etc. Approaches toward h e u ristic evaluation provide c h e c k lists of c h a ra c te ris tic s o f the user in te rface or UVM that describe the d iffe re n t u s a b ility aspects. Such a c h e ck list may a ls o be applied as a guideline fo r making design decisio n s. C hecklists e x is t in d iffe re n t forms, and often include a technique fo r c alc u latin g u s a b ility indexes. Each item may be checked when app licable and, ad d itio n ally , each item that i s diagnosed as non-optimal may give r i s e to design changes. Example items from such a c h e ck list are (Shneiderman and P laisa n t, 2004; Nielsen, 1993): Use a simple and natural d ialo g Humans can only pay atten tio n to a few things at a time. Therefore, d ia lo g s should not contain information that i s irrelev an t or ra re ly needed. Information should appear in a lo g ic a l and natural order to ease comprehension. Speak the u ser s language A d ialo g i s e a s ie r to understand i f i t uses concepts and phrases that are fam ilia r to the user. The d ialo g should not be expressed in computer-oriented terms, but in terms from the task domain"
"Explain why maintenance activities differ in degrees from initial development, rather than fundamentally","However, during initial development we often do not start from scratch either. If an existing organization decides to automate its order administration, the system may have to interface with already existing systems for, say, stock administration and bookkeeping. Hence, maintenance activities differ in degrees from initial development, rather than fundamentally."," The first big difference is that these changes are being made to an existing product. However, during initial development we often do not start from scratch either. If an existing organization decides to automate its order administration, the system may have to interface with already existing systems for, say, stock administration and bookkeeping. Thus, maintenance activities differ in degrees from initial development, rather than fundamentally. This relative difference is even more apparent when the system is prototyped or developed incrementally. The second main difference, time pressure, has a much larger impact. Time pressure is most strongly felt when repairing errors, for then it is quite possible that certain parts of the organization have to shut down because the software is not operational. In such cases, we have to work against time to identify and repair the errors. Often one patches the code and skips a thorough analysis and design step"
"Explain why at each point in time during a project, different versions of the same document may exist in parallel","In the course of a software development project, quite a few documents are produced. These documents are also changed from time to time. Errors have to be corrected, change requests have to be taken care of, etc. Hence, at each point in time during a project, different versions of the same document may exist in parallel."," Configuration Management LEARNING OBJECTIVES    To understand the main tasks and responsibilities of software configuration management To be aware of the contents of a configuration management plan To appreciate the interplay between the role of configuration management in software development and the capabilities of supporting tools CONFIGURATION MANAGEMENT Careful procedures are needed to manage the vast number of elements (source code components, documentation, change requests, etc.) that are created and updated over the lifetime of a large software system. This is especially true in distributed development projects. It is called configuration management. In the course of a software development project, quite a few documents are produced. These documents are also changed from time to time. Errors have to be corrected, change requests have to be taken care of, etc. Thus, at each point in time during a project, different versions of the same document may exist in parallel. Often too, a software system itself is not monolithic. Software systems exist in different versions or configurations. Different versions come about when changes are implemented after the system has been delivered to the customer. From time to time, the customer is then confronted with a new release. Different versions of components of a system may also exist during development. For instance, if a change request has been approved, a programmer may be implementing that change by rewriting one or more components. Another programmer, however, may still be using the previous version of those same components"
Explain why maintenance costs are not taken into account,"The notion of total cost is usually taken to indicate the cost of the initial software development effort, i.e. the cost of the requirements engineering, design, implementation and testing phases. Hence, maintenance costs are not taken into account."," In most cost estimation models, a simple relation between cost and effort is assumed. The effort may be measured in man-months, for instance, and each manmonth is taken to incur a fixed amount, say, of $5000. The total estimated cost is then obtained by simply multiplying the estimated number of man-months by this constant factor. In this chapter, we freely use the terms cost and effort as if they are synonymous. The notion of total cost is usually taken to indicate the cost of the initial software development effort, i.e. the cost of the requirements engineering, design, implementation and testing phases. Thus, maintenance costs are not taken into account. Unless explicitly stated otherwise, this notion of cost will also be used by us. In the same vein, development time will be taken to mean: the time between the start of the requirements engineering phase and the point in time when the software is delivered to the customer. Lastly, the notion of cost as it is used here, does not include possible hardware costs either. It concerns only personnel costs involved in software development. Research in the area of cost estimation is far from crystallized. Different models use different measures and cost drivers, so that mutual comparisons are very difficult"
Explain why each additional line of code requires more effort,"There is a larger overhead because of the increased need for communication and management control, because of the problems and interfaces getting more complex, and so on. Hence, each additional line of code requires more effort."," Origin Halstead Boehm Walston--Felix E = 0:7KLOC 1:50 E = 2:4KLOC 1:05 E = 5:2KLOC 0:91 See section 7.1.2 This phenomenon is well known from the theory of economics. In a so-called economy of scale, one assumes that it is cheaper to produce large quantities of the same product. The fixed costs are then distributed over a larger number of units, which COST ESTIMATION decreases the cost per unit. We thus realize an increasing return on investment. In the opposite case, we find a diseconomy of scale: after a certain point the production of additional units incurs extra costs. In the case of software, the lines of code are the product. If we assume that producing a lot of code will cost less per line of code, formulae like those of Walston--Felix ( < ) result. This may occur, for example, because the cost of expensive tools like program generators, programming environments and test tools can be distributed over a larger number of lines of code. Alternatively, we may reason that large software projects are more expensive, relatively speaking. There is a larger overhead because of the increased need for communication and management control, because of the problems and interfaces getting more complex, and so on. Thus, each additional line of code requires more effort. In such cases, we obtain formulae like those of Boehm and Halstead ( > ). There is no really convincing argument for either type of relation, though the latter ( > ) may seem more plausible. Certainly for large projects, the effort required does seem to increase more than linearly with size"
Explain why some development strategy involving many small steps and frequent user feedback is to be used,"Since requirements are not precisely known, some agile approach is appropriate as a process model. The larger the uncertainty, the more often we will have to check whether we are still on the right track. Hence, some development strategy involving many small steps and frequent user feedback is to be used."," Controlling a project of this kind is a difficult and challenging activity. To make a project of this kind manageable, our goal will be to maximize output, given the resources available to the project. This maximization may concern the quality of the product, or its functionality, or both. Since requirements are not precisely known, some agile approach is appropriate as a process model. The larger the uncertainty, the more often we will have to check whether we are still on the right track. Thus, some development strategy involving many small steps and frequent user feedback is to be used. Cost estimation using some formalized model clearly is not feasible in these circumstances. The use of such models presupposes that we know enough of the project at hand to be able to compare it with previous projects. Such is not the case, though"
Explain how requirements elicitation is about understanding the problem,"Requirements elicitation In general, the requirements analyst is not an expert in the domain being modeled. Through interaction with domain specialists, such as professional librarians, he has to build himself a sufficiently rich model of that domain. Hence, requirements elicitation is about understanding the problem."," For different reasons (cost, time to market, quality) we may want to employ commercial off the shelf (COTS) components in our library system. We then have to tradeoff our requirements against the possibilities offered by those COTS components.  Requirements elicitation In general, the requirements analyst is not an expert in the domain being modeled. Through interaction with domain specialists, such as professional librarians, he has to build himself a sufficiently rich model of that domain. Thus, requirements elicitation is about understanding the problem. The fact that different disciplines are involved in this process complicates matters. In many cases, the analyst is not a mere outside observer of the domain to be modeled, simply eliciting facts from domain specialists. He may have to take a stand in a power struggle or decide between conflicting requirements, thereby actively participating in the construction of the domain of interest"
"Explain why the document must be organized in such a way that changes can be accommodated readily (a tabular or database format, for example)","A requirements specification should be modifiable. Software models part of reality. Therefore it changes. The corresponding requirements specification has to evolve together with the reality being modeled. Hence, the document must be organized in such a way that changes can be accommodated readily (a tabular or database format, for example)."," Phrases like the system should be user-friendly are not verifiable. Likewise, the use of quantities that cannot be measured, as in the systems response time should usually be less than two seconds, should be avoided. A requirement like for requests of type X, the systems response time is less than two seconds in 80% of cases, with a maximum machine load of Y, is verifiable. A requirements specification should be modifiable. Software models part of reality. Therefore it changes. The corresponding requirements specification has to evolve together with the reality being modeled. Thus, the document must be organized in such a way that changes can be accommodated readily (a tabular or database format, for example). Redundancy must be prevented as much as possible, for otherwise there is the danger that changes lead to inconsistencies. A requirements specification should be traceable. The origin and rationale of each and every requirement must be traceable. A clear and consistent numbering scheme makes it possible that other documents can uniquely refer to parts of the requirements specification"
"Explain how the input can be viewed as a sequence of words separated by breaks, with possible leading and trailing breaks, and ending with ET","A word is a non-empty sequence of nonbreak characters. A break is a sequence of one or more break characters. Hence, the input can be viewed as a sequence of words separated by breaks, with possible leading and trailing breaks, and ending with ET."," ~ Study the following specification for a simple line formatter: The programs input is a stream of characters whose end is signaled with a special end-of-text character, ET. There is exactly one ET character in each input stream. Characters are classified as: break characters -- BL (blank) and NL (new line); nonbreak characters -- all others except ET; the end-of-text indicator - ET. A word is a non-empty sequence of nonbreak characters. A break is a sequence of one or more break characters. Thus, the input can be viewed as a sequence of words separated by breaks, with possible leading and trailing breaks, and ending with ET. The programs output should be the same sequence of words as in the input, with the exception that an oversize word (i.e. a word containing more than MAXPOS characters, where MAXPOS is a positive integer) should cause an error exit from the program (i.e. a variable, Alarm, should have the value TRUE). Up to the point of an error, the programs output should have the following properties: 1 A new line should start only between words and at the beginning of the output text, if any"
Explain why an object-oriented design is less susceptible to changes in the world being modeled,"The object-oriented approach promotes reuse by focusing on the identification of real-world objects from the application domain. In contrast, more traditional approaches focus on identifying functions. In an evolving world, the objects tend to be stable, while the functions tend to change. For instance, in an office environment the functions performed are likely to change with time, but there will always be letters, folders, and so on. Hence, an object-oriented design is less susceptible to changes in the world being modeled."," The object-oriented approach leads to more flexible systems that are easier to adapt and change. Because the real-world objects have a direct counterpart in the implementation, it becomes easy to link change requests to the corresponding program modules. Through the inheritance mechanism, changes can often be realized by adding another specialized object rather than through tinkering with the code. For example, if we wish to extend our system dealing with furniture by adding another type of chair, say armchair, we do so by defining a new object ArmChair, together with its own set of attributes, as another specialization of Chair. The object-oriented approach promotes reuse by focusing on the identification of real-world objects from the application domain. In contrast, more traditional approaches focus on identifying functions. In an evolving world, the objects tend to be stable, while the functions tend to change. For instance, in an office environment the functions performed are likely to change with time, but there will always be letters, folders, and so on. Thus, an object-oriented design is less susceptible to changes in the world being modeled.  The inheritance mechanism adds to reusability. New objects can be created as specializations of existing objects, inheriting attributes from the existing objects. At the implementation level, this kind of reuse is accomplished through code sharing. The increasing availability of class libraries contributes to this type of code reuse"
Explain why a typical development environment will have a pyramid shape,"In the ideal case, the choice of a specific set of tools will be made as follows. First, a certain approach to the software development process is selected. Next, techniques are selected that support the various phases in that development process. As a last step, tools are selected that support those techniques. Some steps in the development process may not be supported by well-defined techniques. Some techniques may not be supported by tools. Hence, a typical development environment will have a pyramid shape"," Integrated Environments and Workbenches This section is devoted to CASE products that support (parts of) the software development process. Depending on the scope of the set of tools available, such an environment is called an Analyst WorkBench (AWB), a Programmer WorkBench (PWB), a Management WorkBench (MWB), or an Integrated Project Support Environment (IPSE); see also figure 15.3. The acronym CASE (Computer-Aided Software Engineering) is often used to indicate any type of tool support in the software development process. The qualified terms Upper-CASE and Lower-CASE refer to tool support during the analysis--design and implementation--test phases, respectively. In the ideal case, the choice of a specific set of tools will be made as follows. First, a certain approach to the software development process is selected. Next, techniques are selected that support the various phases in that development process. As a last step, tools are selected that support those techniques. Some steps in the development process may not be supported by well-defined techniques. Some techniques may not be supported by tools. Thus, a typical development environment will have a pyramid shape as in figure 15.4. In practice, we often find the reverse conical form: a barely-developed model of the development process, few well-defined techniques, and a lot of tools. In this way, the benefits of the tools will be limited at best. To paraphrase the situation: for many a CASE, there is a lot of Computer-Aided, and precious little Software Engineering"
"Explain how PSEEs provide support for software development by automating routine tasks, invoking appropriate development tools, and enforcing rules and practices","If the developer indicates that some piece of code is ready for review, the environment is notified and comes into a state as indicated in figure 15.7. Parallel to the coding activity, management schedules a review meeting. Once this is done, the place1 labeled review scheduled is marked. The support environment then knows that the review can be held and may offer support for doing so. In this way, the environment guides the developers and other participants through the steps of the review process, alerts them when certain actions are required, maintains status information of code products and other pieces of information, etc. Hence, PSEEs provide support for software development by automating routine tasks, invoking appropriate development tools, and enforcing rules and practices."," notation is that of Petri nets. In section 3.6, this same figure was used to explain the SOFTWARE TOOLS role of different formalisms in process modeling. Here, we will use it to discuss its role in a process-centered software engineering environment. If the developer indicates that some piece of code is ready for review, the environment is notified and comes into a state as indicated in figure 15.7. Parallel to the coding activity, management schedules a review meeting. Once this is done, the place1 labeled review scheduled is marked. The support environment then knows that the review can be held and may offer support for doing so. In this way, the environment guides the developers and other participants through the steps of the review process, alerts them when certain actions are required, maintains status information of code products and other pieces of information, etc. Thus, PSEEs provide support for software development by automating routine tasks, invoking appropriate development tools, and enforcing rules and practices. Formal models of a software process are rigid. In practice, this rigidity is a hindrance, since there will always be exceptions. For example, the minutes of a review meeting might get lost, management may decide to skip a certain review meeting, a review meeting may have to be rescheduled because some participant got ill, etc"
Explain why people are less likely to fully understand and be able to act on the feedback they are being given,"If someone isnt on the right track with something that theyre doing, waiting up to a year for their next annual review isnt good for anyone involved. They will likely go through this time thinking they are doing well, leading to a nasty surprise come review time. The psychology of getting feedback shows that people generally react to these sorts of negative surprises emotionally rather than intellectually, a phenomenon known as amygdala hijacking.4 Hence, people are less likely to fully understand and be able to act on the feedback they are being given."," | Frequency of Feedback According to a 2011 Wall Street Journal article, 51 percent of companies do annual performance reviews, while 41 percent do them semiannually. However, more and more companies are beginning to realize that feedback and reviews can have much more of an impact if given more frequently, if the feedback itself is helpful to those receiving it. Obviously if feedback provides no new or actionable information, there will be no benefit to getting it more often. However, for feedback that is useful and actionable, greater frequency does lead to greater benefit for both individuals and organizations. If someone isnt on the right track with something that theyre doing, waiting up to a year for their next annual review isnt good for anyone involved. They will likely go through this time thinking they are doing well, leading to a nasty surprise come review time. The psychology of getting feedback shows that people generally react to these sorts of negative surprises emotionally rather than intellectually, a phenomenon known as amygdala hijacking.4 As a result, people are less likely to fully understand and be able to act on the feedback they are being given. Smaller, shorter feedback cycles mean that adjustments are smaller and thus easier to make. This is a big driving factor behind teams moving away from the waterfall model for software development toward more Agile practices and a reason why continuous delivery works so well. Annual performance reviews are similar to waterfall in that the delay in getting feedback slows down problem resolu tion, so organizations should move toward the more Agile idea of continuous feedback"
"Explain why it is important to consider how people work, how they think, and what motivates them","Culture is obviously strongly tied to people; the same tools and policies being used by a different group of people with different goals, working styles, and interpretations can result in a very different culture or work environment. Were it not for the fact that the work were discussing is done by people, ultimately for other people, these types of conversations would not be happening. Hence, it is important to consider how people work, how they think, and what motivates them"," | The question of whether tools or culture is more important to the concept of devops is one that has been around nearly as long as the term itself. As we have explained throughout this book, we strongly believe that culturewith its effects on how people work, why they work, how they interact when working together, and how they make decisions related to work, including which tools and technologies are used and how is more encompassing of the essence of the movement. Culture is obviously strongly tied to people; the same tools and policies being used by a different group of people with different goals, working styles, and interpretations can result in a very different culture or work environment. Were it not for the fact that the work were discussing is done by people, ultimately for other people, these types of conversations would not be happening. Therefore, it is important to consider how people work, how they think, and what motivates them, as we discussed in Part II. Culture is also very strongly tied with values, both at an individual and an organiza tional level. Recognizing the importance of individuals, respecting their expertise around their own areas of work, and letting people shape both their own narratives and influence those of their team and organization are key values of a healthy and effective devops culture. In these next sections, well look at how values can manifest themselves both implicitly and explicitly"
"Explain why, it is essential to instantiate asynchronous communication between the software process activities applied to the engineering and construction of aspects and components","A distinct aspect-oriented process has not yet matured. However, it is likely that such a process will adopt characteristics of both evolutionary and concurrent process models. The evolutionary model is appropriate as aspects are identified and then constructed. The parallel nature of concurrent development is essential because aspects are engineered independently of localized software components and yet, aspects have a direct impact on these components. Hence,  it is essential to instantiate asynchronous communication between the software process activities applied to the engineering and construction of aspects and components."," Grundy [Gru02] provides further discussion of aspects in the context of what he calls aspect-oriented component engineering (AOCE): AOCE uses a concept of horizontal slices through vertically-decomposed software components, called aspects, to characterize cross-cutting functional and non-functional properties of components. Common, systemic aspects include user interfaces, collaborative work, distribution, persistency, memory management, transaction processing, security, integrity and so on. Components may provide or require one or more aspect details relating to a particular aspect, such as a viewing mechanism, extensible affordance and interface kind (user interface aspects); event generation, transport and receiving (distribution aspects); data store/retrieve and indexing (persistency aspects); authentication, encoding and access rights (security aspects); transaction atomicity, concurrency control and logging strategy (transaction aspects); and so on. Each aspect detail has a number of properties, relating to functional and/or non-functional characteristics of the aspect detail. A distinct aspect-oriented process has not yet matured. However, it is likely that such a process will adopt characteristics of both evolutionary and concurrent process models. The evolutionary model is appropriate as aspects are identified and then constructed. The parallel nature of concurrent development is essential because aspects are engineered independently of localized software components and yet, aspects have a direct impact on these components. Hence, it is essential to 11/27/08 CHAPTER 2 instantiate asynchronous communication between the software process activities applied to the engineering and construction of aspects and components. A detailed discussion of aspect-oriented software development is best left to books dedicated to the subject. If you have further interest, see [Saf08], [Cla05], [Jac04], and [Gra03]"
"Explain why, an incremental development strategy should be instituted","But continual adaptation without forward progress accomplishes little. Therefore, an agile software process must adapt incrementally. To accomplish incremental adaptation, an agile team requires customer feedback (so that the appropriate adaptations can be made). An effective catalyst for customer feedback is an operational prototype or a portion of an operational system. Hence,  an incremental development strategy should be instituted."," Given these three assumptions, an important question arises: How do we create a process that can manage unpredictability? The answer, as I have already noted, lies in process adaptability (to rapidly changing project and technical conditions). An agile process, therefore, must be adaptable. But continual adaptation without forward progress accomplishes little. Therefore, an agile software process must adapt incrementally. To accomplish incremental adaptation, an agile team requires customer feedback (so that the appropriate adaptations can be made). An effective catalyst for customer feedback is an operational prototype or a portion of an operational system. Hence, an incremental development strategy should be instituted. Software increments (executable prototypes or portions of an operational system) must be delivered in short time periods so that adaptation keeps pace with change (unpredictability). This iterative approach enables the customer to evaluate the software increment regularly, provide necessary feedback to the software team, and influence the process adaptations that are made to accommodate the feedback. 3.3.1 Agility Principles The Agile Alliance (see [Agi03], [Fow01]) defines 12 agility principles for those who Although agile processes embrace change, it is still important to examine the reasons for change"
"Explain why, requirements modeling with UML6 begins with the creation of scenarios in the form of use cases, activity diagrams, and swimlane diagrams","Although the success of a computer-based system or product is measured in many ways, user satisfaction resides at the top of the list. If you understand how end users (and other actors) want to interact with a system, your software team will be better able to properly characterize requirements and build meaningful analysis and design models. Hence,  requirements modeling with UML6 begins with the creation of scenarios in the form of use cases, activity diagrams, and swimlane diagrams."," However, the specific content of each element (i.e., the diagrams that are used to construct the element and the model) may differ from project to project. As we have noted a number of times in this book, the software team must work to keep it simple. Only those modeling elements that add value to the model should be used. 6.2 S C E N A R I O -B A S E D M O D E L I N G Although the success of a computer-based system or product is measured in many ways, user satisfaction resides at the top of the list. If you understand how end users (and other actors) want to interact with a system, your software team will be better able to properly characterize requirements and build meaningful analysis and design 11/27/08 CHAPTER 6 REQUIREMENTS MODELING: SCENARIOS, INFORMATION, AND ANALYSIS CLASSES models. Hence, requirements modeling with UML6 begins with the creation of scenarios in the form of use cases, activity diagrams, and swimlane diagrams. 6.2.1 Creating a Preliminary Use Case uote: [Use cases] are simply an aid to defining what exists outside the system (actors) and what should be performed by the system (use cases)"
"Explain how, the application domain is one source for the derivation and refinement of components","As the software architecture is refined into components, the structure of the system begins to emerge. But how are these components chosen? In order to answer this question, you begin with the classes that were described as part of the requirements model.4 These analysis classes represent entities within the application (business) domain that must be addressed within the software architecture. Hence,  the application domain is one source for the derivation and refinement of components."," A well-designed habitat allows for the successful evolution of all the components needed in a software system. R. Pattis 9.4.3 Refining the Architecture into Components As the software architecture is refined into components, the structure of the system begins to emerge. But how are these components chosen? In order to answer this question, you begin with the classes that were described as part of the requirements model.4 These analysis classes represent entities within the application (business) domain that must be addressed within the software architecture. Hence, the application domain is one source for the derivation and refinement of components. Another source is the infrastructure domain. The architecture must accommodate many infrastructure components that enable application components but have no business connection to the application domain. For example, memory management components, communication components, database components, and task management components are often integrated into the software architecture. If a conventional (non-object-oriented) approach is chosen, components are derived from the data flow model. I discuss this approach briefly in Section 9.6"
"Explain why, if you think in patterns, it is important to determine whether a pattern for setting up an account exists"," To illustrate, consider the SafeHomeAssured.com WebApp. If you consider the big picture, the WebApp must address a number of fundamental problems such as: How to provide information about SafeHome products and services How to sell SafeHome products and services to customers How to establish Internet-based monitoring and control of an installed security system Each of these fundamental problems can be further refined into a set of subproblems. For example How to sell via the Internet implies an E-commerce pattern that itself implies a large number of patterns at lower levels of abstraction. The E-commerce pattern (likely, an architectural pattern) implies mechanisms for setting up a customer account, displaying the products to be sold, selecting products for purchase, and so forth. Hence,  if you think in patterns, it is important to determine whether a pattern for setting up an account exists."," Its important to note that patterns are not independent entities. Design patterns that are present at a high level of abstraction will invariably influence the manner in which other patterns are applied at lower levels of abstraction. In addition, patterns often collaborate with one another. The implicationwhen you select an architectural pattern, it may very well influence the component-level design patterns you choose. Likewise, when you select a specific interface design pattern, you are sometimes forced to use other patterns that collaborate with it. To illustrate, consider the SafeHomeAssured.com WebApp. If you consider the big picture, the WebApp must address a number of fundamental problems such as: How to provide information about SafeHome products and services How to sell SafeHome products and services to customers How to establish Internet-based monitoring and control of an installed security system pre75977_ch12.qxd 3:58 PM CHAPTER 12 Each of these fundamental problems can be further refined into a set of subproblems. For example How to sell via the Internet implies an E-commerce pattern that itself implies a large number of patterns at lower levels of abstraction. The E-commerce pattern (likely, an architectural pattern) implies mechanisms for setting up a customer account, displaying the products to be sold, selecting products for purchase, and so forth. Hence, if you think in patterns, it is important to determine whether a pattern for setting up an account exists. If SetUpAccount is available as a viable pattern for the problem context, it may collaborate with other patterns such as BuildInputForm, ManageFormsInput, and ValidateFormsEntry. Each of these patterns delineates problems to be solved and solutions that may be applied. 12.2.3 Design Tasks The following design tasks are applied when a pattern-based design philosophy is used: 1. Examine the requirements model and develop a problem hierarchy"
"Explain why, content presented on the far left-hand branch of the hierarchy can have hypertext links that lead directly to content that exists in the middle or right-hand branch of the structure","Hierarchical structures (Figure 13.6) are undoubtedly the most common WebApp architecture. Unlike the partitioned software hierarchies discussed in Chapter 9 that encourage flow of control only along vertical branches of the hierarchy, a WebApp hierarchical structure can be designed in a manner that enables (via hypertext branching) flow of control horizontally across vertical branches of the structure. Hence,  content presented on the far left-hand branch of the hierarchy can have hypertext links that lead directly to content that exists in the middle or right-hand branch of the structure."," Grid structures (Figure 13.5) are an architectural option that you can apply when WebApp content can be organized categorically in two (or more) dimensions. For example, consider a situation in which an e-commerce site sells golf clubs. The horizontal dimension of the grid represents the type of club to be sold (e.g., woods, irons, wedges, putters). The vertical dimension represents the offerings provided by various golf club manufacturers. Hence, a user might navigate the grid horizontally to find the putters column and then vertically to examine the offerings provided by FIGURE 13.4 Linear structures Linear with optional flow Linear with diversions 11/27/08 Page 385 WEBAPP DESIGN FIGURE 13.5 Grid structure FIGURE 13.6 Hierarchical structure those manufacturers that sell putters. This WebApp architecture is useful only when highly regular content is encountered [Pow00]. Hierarchical structures (Figure 13.6) are undoubtedly the most common WebApp architecture. Unlike the partitioned software hierarchies discussed in Chapter 9 that encourage flow of control only along vertical branches of the hierarchy, a WebApp hierarchical structure can be designed in a manner that enables (via hypertext branching) flow of control horizontally across vertical branches of the structure. Hence, content presented on the far left-hand branch of the hierarchy can have hypertext links that lead directly to content that exists in the middle or right-hand branch of the structure. It should be noted, however, that although such branching allows rapid navigation across WebApp content, it can lead to confusion on the part of the user. A networked or pure web structure (Figure 13.7) is similar in many ways to the architecture that evolves for object-oriented systems. Architectural components 5:47 PM Page 386 FIGURE 13.7 Network structure (in this case, Web pages) are designed so that they may pass control (via hypertext links) to virtually every other component in the system. This approach allows considerable navigation flexibility, but at the same time, can be confusing to a user"
Explain why syntactic correctness is judged on proper use of the symbology; each model is reviewed to ensure that proper modeling conventions have been maintained,"The notation and syntax used to represent analysis and design models will be tied to the specific analysis and design methods that are chosen for the project. Hence, syntactic correctness is judged on proper use of the symbology; each model is reviewed to ensure that proper modeling conventions have been maintained."," AND Analysis and design models cannot be tested in the conventional sense, because they cannot be executed. However, technical reviews (Chapter 15) can be used to examine their correctness and consistency. 19.2.1 Correctness of OOA and OOD Models The notation and syntax used to represent analysis and design models will be tied to the specific analysis and design methods that are chosen for the project. Hence syntactic correctness is judged on proper use of the symbology; each model is reviewed to ensure that proper modeling conventions have been maintained. During analysis and design, you can assess semantic correctness based on the models conformance to the real-world problem domain. If the model accurately reflects the real world (to a level of detail that is appropriate to the stage of development at which the model is reviewed), then it is semantically correct. To determine whether the model does, in fact, reflect real-world requirements, it should be presented to problem domain experts who will examine the class definitions and hierarchy for omissions and ambiguity. Class relationships (instance connections) are evaluated to determine whether they accurately reflect real-world object connections.2 Use cases can be invaluable in tracking analysis and design models against real-world usage scenarios for the OO system"
"Explain why, functionality of the system grows with time","A pipeline of software increments is developed by small independent software teams. As each increment is certified, it is integrated into the whole. Hence,  functionality of the system grows with time."," Right practice aims at preventing insertion of errors and, failing that, removing them before testing or any other running of the program. Harlan Mills mental software model introduced in Chapter 2. A pipeline of software increments [Lin94b] is developed by small independent software teams. As each increment is certified, it is integrated into the whole. Hence, functionality of the system grows with time. The sequence of cleanroom tasks for each increment is illustrated in Figure 21.1"
"Explain why, the sequence of usage test cases is defined randomly but corresponds to the appropriate probability of stimuli occurrence","To generate a sequence of usage test cases that conform to the usage probability distribution, random numbers between 1 and 99 are generated. Each random number corresponds to an interval on the preceding probability distribution. Hence,  the sequence of usage test cases is defined randomly but corresponds to the appropriate probability of stimuli occurrence."," Test cases are generated for each set of stimuli4 according to the usage probability distribution. To illustrate, consider the SafeHome system discussed earlier in this book. Cleanroom software engineering is being used to develop a software increment that manages user interaction with the security system keypad. Five stimuli have been identified for this increment. Analysis indicates the percent probability distribution of each stimulus. To make selection of test cases easier, these probabilities are mapped into intervals numbered between 1 and 99 [Lin94] and illustrated in the following table: Program Stimulus Interval 50% Zone set (ZS) 15% Test (T) 9599 pre75977_ch21.qxd 6:18 PM CHAPTER 21 To generate a sequence of usage test cases that conform to the usage probability distribution, random numbers between 1 and 99 are generated. Each random number corresponds to an interval on the preceding probability distribution. Hence, the sequence of usage test cases is defined randomly but corresponds to the appropriate probability of stimuli occurrence. For example, assume the following randomnumber sequences are generated: 13-94-22-24-45-56 38-21-52-84-86-4 Selecting the appropriate stimuli based on the distribution interval shown in the table, the following use cases are derived: ADTADADADZS TADADADQADAD ADADZSTTAD The testing team executes these use cases and verifies software behavior against the specification for the system. Timing for tests is recorded so that interval times may be determined. Using interval times, the certification team can compute mean-timeto-failure. If a long sequence of tests is conducted without failure, the MTTF is low and software reliability may be assumed high. 21.4.2 Certification The verification and testing techniques discussed earlier in this chapter lead to software components (and entire increments) that can be certified. Within the context of the cleanroom software engineering approach, certification implies that the reliability [measured by mean-time-to-failure (MTTF)] can be specified for each component"
"Explain why, the manager monitors the progress of development and recognizes and reacts to problems","At the operational level, the scenario involves various roles and tasks. For the project manager, the goal is to ensure that the product is developed within a certain time frame. Hence,  the manager monitors the progress of development and recognizes and reacts to problems."," 11/27/08 6:20 PM Page 586 Software configuration management is a set of activities that have been developed to manage change throughout the life cycle of computer software. SCM can be viewed as a software quality assurance activity that is applied throughout the software process. In the sections that follow, I describe major SCM tasks and important concepts that can help you to manage change. 22.1.1 An SCM Scenario1 A typical CM operational scenario involves a project manager who is in charge of a software group, a configuration manager who is in charge of the CM procedures and policies, the software engineers who are responsible for developing and maintaining the software product, and the customer who uses the product. In the scenario, assume that the product is a small one involving about 15,000 lines of code being developed by a team of six people. (Note that other scenarios of smaller or larger teams are possible, but, in essence, there are generic issues that each of these projects face concerning CM.) What are the goals of and the activities performed by each of the constituencies involved in change management? At the operational level, the scenario involves various roles and tasks. For the project manager, the goal is to ensure that the product is developed within a certain time frame. Hence, the manager monitors the progress of development and recognizes and reacts to problems. This is done by generating and analyzing reports about the status of the software system and by performing reviews on the system. The goals of the configuration manager are to ensure that procedures and policies for creating, changing, and testing of code are followed, as well as to make information about the project accessible. To implement techniques for maintaining control over code changes, this manager introduces mechanisms for making official requests for changes, for evaluating them (via a Change Control Board that is responsible for approving changes to the software system), and for authorizing changes. The manager creates and disseminates task lists for the engineers and basically creates the project context. Also, the manager collects statistics about components in the software system, such as information determining which components in the system are problematic"
Explain why the average incremental growth remains invariant as the system evolves,"The Law of Conservation of Familiarity (1980): As an E-type system evolves all associated with it, developers, sales personnel, users, for example, must maintain mastery of its content and behavior to achieve satisfactory evolution. Excessive growth diminishes that mastery. Hence, the average incremental growth remains invariant as the system evolves."," The Law of Conservation of Organizational Stability (1980): The average effective global activity rate in an evolving E-type system is invariant over product lifetime. The Law of Conservation of Familiarity (1980): As an E-type system evolves all associated with it, developers, sales personnel, users, for example, must maintain mastery of its content and behavior to achieve satisfactory evolution. Excessive growth diminishes that mastery. Hence the average incremental growth remains invariant as the system evolves. How do legacy systems evolve as time passes? The Law of Continuing Growth (1980): The functional content of E-type systems must be continually increased to maintain user satisfaction over their lifetime"
Explain why people are required to conform to technology,"To make technology that fits human beings, it is necessary to study human beings. But now we tend to study only the technology. Hence, people are required to conform to technology."," Whether an interface has been designed for a digital music player or the weapons control system for a fighter aircraft, usability matters. If interface mechanisms have been well designed, the user glides through the interaction using a smooth rhythm that allows work to be accomplished effortlessly. But if the interface is poorly conceived, the user moves in fits and starts, and the end result is frustration and poor work efficiency. For the first three decades of the computing era, usability was not a dominant concern among those who built software. In his classic book on design, Donald Norman [Nor88] argued that it was time for a change in attitude: To make technology that fits human beings, it is necessary to study human beings. But now we tend to study only the technology. As a result, people are required to conform to technology. It is time to reverse this trend, time to make technology that conforms to people. QUICK LOOK creates an effective communication medium between a human and a computer. Following a set of interface design principles, design identifies interface objects and actions and then creates a screen layout that forms the basis for a user interface prototype"
Explain why the software maintenance tasks that accommodate requests for change involve considerably more complexity than hardware maintenance,"Another aspect of wear illustrates the difference between hardware and software. When a hardware component wears out, it is replaced by a spare part. There are no software spare parts. Every software failure indicates an error in design or in the process through which design was translated into machine executable code. Hence, the software maintenance tasks that accommodate requests for change involve considerably more complexity than hardware maintenance."," failure rate curve to spike as shown in the actual curve (Figure 1.2). Before the curve can return to the original steady-state failure rate, another change is requested, causing the curve to spike again. Slowly, the minimum failure rate level begins to risethe software is deteriorating due to change. Another aspect of wear illustrates the difference between hardware and software. When a hardware component wears out, it is replaced by a spare part. There are no software spare parts. Every software failure indicates an error in design or in the process through which design was translated into machine executable code. Therefore, the software maintenance tasks that accommodate requests for change involve considerably more complexity than hardware maintenance. 3. Although the industry is moving toward component-based construction, most software continues to be custom built"
"Explain why it makes sense to track progress on a daily basis, looking for problem areas and situations in which scheduled work does not conform to actual work conducted","Track the plan frequently and make adjustments as required. Software projects fall behind schedule one day at a time. Hence, it makes sense to track progress on a daily basis, looking for problem areas and situations in which scheduled work does not conform to actual work conducted."," Principle 8. Define how you intend to ensure quality. The plan should identify how the software team intends to ensure quality. If technical reviews3 are to be conducted, they should be scheduled. If pair programming (Chapter 3) is to be used during construction, it should be explicitly defined within the plan. Principle 9. Describe how you intend to accommodate change. Even the best planning can be obviated by uncontrolled change. You should identify how changes are to be accommodated as software engineering work proceeds. For example, can the customer request a change at any time? If a change is requested, is the team obliged to implement it immediately? How is the impact and cost of the change assessed? Principle 10. Track the plan frequently and make adjustments as required. Software projects fall behind schedule one day at a time. Therefore, it makes sense to track progress on a daily basis, looking for problem areas and situations in which scheduled work does not conform to actual work conducted. When slippage is encountered, the plan is adjusted accordingly. To be most effective, everyone on the software team should participate in the planning activity. Only then will team members sign up to the plan"
Explain why create only those models that make it easier and faster to construct the software,"Every model that is created must be kept up-to-date as changes occur. More importantly, every new model takes time that might otherwise be spent on construction (coding and testing). Hence, create only those models that make it easier and faster to construct the software."," Principle 2. Travel lightdont create more models than you need. Every model that is created must be kept up-to-date as changes occur. More importantly, every new model takes time that might otherwise be spent on construction (coding and testing). Therefore, create only those models that make it easier and faster to construct the software. Principle 3. Strive to produce the simplest model that will describe the problem or the software"
Explain why component coupling should be kept as low as is reasonable,"Components should be loosely coupled to one another and to the external environment. Coupling is achieved in many ways via a component interface, by messaging, through global data. As the level of coupling increases, the likelihood of error propagation also increases and the overall maintainability of the software decreases. Hence, component coupling should be kept as low as is reasonable."," Principle 6. Component-level design should be functionally independent. Functional independence is a measure of the single-mindedness of a software component. The functionality that is delivered by a component Frederick P. Brooks should be cohesivethat is, it should focus on one and only one function or subfunction.5 Principle 7. Components should be loosely coupled to one another and to the external environment. Coupling is achieved in many ways via a component interface, by messaging, through global data. As the level of coupling increases, the likelihood of error propagation also increases and the overall maintainability of the software decreases. Therefore, component coupling should be kept as low as is reasonable. Principle 8. Design representations (models) should be easily understandable"
Explain why all tests can be planned and designed before any code has been generated,"Tests should be planned long before testing begins. Test planning (Chapter 17) can begin as soon as the requirements model is complete. Detailed definition of test cases can begin as soon as the design model has been solidified. Hence, all tests can be planned and designed before any code has been generated."," 11/27/08 CHAPTER 4 PRINCIPLES THAT GUIDE PRACTICE Davis [Dav95b] suggests a set of testing principles6 that have been adapted for use in this book: Principle 1. All tests should be traceable to customer requirements.7 The objective of software testing is to uncover errors. It follows that the most severe defects (from the customers point of view) are those that cause the program to fail to meet its requirements. Principle 2. Tests should be planned long before testing begins. Test planning (Chapter 17) can begin as soon as the requirements model is complete. Detailed definition of test cases can begin as soon as the design model has been solidified. Therefore, all tests can be planned and designed before any code has been generated. Principle 3. The Pareto principle applies to software testing. In this context the Pareto principle implies that 80 percent of all errors uncovered during testing will likely be traceable to 20 percent of all program components. The problem, of course, is to isolate these suspect components and to thoroughly test them"
Explain why the requirements model must provide modeling elements that depict behavior,"The behavior of a computer-based system can have a profound effect on the design that is chosen and the implementation approach that is applied. Hence, the requirements model must provide modeling elements that depict behavior."," More on this in the security function (Figure 5.4). Note that the diagram lists the attributes of sensors (e.g., name, type) and the operations (e.g., identify, enable) that can be applied to modify these attributes. In addition to class diagrams, other analysis modeling elements depict the manner in which classes collaborate with one another and the relationships and interactions between classes. These are discussed in more detail in Chapter 7. Behavioral elements. The behavior of a computer-based system can have a profound effect on the design that is chosen and the implementation approach that is applied. Therefore, the requirements model must provide modeling elements that depict behavior. 17 A brief UML tutorial is presented in Appendix 1 for those who are unfamiliar with the notation"
"Explain why width (a single value) would not be a valid data object, but dimensions (incorporating height, width, and depth) could be defined as an object","A data object is a representation of composite information that must be understood by software. By composite information, I mean something that has a number of different properties or attributes. Hence, width (a single value) would not be a valid data object, but dimensions (incorporating height, width, and depth) could be defined as an object."," If software requirements include the need to create, extend, or interface with a database or if complex data structures must be constructed and manipulated, the software team may choose to create a data model as part of overall requirements modeling. A software engineer or analyst defines all data objects that are processed within the system, the relationships between the data objects, and other information that is pertinent to the relationships. The entity-relationship diagram (ERD) addresses these issues and represents all data objects that are entered, stored, transformed, and produced within an application. ? How does a data object manifest itself within the context of an application? A data object is a representation of composite information that must be understood by software. By composite information, I mean something that has a number of different properties or attributes. Therefore, width (a single value) would not be a valid data object, but dimensions (incorporating height, width, and depth) could be defined as an object. A data object can be an external entity (e.g., anything that produces or consumes information), a thing (e.g., a report or a display), an occurrence (e.g., a telephone call) or event (e.g., an alarm), a role (e.g., salesperson), an organizational unit (e.g., accounting department), a place (e.g., a warehouse), or a structure (e.g., a file). For example, a person or a car can be viewed as a data object in the sense that either can be defined in terms of a set of attributes. The description of the data object A data object is a representation of any composite information that is processed by software"
Explain why a designer should ensure that the specification for the interface is accurate and complete," The design of internal interfaces is closely aligned with component-level design (Chapter 10). Design realizations of analysis classes represent all operations and the messaging schemes required to enable communication and collaboration between operations in various classes. Each message must be designed to accommodate the requisite information transfer and the specific functional requirements of the Interface characteristics can change with time. Hence, a designer should ensure that the specification for the interface is accurate and complete."," The design of external interfaces requires definitive information about the entity to which information is sent or received. In every case, this information should be collected during requirements engineering (Chapter 5) and verified once the interface design commences.8 The design of external interfaces should incorporate error checking and (when necessary) appropriate security features. The design of internal interfaces is closely aligned with component-level design (Chapter 10). Design realizations of analysis classes represent all operations and the messaging schemes required to enable communication and collaboration between operations in various classes. Each message must be designed to accommodate the requisite information transfer and the specific functional requirements of the Interface characteristics can change with time. Therefore, a designer should ensure that the specification for the interface is accurate and complete. 11/27/08 3:39 PM Page 236 operation that has been requested. If the classic input-process-output approach to design is chosen, the interface of each software component is designed based on data flow representations and the functionality described in a processing narrative"
Explain why the architect must develop a description using views of the building that address the owners concerns,"For example, the architect of a major office building must work with a variety of different stakeholders. The primary concern of the owner of the building (one stakeholder) is to ensure that it is aesthetically pleasing and that it provides sufficient office space and infrastructure to ensure its profitability. Hence, the architect must develop a description using views of the building that address the owners concerns."," 9.1.3 Architectural Descriptions Each of us has a mental image of what the word architecture means. In reality, however, it means different things to different people. The implication is that different stakeholders will see an architecture from different viewpoints that are driven by different sets of concerns. This implies that an architectural description is actually a set of work products that reflect different views of the system. For example, the architect of a major office building must work with a variety of different stakeholders. The primary concern of the owner of the building (one stakeholder) is to ensure that it is aesthetically pleasing and that it provides sufficient office space and infrastructure to ensure its profitability. Therefore, the architect must develop a description using views of the building that address the owners concerns. Your effort should focus on architectural representations that will guide all other aspects of design"
Explain why architectural decisions themselves can be considered to be one view of the architecture,"Each view developed as part of an architectural description addresses a specific stakeholder concern. To develop each view (and the architectural description as a whole) the system architect considers a variety of alternatives and ultimately decides on the specific architectural features that best meet the concern. Hence, architectural decisions themselves can be considered to be one view of the architecture."," The IEEE standard defines an architectural description (AD) as a collection of products to document an architecture. The description itself is represented using multiple views, where each view is a representation of a whole system from the perpective of a related set of [stakeholder] concerns. A view is created according to rules and conventions defined in a viewpointa specification of the conventions for constructing and using a view [IEE00]. A number of different work products that are used to develop different views of the software architecture are discussed later in this chapter. 9.1.4 Architectural Decisions Each view developed as part of an architectural description addresses a specific stakeholder concern. To develop each view (and the architectural description as a whole) the system architect considers a variety of alternatives and ultimately decides on the specific architectural features that best meet the concern. Therefore, architectural decisions themselves can be considered to be one view of the architecture. The reasons that decisions were made provide insight into the structure of a system and its conformance to stakeholder concerns"
Explain why data design is an integral part of the derivation of the software architecture,"Software architecture provides a holistic view of the system to be built. It depicts the structure and organization of software components, their properties, and the connections between them. Software components include program modules and the various data representations that are manipulated by the program. Hence, data design is an integral part of the derivation of the software architecture."," It is important to note that structural simplicity often reflects both elegance and efficiency. Design refinement should strive for the smallest number of components that is consistent with effective modularity and the least complex data structure that adequately serves information requirements. 9.7 S U M M A R Y Software architecture provides a holistic view of the system to be built. It depicts the structure and organization of software components, their properties, and the connections between them. Software components include program modules and the various data representations that are manipulated by the program. Therefore, data design is an integral part of the derivation of the software architecture. Architecture highlights early design decisions and provides a mechanism for considering the benefits of alternative system structures. A number of different architectural styles and patterns are available to the software engineer and may be applied within a given architectural genre. Each style describes a system category that encompasses a set of components that perform a function required by a system; a set of connectors that enable communication, coordination, and cooperation among components; constraints that define how components can be integrated to form the system; and semantic models that enable a designer to understand the overall properties of a system"
Explain why coupling is a fact of life," Software must communicate internally and externally. Hence, coupling is a fact of life."," External coupling. Occurs when a component communicates or collaborates with infrastructure components (e.g., operating system functions, database capability, telecommunication functions). Although this type of coupling is necessary, it should be limited to a small number of components or classes within a system. Software must communicate internally and externally. Therefore, coupling is a fact of life. However, the designer should work to reduce coupling whenever possible and understand the ramifications of high coupling when it cannot be avoided. 11/27/08 3:47 PM Page 290 S AFE H OME Coupling in Action The scene: Shakiras cubicle"
Explain why component-level design for WebApps often incorporates elements of content design and functional design,"The boundary between content and function is often blurred when Web-based systems and applications (WebApps) are considered. Therefore, it is reasonable to ask: What is a WebApp component? In the context of this chapter, a WebApp component is (1) a well-defined cohesive function that manipulates content or provides computational or data processing for an end user or (2) a cohesive package of content and functionality that provides the end user with some required capability. Hence, component-level design for WebApps often incorporates elements of content design and functional design."," In addition, you should not suffer from tunnel vision. There are always alternative design solutions, and the best designers consider all (or most) of them before settling on the final design model. Develop alternatives and consider each carefully, using the design principles and concepts presented in Chapter 8 and in this chapter. FOR The boundary between content and function is often blurred when Web-based systems and applications (WebApps) are considered. Therefore, it is reasonable to ask: What is a WebApp component? In the context of this chapter, a WebApp component is (1) a well-defined cohesive function that manipulates content or provides computational or data processing for an end user or (2) a cohesive package of content and functionality that provides the 11/27/08 Page 297 COMPONENT-LEVEL DESIGN end user with some required capability. Therefore, component-level design for WebApps often incorporates elements of content design and functional design. 10.4.1 Content Design at the Component Level Content design at the component level focuses on content objects and the manner in which they may be packaged for presentation to a WebApp end user"
Explain why you should apply proven techniques for translating the abstractions contained in the requirements model into a more concrete form that is the software design,"As you begin your work as a designer, its always important to keep quality attributes in mind. These attributes (e.g., a design must implement all explicit requirements addressed in the requirements model) establish a way to assess software quality but do little to help you actually achieve it. The design you create should exhibit the fundamental design concepts. Hence, you should apply proven techniques for translating the abstractions contained in the requirements model into a more concrete form that is the software design."," The role of pattern-based design in all of this is illustrated in Figure 12.1. A software designer begins with a requirements model (either explicit or implied) that presents an abstract representation of the system. The requirements model describes the problem set, establishes the context, and identifies the system of forces that hold sway. It may imply the design in an abstract manner, but the requirements model does little to represent the design explicitly. As you begin your work as a designer, its always important to keep quality attributes in mind. These attributes (e.g., a design must implement all explicit requirements addressed in the requirements model) establish a way to assess software quality but do little to help you actually achieve it. The design you create should exhibit the fundamental design concepts discussed in Chapter 8. Therefore, you should apply proven techniques for translating the abstractions contained in the requirements model into a more concrete form that is the software design. To accomplish this, youll use the methods and modeling tools available for architectural, componentlevel, and interface design. But only when youre faced with a problem, context, and system of forces that have not been solved before. If a solution already exists, use it! And that means applying a pattern-based design approach. 11/27/08 3:58 PM Page 356 12.2.2 Thinking in Patterns In an excellent book on pattern-based design, Shalloway and Trott [Sha05] comment on a new way of thinking when one uses patterns as part of the design activity: I had to open my mind to a new way of thinking. And when I did so, I heard [Christopher] Alexander say that good software design cannot be achieved simply by adding together performing parts"
Explain why the overall navigation structure for a WebApp may be organized as a hierarchy of NSUs,"An NSU describes the navigation requirements for each use case. In essence, the NSU shows how an actor moves between content objects or WebApp functions. (WoN) [Gna99]. A WoN represents the best navigation pathway to achieve a navigational goal for a specific type of user. Each WoN is organized as a set of navigational nodes (NN) that are connected by navigational links. In some cases, a navigational link may be another NSU. Hence, the overall navigation structure for a WebApp may be organized as a hierarchy of NSUs."," An NSU is composed of a set of navigation elements called ways of navigating An NSU describes the navigation requirements for each use case. In essence, the NSU shows how an actor moves between content objects or WebApp functions. (WoN) [Gna99]. A WoN represents the best navigation pathway to achieve a navigational goal for a specific type of user. Each WoN is organized as a set of navigational nodes (NN) that are connected by navigational links. In some cases, a navigational link may be another NSU. Therefore, the overall navigation structure for a WebApp may be organized as a hierarchy of NSUs. To illustrate the development of an NSU, consider the use case Select SafeHome Components: Use Case: Select SafeHome Components The WebApp will recommend product components (e.g., control panels, sensors, cameras) and other features (e.g., PC-based functionality implemented in software) for each room and exterior entrance. If I request alternatives, the WebApp will provide them, if they exist. I will be able to get descriptive and pricing information for each product component. The WebApp will create and display a bill-of-materials as I select various components. Ill be able to give the bill-of-materials a name and save it for future reference (see use case Save Configuration)"
"Explain why testing and test case design is an admission of failure, which instills a goodly dose of guilt","Theres a myth that if we were really good at programming, there would be no bugs to catch. If only we could really concentrate, if only everyone used structured programming, top-down design, . . . then there would be no bugs. So goes the myth. There are bugs, the myth says, because we are bad at what we do; and if we are bad at it, we should feel guilty about it. Hence, testing and test case design is an admission of failure, which instills a goodly dose of guilt."," A wide variety of information sources on software testing strategies are available on the Internet. An up-to-date list of World Wide Web references that are relevant to software testing strategies can be found at the SEPA website: www.mhhe.com/engcs/compsci/pressman/ professional/olc/ser.htm. 11/27/08 Page 481 TESTING CONVENTIONAL APPLICATIONS KEY CONCEPTS basis path testing . . . . . . . . .485 black-box testing . . . . . . . . .495 boundary value analysis . . . . . . . .498 control structure testing . . . . . . . . .492 cyclomatic complexity . . . . . .488 equivalence partitioning . . . . . .497 esting presents an interesting anomaly for software engineers, who by their nature are constructive people. Testing requires that the developer discard preconceived notions of the correctness of software just developed and then work hard to design test cases to break the software. Beizer [Bei90] describes this situation effectively when he states: Theres a myth that if we were really good at programming, there would be no bugs to catch. If only we could really concentrate, if only everyone used structured programming, top-down design, . . . then there would be no bugs. So goes the myth. There are bugs, the myth says, because we are bad at what we do; and if we are bad at it, we should feel guilty about it. Therefore, testing and test case design is an admission of failure, which instills a goodly dose of guilt. And the tedium of testing is just punish- ment for our errors. Punishment for what? For being human? Guilt for what? For fail- graph-based testing methods . . . . . . . .495 graph matrices . . .491 model-based testing . . . . . . . . .502 orthogonal array testing . . . . . . . . .499 patterns . . . . . . . .507 specialized environments . . . .503 white-box testing . . . . . . . . .485 QUICK LOOK programmer thinks and what he says? For failing to be telepathic? For not solving human communications problems that have been kicked around . . . for forty centuries? Should testing instill guilt? Is testing really destructive? The answer to these questions is No! In this chapter, I discuss techniques for software test-case design for conventional applications. Test-case design focuses on a set of techniques for the creation of test cases that meet overall testing objectives and the testing strategies discussed in Chapter 17. Who does it? During early stages of testing, a been generated, software must be tested to uncover (and correct) as many errors as possible before delivery to your customer. Your goal is to design a series of test cases that have a high likelihood of finding errorsbut how? Thats where software testing techniques enter the picture. These techniques provide systematic guidance for designing tests that (1) exercise the internal logic and interfaces of every software component and (2) exercise the input and output domains of the program to uncover errors in program function, behavior, and performance"
Explain why you have to execute the program before it gets to the customer with the specific intent of finding and removing all errors," Reviews and other SQA actions can and do uncover errors, but they are not sufficient. Every time the program is executed, the customer tests it! Hence, you have to execute the program before it gets to the customer with the specific intent of finding and removing all errors."," software engineer performs all tests. However, as the testing process progresses, testing specialists may become involved. Why is it important? Reviews and other SQA actions can and do uncover errors, but they are not sufficient. Every time the program is executed, the customer tests it! Therefore, you have to execute the program before it gets to the customer with the specific intent of finding and removing all errors. In order to find the highest possible number of errors, tests must be conducted systematically and test cases must be designed using disciplined techniques. 11/27/08 6:12 PM Page 482 What are the steps? For conventional applica- tions, software is tested from two different perspectives: (1) internal program logic is exercised using white box test-case design techniques and (2) software requirements are exercised using black box test-case design techniques"
Explain why testing the handling of these Boolean events is essential,"Software and hardware are integrated, and a full range of system tests are conducted in an attempt to uncover errors at the softwarehardware interface. Most real-time systems process interrupts. Hence, testing the handling of these Boolean events is essential."," 11/27/08 Page 507 TESTING CONVENTIONAL APPLICATIONS Intertask testing. Once errors in individual tasks and in system behavior have been isolated, testing shifts to time-related errors. Asynchronous tasks that are known to communicate with one another are tested with different data rates and processing load to determine if intertask synchronization errors will occur. In addition, tasks that communicate via a message queue or data store are tested to uncover errors in the sizing of these data storage areas. System testing. Software and hardware are integrated, and a full range of system tests are conducted in an attempt to uncover errors at the softwarehardware interface. Most real-time systems process interrupts. Therefore, testing the handling of these Boolean events is essential. Using the state diagram (Chapter 7), the tester develops a list of all possible interrupts and the processing that occurs as a consequence of the interrupts. Tests are then designed to assess the following system characteristics: Are interrupt priorities properly assigned and properly handled? Is processing for each interrupt handled correctly? Does the performance (e.g., processing time) of each interrupt-handling procedure conform to requirements? Does a high volume of interrupts arriving at critical times create problems in function or performance? In addition, global data areas that are used to transfer information as part of interrupt processing should be tested to assess the potential for the generation of side effects. 18.9 P AT T E R N S WebRef A software testing patterns catalog can be found at www.rbsc .com/pages/ TestPatternList.htm"
Explain why tests that uncover errors in communication between the WebApp and the remote database must be developed,"The database may be remote to the server that houses the WebApp. Hence, tests that uncover errors in communication between the WebApp and the remote database must be developed."," The composite content object that presents this information is created dynamically after the user has made a request for information about a specific equity. To accomplish this, the following steps are required: (1) a large equities database is queried, (2) relevant data are extracted from the database, (3) the extracted data must be organized as a content object, and (4) this content object (representing customized information requested by an end user) is transmitted to the client environment for display. Errors can and do occur as a consequence of each of these steps. The objective of database testing is to uncover these errors, but database testing is complicated by a variety of factors: 1. The original client-side request for information is rarely presented in the form What issues complicate database testing for WebApps? [e.g., structured query language (SQL)] that can be input to a database management system (DBMS). Therefore, tests should be designed to uncover errors made in translating the users request into a form that can be processed by the DBMS. 11/28/08 10:22 AM Page 536 2. The database may be remote to the server that houses the WebApp. Therefore, tests that uncover errors in communication between the WebApp and the remote database must be developed.3 3. Raw data acquired from the database must be transmitted to the WebApp server and properly formatted for subsequent transmittal to the client. Therefore, tests that demonstrate the validity of the raw data received by the WebApp server must be developed, and additional tests that demonstrate the validity of the transformations applied to the raw data to create valid content objects must also be created. 4. The dynamic content object(s) must be transmitted to the client in a form that can be displayed to the end user. Therefore, a series of tests must be designed to (1) uncover errors in the content object format and (2) test compatibility with different client environment configurations"
Explain why database testing becomes an integral part of the component-testing regime,"In many situations, the correct execution of a WebApp function is tied to proper interfacing with a database that may be external to the WebApp. Hence, database testing becomes an integral part of the component-testing regime."," Each component-level test case specifies all input values and the expected output to be provided by the component. The actual output produced as a consequence of the test is recorded for future reference during support and maintenance. In many situations, the correct execution of a WebApp function is tied to proper interfacing with a database that may be external to the WebApp. Therefore, database testing becomes an integral part of the component-testing regime. In this case, a better input design might eliminate potential errors. The maximum number of days could be selected from a pull-down menu, precluding the user from specifying out-of-bounds input"
"Explain why in the agile world of WebApps, change is viewed somewhat differently","Subsequent increments add additional content and functionality, and each is likely to implement changes that lead to enhanced content, better usability, improved aesthetics, better navigation, enhanced performance, and stronger security. Hence, in the agile world of WebApps, change is viewed somewhat differently."," Among the many characteristics that differentiate WebApps from traditional software is the ubiquitous nature of change. WebApp developers often use an iterative, incremental process model that applies many principles derived from agile software development (Chapter 3). Using this approach, an engineering team often develops a WebApp increment in a very What impact does uncontrolled change have on a WebApp? short time period using a customer-driven approach. Subsequent increments add additional content and functionality, and each is likely to implement changes that lead to enhanced content, better usability, improved aesthetics, better navigation, enhanced performance, and stronger security. Therefore, in the agile world of WebApps, change is viewed somewhat differently. If youre a member of a WebApp team, you must embrace change. And yet, a typical agile team eschews all things that appear to be process-heavy, bureaucratic, and formal. Software configuration management is often viewed (albeit incorrectly) to have these characteristics. This seeming contradiction is remedied not by rejecting SCM principles, practices, and tools, but rather, by molding them to meet the special needs of WebApp projects"
Explain why the rigor of configuration control mechanisms should be directly proportional to application scale,"Scalability. upward well. It is not uncommon for a simple WebApp to grow significantly as interconnections with existing information systems, databases, data warehouses, and portal gateways are implemented. As size and complexity grow, small changes can have far-reaching and unintended effects that can be problematic. Hence, the rigor of configuration control mechanisms should be directly proportional to application scale."," Scalability. upward well. It is not uncommon for a simple WebApp to grow significantly as interconnections with existing information systems, databases, data warehouses, and portal gateways are implemented. As size and complexity grow, small changes can have far-reaching and unintended effects that can be problematic. Therefore, the rigor of configuration control mechanisms should be directly proportional to application scale. Politics"
Explain why product metrics that provide insight into the quality of the analysis model are desirable,"Technical work in software engineering begins with the creation of the requirements model. It is at this stage that requirements are derived and a foundation for design is established. Hence, product metrics that provide insight into the quality of the analysis model are desirable."," Ed: Im not real familiar with class-oriented metrics. Vinod: Ill spend some time checking them out and make a recommendation . . . okay with you guys? [Ed and Jamie nod without much enthusiasm.] 23.2 M E T R I C S REQUIREMENTS MODEL Technical work in software engineering begins with the creation of the requirements model. It is at this stage that requirements are derived and a foundation for design is established. Therefore, product metrics that provide insight into the quality of the analysis model are desirable. Although relatively few analysis and specification metrics have appeared in the literature, it is possible to adapt metrics that are often used for project estimation and apply them in this context. These metrics examine the requirements model with the intent of predicting the size of the resultant system. Size is sometimes (but not always) an indicator of design complexity and is almost always an indicator of increased coding, integration, and testing effort"
Explain why errors are to be expected as work products are developed,"The overriding goal of software engineering is to produce a high-quality system, application, or product within a time frame that satisfies a market need. To achieve this goal, you must apply effective methods coupled with modern tools within the con- Software is a complex entity. Hence, errors are to be expected as work products are developed."," (www.predicate.com), is a tool suite for management metrics collection and reporting. The overriding goal of software engineering is to produce a high-quality system, application, or product within a time frame that satisfies a market need. To achieve this goal, you must apply effective methods coupled with modern tools within the con- Software is a complex entity. Therefore, errors are to be expected as work products are developed. Process metrics are intended to improve the software process so that errors are uncovered in the most effective manner. text of a mature software process. In addition, a good software engineer (and good software engineering managers) must measure if high quality is to be realized"
Explain why update your estimates as the project progresses,"The objective of software project planning is to provide a framework that enables the manager to make reasonable estimates of resources, cost, and schedule. In addition, The more you know, the better you estimate. Hence, update your estimates as the project progresses."," However, you should not become obsessive about estimation. Modern software engineering approaches (e.g., evolutionary process models) take an iterative view of development. In such approaches, it is possiblealthough not always politically acceptableto revisit the estimate (as more information is known) and revise it when the customer makes changes to requirements. 26.2 T H E P R O J E C T P L A N N I N G P R O C E S S The objective of software project planning is to provide a framework that enables the manager to make reasonable estimates of resources, cost, and schedule. In addition, The more you know, the better you estimate. Therefore, update your estimates as the project progresses. estimates should attempt to define best-case and worst-case scenarios so that project outcomes can be bounded. Although there is an inherent degree of uncertainty, the software team embarks on a plan that has been established as a consequence of these tasks. Therefore, the plan must be adapted and updated as the project proceeds. In the following sections, each of the actions associated with software project planning is discussed"
Explain why the plan must be adapted and updated as the project proceeds,"The more you know, the better you estimate. Therefore, update your estimates as the project progresses. estimates should attempt to define best-case and worst-case scenarios so that project outcomes can be bounded. Although there is an inherent degree of uncertainty, the software team embarks on a plan that has been established as a consequence of these tasks. Hence, the plan must be adapted and updated as the project proceeds."," 26.2 T H E P R O J E C T P L A N N I N G P R O C E S S The objective of software project planning is to provide a framework that enables the manager to make reasonable estimates of resources, cost, and schedule. In addition, The more you know, the better you estimate. Therefore, update your estimates as the project progresses. estimates should attempt to define best-case and worst-case scenarios so that project outcomes can be bounded. Although there is an inherent degree of uncertainty, the software team embarks on a plan that has been established as a consequence of these tasks. Therefore, the plan must be adapted and updated as the project proceeds. In the following sections, each of the actions associated with software project planning is discussed. Size often increases due to scope creep that occurs when problem requirements change. Increases in project size can have a geometric impact on project cost and schedule (Michael Mah, personal communication)"
Explain why modifications required for full-experience components will be relatively low risk," Existing specifications, designs, code, or test data developed for past projects that are similar to the software to be built for the current project. Members of the current software team have had full experience in the application area represented by these components. Hence, modifications required for full-experience components will be relatively low risk."," party or from a past project. COTS (commercial off-the-shelf) components are purchased from a third party, are ready for use on the current project, and have been fully validated. Full-experience components. Existing specifications, designs, code, or test data developed for past projects that are similar to the software to be built for the current project. Members of the current software team have had full experience in the application area represented by these components. Therefore, modifications required for full-experience components will be relatively low risk. Partial-experience components. Existing specifications, designs, code, or test data developed for past projects that are related to the software to be built for the current project but will require substantial modification. Members of the current software team have only limited experience in the application area represented by these components. Therefore, modifications required for partial-experience components have a fair degree of risk"
Explain why modifications required for partial-experience components have a fair degree of risk,"Existing specifications, designs, code, or test data developed for past projects that are related to the software to be built for the current project but will require substantial modification. Members of the current software team have only limited experience in the application area represented by these components. Hence, modifications required for partial-experience components have a fair degree of risk."," Full-experience components. Existing specifications, designs, code, or test data developed for past projects that are similar to the software to be built for the current project. Members of the current software team have had full experience in the application area represented by these components. Therefore, modifications required for full-experience components will be relatively low risk. Partial-experience components. Existing specifications, designs, code, or test data developed for past projects that are related to the software to be built for the current project but will require substantial modification. Members of the current software team have only limited experience in the application area represented by these components. Therefore, modifications required for partial-experience components have a fair degree of risk. New components. Software components must be built by the software team specifically for the needs of the current project"
Explain why you should use  An estimation model reflects the population of projects from which it has been derived,"The empirical data that support most estimation models are derived from a limited sample of projects. For this reason, no estimation model is appropriate for all classes of software and in all development environments. Hence, you should use  An estimation model reflects the population of projects from which it has been derived."," 26.7 E M P I R I C A L E S T I M AT I O N M O D E L S An estimation model for computer software uses empirically derived formulas to predict effort as a function of LOC or FP.11 Values for LOC or FP are estimated using the approach described in Sections 26.6.3 and 26.6.4. But instead of using the tables described in those sections, the resultant values for LOC or FP are plugged into the estimation model. The empirical data that support most estimation models are derived from a limited sample of projects. For this reason, no estimation model is appropriate for all classes of software and in all development environments. Therefore, you should use An estimation model reflects the population of projects from which it has been derived. Therefore, the model is domain sensitive"
"Explain why an effective software process should define a collection of task sets, each designed to meet the needs of different types of projects"," The set of tasks that would be appropriate for a large, complex system would likely be perceived as overkill for a small, relatively simple software product. Hence, an effective software process should define a collection of task sets, each designed to meet the needs of different types of projects."," A FOR THE Regardless of the process model that is chosen, the work that a software team performs is achieved through a set of tasks that enable you to define, develop, and ultimately support computer software. No single task set is appropriate for all projects. The set of tasks that would be appropriate for a large, complex system would likely be perceived as overkill for a small, relatively simple software product. Therefore, an effective software process should define a collection of task sets, each designed to meet the needs of different types of projects. As I noted in Chapter 2, a task set is a collection of software engineering work tasks, milestones, work products, and quality assurance filters that must be accomplished to complete a particular project. The task set must provide enough discipline to achieve high software quality. But, at the same time, it must not burden the project team with unnecessary work"
"Explain how an incremental software paradigm is chosen, and a schedule is derived for each incremental delivery","When faced with severe deadline pressure, experienced project managers sometimes use a project scheduling and control technique called time-boxing. The time-boxing strategy recognizes that the complete product may not be deliverable by the predefined deadline. Hence, an incremental software paradigm is chosen, and a schedule is derived for each incremental delivery."," Control is employed by a software project manager to administer project resources, cope with problems, and direct project staff. If things are going well (i.e., the project is on schedule and within budget, reviews indicate that real progress is being made and milestones are being reached), control is light. But when problems 11/27/08 Page 735 PROJECT SCHEDULING occur, you must exercise control to reconcile them as quickly as possible. After a problem has been diagnosed, additional resources may be focused on the problem area: staff may be redeployed or the project schedule can be redefined. When faced with severe deadline pressure, experienced project managers sometimes use a project scheduling and control technique called time-boxing [Jal04]. The time-boxing strategy recognizes that the complete product may not be deliverable by the predefined deadline. Therefore, an incremental software paradigm (Chapter 2) is chosen, and a schedule is derived for each incremental delivery. The tasks associated with each increment are then time-boxed. This means that the schedule for each task is adjusted by working backward from the delivery date When the defined completion date of a time-boxed task is reached, work ceases for that task and the next task begins"
Explain why reengineering one database schema into another requires an understanding of existing objects and their relationships,"Regardless of its logical organization and physical structure, a database allows the definition of data objects and supports some method for establishing relationships among the objects. Hence, reengineering one database schema into another requires an understanding of existing objects and their relationships."," Internal data structures. Reverse engineering techniques for internal program data focus on the definition of classes of objects. This is accomplished by examining the program code with the intent of grouping related program variables. In many cases, the data organization within the code identifies abstract data types. For example, record structures, files, lists, and other data structures often provide an initial indicator of classes. Database structure. Regardless of its logical organization and physical structure, a database allows the definition of data objects and supports some method for establishing relationships among the objects. Therefore, reengineering one database schema into another requires an understanding of existing objects and their relationships. The following steps [Pre94] may be used to define the existing data model as a precursor to reengineering a new database model: (1) build an initial object model, (2) determine candidate keys (the attributes are examined to determine whether they are used to point to another record or table; those that serve as pointers become candidate keys), (3) refine the tentative classes, (4) define generalizations, and (5) discover associations using techniques that are analogous to the CRC approach. Once information defined in the preceding steps is known, a series of transformations [Pre94] can be applied to map the old database structure into a new database structure"
Explain why the redevelopment of user interfaces has become one of the most common types of reengineering activity,"Reverse Engineering User Interfaces Sophisticated GUIs have become de rigueur for computer-based products and systems of every type. Hence, the redevelopment of user interfaces has become one of the most common types of reengineering activity."," For large systems, reverse engineering is generally accomplished using a semiautomated approach. Automated tools can be used to help you understand the semantics of existing code. The output of this process is then passed to restructuring and forward engineering tools to complete the reengineering process. 29.6.3 Reverse Engineering User Interfaces Sophisticated GUIs have become de rigueur for computer-based products and systems of every type. Therefore, the redevelopment of user interfaces has become one of the most common types of reengineering activity. But before a user interface can be rebuilt, reverse engineering should occur. To fully understand an existing user interface, the structure and behavior of the interface must be specified. Merlo and his colleagues [Mer93] suggest three basic questions that must be answered as reverse engineering of the UI commences: do I ? How understand the workings of an existing user interface? What are the basic actions (e.g., keystrokes and mouse clicks) that the interface must process? What is a compact description of the behavioral response of the system to these actions? What is meant by a replacement, or more precisely, what concept of equivalence of interfaces is relevant here? Behavioral modeling notation (Chapter 7) can provide a means for developing answers to the first two questions. Much of the information necessary to create a behavioral model can be obtained by observing the external manifestation of the existing interface. But additional information necessary to create the behavioral model must be extracted from the code"
"Explain why regardless of the size of the software organization, its reasonable to consider the business motivation for SPI","Within small organizations the implementation of an SPI framework requires resources that may be in short supply. Managers must allocate people and money to make software engineering happen. Hence, regardless of the size of the software organization, its reasonable to consider the business motivation for SPI."," They also tend to pride themselves on the creativity of individual members of the 11/27/08 Page 791 SOFTWARE PROCESS IMPROVEMENT software organization, and initially view an SPI framework as overly bureaucratic and ponderous. Yet, process improvement is as important for a small organization as it is for a large one. Within small organizations the implementation of an SPI framework requires resources that may be in short supply. Managers must allocate people and money to make software engineering happen. Therefore, regardless of the size of the software organization, its reasonable to consider the business motivation for SPI. SPI will be approved and implemented only after its proponents demonstrate financial leverage [Bir98]. Financial leverage is demonstrated by examining technical benefits (e.g., fewer defects delivered to the field, reduced rework, lower maintenance costs, or more rapid time-to-market) and translating them into dollars. In essence, you must show a realistic return on investment (Section 30.7) for SPI costs"
Explain why an incremental migration from one process (that doesnt work as well as desired) to another process is a more effective strategy,"In other cases, changes associated with SPI are relatively minor, representing small, but meaningful modifications to an existing process model. Such changes are often referred to as process migration. Today, many software organizations have a process in place. The problem is that it doesnt work in an effective manner. Hence, an incremental migration from one process (that doesnt work as well as desired) to another process is a more effective strategy."," 30.2.4 Installation/Migration Installation is the first point at which a software organization feels the effects of changes implemented as a consequence of the SPI road map. In some cases, an entirely new process is recommended for an organization. Framework activities, software engineering actions, and individual work tasks must be defined and installed as part of a new software engineering culture. Such changes represent a substantial organizational and technological transition and must be managed very carefully. In other cases, changes associated with SPI are relatively minor, representing small, but meaningful modifications to an existing process model. Such changes are often referred to as process migration. Today, many software organizations have a process in place. The problem is that it doesnt work in an effective manner. Therefore, an incremental migration from one process (that doesnt work as well as desired) to another process is a more effective strategy. Installation and migration are actually software process redesign (SPR) activities"
Explain why the P-com software must be designed so that it can adapt to the requirements that emerge as some new amI systems go online," New amI-capable systems will be added to the network constantly, each providing useful capabilities and demanding access to your P-com. Hence, the P-com software must be designed so that it can adapt to the requirements that emerge as some new amI systems go online."," It should be obvious that significant privacy and security issues come into play. A trust management system [Duc01] will be an integral part of amI and will manage privileges that enable communication with networks, health, entertainment, financial, employment, and personal systems. New amI-capable systems will be added to the network constantly, each providing useful capabilities and demanding access to your P-com. Therefore, the P-com software must be designed so that it can adapt to the requirements that emerge as some new amI systems go online. There are many ways to accomplish this, but the bottom line is this: the P-com software must be flexible and robust in ways that conventional software cant match. A worthwhile and quite detailed introduction to ambient intelligence can be found at www.emergingcommunication.com/volume6.html. More information can be obtained at www.ambientintelligence.org/"
Explain why the omission of a particular feature does not mean that the feature is absent; it may mean that the feature was suppressed,"UML language provides these (sometimes arcane) options so that you can express all the important aspects of a system. At the same time, you have the flexibility to suppress those parts of the diagram that are not relevant to the aspect being modeled in order to avoid cluttering the diagram with irrelevant details. Hence, the omission of a particular feature does not mean that the feature is absent; it may mean that the feature was suppressed."," UML 2.0 provides 13 different diagrams for use in software modeling. In this appendix, I will discuss only class, deployment, use case, sequence, communication, activity, and state diagrams. These diagrams are used in this edition of Software Engineering: A Practitioners Approach. You should note that there are many optional features in UML diagrams. The UML language provides these (sometimes arcane) options so that you can express all the important aspects of a system. At the same time, you have the flexibility to suppress those parts of the diagram that are not relevant to the aspect being modeled in order to avoid cluttering the diagram with irrelevant details. Therefore, the omission of a particular feature does not mean that the feature is absent; it may mean that the feature was suppressed. In this appendix, exhaustive coverage of all the features of the UML diagrams is not presented. Instead, I will focus on the standard options, especially those options that have been used in this book"
Explain why it would be appropriate to list the current time as an attribute of that class of objects,"For example, an object might always know the current time and be able to return it to you whenever you ask. Hence, it would be appropriate to list the current time as an attribute of that class of objects."," The main elements of a class diagram are boxes, which are the icons used to represent classes and interfaces. Each box is divided into horizontal parts. The top part contains the name of the class. The middle section lists the attributes of the class. An attribute refers to something that an object of that class knows or can provide all the time. Attributes are usually implemented as fields of the class, but they need not be. They could be values that the class can compute from its instance variables or values that the class can get from other objects of which it is composed. For example, an object might always know the current time and be able to return it to you whenever you ask. Therefore, it would be appropriate to list the current time as an attribute of that class of objects. However, the object would most likely not have that time stored in one of its instance variables, because it would need to continually update that field. Instead, the object would likely compute the current time (e.g., through consultation with objects of other classes) at the moment when the time is requested. The third section of the class diagram contains the operations or behaviors of the class. An operation refers to what objects of the class can do. It is usually implemented as a method of the class. thoroughbred horses. It has three attributes displayedmother, father, and birthyear"
Explain how the class encapsulates data (inside the wall) and the processing that manipulates the data (the methods that make up the wall),"A class is an OO concept that encapsulates the data and procedural abstractions required to describe the content and behavior of some real-world entity. Data abstractions that describe the class are enclosed by a wall of procedural abstractions that are capable of manipulating the data in some way. In a well-designed class, the only way to reach the attributes (and operate on them) is to go through one of the methods that form the wall illustrated in the figure. Hence, the class encapsulates data (inside the wall) and the processing that manipulates the data (the methods that make up the wall)."," 11/27/08 6:45 PM Page 864 Now that I have introduced a few basic concepts, a more formal definition of object oriented will prove more meaningful. Coad and Yourdon [Coa91] define the term this way: Object oriented  objects  classification  inheritance  communication Three of these concepts have already been introduced. Communication is discussed later in this appendix. AND A class is an OO concept that encapsulates the data and procedural abstractions required to describe the content and behavior of some real-world entity. Data abstractions that describe the class are enclosed by a wall of procedural abstractions [Tay90] (represented in Figure A2.1) that are capable of manipulating the data in some way. In a well-designed class, the only way to reach the attributes (and operate on them) is to go through one of the methods that form the wall illustrated in the figure. Therefore, the class encapsulates data (inside the wall) and the processing that manipulates the data (the methods that make up the wall). This achieves information hiding (Chapter 8) and reduces the impact of side effects associated with change. Since the methods tend to manipulate a limited number of attributes, their cohesion is improved, and because communication occurs only through the methods that make up the wall, the class tends to be less strongly coupled from other elements of a system.1 FIGURE A2.1 A schematic representation of a class Method1() Method2() Attributes It should be noted, however, that coupling can become a serious problem in OO systems. It arises when classes from various parts of the system are used as the data types of attributes, and arguments to methods. Even though access to the objects may only be through procedure calls, this does not mean that coupling is necessarily low, just lower than if direct access to the internals of objects were allowed. 11/27/08 Page 865 Stated another way, a class is a generalized description (e.g., a template or blueprint) that describes a collection of similar objects. By definition, objects are instances of a specific class and inherit its attributes and the operations that are available to manipulate the attributes. A superclass (often called a base class) is a generalization of a set of classes that are related to it. A subclass is a specialization of the superclass. For example, the superclass MotorVehicle is a generalization of the classes Truck, SUV, Automobile, and Van. The subclass Automobile inherits all attributes of MotorVehicle, but in addition, incorporates additional attributes that are specific only to automobiles"
Explain how the class hierarchy becomes a mechanism through which changes (at high levels) can be immediately propagated through a system," Any change to the attributes or operations contained within a superclass is immediately inherited by all subclasses. Hence, the class hierarchy becomes a mechanism through which changes (at high levels) can be immediately propagated through a system."," Inheritance. Inheritance is one of the key differentiators between conventional and object-oriented systems. A subclass Y inherits all of the attributes and operations associated with its superclass X. This means that all data structures and algorithms originally designed and implemented for X are immediately available for Yno further work need be done. Reuse has been accomplished directly. Any change to the attributes or operations contained within a superclass is immediately inherited by all subclasses. Therefore, the class hierarchy becomes a mechanism through which changes (at high levels) can be immediately propagated through a system. It is important to note that at each level of the class hierarchy new attributes and operations may be added to those that have been inherited from higher levels in the hierarchy. In fact, whenever a new class is to be created, you have a number of options: The class can be designed and built from scratch. That is, inheritance is not used"
Explain why programs written using these languages tended to produce spaghetti codea mass of tangled jumps and conditional branches that make a program virtually impossible to understand,"COBOL, and FORTRAN were not designed around structured principles. Instead, they relied upon the GOTO as a primary means of program control. Hence, programs written using these languages tended to produce spaghetti codea mass of tangled jumps and conditional branches that make a program virtually impossible to understand."," 01-ch01.indd 3 CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 The Java Language The Birth of Modern Programming: C The C language shook the computer world. Its impact should not be underestimated, because it fundamentally changed the way programming was approached and thought about. The creation of C was a direct result of the need for a structured, efficient, high-level language that could replace assembly code when creating systems programs. As you may know, when a computer language is designed, trade-offs are often made, such as the following: Ease-of-use versus power Safety versus efficiency Rigidity versus extensibility Prior to C, programmers usually had to choose between languages that optimized one set of traits or the other. For example, although FORTRAN could be used to write fairly efficient programs for scientific applications, it was not very good for system code. And while BASIC was easy to learn, it wasnt very powerful, and its lack of structure made its usefulness questionable for large programs. Assembly language can be used to produce highly efficient programs, but it is not easy to learn or use effectively. Further, debugging assembly code can be quite difficult. Another compounding problem was that early computer languages such as BASIC, COBOL, and FORTRAN were not designed around structured principles. Instead, they relied upon the GOTO as a primary means of program control. As a result, programs written using these languages tended to produce spaghetti codea mass of tangled jumps and conditional branches that make a program virtually impossible to understand. While languages like Pascal are structured, they were not designed for efficiency, and failed to include certain features necessary to make them applicable to a wide range of programs. (Specifically, given the standard dialects of Pascal available at the time, it was not practical to consider using Pascal for systems-level code.) So, just prior to the invention of C, no one language had reconciled the conflicting attributes that had dogged earlier efforts. Yet the need for such a language was pressing. By the early 1970s, the computer revolution was beginning to take hold, and the demand for software was rapidly outpacing programmers ability to produce it. A great deal of effort was being expended in academic circles in an attempt to create a better computer language. But, and perhaps most importantly, a secondary force was beginning to be felt. Computer hardware was finally becoming common enough that a critical mass was being reached. No longer were computers kept behind locked doors. For the first time, programmers were gaining virtually unlimited access to their machines. This allowed the freedom to experiment. It also allowed programmers to begin to create their own tools. On the eve of Cs creation, the stage was set for a quantum leap forward in computer languages"
Explain why today there are fewer cases in which explicit type arguments are needed," Although type inference will be sufficient for most generic method calls, you can explicitly specify the type argument if needed. For example, here is how the first call to isIn( ) looks when the type arguments are specified: GenMethDemo.<Integer, Integer>isIn(2, nums) Of course, in this case, there is nothing gained by specifying the type arguments. Furthermore, JDK 8 improved type inference as it relates to methods. Hence, today there are fewer cases in which explicit type arguments are needed."," Now, notice how isIn( ) is called within main( ) by use of the normal call syntax, without the need to specify type arguments. This is because the types of the arguments are automatically discerned, and the types of T and V are adjusted accordingly. For example, in the first call: if(isIn(2, nums)) the type of the first argument is Integer (due to autoboxing), which causes Integer to be substituted for T. The base type of the second argument is also Integer, which makes Integer a substitute for V, too. In the second call, String types are used, and the types of T and V are replaced by String. Although type inference will be sufficient for most generic method calls, you can explicitly specify the type argument if needed. For example, here is how the first call to isIn( ) looks when the type arguments are specified: GenMethDemo.<Integer, Integer>isIn(2, nums) Of course, in this case, there is nothing gained by specifying the type arguments. Furthermore, JDK 8 improved type inference as it relates to methods. As a result, today there are fewer cases in which explicit type arguments are needed. Now, notice the commented-out code, shown here: // // if(isIn(""two"", nums)) System.out.println(""two is in strs""); If you remove the comments and then try to compile the program, you will receive an error"
"Explain why today, an interface method is abstract only if it does not specify an implementation","As stated, a functional interface is an interface that specifies only one abstract method. If you have been programming in Java for some time, you might at first think that all interface methods are implicitly abstract. Although this was true prior to JDK 8, the situation has changed. As explained in Chapter 9, beginning with JDK 8, it is possible to specify a default implementation for a method declared in an interface. Private and static interface methods also supply an implementation. Hence, today, an interface method is abstract only if it does not specify an implementation."," When a lambda expression requires a parameter, it is specified in the parameter list on the left side of the lambda operator. Here is a simple example: (n) -> (n % 2)==0 21/09/21 5:47 PM Lambda Expressions This lambda expression returns true if the value of parameter n is even. Although it is possible to explicitly specify the type of a parameter, such as n in this case, often you wont need to do so because in many cases its type can be inferred. Like a named method, a lambda expression can specify as many parameters as needed. Part I As stated, a functional interface is an interface that specifies only one abstract method. If you have been programming in Java for some time, you might at first think that all interface methods are implicitly abstract. Although this was true prior to JDK 8, the situation has changed. As explained in Chapter 9, beginning with JDK 8, it is possible to specify a default implementation for a method declared in an interface. Private and static interface methods also supply an implementation. As a result, today, an interface method is abstract only if it does not specify an implementation. Because non-default, non-static, non-private interface methods are implicitly abstract, there is no need to use the abstract modifier (although you can specify it, if you like). Here is an example of a functional interface: interface MyNumber { double getValue(); } In this case, the method getValue( ) is implicitly abstract, and it is the only method defined by MyNumber. Thus, MyNumber is a functional interface, and its function is defined by getValue( )"
Explain why a lambda expression can be specified only in a context in which a target type is defined,"As mentioned earlier, a lambda expression is not executed on its own. Rather, it forms the implementation of the abstract method defined by the functional interface that specifies its target type. Hence, a lambda expression can be specified only in a context in which a target type is defined."," Here is an example of a functional interface: interface MyNumber { double getValue(); } In this case, the method getValue( ) is implicitly abstract, and it is the only method defined by MyNumber. Thus, MyNumber is a functional interface, and its function is defined by getValue( ). As mentioned earlier, a lambda expression is not executed on its own. Rather, it forms the implementation of the abstract method defined by the functional interface that specifies its target type. As a result, a lambda expression can be specified only in a context in which a target type is defined. One of these contexts is created when a lambda expression is assigned to a functional interface reference. Other target type contexts include variable initialization, return statements, and method arguments, to name a few. Lets work through an example that shows how a lambda expression can be used in an assignment context. First, a reference to the functional interface MyNumber is declared: // Create a reference to a MyNumber instance"
Explain why the JDK and the run-time system were substantially upgraded to support modules,"Support for modules is provided both by language elements, including several keywords, and by enhancements to javac, java, and other JDK tools. Furthermore, new tools and file formats were introduced. Hence, the JDK and the run-time system were substantially upgraded to support modules."," As a general rule, modules are most helpful to large applications because they help reduce the management complexity often associated with a large software system. However, small programs also benefit from modules because the Java API library has now been organized into modules. Thus, it is now possible to specify which parts of the API are required by your program and which are not. This makes it possible to deploy programs with a smaller run-time footprint, which is especially important when creating code for small devices, such as those intended to be part of the Internet of Things (IoT). Support for modules is provided both by language elements, including several keywords, and by enhancements to javac, java, and other JDK tools. Furthermore, new tools and file formats were introduced. As a result, the JDK and the run-time system were substantially upgraded to support modules. In short, modules constitute a major addition to, and evolution of, the Java language. Module Basics In its most fundamental sense, a module is a grouping of packages and resources that can be collectively referred to by the modules name. A module declaration specifies the name of a module and defines the relationship a module and its packages have to other modules"
Explain why the program is compiled and run in the same way it was prior to the advent of modules,"The second key feature that supports legacy code is the automatic use of the class path, rather than the module path. When you compile a program that does not use modules, the class path mechanism is employed, just as it has been since Javas original release. Hence, the program is compiled and run in the same way it was prior to the advent of modules."," When you use code that is not part of a named module, it automatically becomes part of the unnamed module. The unnamed module has two important attributes. First, all of the packages in the unnamed module are automatically exported. Second, the unnamed module can access any and all other modules. Thus, when a program does not use modules, all API modules in the Java platform are automatically accessible through the unnamed module. The second key feature that supports legacy code is the automatic use of the class path, rather than the module path. When you compile a program that does not use modules, the class path mechanism is employed, just as it has been since Javas original release. As a result, the program is compiled and run in the same way it was prior to the advent of modules. Because of the unnamed module and the automatic use of the class path, there was no need to declare any modules for the sample programs shown elsewhere in this book. They run properly whether you compile them with a modern compiler or an earlier one, such as JDK 8. Thus, even though modules are a feature that has significant impact on Java, compatibility with legacy code is maintained. This approach also provides a smooth, nonintrusive, nondisruptive transition path to modules. Thus, it enables you to move a legacy application to modules at your own pace. Furthermore, it allows you to avoid the use of modules when they are not needed"
Explain how records make it much easier to work with groups of data,"Beginning with JDK 16, Java supports a special-purpose class called a record. A record is designed to provide an efficient, easy-to-use way to hold a group of values. For example, you might use a record to hold a set of coordinates; bank account numbers and balances; the length, width, and height of a shipping container; and so on. Because it holds a group of values, a record is commonly referred to as an aggregate type. However, the record is more than simply a means of grouping data, because records also have some of the capabilities of a class. In addition, a record has unique features that simplify its declaration and streamline access to its values. Hence, records make it much easier to work with groups of data."," One final point before leaving the topic of text blocks: Because text blocks provide a better, easier way to enter many types of strings into your program, it is anticipated that they will be widely used by the Java programmer community. It is likely that you will begin to encounter them in code that you work on, or use them in code that you create. Just remember, your JDK release must be at least 15 or later. 21/09/21 5:48 PM 464PART I Records Beginning with JDK 16, Java supports a special-purpose class called a record. A record is designed to provide an efficient, easy-to-use way to hold a group of values. For example, you might use a record to hold a set of coordinates; bank account numbers and balances; the length, width, and height of a shipping container; and so on. Because it holds a group of values, a record is commonly referred to as an aggregate type. However, the record is more than simply a means of grouping data, because records also have some of the capabilities of a class. In addition, a record has unique features that simplify its declaration and streamline access to its values. As a result, records make it much easier to work with groups of data. Records are supported by the new context-sensitive keyword record"
Explain why frequent checks for a null value were necessary to avoid generating an exception," However, this can lead to null pointer exceptions if an attempt is made to dereference a null reference. Hence, frequent checks for a null value were necessary to avoid generating an exception."," In the past, you would normally use the value null to indicate that no value is present. However, this can lead to null pointer exceptions if an attempt is made to dereference a null reference. As a result, frequent checks for a null value were necessary to avoid generating an exception. These classes provide a better way to handle such situations. One other point: These classes are value-based. (See Chapter 13 for a description of value-based classes.) The first and most general of these classes is Optional. For this reason, it is the primary focus of this discussion. It is shown here: class Optional<T> Here, T specifies the type of value stored. It is important to understand that an Optional instance can either contain a value of type T or be empty. In other words, an Optional object does not necessarily contain a value. Optional does not define any constructors, but it does define several methods that let you work with Optional objects. For example, you can determine if a value is present, obtain the value if it is present, obtain a default value when no value is present, and construct an Optional value. The Optional methods are shown in Table 21-3. Method static <T> Optional<T> empty( ) boolean equals(Object optional) Returns true if the invoking object equals optional"
Explain why a solid knowledge of the AWT is still required to use Swing effectively,"Instead, an understanding of the AWT is still important because the AWT underpins Swing, with many AWT classes being used either directly or indirectly by Swing. Hence, a solid knowledge of the AWT is still required to use Swing effectively."," To understand why, consider the following. At the time of this writing, the framework that is most widely used is Swing. Because Swing provides a richer, more flexible GUI framework than does the AWT, it is easy to jump to the conclusion that the AWT is no longer relevantthat it has been fully superseded by Swing. This assumption is, however, false. Instead, an understanding of the AWT is still important because the AWT underpins Swing, with many AWT classes being used either directly or indirectly by Swing. As a result, a solid knowledge of the AWT is still required to use Swing effectively. Also, for some types of small programs that make only minimal use of a GUI, using the AWT may still be appropriate. Therefore, even though the AWT constitutes Javas oldest GUI framework, a basic working knowledge of its fundamentals is still important today. One last point before beginning: The AWT is quite large and a full description would easily fill an entire book. Therefore, it is not possible to describe in detail every AWT class, method, or instance variable. However, this and the following chapters explain the basic 26-ch26.indd 851 CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 The Java Library techniques needed to use the AWT. From there, you will be able to explore other parts of the AWT on your own. You will also be ready to move on to Swing"
Explain why it is the GUI that has been widely used by Java programmers for more than two decades,"Although the AWT is still a crucial part of Java, its component set is no longer widely used to create graphical user interfaces. Today, programmers typically use Swing for this purpose. Swing is a framework that provides more powerful and flexible GUI components than does the AWT. Hence, it is the GUI that has been widely used by Java programmers for more than two decades."," Duration encapsulates a length of time. Period encapsulates a length of date. You will also want to explore the new InstantSource interface added by JDK 17, which is implemented by the Clock class. Part II 21/09/21 5:57 PM This page intentionally left blank CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 / blind folio: 1059 III Introducing GUI Programming with Swing Introducing Swing Exploring Swing Introducing Swing Menus 22/09/21 6:42 PM This page intentionally left blank CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 In Part II, you saw how to build very simple user interfaces with the AWT classes. Although the AWT is still a crucial part of Java, its component set is no longer widely used to create graphical user interfaces. Today, programmers typically use Swing for this purpose. Swing is a framework that provides more powerful and flexible GUI components than does the AWT. As a result, it is the GUI that has been widely used by Java programmers for more than two decades. Coverage of Swing is divided between three chapters. This chapter introduces Swing. It begins by describing Swings core concepts. It then presents a simple example that shows the general form of a Swing program. This is followed by an example that uses event handling. The chapter concludes by explaining how painting is accomplished in Swing. The next chapter presents several commonly used Swing components. The third chapter introduces Swing-based menus. It is important to understand that the number of classes and interfaces in the Swing packages is quite large, and they cant all be covered in this book. (In fact, full coverage of Swing requires an entire book of its own.) However, these three chapters will give you a basic understanding of this important topic"
Explain why each component will work in a consistent manner across all platforms,"With very few exceptions, Swing components are lightweight. This means that they are written entirely in Java and do not map directly to platform-specific peers. Thus, lightweight components are more efficient and more flexible. Furthermore, because lightweight components do not translate into native peers, the look and feel of each component is determined by Swing, not by the underlying operating system. Hence, each component will work in a consistent manner across all platforms."," More than anything else, it is these two features that define the essence of Swing. Each is examined here. Swing Components Are Lightweight With very few exceptions, Swing components are lightweight. This means that they are written entirely in Java and do not map directly to platform-specific peers. Thus, lightweight components are more efficient and more flexible. Furthermore, because lightweight components do not translate into native peers, the look and feel of each component is determined by Swing, not by the underlying operating system. As a result, each component will work in a consistent manner across all platforms. Swing Supports a Pluggable Look and Feel Swing supports a pluggable look and feel (PLAF). Because each Swing component is rendered by Java code rather than by native peers, the look and feel of a component is under the control of Swing. This fact means that it is possible to separate the look and feel of a component from the logic of the component, and this is what Swing does. Separating out the look and feel provides a significant advantage: it becomes possible to change the way that a 22/09/21 6:42 PM component is rendered without affecting any of its other aspects. In other words, it is possible to plug in a new look and feel for any given component without creating any side effects in the code that uses that component. Moreover, it becomes possible to define entire sets of look-and-feels that represent different GUI styles. To use a specific style, its look and feel is simply plugged in. Once this is done, all components are automatically rendered using that style"
"Explain how by using the Fork/Join Framework, you can easily create scaleable applications that automatically take advantage of the processors available in the execution environment","The Fork/Join Framework provides important support for parallel programming. Parallel programming is the name commonly given to the techniques that make effective use of computers that contain more than one processor, including multicore systems. The advantage that multicore environments offer is the prospect of significantly increased program performance. The Fork/Join Framework addressed parallel programming by: Simplifying the creation and use of tasks that can execute concurrently Automatically making use of multiple processors.Hence, by using the Fork/Join Framework, you can easily create scaleable applications that automatically take advantage of the processors available in the execution environment."," Java SE 7 made several additions to the Java API library. Two of the most important were the enhancements to the NIO Framework and the addition of the Fork/Join Framework. NIO (which originally stood for New I/O) was added to Java in version 1.4. However, the changes added by Java SE 7 fundamentally expanded its capabilities. So significant were the changes, that the term NIO.2 is often used. The Fork/Join Framework provides important support for parallel programming. Parallel programming is the name commonly given to the techniques that make effective use of computers that contain more than one processor, including multicore systems. The advantage that multicore environments offer is the prospect of significantly increased program performance. The Fork/Join Framework addressed parallel programming by: Simplifying the creation and use of tasks that can execute concurrently Automatically making use of multiple processors 21/09/21 5:35 PM 18PART I Therefore, by using the Fork/Join Framework, you can easily create scaleable applications that automatically take advantage of the processors available in the execution environment. Of course, not all algorithms lend themselves to parallelization, but for those that do, a significant improvement in execution speed can be obtained"
Explain why any other code that is not a member of the class cannot access a private method or variable,"Since the purpose of a class is to encapsulate complexity, there are mechanisms for hiding the complexity of the implementation inside the class. Each method or variable in a class may be marked private or public. The public interface of a class represents everything that external users of the class need to know, or may know. The private methods and data can only be accessed by code that is a member of the class. Hence, any other code that is not a member of the class cannot access a private method or variable."," Collectively, these elements are called members of the class. Specifically, the data defined by the class are referred to as member variables or instance variables. The code that operates on that data is referred to as member methods or just methods. (If you are familiar with C/C++, it may help to know that what a Java programmer calls a method, a C/C++ programmer calls a function.) In properly written Java programs, the methods define how the member variables can be used. This means that the behavior and interface of a class are defined by the methods that operate on its instance data. Since the purpose of a class is to encapsulate complexity, there are mechanisms for hiding the complexity of the implementation inside the class. Each method or variable in a class may be marked private or public. The public interface of a class represents everything that external users of the class need to know, or may know. The private methods and data can only be accessed by code that is a member of the class. Therefore, any other code that is not a member of the class cannot access a private method or variable. Since the private members of a class may only be accessed by other parts of your program through the class public methods, you can ensure that no improper actions take place. Of course, this means that the public interface should be carefully designed not to expose too much of the inner workings of a class (see Figure 2-1). Inheritance is the process by which one object acquires the properties of another object. This is important because it supports the concept of hierarchical classification. As mentioned earlier, most knowledge is made manageable by hierarchical (that is, top-down) classifications. For example, a Golden Retriever is part of the classification dog, which in turn is part of the mammal class, which is under the larger class animal. Without the use of hierarchies, each object would need to define all of its characteristics explicitly. However, by use of inheritance, an object need only define those qualities that make it unique within its class. It can inherit its general attributes from its parent. Thus, it is the inheritance mechanism that makes it possible for one object to be a specific instance of a more general case. Lets take a closer look at this process"
Explain why int is often the best choice when an integer is needed,"The most commonly used integer type is int. It is a signed 32-bit type that has a range from 2,147,483,648 to 2,147,483,647. In addition to other uses, variables of type int are commonly employed to control loops and to index arrays. Although you might think that using a byte or short would be more efficient than using an int in situations in which the larger range of an int is not needed, this may not be the case. The reason is that when byte and short values are used in an expression, they are promoted to int when the expression is evaluated. (Type promotion is described later in this chapter.) Hence, int is often the best choice when an integer is needed."," The width of an integer type should not be thought of as the amount of storage it consumes, but rather as the behavior it defines for variables and expressions of that type. The Java run-time environment is free to use whatever size it wants, as long as the types behave as you declared them. The width and ranges of these integer types vary widely, as shown in this table: Name Range 32 int short byte byte The smallest integer type is byte. This is a signed 8-bit type that has a range from 128 to 127. Variables of type byte are especially useful when youre working with a stream of data from a network or file. They are also useful when youre working with raw binary data that may not be directly compatible with Javas other built-in types. 22/09/21 6:22 PM Byte variables are declared by use of the byte keyword. For example, the following declares two byte variables called b and c: byte b, c; short short is a signed 16-bit type. It has a range from 32,768 to 32,767. It is probably the leastused Java type. Here are some examples of short variable declarations: short s; short t; int The most commonly used integer type is int. It is a signed 32-bit type that has a range from 2,147,483,648 to 2,147,483,647. In addition to other uses, variables of type int are commonly employed to control loops and to index arrays. Although you might think that using a byte or short would be more efficient than using an int in situations in which the larger range of an int is not needed, this may not be the case. The reason is that when byte and short values are used in an expression, they are promoted to int when the expression is evaluated. (Type promotion is described later in this chapter.) Therefore, int is often the best choice when an integer is needed. long long is a signed 64-bit type and is useful for those occasions where an int type is not large enough to hold the desired value. The range of a long is quite large. This makes it useful when big, whole numbers are needed. For example, here is a program that computes the number of miles that light will travel in a specified number of days: // Compute distance light travels using long variables"
Explain why variables declared within a method will not hold their values between calls to that method,"Here is another important point to remember: variables are created when their scope is entered, and destroyed when their scope is left. This means that a variable will not hold its value once it has gone out of scope. Hence, variables declared within a method will not hold their values between calls to that method."," System.out.println(""x is "" + x); } } As the comments indicate, the variable x is declared at the start of main( )s scope and is accessible to all subsequent code within main( ). Within the if block, y is declared. Since a 22/09/21 6:22 PM block defines a scope, y is only visible to other code within its block. This is why outside of its block, the line y = 100; is commented out. If you remove the leading comment symbol, a compile-time error will occur, because y is not visible outside of its block. Within the if block, x can be used because code within a block (that is, a nested scope) has access to variables declared by an enclosing scope. Within a block, variables can be declared at any point, but are valid only after they are declared. Thus, if you define a variable at the start of a method, it is available to all of the code within that method. Conversely, if you declare a variable at the end of a block, it is effectively useless, because no code will have access to it. For example, this fragment is invalid because count cannot be used prior to its declaration: // This fragment is wrong! count = 100; // oops! cannot use count before it is declared! int count; Here is another important point to remember: variables are created when their scope is entered, and destroyed when their scope is left. This means that a variable will not hold its value once it has gone out of scope. Therefore, variables declared within a method will not hold their values between calls to that method. Also, a variable declared within a block will lose its value when the block is left. Thus, the lifetime of a variable is confined to its scope. If a variable declaration includes an initializer, then that variable will be reinitialized each time the block in which it is declared is entered. For example, consider the next program: // Demonstrate lifetime of a variable"
"Explain why to test for zero or nonzero, you must explicitly employ one or more of the relational operators","In C/C++, true is any nonzero value and false is zero. In Java, true and false are nonnumeric values that do not relate to zero or nonzero. Hence, to test for zero or nonzero, you must explicitly employ one or more of the relational operators."," The reason is that Java does not define true and false in the same way as C/C++. In C/C++, true is any nonzero value and false is zero. In Java, true and false are nonnumeric values that do not relate to zero or nonzero. Therefore, to test for zero or nonzero, you must explicitly employ one or more of the relational operators. Boolean Logical Operators The Boolean logical operators shown here operate only on boolean operands. All of the binary logical operators combine two boolean values to form a resultant boolean value"
"Explain why if you need to select among a large group of values, a switch statement will run much faster than the equivalent logic coded using a sequence of if-elses","When it compiles a switch statement, the Java compiler will inspect each of the case constants and create a jump table that it will use for selecting the path of execution depending on the value of the expression. Hence, if you need to select among a large group of values, a switch statement will run much faster than the equivalent logic coded using a sequence of if-elses."," 21/09/21 5:40 PM No two case constants in the same switch can have identical values. Of course, a A switch statement is usually more efficient than a set of nested ifs. The last point is particularly interesting because it gives insight into how the Java compiler works. When it compiles a switch statement, the Java compiler will inspect each of the case constants and create a jump table that it will use for selecting the path of execution depending on the value of the expression. Therefore, if you need to select among a large group of values, a switch statement will run much faster than the equivalent logic coded using a sequence of if-elses. The compiler can do this because it knows that the case constants are all the same type and simply must be compared for equality with the switch expression. The compiler has no such knowledge of a long list of if expressions. switch statement and an enclosing outer switch can have case constants in common"
Explain why changes made to the parameter of the subroutine have no effect on the argument,"This approach copies the value of an argument into the formal parameter of the subroutine. Hence, changes made to the parameter of the subroutine have no effect on the argument."," A Closer Look at Argument Passing In general, there are two ways that a computer language can pass an argument to a subroutine. The first way is call-by-value. This approach copies the value of an argument into the formal parameter of the subroutine. Therefore, changes made to the parameter of the subroutine have no effect on the argument. The second way an argument can be passed is call-byreference. In this approach, a reference to an argument (not the value of the argument) is passed to the parameter. Inside the subroutine, this reference is used to access the actual argument specified in the call. This means that changes made to the parameter will affect the argument used to call the subroutine. As you will see, although Java uses call-by-value to pass all arguments, the precise effect differs between whether a primitive type or a reference type is passed. When you pass a primitive type to a method, it is passed by value. Thus, a copy of the argument is made, and what occurs to the parameter that receives the argument has no effect outside the method. For example, consider the following program: // Primitive types are passed by value"
Explain why a subclass is a specialized version of a superclass,"Inheritance is one of the cornerstones of object-oriented programming because it allows the creation of hierarchical classifications. Using inheritance, you can create a general class that defines traits common to a set of related items. This class can then be inherited by other, more specific classes, each adding those things that are unique to it. In the terminology of Java, a class that is inherited is called a superclass. The class that does the inheriting is called a subclass. Hence, a subclass is a specialized version of a superclass."," As explained earlier in this book, for the benefit of readers working in Java environments that do not support local variable type inference, it will not be used by most examples in the remainder of this edition of this book. This way, the majority of examples will compile and run for the largest number of readers. 21/09/21 5:41 PM CHAPTER Inheritance Inheritance is one of the cornerstones of object-oriented programming because it allows the creation of hierarchical classifications. Using inheritance, you can create a general class that defines traits common to a set of related items. This class can then be inherited by other, more specific classes, each adding those things that are unique to it. In the terminology of Java, a class that is inherited is called a superclass. The class that does the inheriting is called a subclass. Therefore, a subclass is a specialized version of a superclass. It inherits all of the members defined by the superclass and adds its own, unique elements. Inheritance Basics To inherit a class, you simply incorporate the definition of one class into another by using the extends keyword. To see how, lets begin with a short example. The following program creates a superclass called A and a subclass called B. Notice how the keyword extends is used to create a subclass of A"
"Explain why in a single-core system, two or more threads do not actually run at the same time, but idle CPU time is utilized","In a single-core system, concurrently executing threads share the CPU, with each thread receiving a slice of CPU time. Hence, in a single-core system, two or more threads do not actually run at the same time, but idle CPU time is utilized."," The benefit of Javas multithreading is that the main loop/polling mechanism is eliminated. One thread can pause without stopping other parts of your program. For example, the idle time created when a thread reads data from a network or waits for user input can be utilized elsewhere. Multithreading allows animation loops to sleep for a second between each frame without causing the whole system to pause. When a thread blocks in a Java program, only the single thread that is blocked pauses. All other threads continue to run. As most readers know, over the past few years, multicore systems have become commonplace. Of course, single-core systems are still in widespread use. It is important to understand that Javas multithreading features work in both types of systems. In a single-core system, concurrently executing threads share the CPU, with each thread receiving a slice of CPU time. Therefore, in a single-core system, two or more threads do not actually run at the same time, but idle CPU time is utilized. However, in multicore systems, it is possible for two or more threads to actually execute simultaneously. In many cases, this can further improve program efficiency and increase the speed of certain operations. 22/09/21 6:37 PM the Fork/Join Framework. It provides a powerful means of creating multithreaded applications that automatically scale to make best use of multicore environments. The Fork/Join Framework is part of Javas support for parallel programming, which is the name commonly given to the techniques that optimize some types of algorithms for parallel execution in systems that have more than one CPU. For a discussion of the Fork/Join Framework and other concurrency utilities, see Chapter 29. Javas traditional multithreading capabilities are described here"
Explain why the newline after the second line is also preserved,"Thus, the text block automatically preserves the newlines in the text. Again, the text block begins after the newline following the opening delimiter and ends at the start of the closing delimiter. Hence, the newline after the second line is also preserved."," """""" This example creates a string in which the line ""Text blocks make"" is separated from ""multiple lines easy."" by a newline. It is not necessary to use the \n escape sequence to obtain the newline. Thus, the text block automatically preserves the newlines in the text. Again, the text block begins after the newline following the opening delimiter and ends at the start of the closing delimiter. Therefore, the newline after the second line is also preserved. It is important to emphasize that even though a text block uses the """""" delimiter, it is of type String. Thus, the preceding text block could be assigned to a String variable, as shown here: String str = """""" Text blocks make multiple lines easy"
Explain why the inheritance chain ends with Alpha and Beta,"Notice that each is specified as final. In general, a subclass of a sealed class must be declared as either final, sealed, or non-sealed. Lets look at each option. First, in this example, each subclass is declared final. This means that the only subclasses of MySealedClass are Alpha and Beta, and no subclasses of either of those can be created. Hence, the inheritance chain ends with Alpha and Beta."," } public final class Beta extends MySealedClass { // ... } Notice that each is specified as final. In general, a subclass of a sealed class must be declared as either final, sealed, or non-sealed. Lets look at each option. First, in this example, each subclass is declared final. This means that the only subclasses of MySealedClass are Alpha and Beta, and no subclasses of either of those can be created. Therefore, the inheritance chain ends with Alpha and Beta. To indicate that a subclass is itself sealed, it must be declared sealed and its permitted subclasses must be specified. For example, this version of Alpha permits Gamma: public sealed class Alpha extends MySealedClass permits Gamma { // .."
Explain why the order in which elements are added to a hash map is not necessarily the order in which they are read by an iterator," You should note that a hash map does not guarantee the order of its elements. Hence, the order in which elements are added to a hash map is not necessarily the order in which they are read by an iterator."," HashMap implements Map and extends AbstractMap. It does not add any methods of its own. You should note that a hash map does not guarantee the order of its elements. Therefore, the order in which elements are added to a hash map is not necessarily the order in which they are read by an iterator. 21/09/21 5:49 PM The following program illustrates HashMap. It maps names to account balances. Notice how a set-view is obtained and used"
Explain why it is necessary for the current position to be reset to the start of the buffer before calling write( ),"It is useful to emphasize an important aspect of this program. As mentioned, after data is written to mBuf, but before it is written to the file, a call to rewind( ) on mBuf is made. This is necessary in order to reset the current position to zero after data has been written to mBuf. Remember, each call to put( ) on mBuf advances the current position. Hence, it is necessary for the current position to be reset to the start of the buffer before calling write( )."," mBuf.rewind(); 21/09/21 5:53 PM 782PART II // Write the buffer to the output file. fChan.write(mBuf); } catch(InvalidPathException e) { System.out.println(""Path Error "" + e); } catch (IOException e) { System.out.println(""I/O Error: "" + e); System.exit(1); } } } It is useful to emphasize an important aspect of this program. As mentioned, after data is written to mBuf, but before it is written to the file, a call to rewind( ) on mBuf is made. This is necessary in order to reset the current position to zero after data has been written to mBuf. Remember, each call to put( ) on mBuf advances the current position. Therefore, it is necessary for the current position to be reset to the start of the buffer before calling write( ). If this is not done, write( ) will think that there is no data in the buffer"
"Explain why by default, components are laid out line-by-line beginning at the upper-left corner","You have already seen FlowLayout in action. It is the layout manager that the preceding examples have used. FlowLayout implements a simple layout style, which is similar to how words flow in a text editor. The direction of the layout is governed by the containers component orientation property, which, by default, is left to right, top to bottom. Hence, by default, components are laid out line-by-line beginning at the upper-left corner."," You can use the layout manager that best fits your application. FlowLayout You have already seen FlowLayout in action. It is the layout manager that the preceding examples have used. FlowLayout implements a simple layout style, which is similar to how words flow in a text editor. The direction of the layout is governed by the containers component orientation property, which, by default, is left to right, top to bottom. Therefore, by default, components are laid out line-by-line beginning at the upper-left corner. In all cases, when a line is filled, layout advances to the next line. A small space is left between each component, above and below, as well as left and right. Here are the constructors for FlowLayout: FlowLayout( ) FlowLayout(int how) FlowLayout(int how, int horz, int vert) 22/09/21 6:39 PM 906PART II The first form creates the default layout, which centers components and leaves five pixels of space between each component. The second form lets you specify how each line is aligned. Valid values for how are as follows: FlowLayout.LEFT FlowLayout.CENTER FlowLayout.RIGHT FlowLayout.LEADING FlowLayout.TRAILING These values specify left, center, right, leading edge, and trailing edge alignment, respectively"
"Explain why in order to execute a ForkJoinTask, you must first have a ForkJoinPool"," In general, RecursiveTask is used to implement a recursive, divide-and-conquer strategy for tasks that return results. The execution of ForkJoinTasks takes place within a ForkJoinPool, which also manages the execution of the tasks. Hence, in order to execute a ForkJoinTask, you must first have a ForkJoinPool."," The compute( ) method is defined by RecursiveTask<V> like this: protected abstract V compute( ) Notice that compute( ) is protected and abstract. This means that it must be implemented by a subclass. When implemented, it must return the result of the task. In general, RecursiveTask is used to implement a recursive, divide-and-conquer strategy for tasks that return results. (See The Divide-and-Conquer Strategy later in this chapter.) The execution of ForkJoinTasks takes place within a ForkJoinPool, which also manages the execution of the tasks. Therefore, in order to execute a ForkJoinTask, you must first have a ForkJoinPool. There are two ways to acquire a ForkJoinPool. First, you can explicitly create one by using a ForkJoinPool constructor. Second, you can use what is referred to as the common pool. The common pool (which was added by JDK 8) is a static ForkJoinPool that is automatically available for your use. Each method is introduced here, beginning with manually constructing a pool. ForkJoinPool defines several constructors. Here are two commonly used ones: ForkJoinPool( ) ForkJoinPool(int pLevel) 21/09/21 5:55 PM The first creates a default pool that supports a level of parallelism equal to the number of processors available in the system. The second lets you specify the level of parallelism. Its value must be greater than zero and not more than the limits of the implementation. The level of parallelism determines the number of threads that can execute concurrently. As a result, the level of parallelism effectively determines the number of tasks that can be executed simultaneously. (Of course, the number of tasks that can execute simultaneously cannot exceed the number of processors.) It is important to understand that the level of parallelism does not, however, limit the number of tasks that can be managed by the pool"
"Explain why in general, a ForkJoinTask will not perform I/O","ForkJoinTask is the divide-and-conquer strategy. Such an approach does not normally lend itself to situations in which outside synchronization is needed. Also, avoid situations in which substantial blocking will occur through I/O. Hence, in general, a ForkJoinTask will not perform I/O."," Second, usually it is best to use the default level of parallelism. If you specify a smaller number, it may significantly reduce the benefits of using the Fork/Join Framework. In general, a ForkJoinTask should not use synchronized methods or synchronized blocks of code. Also, you will not normally want to have the compute( ) method use other types of synchronization, such as a semaphore. (The Phaser can, however, be used when appropriate because it is compatible with the fork/join mechanism.) Remember, the main idea behind a ForkJoinTask is the divide-and-conquer strategy. Such an approach does not normally lend itself to situations in which outside synchronization is needed. Also, avoid situations in which substantial blocking will occur through I/O. Therefore, in general, a ForkJoinTask will not perform I/O. Simply put, to best utilize the Fork/Join Framework, a task should perform a computation that can run without outside blocking or synchronization. One last point: Except under unusual circumstances, do not make assumptions about the execution environment that your code will run in. This means you should not assume that some specific number of processors will be available, or that the execution characteristics of your program wont be affected by other processes running at the same time"
"Explain why before you paint to a component, you must obtain the width of the border and then adjust your drawing accordingly","When drawing to the surface of a component, you must be careful to restrict your output to the area that is inside the border. Although Swing automatically clips any output that will exceed the boundaries of a component, it is still possible to paint into the border, which will then get overwritten when the border is drawn. To avoid this, you must compute the paintable area of the component. This is the area defined by the current size of the component minus the space used by the border. Hence, before you paint to a component, you must obtain the width of the border and then adjust your drawing accordingly."," Inside the overridden paintComponent( ), you will draw the stored output. protected void paintComponent(Graphics g) Compute the Paintable Area When drawing to the surface of a component, you must be careful to restrict your output to the area that is inside the border. Although Swing automatically clips any output that will exceed the boundaries of a component, it is still possible to paint into the border, which will then get overwritten when the border is drawn. To avoid this, you must compute the paintable area of the component. This is the area defined by the current size of the component minus the space used by the border. Therefore, before you paint to a component, you must obtain the width of the border and then adjust your drawing accordingly. To obtain the border width, call getInsets( ), shown here: Insets getInsets( ) This method is defined by Container and overridden by JComponent. It returns an Insets object that contains the dimensions of the border. The inset values can be obtained by using these fields: int top; int bottom; int left; int right; 22/09/21 6:42 PM 1076PART III These values are then used to compute the drawing area given the width and the height of the component. You can obtain the width and height of the component by calling getWidth( ) and getHeight( ) on the component. They are shown here: int getWidth( ) int getHeight( ) By subtracting the value of the insets, you can compute the usable width and height of the component"
Explain how using setText( ) it is possible to change the text inside a label during program execution," The icon and text associated with the label can be obtained by the following methods: Icon getIcon( ) String getText( ) The icon and text associated with a label can be set by these methods: void setIcon(Icon icon) void setText(String str) Here, icon and str are the icon and text, respectively. Hence, using setText( ) it is possible to change the text inside a label during program execution."," Notice that icons are specified by objects of type Icon, which is an interface defined by Swing. The easiest way to obtain an icon is to use the ImageIcon class. ImageIcon implements Icon and encapsulates an image. Thus, an object of type ImageIcon can be passed as an argument to the Icon parameter of JLabels constructor. There are several ways to provide the image, including reading it from a file or downloading it from a URL. Here is the ImageIcon constructor used by the example in this section: ImageIcon(String filename) It obtains the image in the file named filename. The icon and text associated with the label can be obtained by the following methods: Icon getIcon( ) String getText( ) The icon and text associated with a label can be set by these methods: void setIcon(Icon icon) void setText(String str) Here, icon and str are the icon and text, respectively. Therefore, using setText( ) it is possible to change the text inside a label during program execution. The following program illustrates how to create and display a label containing both an icon and a string. It begins by creating an ImageIcon object for the file hourglass.png, which depicts an hourglass. This is used as the second argument to the JLabel constructor"
"Explain why initially the menu bar will be empty, and you will need to populate it with menus prior to use","JComponent (which inherits Container and Component). It has only one constructor, which is the default constructor. Hence, initially the menu bar will be empty, and you will need to populate it with menus prior to use."," Menus can also generate other types of events. For example, each time that a menu is activated, selected, or canceled, a MenuEvent is generated that can be listened for via a MenuListener. Other menu-related events include MenuKeyEvent, MenuDragMouseEvent, and PopupMenuEvent. In many cases, however, you need only watch for action events, and in this chapter, we will use only action events. JMenuBar As mentioned, JMenuBar is essentially a container for menus. Like all components, it inherits JComponent (which inherits Container and Component). It has only one constructor, which is the default constructor. Therefore, initially the menu bar will be empty, and you will need to populate it with menus prior to use. Each application has one and only one menu bar. JMenuBar defines several methods, but often you will only need to use one: add( )"
"Explain why when a menu item is selected, an action event is generated","JMenuItem encapsulates an element in a menu. This element can be a selection linked to some program action, such as Save or Close, or it can cause a submenu to be displayed. As mentioned, JMenuItem is derived from AbstractButton, and every item in a menu can be thought of as a special kind of button. Hence, when a menu item is selected, an action event is generated."," 22/09/21 6:43 PM You can remove an item from a menu by calling remove( ). Two of its forms are shown here: void remove(JMenuItem menu) void remove(int idx) In this case, menu is a reference to the item to remove and idx is the index of the item to remove. You can obtain the number of items in the menu by calling getMenuComponentCount( ), shown here: int getMenuComponentCount( ) You can get an array of the items in the menu by calling getMenuComponents( ), shown next: Component[ ] getMenuComponents( ) JMenuItem JMenuItem encapsulates an element in a menu. This element can be a selection linked to some program action, such as Save or Close, or it can cause a submenu to be displayed. As mentioned, JMenuItem is derived from AbstractButton, and every item in a menu can be thought of as a special kind of button. Therefore, when a menu item is selected, an action event is generated. (This is similar to the way a JButton fires an action event when it is pressed.) JMenuItem defines many constructors. The ones used in this chapter are shown here: An array containing the components is returned. JMenuItem(String name) JMenuItem(Icon image) JMenuItem(String name, Icon image) JMenuItem(String name, int mnem) JMenuItem(Action action) The first constructor creates a menu item with the name specified by name. The second creates a menu item that displays the image specified by image. The third creates a menu item with the name specified by name and the image specified by image. The fourth creates a menu item with the name specified by name and uses the keyboard mnemonic specified by mnem. This mnemonic enables you to select an item from the menu by pressing the specified key. The last constructor creates a menu item using the information specified in action. A default constructor is also supported"
"Explain why if you are using an IDE, you must refer to the instructions for that environment for information concerning the development and deployment of servlets","Although IDEs such as NetBeans and Eclipse are very useful and can streamline the creation of servlets, they are not used in this chapter. The way you develop and deploy servlets differs among IDEs, and it is simply not possible for this book to address each environment. Furthermore, many readers will be using the command-line tools rather than an IDE. Hence, if you are using an IDE, you must refer to the instructions for that environment for information concerning the development and deployment of servlets."," Servlet Development Options To experiment with servlets, you will need access to a servlet container/server. Two popular ones are Glassfish and Apache Tomcat. The one used in this chapter is Tomcat. Apache Tomcat is an open-source product maintained by the Apache Software Foundation. Although IDEs such as NetBeans and Eclipse are very useful and can streamline the creation of servlets, they are not used in this chapter. The way you develop and deploy 21/09/21 5:59 PM servlets differs among IDEs, and it is simply not possible for this book to address each environment. Furthermore, many readers will be using the command-line tools rather than an IDE. Therefore, if you are using an IDE, you must refer to the instructions for that environment for information concerning the development and deployment of servlets. For this reason, the instructions given here and elsewhere in this chapter assume that only the command-line tools are employed. Thus, they will work for nearly any reader. As stated, this chapter uses Tomcat in the examples. It provides a simple, yet effective way to experiment with servlets using only the command line tools. It is also widely available in various programming environments. Furthermore, since only command-line tools are used, you dont need to download and install an IDE just to experiment with servlets"
Explain why Java is a programmers language,"The Java designers knew that using the familiar syntax of C and echoing the object-oriented features of C++ would make their language appealing to the legions of experienced C/C++ programmers. In addition to the surface similarities, Java shares some of the other attributes that helped make C and C++ successful. First, Java was designed, tested, and refined by real, working programmers. It is a language grounded in the needs and experiences of the people who devised it. Hence, Java is a programmers language."," As mentioned earlier, Java derives much of its character from C and C++. This is by intent. The Java designers knew that using the familiar syntax of C and echoing the object-oriented features of C++ would make their language appealing to the legions of experienced C/C++ programmers. In addition to the surface similarities, Java shares some of the other attributes that helped make C and C++ successful. First, Java was designed, tested, and refined by real, working programmers. It is a language grounded in the needs and experiences of the people who devised it. Thus, Java is a programmers language. Second, Java is cohesive and logically consistent. Third, except for those constraints imposed by the Internet environment, Java gives you, the programmer, full control. If you program well, your programs reflect it. If you program poorly, your programs reflect that, too. Put differently, Java is not a language with training wheels. It is a language for professional programmers. Because of the similarities between Java and C++, it is tempting to think of Java as simply the Internet version of C++. However, to do so would be a large mistake. Java has significant practical and philosophical differences. While it is true that Java was influenced by C++, it is not an enhanced version of C++. For example, Java is neither upwardly nor downwardly compatible with C++. Of course, the similarities with C++ are significant, and if you are a CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 The Java Language C++ programmer, then you will feel right at home with Java. One other point: Java was not designed to replace C++. Java was designed to solve a certain set of problems. C++ was designed to solve a different set of problems. Both will coexist for many years to come"
"Explain why it is possible for the JVM to create a restricted execution environment, called the sandbox, that contains the program, preventing unrestricted access to the machine","The fact that a Java program is executed by the JVM also helps to make it secure. Because the JVM is in control, it manages program execution. Hence, it is possible for the JVM to create a restricted execution environment, called the sandbox, that contains the program, preventing unrestricted access to the machine."," This is, of course, not a feasible solution. Thus, the execution of bytecode by the JVM is the easiest way to create truly portable programs. The fact that a Java program is executed by the JVM also helps to make it secure. Because the JVM is in control, it manages program execution. Thus, it is possible for the JVM to create a restricted execution environment, called the sandbox, that contains the program, preventing unrestricted access to the machine. Safety is also enhanced by certain restrictions that exist in the Java language. In general, when a program is compiled to an intermediate form and then interpreted by a virtual machine, it runs slower than it would run if compiled to executable code. However, with Java, the differential between the two is not so great. Because bytecode has been highly optimized, the use of bytecode enables the JVM to execute programs much faster than you might expect"
Explain why a deprecated feature should not be used for new code,"In the language of Java, deprecated means that a feature is still available but flagged as obsolete. Hence, a deprecated feature should not be used for new code."," As explained previously, in the early years of Java, applets were a crucial part of Java programming. They not only added excitement to a web page, they were also a highly visible part of Java, which added to its charisma. However, applets rely on a Java browser plug-in. Thus, for an applet to work, the browser must support it. Over the past few years, support for the Java browser plug-in has been waning. Simply put, without browser support, applets are not viable. Because of this, beginning with JDK 9, the phase-out of applets was begun, with support for applets being deprecated. In the language of Java, deprecated means that a feature is still available but flagged as obsolete. Thus, a deprecated feature should not be used for new code. The phase-out became complete with the release of JDK 11 because run-time support for applets was removed. Beginning with JDK 17, the entire Applet API was deprecated for removal. As a point of interest, a few years after Javas creation an alternative to applets was added to Java. Called Java Web Start, it enabled an application to be dynamically downloaded from a web page. It was a deployment mechanism that was especially useful for larger Java applications that were not appropriate for applets. The difference between an applet and a Web Start application is that a Web Start application runs on its own, not inside the browser"
Explain why the same servlet can be used in a variety of different server environments," Because servlets (like all Java programs) are compiled into bytecode and executed by the JVM, they are highly portable. Hence, the same servlet can be used in a variety of different server environments."," Servlets are used to create dynamically generated content that is then served to the client. For example, an online store might use a servlet to look up the price for an item in a database. The price information is then used to dynamically generate a web page that is sent to the browser. Although dynamically generated content was available through mechanisms such as CGI (Common Gateway Interface), the servlet offered several advantages, including increased performance. Because servlets (like all Java programs) are compiled into bytecode and executed by the JVM, they are highly portable. Thus, the same servlet can be used in a variety of different server environments. The only requirements are that the server support the JVM and a servlet container. Today, server-side code in general constitutes a major use of Java. 21/09/21 5:35 PM No discussion of Javas history is complete without a look at the Java buzzwords. Although the fundamental forces that necessitated the invention of Java are portability and security, other factors also played an important role in molding the final form of the language. The key considerations were summed up by the Java team in the following list of buzzwords: Simple Secure Portable Object-oriented Robust Multithreaded Architecture-neutral Interpreted High performance Distributed Dynamic Two of these buzzwords have already been discussed: secure and portable. Lets examine what each of the others implies"
Explain why each of these objects describes its own unique behavior,"The data from a traditional process-oriented program can be transformed by abstraction into its component objects. A sequence of process steps can become a collection of messages between these objects. Hence, each of these objects describes its own unique behavior."," Hierarchical abstractions of complex systems can also be applied to computer programs. The data from a traditional process-oriented program can be transformed by abstraction into its component objects. A sequence of process steps can become a collection of messages between these objects. Thus, each of these objects describes its own unique behavior. You can treat these objects as concrete entities that respond to messages telling them to do something. This is the essence of object-oriented programming. Object-oriented concepts form the heart of Java just as they form the basis for human understanding. It is important that you understand how these concepts translate into programs. As you will see, object-oriented programming is a powerful and natural paradigm for creating programs that survive the inevitable changes accompanying the life cycle of any major software project, including conception, growth, and aging. For example, once you have well-defined objects and clean, reliable interfaces to those objects, you can gracefully decommission or replace parts of an older system without fear"
Explain why the gear-shift lever is a interface to the transmission,"Encapsulation is the mechanism that binds together code and the data it manipulates, and keeps both safe from outside interference and misuse. One way to think about encapsulation is as a protective wrapper that prevents the code and data from being arbitrarily accessed by other code defined outside the wrapper. Access to the code and data inside the wrapper is tightly controlled through a well-defined interface. To relate this to the real world, consider the automatic transmission on an automobile. It encapsulates hundreds of bits of information about your engine, such as how much you are accelerating, the pitch of the surface you are on, and the position of the shift lever. You, as the user, have only one method of affecting this complex encapsulation: by moving the gear-shift lever. You cant affect the transmission by using the turn signal or windshield wipers, for example. Hence, the gear-shift lever is a interface to the transmission."," The Three OOP Principles All object-oriented programming languages provide mechanisms that help you implement the object-oriented model. They are encapsulation, inheritance, and polymorphism. Lets take a look at these concepts now. Encapsulation is the mechanism that binds together code and the data it manipulates, and keeps both safe from outside interference and misuse. One way to think about encapsulation is as a protective wrapper that prevents the code and data from being arbitrarily accessed by other code defined outside the wrapper. Access to the code and data inside the wrapper is tightly controlled through a well-defined interface. To relate this to the real world, consider the automatic transmission on an automobile. It encapsulates hundreds of bits of information about your engine, such as how much you are accelerating, the pitch of the surface you are on, and the position of the shift lever. You, as the user, have only one method of affecting this complex encapsulation: by moving the gear-shift lever. You cant affect the transmission by using the turn signal or windshield wipers, for example. Thus, the gear-shift lever is a 22/09/21 6:22 PM An Overview of Java well-defined (indeed, unique) interface to the transmission. Further, what occurs inside the transmission does not affect objects outside the transmission. For example, shifting gears does not turn on the headlights! Because an automatic transmission is encapsulated, dozens of car manufacturers can implement one in any way they please. However, from the drivers point of view, they all work the same. This same idea can be applied to programming. The power of encapsulated code is that everyone knows how to access it and thus can use it regardless of the implementation detailsand without fear of unexpected side effects. In Java, the basis of encapsulation is the class. Although the class will be examined in great detail later in this book, the following brief discussion will be helpful now. A class defines the structure and behavior (data and code) that will be shared by a set of objects. Each object of a given class contains the structure and behavior defined by the class, as if it were stamped out by a mold in the shape of the class. For this reason, objects are sometimes referred to as instances of a class. Thus, a class is a logical construct; an object has physical reality"
Explain why a class is a logical construct; an object has physical reality,"A class defines the structure and behavior (data and code) that will be shared by a set of objects. Each object of a given class contains the structure and behavior defined by the class, as if it were stamped out by a mold in the shape of the class. For this reason, objects are sometimes referred to as instances of a class. Hence, a class is a logical construct; an object has physical reality."," Encapsulation is the mechanism that binds together code and the data it manipulates, and keeps both safe from outside interference and misuse. One way to think about encapsulation is as a protective wrapper that prevents the code and data from being arbitrarily accessed by other code defined outside the wrapper. Access to the code and data inside the wrapper is tightly controlled through a well-defined interface. To relate this to the real world, consider the automatic transmission on an automobile. It encapsulates hundreds of bits of information about your engine, such as how much you are accelerating, the pitch of the surface you are on, and the position of the shift lever. You, as the user, have only one method of affecting this complex encapsulation: by moving the gear-shift lever. You cant affect the transmission by using the turn signal or windshield wipers, for example. Thus, the gear-shift lever is a 22/09/21 6:22 PM An Overview of Java well-defined (indeed, unique) interface to the transmission. Further, what occurs inside the transmission does not affect objects outside the transmission. For example, shifting gears does not turn on the headlights! Because an automatic transmission is encapsulated, dozens of car manufacturers can implement one in any way they please. However, from the drivers point of view, they all work the same. This same idea can be applied to programming. The power of encapsulated code is that everyone knows how to access it and thus can use it regardless of the implementation detailsand without fear of unexpected side effects. In Java, the basis of encapsulation is the class. Although the class will be examined in great detail later in this book, the following brief discussion will be helpful now. A class defines the structure and behavior (data and code) that will be shared by a set of objects. Each object of a given class contains the structure and behavior defined by the class, as if it were stamped out by a mold in the shape of the class. For this reason, objects are sometimes referred to as instances of a class. Thus, a class is a logical construct; an object has physical reality. When you create a class, you will specify the code and data that constitute that class"
Explain how it is the inheritance mechanism that makes it possible for one object to be a specific instance of a more general case,"Inheritance is the process by which one object acquires the properties of another object. This is important because it supports the concept of hierarchical classification. As mentioned earlier, most knowledge is made manageable by hierarchical (that is, top-down) classifications. However, by use of inheritance, an object need only define those qualities that make it unique within its class. It can inherit its general attributes from its parent. Hence, it is the inheritance mechanism that makes it possible for one object to be a specific instance of a more general case."," Since the purpose of a class is to encapsulate complexity, there are mechanisms for hiding the complexity of the implementation inside the class. Each method or variable in a class may be marked private or public. The public interface of a class represents everything that external users of the class need to know, or may know. The private methods and data can only be accessed by code that is a member of the class. Therefore, any other code that is not a member of the class cannot access a private method or variable. Since the private members of a class may only be accessed by other parts of your program through the class public methods, you can ensure that no improper actions take place. Of course, this means that the public interface should be carefully designed not to expose too much of the inner workings of a class (see Figure 2-1). Inheritance is the process by which one object acquires the properties of another object. This is important because it supports the concept of hierarchical classification. As mentioned earlier, most knowledge is made manageable by hierarchical (that is, top-down) classifications. For example, a Golden Retriever is part of the classification dog, which in turn is part of the mammal class, which is under the larger class animal. Without the use of hierarchies, each object would need to define all of its characteristics explicitly. However, by use of inheritance, an object need only define those qualities that make it unique within its class. It can inherit its general attributes from its parent. Thus, it is the inheritance mechanism that makes it possible for one object to be a specific instance of a more general case. Lets take a closer look at this process. Most people naturally view the world as made up of objects that are related to each other in a hierarchical way, such as animals, mammals, and dogs. If you wanted to describe animals in an abstract way, you would say they have some attributes, such as size, intelligence, and type of skeletal system. Animals also have certain behavioral aspects; they eat, breathe, and sleep. This description of attributes and behavior is the class definition for animals"
Explain why the output of javac is not code that can be directly executed,"The javac compiler creates a file called Example.class that contains the bytecode version of the program. As discussed earlier, the Java bytecode is the intermediate representation of your program that contains instructions the Java Virtual Machine will execute. Hence, the output of javac is not code that can be directly executed."," As you can see by looking at the program, the name of the class defined by the program is also Example. This is not a coincidence. In Java, all code must reside inside a class. By convention, the name of the main class should match the name of the file that holds the program. You should also make sure that the capitalization of the filename matches the class name. The reason for this is that Java is case-sensitive. At this point, the convention that filenames correspond to class names may seem arbitrary. However, this convention makes it easier to maintain and organize your programs. Furthermore, as you will see later in this book, in some cases, it is required. Compiling the Program To compile the Example program, execute the compiler, javac, specifying the name of the source file on the command line, as shown here: C:\>javac Example.java 22/09/21 6:22 PM 28PART I The javac compiler creates a file called Example.class that contains the bytecode version of the program. As discussed earlier, the Java bytecode is the intermediate representation of your program that contains instructions the Java Virtual Machine will execute. Thus, the output of javac is not code that can be directly executed. To actually run the program, you must use the Java application launcher called java. To do so, pass the class name Example as a command-line argument, as shown here: C:\>java Example When the program is run, the following output is displayed: This is a simple Java program"
Explain why Main is different from main," As stated, main( ) is the method called when a Java application begins. Keep in mind that Java is case-sensitive. Hence, Main is different from main."," The public keyword is an access modifier, which allows the programmer to control the visibility of class members. When a class member is preceded by public, then that member may be accessed by code outside the class in which it is declared. (The opposite of public is private, which prevents a member from being used by code defined outside of its class.) In this case, main( ) must be declared as public, since it must be called by code outside of its class when the program is started. The keyword static allows main( ) to be called without having to instantiate a particular instance of the class. This is necessary since main( ) is called by the Java Virtual Machine before any objects are made. The keyword void simply tells the compiler that main( ) does not return a value. As you will see, methods may also return values. If all this seems a bit confusing, dont worry. All of these concepts will be discussed in detail in subsequent chapters. As stated, main( ) is the method called when a Java application begins. Keep in mind that Java is case-sensitive. Thus, Main is different from main. It is important to understand that the Java compiler will compile classes that do not contain a main( ) method. But java has no way to run these classes. So, if you had typed Main instead of main, the compiler would still compile your program. However, java would report an error because it would be unable to find the main( ) method. Any information that you need to pass to a method is received by variables specified within the set of parentheses that follow the name of the method. These variables are called parameters"
Explain why the need for an unsigned integer type was eliminated,"Java defines four integer types: byte, short, int, and long. All of these are signed, positive and negative values. Java does not support unsigned, positive-only integers. Many other computer languages support both signed and unsigned integers. However, Javas designers felt that unsigned integers were unnecessary. Specifically, they felt that the concept of unsigned was used mostly to specify the behavior of the high-order bit, which defines the sign of an integer value. As you will see in Chapter 4, Java manages the meaning of the highorder bit differently, by adding a special unsigned right shift operator. Hence, the need for an unsigned integer type was eliminated."," Lets look at each type of data in turn. Integers Java defines four integer types: byte, short, int, and long. All of these are signed, positive and negative values. Java does not support unsigned, positive-only integers. Many other computer languages support both signed and unsigned integers. However, Javas designers felt that unsigned integers were unnecessary. Specifically, they felt that the concept of unsigned was used mostly to specify the behavior of the high-order bit, which defines the sign of an integer value. As you will see in Chapter 4, Java manages the meaning of the highorder bit differently, by adding a special unsigned right shift operator. Thus, the need for an unsigned integer type was eliminated. The width of an integer type should not be thought of as the amount of storage it consumes, but rather as the behavior it defines for variables and expressions of that type. The Java run-time environment is free to use whatever size it wants, as long as the types behave as you declared them. The width and ranges of these integer types vary widely, as shown in this table: Name Range 32 int short byte byte The smallest integer type is byte. This is a signed 8-bit type that has a range from 128 to 127. Variables of type byte are especially useful when youre working with a stream of data from a network or file. They are also useful when youre working with raw binary data that may not be directly compatible with Javas other built-in types"
Explain why in Java char is a 16-bit type,"In Java, the data type used to store characters is char. A key point to understand is that Java uses Unicode to represent characters. Unicode defines a fully international character set that can represent all of the characters found in all human languages. It is a unification of dozens of character sets, such as Latin, Greek, Arabic, Cyrillic, Hebrew, Katakana, Hangul, and many more. At the time of Javas creation, Unicode required 16 bits. Hence, in Java char is a 16-bit type."," Here is a short program that uses double variables to compute the area of a circle: // Compute the area of a circle. class Area { public static void main(String[] args) { double pi, r, a; 22/09/21 6:22 PM r = 10.8; // radius of circle pi = 3.1416; // pi, approximately a = pi * r * r; // compute area System.out.println(""Area of circle is "" + a); } } Characters In Java, the data type used to store characters is char. A key point to understand is that Java uses Unicode to represent characters. Unicode defines a fully international character set that can represent all of the characters found in all human languages. It is a unification of dozens of character sets, such as Latin, Greek, Arabic, Cyrillic, Hebrew, Katakana, Hangul, and many more. At the time of Javas creation, Unicode required 16 bits. Thus, in Java char is a 16-bit type. The range of a char is 0 to 65,535. There are no negative chars. The standard set of characters known as ASCII still ranges from 0 to 127 as always, and the extended 8-bit character set, ISO-Latin-1, ranges from 0 to 255. Since Java is designed to allow programs to be written for worldwide use, it makes sense that it would use Unicode to represent characters. Of course, the use of Unicode is somewhat inefficient for languages such as English, German, Spanish, or French, whose characters can easily be contained within 8 bits. But such is the price that must be paid for global portability"
"Explain why each time you start a new block, you are creating a new scope","However, Java allows variables to be declared within any block. A block is begun with an opening curly brace and ended by a closing curly brace. A block defines a scope. Hence, each time you start a new block, you are creating a new scope."," The Scope and Lifetime of Variables So far, all of the variables used have been declared at the start of the main( ) method. However, Java allows variables to be declared within any block. As explained in Chapter 2, a block is begun with an opening curly brace and ended by a closing curly brace. A block defines a scope. Thus, each time you start a new block, you are creating a new scope. A scope 22/09/21 6:22 PM 50PART I determines what objects are visible to other parts of your program. It also determines the lifetime of those objects. It is not uncommon to think in terms of two general categories of scopes: global and local. However, these traditional scopes do not fit well with Javas strict, object-oriented model. While it is possible to create what amounts to being a global scope, it is by far the exception, not the rule. In Java, the two major scopes are those defined by a class and those defined by a method. Even this distinction is somewhat artificial. However, since the class scope has several unique properties and attributes that do not apply to the scope defined by a method, this distinction makes some sense. Because of the differences, a discussion of class scope (and variables declared within it) is deferred until Chapter 6, when classes are described. For now, we will only examine the scopes defined by or within a method"
"Explain how when you declare a variable within a scope, you are localizing that variable and protecting it from unauthorized access and/or modification"," As a general rule, variables declared inside a scope are not visible (that is, accessible) to code that is defined outside that scope. Hence, when you declare a variable within a scope, you are localizing that variable and protecting it from unauthorized access and/or modification."," The scope defined by a method begins with its opening curly brace. However, if that method has parameters, they too are included within the methods scope. A methods scope ends with its closing curly brace. This block of code is called the method body. As a general rule, variables declared inside a scope are not visible (that is, accessible) to code that is defined outside that scope. Thus, when you declare a variable within a scope, you are localizing that variable and protecting it from unauthorized access and/or modification. Indeed, the scope rules provide the foundation for encapsulation. A variable declared within a block is called a local variable"
"Explain why if you define a variable at the start of a method, it is available to all of the code within that method"," Within a block, variables can be declared at any point, but are valid only after they are declared. Hence, if you define a variable at the start of a method, it is available to all of the code within that method."," System.out.println(""x is "" + x); } } As the comments indicate, the variable x is declared at the start of main( )s scope and is accessible to all subsequent code within main( ). Within the if block, y is declared. Since a 22/09/21 6:22 PM block defines a scope, y is only visible to other code within its block. This is why outside of its block, the line y = 100; is commented out. If you remove the leading comment symbol, a compile-time error will occur, because y is not visible outside of its block. Within the if block, x can be used because code within a block (that is, a nested scope) has access to variables declared by an enclosing scope. Within a block, variables can be declared at any point, but are valid only after they are declared. Thus, if you define a variable at the start of a method, it is available to all of the code within that method. Conversely, if you declare a variable at the end of a block, it is effectively useless, because no code will have access to it. For example, this fragment is invalid because count cannot be used prior to its declaration: // This fragment is wrong! count = 100; // oops! cannot use count before it is declared! int count; Here is another important point to remember: variables are created when their scope is entered, and destroyed when their scope is left. This means that a variable will not hold its value once it has gone out of scope. Therefore, variables declared within a method will not hold their values between calls to that method. Also, a variable declared within a block will lose its value when the block is left. Thus, the lifetime of a variable is confined to its scope. If a variable declaration includes an initializer, then that variable will be reinitialized each time the block in which it is declared is entered. For example, consider the next program: // Demonstrate lifetime of a variable"
Explain why the lifetime of a variable is confined to its scope," variables are created when their scope is entered, and destroyed when their scope is left. This means that a variable will not hold its value once it has gone out of scope. Therefore, variables declared within a method will not hold their values between calls to that method. Also, a variable declared within a block will lose its value when the block is left. Hence, the lifetime of a variable is confined to its scope."," System.out.println(""x is "" + x); } } As the comments indicate, the variable x is declared at the start of main( )s scope and is accessible to all subsequent code within main( ). Within the if block, y is declared. Since a 22/09/21 6:22 PM block defines a scope, y is only visible to other code within its block. This is why outside of its block, the line y = 100; is commented out. If you remove the leading comment symbol, a compile-time error will occur, because y is not visible outside of its block. Within the if block, x can be used because code within a block (that is, a nested scope) has access to variables declared by an enclosing scope. Within a block, variables can be declared at any point, but are valid only after they are declared. Thus, if you define a variable at the start of a method, it is available to all of the code within that method. Conversely, if you declare a variable at the end of a block, it is effectively useless, because no code will have access to it. For example, this fragment is invalid because count cannot be used prior to its declaration: // This fragment is wrong! count = 100; // oops! cannot use count before it is declared! int count; Here is another important point to remember: variables are created when their scope is entered, and destroyed when their scope is left. This means that a variable will not hold its value once it has gone out of scope. Therefore, variables declared within a method will not hold their values between calls to that method. Also, a variable declared within a block will lose its value when the block is left. Thus, the lifetime of a variable is confined to its scope. If a variable declaration includes an initializer, then that variable will be reinitialized each time the block in which it is declared is entered. For example, consider the next program: // Demonstrate lifetime of a variable"
"Explain why when a floating-point value is assigned to an integer type, the fractional component is lost","A different type of conversion will occur when a floating-point value is assigned to an integer type: truncation. As you know, integers do not have fractional components. Hence, when a floating-point value is assigned to an integer type, the fractional component is lost."," 22/09/21 6:22 PM (target-type) value Here, target-type specifies the desired type to convert the specified value to. For example, the following fragment casts an int to a byte. If the integers value is larger than the range of a byte, it will be reduced modulo (the remainder of an integer division by the) bytes range. To create a conversion between two incompatible types, you must use a cast. A cast is simply an explicit type conversion. It has this general form: int a; byte b; // b = (byte) a; A different type of conversion will occur when a floating-point value is assigned to an integer type: truncation. As you know, integers do not have fractional components. Thus, when a floating-point value is assigned to an integer type, the fractional component is lost. For example, if the value 1.23 is assigned to an integer, the resulting value will simply be 1. The 0.23 will have been truncated. Of course, if the size of the whole number component is too large to fit into the target integer type, then that value will be reduced modulo the target types range. The following program demonstrates some type conversions that require casts: // Demonstrate casts"
Explain how the element type for the array determines what type of data the array will hold,"Here, type declares the element type (also called the base type) of the array. The element type determines the data type of each element that comprises the array. Hence, the element type for the array determines what type of data the array will hold."," Arrays An array is a group of like-typed variables that are referred to by a common name. Arrays of any type can be created and may have one or more dimensions. A specific element in an array is accessed by its index. Arrays offer a convenient means of grouping related information. One-Dimensional Arrays A one-dimensional array is, essentially, a list of like-typed variables. To create an array, you first must create an array variable of the desired type. The general form of a one-dimensional array declaration is type[ ] var-name; Here, type declares the element type (also called the base type) of the array. The element type determines the data type of each element that comprises the array. Thus, the element type for the array determines what type of data the array will hold. For example, the following declares an array named month_days with the type array of int: int[] month_days; Although this declaration establishes the fact that month_days is an array variable, no array actually exists. To link month_days with an actual, physical array of integers, you 22/09/21 6:22 PM 56PART I must allocate one using new and assign it to month_days. new is a special operator that allocates memory. You will look more closely at new in a later chapter, but you need to use it now to allocate memory for arrays. The general form of new as it applies to one-dimensional arrays appears as follows: array-var = new type [size]; Here, type specifies the type of data being allocated, size specifies the number of elements in the array, and array-var is the array variable that is linked to the array. That is, to use new to allocate an array, you must specify the type and number of elements to allocate. The elements in the array allocated by new will automatically be initialized to zero (for numeric types), false (for boolean), or null (for reference types, which are described in a later chapter). This example allocates a 12-element array of integers and links them to month_days: month_days = new int[12]; After this statement executes, month_days will refer to an array of 12 integers. Further, all elements in the array will be initialized to zero"
Explain how in Java all arrays are dynamically allocated,"Obtaining an array is a two-step process. First, you must declare a variable of the desired array type. Second, you must allocate the memory that will hold the array, using new, and assign it to the array variable. Hence, in Java all arrays are dynamically allocated."," You will look more closely at new in a later chapter, but you need to use it now to allocate memory for arrays. The general form of new as it applies to one-dimensional arrays appears as follows: array-var = new type [size]; Here, type specifies the type of data being allocated, size specifies the number of elements in the array, and array-var is the array variable that is linked to the array. That is, to use new to allocate an array, you must specify the type and number of elements to allocate. The elements in the array allocated by new will automatically be initialized to zero (for numeric types), false (for boolean), or null (for reference types, which are described in a later chapter). This example allocates a 12-element array of integers and links them to month_days: month_days = new int[12]; After this statement executes, month_days will refer to an array of 12 integers. Further, all elements in the array will be initialized to zero. Lets review: Obtaining an array is a two-step process. First, you must declare a variable of the desired array type. Second, you must allocate the memory that will hold the array, using new, and assign it to the array variable. Thus, in Java all arrays are dynamically allocated. If the concept of dynamic allocation is unfamiliar to you, dont worry. It will be described at length later in this book. Once you have allocated an array, you can access a specific element in the array by specifying its index within square brackets. All array indexes start at zero. For example, this statement assigns the value 28 to the second element of month_days: month_days[1] = 28; The next line displays the value stored at index 3: System.out.println(month_days[3]); Putting together all the pieces, here is a program that creates an array of the number of days in each month: // Demonstrate a one-dimensional array"
"Explain why in principle, it would not be necessary to specify an explicit type for an initialized variable because it could be inferred by the type of its initializer","Not long ago, a new feature called local variable type inference was added to the Java language. To begin, lets review two important aspects of variables. First, all variables in Java must be declared prior to their use. Second, a variable can be initialized with a value when it is declared. Furthermore, when a variable is initialized, the type of the initializer must be the same as (or convertible to) the declared type of the variable. Hence, in principle, it would not be necessary to specify an explicit type for an initialized variable because it could be inferred by the type of its initializer."," class ThreeDMatrix { public static void main(String[] args) { int[][][] threeD = new int[3][4][5]; int i, j, k; for(i=0; i<3; i++) for(j=0; j<4; j++) for(k=0; k<5; k++) threeD[i][j][k] = i * j * k; for(i=0; i<3; i++) { for(j=0; j<4; j++) { for(k=0; k<5; k++) System.out.print(threeD[i][j][k] + "" ""); System.out.println(); } System.out.println(); } } } 22/09/21 6:22 PM 62PART I This program generates the following output: 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2 6 3 9 4 0 0 2 6 4 6 8 8 12 16 12 18 24 Alternative Array Declaration Syntax There is a second form that may be used to declare an array: type var-name[ ]; Here, the square brackets follow the array variable name, and not the type specifier. For example, the following two declarations are equivalent: int al[] = new int[3]; int[] a2 = new int[3]; The following declarations are also equivalent: char twod1[][] = new char[3][4]; char[][] twod2 = new char[3][4]; This alternative declaration form offers convenience when converting code from C/C++ to Java. It also lets you declare both array and non-array variables in a single declaration statement. Today, the alternative form of array declaration is less commonly used, but it is still important that you are familiar with it because both forms of array declaration are legal in Java. Introducing Type Inference with Local Variables Not long ago, a new feature called local variable type inference was added to the Java language. To begin, lets review two important aspects of variables. First, all variables in Java must be declared prior to their use. Second, a variable can be initialized with a value when it is declared. Furthermore, when a variable is initialized, the type of the initializer must be the same as (or convertible to) the declared type of the variable. Thus, in principle, it would not be necessary to specify an explicit type for an initialized variable because it could be inferred by the type of its initializer. Of course, in the past, such inference was not supported, and all variables required an explicitly declared type, whether they were initialized or not. Today, that situation has changed. 22/09/21 6:22 PM Data Types, Variables, and Arrays Beginning with JDK 10, it is now possible to let the compiler infer the type of a local variable based on the type of its initializer, thus avoiding the need to explicitly specify the type. Local variable type inference offers a number of advantages. For example, it can streamline code by eliminating the need to redundantly specify a variables type when it can be inferred from its initializer. It can simplify declarations in cases in which the type name is quite lengthy, such as can be the case with some class names. It can also be helpful when a type is difficult to discern or cannot be denoted. (An example of a type that cannot be denoted is the type of an anonymous class, discussed in Chapter 25.) Furthermore, local variable type inference has become a common part of the contemporary programming environment. Its inclusion in Java helps keep Java up-to-date with evolving trends in language design. To support local variable type inference, the context-sensitive keyword var was added"
"Explain why in a local variable declaration, var is a placeholder for the actual, inferred type","As mentioned, var is context-sensitive. When it is used as the type name in the context of a local variable declaration, it tells the compiler to use type inference to determine the type of the variable being declared based on the type of the initializer. Hence, in a local variable declaration, var is a placeholder for the actual, inferred type."," To use local variable type inference, the variable must be declared with var as the type name and it must include an initializer. For example, in the past you would declare a local double variable called avg that is initialized with the value 10.0, as shown here: Part I double avg = 10.0; Using type inference, this declaration can now also be written like this: var avg = 10.0; In both cases, avg will be of type double. In the first case, its type is explicitly specified. In the second, its type is inferred as double because the initializer 10.0 is of type double. As mentioned, var is context-sensitive. When it is used as the type name in the context of a local variable declaration, it tells the compiler to use type inference to determine the type of the variable being declared based on the type of the initializer. Thus, in a local variable declaration, var is a placeholder for the actual, inferred type. However, when used in most other places, var is simply a user-defined identifier with no special meaning. For example, the following declaration is still valid: int var = 1; In this case, the type is explicitly specified as int and var is the name of the variable being declared. Even though it is context-sensitive, there are a few places in which the use of var is illegal. It cannot be used as the name of a class, for example. The following program puts the preceding discussion into action: // A simple demonstration of local variable type inference"
"Explain why when iterating over arrays, type must be compatible with the element type of the array"," Because the iteration variable receives values from the collection, type must be the same as (or compatible with) the elements stored in the collection. Hence, when iterating over arrays, type must be compatible with the element type of the array."," The general form of the for-each version of the for is shown here: for(type itr-var : collection) statement-block Here, type specifies the type and itr-var specifies the name of an iteration variable that will receive the elements from a collection, one at a time, from beginning to end. The collection being cycled through is specified by collection. There are various types of collections that can be used with the for, but the only type used in this chapter is the array. (Other types of collections that can be used with the for, such as those defined by the Collections Framework, are discussed later in this book.) With each iteration of the loop, the next element in the collection is retrieved and stored in itr-var. The loop repeats until all elements in the collection have been obtained. Because the iteration variable receives values from the collection, type must be the same as (or compatible with) the elements stored in the collection. Thus, when iterating over arrays, type must be compatible with the element type of the array. To understand the motivation behind a for-each style loop, consider the type of for loop that it is designed to replace. The following fragment uses a traditional for loop to compute the sum of the values in an array: int[] nums = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 }; int sum = 0; for(int i=0; i < 10; i++) sum += nums[i]; 21/09/21 5:40 PM 104PART I To compute the sum, each element in nums is read, in order, from start to finish. Thus, the entire array is read in strictly sequential order. This is accomplished by manually indexing the nums array by i, the loop control variable"
Explain how the return statement immediately terminates the method in which it is executed," At any time in a method, the return statement can be used to cause execution to branch back to the caller of the method. Hence, the return statement immediately terminates the method in which it is executed."," 21/09/21 5:40 PM The last control statement is return. The return statement is used to explicitly return from a method. That is, it causes program control to transfer back to the caller of the method. As such, it is categorized as a jump statement. Although a full discussion of return must wait until methods are discussed in Chapter 6, a brief look at return is presented here. At any time in a method, the return statement can be used to cause execution to branch back to the caller of the method. Thus, the return statement immediately terminates the method in which it is executed. The following example illustrates this point. Here, return causes execution to return to the Java run-time system, since it is the run-time system that calls main( ): return // Demonstrate return. class Return { public static void main(String[] args) { boolean t = true; System.out.println(""Before the return.""); if(t) return; // return to caller System.out.println(""This won't execute.""); } } The output from this program is shown here: Before the return"
"Explain how a class is a template for an object, and an object is an instance of a class","Perhaps the most important thing to understand about a class is that it defines a new data type. Once defined, this new type can be used to create objects of that type. Hence, a class is a template for an object, and an object is an instance of a class."," Class Fundamentals Classes have been used since the beginning of this book. However, until now, only the most rudimentary form of a class has been shown. The classes created in the preceding chapters primarily exist simply to encapsulate the main( ) method, which has been used to demonstrate the basics of the Java syntax. As you will see, classes are substantially more powerful than the limited ones presented so far. Perhaps the most important thing to understand about a class is that it defines a new data type. Once defined, this new type can be used to create objects of that type. Thus, a class is a template for an object, and an object is an instance of a class. Because an object is an instance of a class, you will often see the two words object and instance used interchangeably. The General Form of a Class When you define a class, you declare its exact form and nature. You do this by specifying the data that it contains and the code that operates on that data. While very simple classes may contain only code or only data, most real-world classes contain both. As you will see, a class code defines the interface to its data"
"Explain why as a general rule, it is the methods that determine how a class data can be used","The data, or variables, defined within a class are called instance variables. The code is contained within methods. Collectively, the methods and variables defined within a class are called members of the class. In most classes, the instance variables are acted upon and accessed by the methods defined for that class. Hence, as a general rule, it is the methods that determine how a class data can be used."," type instance-variableN; type methodname1(parameter-list) { // body of method } type methodname2(parameter-list) { // body of method } // ... type methodnameN(parameter-list) { // body of method } The data, or variables, defined within a class are called instance variables. The code is contained within methods. Collectively, the methods and variables defined within a class are called members of the class. In most classes, the instance variables are acted upon and accessed by the methods defined for that class. Thus, as a general rule, it is the methods that determine how a class data can be used. Variables defined within a class are called instance variables because each instance of the class (that is, each object of the class) contains its own copy of these variables. Thus, the data for one object is separate and unique from the data for another. We will come back to this point shortly, but it is an important concept to learn early"
Explain why the data for one object is separate and unique from the data for another," Variables defined within a class are called instance variables because each instance of the class (that is, each object of the class) contains its own copy of these variables. Hence, the data for one object is separate and unique from the data for another."," type methodnameN(parameter-list) { // body of method } The data, or variables, defined within a class are called instance variables. The code is contained within methods. Collectively, the methods and variables defined within a class are called members of the class. In most classes, the instance variables are acted upon and accessed by the methods defined for that class. Thus, as a general rule, it is the methods that determine how a class data can be used. Variables defined within a class are called instance variables because each instance of the class (that is, each object of the class) contains its own copy of these variables. Thus, the data for one object is separate and unique from the data for another. We will come back to this point shortly, but it is an important concept to learn early. All methods have the same general form as main( ), which we have been using thus far"
"Explain why in Java, all class objects must be dynamically allocated","Instead, it is simply a variable that can refer to an object. Second, you must acquire an actual, physical copy of the object and assign it to that variable. You can do this using the new operator. The new operator dynamically allocates (that is, allocates at run time) memory for an object and returns a reference to it. This reference is, essentially, the address in memory of the object allocated by new. This reference is then stored in the variable. Hence, in Java, all class objects must be dynamically allocated."," First, you must declare a variable of the class type. This variable does not define an object. Instead, it is simply a variable that can refer to an object. Second, you must acquire an actual, physical copy of the object and assign it to that variable. You can do this using the new operator. The new operator dynamically allocates (that is, allocates at run time) memory for an object and returns a reference to it. This reference is, essentially, the address in memory of the object allocated by new. This reference is then stored in the variable. Thus, in Java, all class objects must be dynamically allocated. Lets look at the details of this procedure. In the preceding sample programs, a line similar to the following is used to declare an object of type Box: Box mybox = new Box(); This statement combines the two steps just described. It can be rewritten like this to show each step more clearly: Box mybox; // declare reference to object mybox = new Box(); // allocate a Box object The first line declares mybox as a reference to an object of type Box. At this point, mybox does not yet refer to an actual object. The next line allocates an object and assigns a reference to it to mybox. After the second line executes, you can use mybox as if it were a Box object"
Explain why a class is a logical construct,"A class creates a new data type that can be used to create objects. That is, a class creates a logical framework that defines the relationship between its members. When you declare an object of a class, you are creating an instance of that class. Hence, a class is a logical construct."," The advantage of this approach is that your program can create as many or as few objects as it needs during the execution of your program. However, since memory is finite, it is possible that new will not be able to allocate memory for an object because insufficient memory exists. If this happens, a run-time exception will occur. (You will learn how to handle exceptions in Chapter 10.) For the sample programs in this book, you wont need to worry about running out of memory, but you will need to consider this possibility in real-world programs that you write. Lets once again review the distinction between a class and an object. A class creates a new data type that can be used to create objects. That is, a class creates a logical framework that defines the relationship between its members. When you declare an object of a class, you are creating an instance of that class. Thus, a class is a logical construct. An object has physical reality. (That is, an object occupies space in memory.) It is important to keep this distinction clearly in mind. Assigning Object Reference Variables Object reference variables act differently than you might expect when an assignment takes place. For example, what do you think the following fragment does? Box b1 = new Box(); Box b2 = b1; 22/09/21 6:36 PM Introducing Classes You might think that b2 is being assigned a reference to a copy of the object referred to by b1. That is, you might think that b1 and b2 refer to separate and distinct objects. However, this would be wrong. Instead, after this fragment executes, b1 and b2 will both refer to the same object. The assignment of b1 to b2 did not allocate any memory or copy any part of the original object. It simply makes b2 refer to the same object as does b1. Thus, any changes made to the object through b2 will affect the object to which b1 is referring, since they are the same object"
Explain why overloaded methods must differ in the type and/or number of their parameters," When an overloaded method is invoked, Java uses the type and/or number of arguments as its guide to determine which version of the overloaded method to actually call. Hence, overloaded methods must differ in the type and/or number of their parameters."," Method overloading is one of the ways that Java supports polymorphism. If you have never used a language that allows the overloading of methods, then the concept may seem strange at first. But as you will see, method overloading is one of Javas most exciting and useful features. When an overloaded method is invoked, Java uses the type and/or number of arguments as its guide to determine which version of the overloaded method to actually call. Thus, overloaded methods must differ in the type and/or number of their parameters. While overloaded methods may have different return types, the return type alone is insufficient to distinguish two versions of a method. When Java encounters a call to an overloaded method, it simply executes the version of the method whose parameters match the arguments used in the call. Here is a simple example that illustrates method overloading: // Demonstrate method overloading"
"Explain why while you can use the same name to overload unrelated methods, you should not","When you overload a method, each version of that method can perform any activity you desire. There is no rule stating that overloaded methods must relate to one another. However, from a stylistic point of view, method overloading implies a relationship. Hence, while you can use the same name to overload unrelated methods, you should not."," Although this example is fairly simple, if you expand the concept, you can see how overloading can help you manage greater complexity. When you overload a method, each version of that method can perform any activity you desire. There is no rule stating that overloaded methods must relate to one another. However, from a stylistic point of view, method overloading implies a relationship. Thus, while you can use the same name to overload unrelated methods, you should not. For example, you could use the name sqr to create methods that return the square of an integer and the square root of a floating-point value. But these two operations are fundamentally different. Applying method overloading in this manner defeats its original purpose. In practice, you should only overload closely related operations. Overloading Constructors In addition to overloading normal methods, you can also overload constructor methods. In fact, for most real-world classes that you create, overloaded constructors will be the norm, not the exception. To understand why, lets return to the Box class developed in the preceding chapter. Following is the latest version of Box: class Box { double width; double height; double depth; // This is the constructor for Box"
"Explain why a copy of the argument is made, and what occurs to the parameter that receives the argument has no effect outside the method"," When you pass a primitive type to a method, it is passed by value. Hence, a copy of the argument is made, and what occurs to the parameter that receives the argument has no effect outside the method."," The first way is call-by-value. This approach copies the value of an argument into the formal parameter of the subroutine. Therefore, changes made to the parameter of the subroutine have no effect on the argument. The second way an argument can be passed is call-byreference. In this approach, a reference to an argument (not the value of the argument) is passed to the parameter. Inside the subroutine, this reference is used to access the actual argument specified in the call. This means that changes made to the parameter will affect the argument used to call the subroutine. As you will see, although Java uses call-by-value to pass all arguments, the precise effect differs between whether a primitive type or a reference type is passed. When you pass a primitive type to a method, it is passed by value. Thus, a copy of the argument is made, and what occurs to the parameter that receives the argument has no effect outside the method. For example, consider the following program: // Primitive types are passed by value. class Test { void meth(int i, int j) { i *= 2; j /= 2; } } 21/09/21 5:41 PM class CallByValue { public static void main(String[] args) { Test ob = new Test(); int a = 15, b = 20; System.out.println(""a and b before call: "" + a + "" "" + b); ob.meth(a, b); System.out.println(""a and b after call: "" + a + "" "" + b); } } The output from this program is shown here: a and b before call: 15 20 a and b after call: 15 20 As you can see, the operations that occur inside meth( ) have no effect on the values of a and b used in the call; their values here did not change to 30 and 10"
"Explain why when correctly implemented, a class creates a black box which may be used, but the inner workings of which are not open to tampering","As you know, encapsulation links data with the code that manipulates it. However, encapsulation provides another important attribute: access control. Through encapsulation, you can control what parts of a program can access the members of a class. By controlling access, you can prevent misuse. For example, allowing access to data only through a welldefined set of methods, you can prevent the misuse of that data. Hence, when correctly implemented, a class creates a black box which may be used, but the inner workings of which are not open to tampering."," // Another example that uses recursion. class RecTest { int[] values; 21/09/21 5:41 PM RecTest(int i) { values = new int[i]; } // display array -- recursively void printArray(int i) { if(i==0) return; else printArray(i-1); System.out.println(""["" + (i-1) + ""] "" + values[i-1]); } } class Recursion2 { public static void main(String[] args) { RecTest ob = new RecTest(10); int i; for(i=0; i<10; i++) ob.values[i] = i; ob.printArray(10); } } This program generates the following output: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] 1 3 5 7 9 Introducing Access Control As you know, encapsulation links data with the code that manipulates it. However, encapsulation provides another important attribute: access control. Through encapsulation, you can control what parts of a program can access the members of a class. By controlling access, you can prevent misuse. For example, allowing access to data only through a welldefined set of methods, you can prevent the misuse of that data. Thus, when correctly implemented, a class creates a black box which may be used, but the inner workings of which are not open to tampering. However, the classes that were presented earlier do not completely meet this goal. For example, consider the Stack class shown at the end of Chapter 6. While it is true that the methods push( ) and pop( ) do provide a controlled interface to the stack, this interface is not enforced. That is, it is possible for another part of the program to bypass these methods and access the stack directly. Of course, in the wrong hands, this could lead to trouble. In this section, you will be introduced to the mechanism by which you can precisely control access to the various members of a class. 21/09/21 5:41 PM 150PART I How a member can be accessed is determined by the access modifier attached to its declaration. Java supplies a rich set of access modifiers. Some aspects of access control are related mostly to inheritance or packages. (A package is, essentially, a grouping of classes.) These parts of Javas access control mechanism will be discussed in subsequent chapters"
Explain why super( ) always refers to the superclass immediately above the calling class,"Lets review the key concepts behind super( ). When a subclass calls super( ), it is calling the constructor of its immediate superclass. Hence, super( ) always refers to the superclass immediately above the calling class."," class BoxWeight extends Box { double weight; // weight of box // construct clone of an object BoxWeight(BoxWeight ob) { // pass object to constructor super(ob); weight = ob.weight; } // constructor when all parameters are specified BoxWeight(double w, double h, double d, double m) { 21/09/21 5:41 PM } // default constructor BoxWeight() { super(); weight = -1; } super(w, h, d); // call superclass constructor weight = m; // constructor used when cube is created BoxWeight(double len, double m) { super(len); weight = m; } } class DemoSuper { public static void main(String[] args) { BoxWeight mybox1 = new BoxWeight(10, 20, 15, 34.3); BoxWeight mybox2 = new BoxWeight(2, 3, 4, 0.076); BoxWeight mybox3 = new BoxWeight(); // default BoxWeight mycube = new BoxWeight(3, 2); BoxWeight myclone = new BoxWeight(mybox1); double vol; vol = mybox1.volume(); System.out.println(""Volume of mybox1 is "" + vol); System.out.println(""Weight of mybox1 is "" + mybox1.weight); System.out.println(); vol = mybox2.volume(); System.out.println(""Volume of mybox2 is "" + vol); System.out.println(""Weight of mybox2 is "" + mybox2.weight); System.out.println(); vol = mybox3.volume(); System.out.println(""Volume of mybox3 is "" + vol); System.out.println(""Weight of mybox3 is "" + mybox3.weight); System.out.println(); vol = myclone.volume(); System.out.println(""Volume of myclone is "" + vol); System.out.println(""Weight of myclone is "" + myclone.weight); System.out.println(); vol = mycube.volume(); System.out.println(""Volume of mycube is "" + vol); System.out.println(""Weight of mycube is "" + mycube.weight); System.out.println(); } } 21/09/21 5:41 PM 180PART I This program generates the following output: Volume of mybox1 is 3000.0 Weight of mybox1 is 34.3 Volume of mybox2 is 24.0 Weight of mybox2 is 0.076 Volume of mybox3 is -1.0 Weight of mybox3 is -1.0 Volume of myclone is 3000.0 Weight of myclone is 34.3 Volume of mycube is 27.0 Weight of mycube is 2.0 Pay special attention to this constructor in BoxWeight: // construct clone of an object BoxWeight(BoxWeight ob) { // pass object to constructor super(ob); weight = ob.weight; } Notice that super( ) is passed an object of type BoxWeightnot of type Box. This still invokes the constructor Box(Box ob). As mentioned earlier, a superclass variable can be used to reference any object derived from that class. Thus, we are able to pass a BoxWeight object to the Box constructor. Of course, Box only has knowledge of its own members. Lets review the key concepts behind super( ). When a subclass calls super( ), it is calling the constructor of its immediate superclass. Thus, super( ) always refers to the superclass immediately above the calling class. This is true even in a multileveled hierarchy. Also, super( ) must always be the first statement executed inside a subclass constructor. A Second Use for super The second form of super acts somewhat like this, except that it always refers to the superclass of the subclass in which it is used. This usage has the following general form: super.member Here, member can be either a method or an instance variable"
"Explain why by combining inheritance with overridden methods, a superclass can define the general form of the methods that will be used by all of its subclasses","Used correctly, the superclass provides all elements that a subclass can use directly. It also defines those methods that the derived class must implement on its own. This allows the subclass the flexibility to define its own methods, yet still enforces a consistent interface. Hence, by combining inheritance with overridden methods, a superclass can define the general form of the methods that will be used by all of its subclasses."," Part of the key to successfully applying polymorphism is understanding that the superclasses and subclasses form a hierarchy which moves from lesser to greater specialization. Used correctly, the superclass provides all elements that a subclass can use directly. It also defines those methods that the derived class must implement on its own. This allows the subclass the flexibility to define its own methods, yet still enforces a consistent interface. Thus, by combining inheritance with overridden methods, a superclass can define the general form of the methods that will be used by all of its subclasses. Dynamic, run-time polymorphism is one of the most powerful mechanisms that objectoriented design brings to bear on code reuse and robustness. The ability of existing code libraries to call methods on instances of new classes without recompiling while maintaining a clean abstract interface is a profoundly powerful tool"
Explain why it is now possible for interface to specify some behavior," Before continuing an important point needs to be made. JDK 8 added a feature to interface that made a significant change to its capabilities. Prior to JDK 8, an interface could not define any implementation whatsoever. This is the type of interface that the preceding simplified form shows, in which no method declaration supplies a body. Thus, prior to JDK 8, an interface could define only what, but not how. JDK 8 changed this. Beginning with JDK 8, it is possible to add a default implementation to an interface method. Furthermore, JDK 8 also added static interface methods, and beginning with JDK 9, an interface can include private methods. Hence, it is now possible for interface to specify some behavior."," return-type method-nameN(parameter-list); type final-varnameN = value; When no access modifier is included, then default access results, and the interface is only available to other members of the package in which it is declared. When it is declared as public, the interface can be used by code outside its package. In this case, the interface must be the only public interface declared in the file, and the file must have the same name as the interface. name is the name of the interface, and can be any valid identifier. Notice that the methods that are declared have no bodies. They end with a semicolon after the parameter list. They are, essentially, abstract methods. Each class that includes such an interface must implement all of the methods. Before continuing an important point needs to be made. JDK 8 added a feature to interface that made a significant change to its capabilities. Prior to JDK 8, an interface could not define any implementation whatsoever. This is the type of interface that the preceding simplified form shows, in which no method declaration supplies a body. Thus, prior to JDK 8, an interface could define only what, but not how. JDK 8 changed this. Beginning with JDK 8, it is possible to add a default implementation to an interface method. Furthermore, JDK 8 also added static interface methods, and beginning with JDK 9, an interface can include private methods. Thus, it is now possible for interface to specify some behavior. However, such methods constitute what are, in essence, special-use features, and the original intent behind interface still remains. Therefore, as a general rule, you will still often create and use interfaces in which no use is made of these new features. For this reason, we will begin by discussing the interface in its traditional form. The newer interface features are described at the end of this chapter"
"Explain why outside of the class or interface in which a nested interface is declared, its name must be fully qualified","An interface can be declared a member of a class or another interface. Such an interface is called a member interface or a nested interface. A nested interface can be declared as public, private, or protected. This differs from a top-level interface, which must either be declared as public or use the default access level, as previously described. When a nested interface is used outside of its enclosing scope, it must be qualified by the name of the class or interface of which it is a member. Hence, outside of the class or interface in which a nested interface is declared, its name must be fully qualified."," } Here, the class Incomplete does not implement callback( ) and must be declared as abstract. Any class that inherits Incomplete must implement callback( ) or be declared abstract itself. Nested Interfaces An interface can be declared a member of a class or another interface. Such an interface is called a member interface or a nested interface. A nested interface can be declared as public, private, or protected. This differs from a top-level interface, which must either be declared as public or use the default access level, as previously described. When a nested interface is used outside of its enclosing scope, it must be qualified by the name of the class or interface of which it is a member. Thus, outside of the class or interface in which a nested interface is declared, its name must be fully qualified. Here is an example that demonstrates a nested interface: // A nested interface example"
Explain why the addition of a default method will not cause preexisting code to break,"A primary motivation for the default method was to provide a means by which interfaces could be expanded without breaking existing code. Recall that there must be implementations for all methods defined by an interface. In the past, if a new method were added to a popular, widely used interface, then the addition of that method would break existing code because no implementation would be found for that new method. The default method solves this problem by supplying an implementation that will be used if no other implementation is explicitly provided. Hence, the addition of a default method will not cause preexisting code to break."," Default Interface Methods As explained earlier, prior to JDK 8, an interface could not define any implementation whatsoever. This meant that for all previous versions of Java, the methods specified by an interface were abstract, containing no body. This is the traditional form of an interface and is the type of interface that the preceding discussions have used. The release of JDK 8 changed this by adding a new capability to interface called the default method. A default method lets you define a default implementation for an interface method. In other words, by use of a default method, it is possible for an interface method to provide a body, rather than being abstract. During its development, the default method was also referred to as an extension method, and you will likely see both terms used. A primary motivation for the default method was to provide a means by which interfaces could be expanded without breaking existing code. Recall that there must be implementations for all methods defined by an interface. In the past, if a new method were added to a popular, widely used interface, then the addition of that method would break existing code because no implementation would be found for that new method. The default method solves this problem by supplying an implementation that will be used if no other implementation is explicitly provided. Thus, the addition of a default method will not cause preexisting code to break. 21/09/21 5:43 PM 220PART I Another motivation for the default method was the desire to specify methods in an interface that are, essentially, optional, depending on how the interface is used. For example, an interface might define a group of methods that act on a sequence of elements. One of these methods might be called remove( ), and its purpose is to remove an element from the sequence. However, if the interface is intended to support both modifiable and nonmodifiable sequences, then remove( ) is essentially optional because it wont be used by nonmodifiable sequences. In the past, a class that implemented a nonmodifiable sequence would have had to define an empty implementation of remove( ), even though it was not needed. Today, a default implementation for remove( ) can be specified in the interface that does nothing (or throws an exception). Providing this default prevents a class used for nonmodifiable sequences from having to define its own, placeholder version of remove( ). Thus, by providing a default, the interface makes the implementation of remove( ) by a class optional"
"Explain how to a limited extent, default methods do support multiple inheritance of behavior","The preceding notwithstanding, default methods do offer a bit of what one would normally associate with the concept of multiple inheritance. For example, you might have a class that implements two interfaces. If each of these interfaces provides default methods, then some behavior is inherited from both. Hence, to a limited extent, default methods do support multiple inheritance of behavior."," Now that an interface can include default methods, you might be wondering if an interface can provide a way around this restriction. The answer is, essentially, no. Recall that there is 21/09/21 5:43 PM Packages and Interfaces still a key difference between a class and an interface: a class can maintain state information (especially through the use of instance variables), but an interface cannot. The preceding notwithstanding, default methods do offer a bit of what one would normally associate with the concept of multiple inheritance. For example, you might have a class that implements two interfaces. If each of these interfaces provides default methods, then some behavior is inherited from both. Thus, to a limited extent, default methods do support multiple inheritance of behavior. As you might guess, in such a situation, it is possible that a name conflict will occur. For example, assume that two interfaces called Alpha and Beta are implemented by a class called MyClass. What happens if both Alpha and Beta provide a method called reset( ) for which both declare a default implementation? Is the version by Alpha or the version by Beta used by MyClass? Or, consider a situation in which Beta extends Alpha. Which version of the default method is used? Or, what if MyClass provides its own implementation of the method? To handle these and other similar types of situations, Java defines a set of rules that resolves such conflicts"
"Explain why no implementation of the interface is necessary, and no instance of the interface is required, in order to call a static method","Like static methods in a class, a static method defined by an interface can be called independently of any object. Hence, no implementation of the interface is necessary, and no instance of the interface is required, in order to call a static method."," In cases in which one interface inherits another, with both defining a common default method, the inheriting interfaces version of the method takes precedence. Therefore, continuing the example, if Beta extends Alpha, then Betas version of reset( ) will be used. It is possible to explicitly refer to a default implementation in an inherited interface by using this form of super. Its general form is shown here: InterfaceName.super.methodName( ) For example, if Beta wants to refer to Alphas default for reset( ), it can use this statement: Alpha.super.reset(); Use static Methods in an Interface Another capability added to interface by JDK 8 is the ability to define one or more static methods. Like static methods in a class, a static method defined by an interface can be called independently of any object. Thus, no implementation of the interface is necessary, and no instance of the interface is required, in order to call a static method. Instead, a static method is called by specifying the interface name, followed by a period, followed by the method name. Here is the general form: InterfaceName.staticMethodName Notice that this is similar to the way that a static method in a class is called"
Explain why a subclass would never be reached if it came after its superclass,"When you use multiple catch statements, it is important to remember that exception subclasses must come before any of their superclasses. This is because a catch statement that uses a superclass will catch exceptions of that type plus any of its subclasses. Hence, a subclass would never be reached if it came after its superclass."," C:\>java MultipleCatches TestArg a = 1 Array index oob: java.lang.ArrayIndexOutOfBoundsException: Index 42 out of bounds for length 1 After try/catch blocks. When you use multiple catch statements, it is important to remember that exception subclasses must come before any of their superclasses. This is because a catch statement that uses a superclass will catch exceptions of that type plus any of its subclasses. Thus, a subclass would never be reached if it came after its superclass. Further, in Java, unreachable code is an error. For example, consider the following program: /* This program contains an error. A subclass must come before its superclass in a series of catch statements. If not, unreachable code will be created and a compile-time error will result"
"Explain why all exceptions, including those that you create, have the methods defined by Throwable available to them","The Exception class does not define any methods of its own. It does, of course, inherit those methods provided by Throwable. Hence, all exceptions, including those that you create, have the methods defined by Throwable available to them."," Part I Table 10-2 Javas Checked Exceptions Defined in java.lang Creating Your Own Exception Subclasses Although Javas built-in exceptions handle most common errors, you will probably want to create your own exception types to handle situations specific to your applications. This is quite easy to do: just define a subclass of Exception (which is, of course, a subclass of Throwable). Your subclasses dont need to actually implement anythingit is their existence in the type system that allows you to use them as exceptions. The Exception class does not define any methods of its own. It does, of course, inherit those methods provided by Throwable. Thus, all exceptions, including those that you create, have the methods defined by Throwable available to them. They are shown in Table 10-3. You may also wish to override one or more of these methods in exception classes that you create"
Explain why you can associate a cause with an exception after the exception has been created,"If there is no underlying exception, null is returned. The initCause( ) method associates causeExc with the invoking exception and returns a reference to the exception. Hence, you can associate a cause with an exception after the exception has been created."," Throwable getCause( ) Throwable initCause(Throwable causeExc) The getCause( ) method returns the exception that underlies the current exception. If there is no underlying exception, null is returned. The initCause( ) method associates causeExc with the invoking exception and returns a reference to the exception. Thus, you can associate a cause with an exception after the exception has been created. However, the cause exception can be set only once. This means that you can call initCause( ) only once for each exception object. Furthermore, if the cause exception was set by a constructor, then you cant set it again using initCause( ). In general, initCause( ) is used to set a cause for legacy exception classes that dont support the two additional constructors described earlier. Here is an example that illustrates the mechanics of handling chained exceptions: // Demonstrate exception chaining"
"Explain why the cause exception can, itself, have a cause"," Chained exceptions can be carried on to whatever depth is necessary. Hence, the cause exception can, itself, have a cause."," class ChainExcDemo { static void demoproc() { // create an exception NullPointerException e = new NullPointerException(""top layer""); // add a cause e.initCause(new ArithmeticException(""cause"")); throw e; } 22/09/21 6:36 PM public static void main(String[] args) { try { demoproc(); } catch(NullPointerException e) { // display top level exception System.out.println(""Caught: "" + e); // display cause exception System.out.println(""Original cause: "" + e.getCause()); } } } The output from the program is shown here: Caught: java.lang.NullPointerException: top layer Original cause: java.lang.ArithmeticException: cause In this example, the top-level exception is NullPointerException. To it is added a cause exception, ArithmeticException. When the exception is thrown out of demoproc( ), it is caught by main( ). There, the top-level exception is displayed, followed by the underlying exception, which is obtained by calling getCause( ). Chained exceptions can be carried on to whatever depth is necessary. Thus, the cause exception can, itself, have a cause. Be aware that overly long chains of exceptions may indicate poor design. Chained exceptions are not something that every program will need. However, in cases in which knowledge of an underlying cause is useful, they offer an elegant solution"
"Explain why when a method can fail, have it throw an exception","Exception handling provides a powerful mechanism for controlling complex programs that have many dynamic run-time characteristics. It is important to think of try, throw, and catch as clean ways to handle errors and unusual boundary conditions in your programs logic. Instead of using error return codes to indicate failure, use Javas exception handling capabilities. Hence, when a method can fail, have it throw an exception."," The more precise rethrow feature restricts the type of exceptions that can be rethrown to only those checked exceptions that the associated try block throws, that are not handled by a preceding catch clause, and that are a subtype or supertype of the parameter. Although this capability might not be needed often, it is now available for use. For the more precise rethrow feature to be in force, the catch parameter must be either effectively final, which means that it must not be assigned a new value inside the catch block, or explicitly declared final. Using Exceptions Exception handling provides a powerful mechanism for controlling complex programs that have many dynamic run-time characteristics. It is important to think of try, throw, and catch as clean ways to handle errors and unusual boundary conditions in your programs logic. Instead of using error return codes to indicate failure, use Javas exception handling capabilities. Thus, when a method can fail, have it throw an exception. This is a cleaner way to handle failure modes. One last point: Javas exception-handling statements should not be considered a general mechanism for nonlocal branching. If you do so, it will only confuse your code and make it hard to maintain"
Explain how multithreading is a specialized form of multitasking,"Java provides built-in support for multithreaded programming. A multithreaded program contains two or more parts that can run concurrently. Each part of such a program is called a thread, and each thread defines a separate path of execution. Hence, multithreading is a specialized form of multitasking."," One last point: Javas exception-handling statements should not be considered a general mechanism for nonlocal branching. If you do so, it will only confuse your code and make it hard to maintain. 22/09/21 6:36 PM CHAPTER Multithreaded Programming Java provides built-in support for multithreaded programming. A multithreaded program contains two or more parts that can run concurrently. Each part of such a program is called a thread, and each thread defines a separate path of execution. Thus, multithreading is a specialized form of multitasking. You are almost certainly acquainted with multitasking because it is supported by virtually all modern operating systems. However, there are two distinct types of multitasking: process-based and thread-based. It is important to understand the difference between the two. For many readers, process-based multitasking is the more familiar form. A process is, in essence, a program that is executing. Thus, process-based multitasking is the feature that allows your computer to run two or more programs concurrently. For example, processbased multitasking enables you to run the Java compiler at the same time that you are using a text editor or visiting a web site. In process-based multitasking, a program is the smallest unit of code that can be dispatched by the scheduler"
"Explain why process-based multitasking deals with the big picture, and thread-based multitasking handles the details","In a thread-based multitasking environment, the thread is the smallest unit of dispatchable code. This means that a single program can perform two or more tasks simultaneously. For instance, a text editor can format text at the same time that it is printing, as long as these two actions are being performed by two separate threads. Hence, process-based multitasking deals with the big picture, and thread-based multitasking handles the details."," You are almost certainly acquainted with multitasking because it is supported by virtually all modern operating systems. However, there are two distinct types of multitasking: process-based and thread-based. It is important to understand the difference between the two. For many readers, process-based multitasking is the more familiar form. A process is, in essence, a program that is executing. Thus, process-based multitasking is the feature that allows your computer to run two or more programs concurrently. For example, processbased multitasking enables you to run the Java compiler at the same time that you are using a text editor or visiting a web site. In process-based multitasking, a program is the smallest unit of code that can be dispatched by the scheduler. In a thread-based multitasking environment, the thread is the smallest unit of dispatchable code. This means that a single program can perform two or more tasks simultaneously. For instance, a text editor can format text at the same time that it is printing, as long as these two actions are being performed by two separate threads. Thus, process-based multitasking deals with the big picture, and thread-based multitasking handles the details. Multitasking threads require less overhead than multitasking processes. Processes are heavyweight tasks that require their own separate address spaces. Interprocess communication is expensive and limited. Context switching from one process to another is also costly. Threads, on the other hand, are lighter weight. They share the same address space and cooperatively share the same heavyweight process. Interthread communication is inexpensive, and context switching from one thread to the next is lower in cost. While Java programs make use of process-based multitasking environments, process-based multitasking is not under Javas direct control. However, multithreaded multitasking is"
Explain how the corrupted data might be used by another thread that is waiting on the same lock," The trouble is that stop( ) causes any lock the calling thread holds to be released. Hence, the corrupted data might be used by another thread that is waiting on the same lock."," If that thread is stopped at that point, that data structure might be left in a corrupted state. The trouble is that stop( ) causes any lock the calling thread holds to be released. Thus, the corrupted data might be used by another thread that is waiting on the same lock. Because you cant now use the suspend( ), resume( ), or stop( ) methods to control a thread, you might be thinking that no way exists to pause, restart, or terminate a thread. But, fortunately, this is not true. Instead, a thread must be designed so that the run( ) method periodically checks to determine whether that thread should suspend, resume, or stop its own execution. Typically, this is accomplished by establishing a flag variable that indicates the execution state of the thread. As long as this flag is set to running, the run( ) method must continue to let the thread execute. If this variable is set to suspend, the thread must pause. If it is set to stop, the thread must terminate. Of course, a variety of ways exist in which to write such code, but the central theme will be the same for all programs"
Explain why an enumeration object can hold only a value that was declared in the list," Enumerations In its simplest form, an enumeration is a list of named constants that define a new data type and its legal values. Hence, an enumeration object can hold only a value that was declared in the list."," 22/09/21 6:37 PM CHAPTER Enumerations, Autoboxing, and Annotations This chapter examines three features that were not originally part of Java, but over time each has become a near indispensable aspect of Java programming: enumerations, autoboxing, and annotations. Originally added by JDK 5, each is a feature upon which Java programmers have come to rely because each offers a streamlined approach to handling common programming tasks. This chapter also discusses Javas type wrappers and introduces reflection. Enumerations In its simplest form, an enumeration is a list of named constants that define a new data type and its legal values. Thus, an enumeration object can hold only a value that was declared in the list. Other values are not allowed. In other words, an enumeration gives you a way to explicitly specify the only values that a data type can legally have. Enumerations are commonly used to define a set of values that represent a collection of items. For example, you might use an enumeration to represent the error codes that can result from some operation, such as success, failed, or pending; or a list of the states that a device might be in, such as running, stopped, or paused. In early versions of Java, such values were defined using final variables, but enumerations offer a far superior approach. Although Java enumerations might, at first glance, appear similar to enumerations in other languages, this similarity may be only skin deep because, in Java, an enumeration defines a class type. By making enumerations into classes, the capabilities of the enumeration are greatly expanded. For example, in Java, an enumeration can have constructors, methods, and instance variables. Because of their power and flexibility, enumerations are widely used throughout the Java API library"
"Explain why when you define a constructor for an enum, the constructor is called when each enumeration constant is created"," It is important to understand that each enumeration constant is an object of its enumeration type. Hence, when you define a constructor for an enum, the constructor is called when each enumeration constant is created."," Java Enumerations Are Class Types As mentioned, a Java enumeration is a class type. Although you dont instantiate an enum using new, it otherwise has much the same capabilities as other classes. The fact that enum defines a class gives the Java enumeration extraordinary power. For example, you can give them constructors, add instance variables and methods, and even implement interfaces. It is important to understand that each enumeration constant is an object of its enumeration type. Thus, when you define a constructor for an enum, the constructor is called when each enumeration constant is created. Also, each enumeration constant has its own copy of any instance variables defined by the enumeration. For example, consider the following version of Apple: // Use an enum constructor, instance variable, and method. enum Apple { Jonathan(10), GoldenDel(9), RedDel(12), Winesap(15), Cortland(8); private int price; // price of each apple // Constructor Apple(int p) { price = p; } int getPrice() { return price; } } 21/09/21 5:45 PM 282PART I class EnumDemo3 { public static void main(String[] args) { Apple ap; // Display price of Winesap"
"Explain why the primitive types are not part of the object hierarchy, and they do not inherit Object","As you know, Java uses primitive types (also called simple types), such as int or double, to hold the basic data types supported by the language. Primitive types, rather than objects, are used for these quantities for the sake of performance. Using objects for these values would add an unacceptable overhead to even the simplest of calculations. Hence, the primitive types are not part of the object hierarchy, and they do not inherit Object."," import java.util.Random; // An enumeration of the possible answers. enum Answers { NO, YES, MAYBE, LATER, SOON, NEVER } class Question { Random rand = new Random(); Answers ask() { int prob = (int) (100 * rand.nextDouble()); if (prob < 15) return Answers.MAYBE; // 15% else if (prob < 30) return Answers.NO; // 15% else if (prob < 60) return Answers.YES; // 30% 21/09/21 5:45 PM 286PART I else if (prob < 75) return Answers.LATER; // 15% else if (prob < 98) return Answers.SOON; // 13% else return Answers.NEVER; // 2% } } class AskMe { static void answer(Answers result) { switch(result) { case NO: System.out.println(""No""); break; case YES: System.out.println(""Yes""); break; case MAYBE: System.out.println(""Maybe""); break; case LATER: System.out.println(""Later""); break; case SOON: System.out.println(""Soon""); break; case NEVER: System.out.println(""Never""); break; } } public static void main(String[] args) { Question q = new Question(); answer(q.ask()); answer(q.ask()); answer(q.ask()); answer(q.ask()); } } Type Wrappers As you know, Java uses primitive types (also called simple types), such as int or double, to hold the basic data types supported by the language. Primitive types, rather than objects, are used for these quantities for the sake of performance. Using objects for these values would add an unacceptable overhead to even the simplest of calculations. Thus, the primitive types are not part of the object hierarchy, and they do not inherit Object. Despite the performance benefit offered by the primitive types, there are times when you will need an object representation. For example, you cant pass a primitive type by reference 21/09/21 5:45 PM to a method. Also, many of the standard data structures implemented by Java operate on objects, which means that you cant use these data structures to store primitive types. To handle these (and other) situations, Java provides type wrappers, which are classes that encapsulate a primitive type within an object. The type wrapper classes are described in detail in Part II, but they are introduced here because they relate directly to Javas autoboxing feature"
Explain how the same I/O classes and methods can be applied to different types of devices,"Java programs perform I/O through streams. A stream is an abstraction that either produces or consumes information. A stream is linked to a physical device by the Java I/O system. All streams behave in the same manner, even if the actual physical devices to which they are linked differ. Hence, the same I/O classes and methods can be applied to different types of devices."," The preceding paragraph notwithstanding, Java does provide strong, flexible support for I/O as it relates to files and networks. Javas I/O system is cohesive and consistent. In fact, once you understand its fundamentals, the rest of the I/O system is easy to master. A general overview of I/O is presented here. A detailed description is found in Chapters 22 and 23. 13-ch13.indd 315 CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 The Java Language Streams Java programs perform I/O through streams. A stream is an abstraction that either produces or consumes information. A stream is linked to a physical device by the Java I/O system. All streams behave in the same manner, even if the actual physical devices to which they are linked differ. Thus, the same I/O classes and methods can be applied to different types of devices. This means that an input stream can abstract many different kinds of input: from a disk file, a keyboard, or a network socket. Likewise, an output stream may refer to the console, a disk file, or a network connection. Streams are a clean way to deal with input/ output without having every part of your code understand the difference between a keyboard and a network, for example. Java implements streams within class hierarchies defined in the java.io package. NOTE In addition to the stream-based I/O defined in java.io, Java also provides buffer- and channel-based I/O, which is defined in java.nio and its subpackages. They are described in Chapter 23"
Explain why there is no need to call close( ) explicitly,"Typically, resource-specification is a statement that declares and initializes a resource, such as a file stream. It consists of a variable declaration in which the variable is initialized with a reference to the object being managed. When the try block ends, the resource is automatically released. In the case of a file, this means that the file is automatically closed. (Hence, there is no need to call close( ) explicitly."," Automatic resource management is based on an expanded form of the try statement. Here is its general form: try (resource-specification) { // use the resource } Typically, resource-specification is a statement that declares and initializes a resource, such as a file stream. It consists of a variable declaration in which the variable is initialized with a reference to the object being managed. When the try block ends, the resource is automatically released. In the case of a file, this means that the file is automatically closed. (Thus, there is no need to call close( ) explicitly.) Of course, this form of try can also include catch and finally clauses. This form of try is called the try-with-resources statement. NOTE Beginning with JDK 9, it is also possible for the resource specification of the try to consist of a variable that has been declared and initialized earlier in the program. However, that variable must be effectively final, which means that it has not been assigned a new value after being given its initial value"
Explain how generics expanded your ability to reuse code and let you do so safely and easily,"Generics added the type safety that was lacking. They also streamlined the process, because it is no longer necessary to explicitly employ casts to translate between Object and the type of data that is actually being operated upon. With generics, all casts are automatic and implicit. Hence, generics expanded your ability to reuse code and let you do so safely and easily."," Because Object is the superclass of all other classes, an Object reference can refer to any type object. Thus, in pre-generics code, generalized classes, interfaces, and methods used Object references to operate on various types of objects. The problem was that they could not do so with type safety. Generics added the type safety that was lacking. They also streamlined the process, because it is no longer necessary to explicitly employ casts to translate between Object and the type of data that is actually being operated upon. With generics, all casts are automatic and implicit. Thus, generics expanded your ability to reuse code and let you do so safely and easily. CAUTION A Warning to C++ Programmers: Although generics are similar to templates in C++, they are not the same. There are some fundamental differences between the two approaches to generic types. If you have a background in C++, it is important not to jump to conclusions about how generics work in Java"
"Explain why superclass defines an inclusive, upper limit"," When specifying a type parameter, you can create an upper bound that declares the superclass from which all type arguments must be derived. This is accomplished through the use of an extends clause when specifying the type parameter, as shown here: <T extends superclass> This specifies that T can only be replaced by superclass, or subclasses of superclass. Hence, superclass defines an inclusive, upper limit."," double average() { double sum = 0.0; for(int i=0; i < nums.length; i++) sum += nums[i].doubleValue(); // Error!!! return sum / nums.length; } } In Stats, the average( ) method attempts to obtain the double version of each number in the nums array by calling doubleValue( ). Because all numeric classes, such as Integer and Double, are subclasses of Number, and Number defines the doubleValue( ) method, this method is available to all numeric wrapper classes. The trouble is that the compiler has no way to know that you are intending to create Stats objects using only numeric types. Thus, when you try to compile Stats, an error is reported that indicates that the doubleValue( ) method is unknown. To solve this problem, you need some way to tell the compiler that you intend to pass only numeric types to T. Furthermore, you need some way to ensure that only numeric types are actually passed. To handle such situations, Java provides bounded types. When specifying a type parameter, you can create an upper bound that declares the superclass from which all type arguments must be derived. This is accomplished through the use of an extends clause when specifying the type parameter, as shown here: <T extends superclass> This specifies that T can only be replaced by superclass, or subclasses of superclass. Thus, superclass defines an inclusive, upper limit. You can use an upper bound to fix the Stats class shown earlier by specifying Number as an upper bound, as shown here: // In this version of Stats, the type argument for // T must be either Number, or a class derived // from Number"
Explain why a lambda expression results in a form of anonymous class,"A lambda expression is, essentially, an anonymous (that is, unnamed) method. However, this method is not executed on its own. Instead, it is used to implement a method defined by a functional interface. Hence, a lambda expression results in a form of anonymous class."," Introducing Lambda Expressions Key to understanding Javas implementation of lambda expressions are two constructs. The first is the lambda expression, itself. The second is the functional interface. Lets begin with a simple definition of each. A lambda expression is, essentially, an anonymous (that is, unnamed) method. However, this method is not executed on its own. Instead, it is used to implement a method defined by a functional interface. Thus, a lambda expression results in a form of anonymous class. Lambda expressions are also commonly referred to as closures"
Explain why a lambda expression gives us a way to transform a code segment into an object,"When a lambda expression occurs in a target type context, an instance of a class is automatically created that implements the functional interface, with the lambda expression defining the behavior of the abstract method declared by the functional interface. When that method is called through the target, the lambda expression is executed. Hence, a lambda expression gives us a way to transform a code segment into an object."," MyNumber myNum; Next, a lambda expression is assigned to that interface reference: // Use a lambda in an assignment context. myNum = () -> 123.45; When a lambda expression occurs in a target type context, an instance of a class is automatically created that implements the functional interface, with the lambda expression defining the behavior of the abstract method declared by the functional interface. When that method is called through the target, the lambda expression is executed. Thus, a lambda expression gives us a way to transform a code segment into an object. 21/09/21 5:47 PM 394PART I In the preceding example, the lambda expression becomes the implementation for the getValue( ) method. As a result, the following displays the value 123.45: // Call getValue(), which is implemented by the previously assigned // lambda expression"
Explain why a lambda expression cannot be generic," Generic Functional Interfaces A lambda expression, itself, cannot specify type parameters. Hence, a lambda expression cannot be generic."," StringFunc reverse = (str) -> { String result = """"; int i; for(i = str.length()-1; i >= 0; i--) result += str.charAt(i); return result; }; System.out.println(""Lambda reversed is "" + reverse.func(""Lambda"")); System.out.println(""Expression reversed is "" + reverse.func(""Expression"")); } } The output is shown here: Lambda reversed is adbmaL Expression reversed is noisserpxE In this example, the functional interface StringFunc declares the func( ) method. This method takes a parameter of type String and has a return type of String. Thus, in the reverse lambda expression, the type of str is inferred to be String. Notice that the charAt( ) method is called on str. This is legal because of the inference that str is of type String. Generic Functional Interfaces A lambda expression, itself, cannot specify type parameters. Thus, a lambda expression cannot be generic. (Of course, because of type inference, all lambda expressions exhibit some generic-like qualities.) However, the functional interface associated with a lambda expression can be generic. In this case, the target type of the lambda expression is determined, in part, by the type argument or arguments specified when a functional interface reference is declared. 21/09/21 5:47 PM 400PART I To understand the value of generic functional interfaces, consider this. The two examples in the previous section used two different functional interfaces, one called NumericFunc and the other called StringFunc. However, both defined a method called func( ) that took one parameter and returned a result. In the first case, the type of the parameter and return type was int. In the second case, the parameter and return type was String. Thus, the only difference between the two methods was the type of data they required. Instead of having two functional interfaces whose methods differ only in their data types, it is possible to declare one generic interface that can be used to handle both circumstances. The following program shows this approach: // Use a generic functional interface with lambda expressions"
Explain why a module may have several exports statements,"Here, packageName specifies the name of the package that is exported by the module in which this statement occurs. A module can export as many packages as needed, with each one specified in a separate exports statement. Hence, a module may have several exports statements."," Here is the form of the requires statement used in the example: requires moduleName; Here, moduleName specifies the name of a module that is required by the module in which the requires statement occurs. This means that the required module must be present in order for the current module to compile. In the language of modules, the current module is said to read the module specified in the requires statement. When more than one module is required, it must be specified in its own requires statement. Thus, a module declaration may include several different requires statements. In general, the requires statement gives you a way to ensure that your program has access to the modules that it needs. Here is the general form of the exports statement used in the example: exports packageName; Here, packageName specifies the name of the package that is exported by the module in which this statement occurs. A module can export as many packages as needed, with each one specified in a separate exports statement. Thus, a module may have several exports statements. When a module exports a package, it makes all of the public and protected types in the package accessible to other modules. Furthermore, the public and protected members of those types are also accessible. However, if a package within a module is not exported, then it is private to that module, including all of its public types. For example, even though a class is declared as public within a package, if that package is not explicitly exported by an exports statement, then that class is not accessible to other modules. It is important to understand that the public and protected types of a package, whether exported or not, are always accessible within that packages module. The exports statement simply makes them accessible to outside modules. Thus, any nonexported package is only for the internal use of its module"
Explain why any nonexported package is only for the internal use of its module,"When a module exports a package, it makes all of the public and protected types in the package accessible to other modules. Furthermore, the public and protected members of those types are also accessible. However, if a package within a module is not exported, then it is private to that module, including all of its public types. For example, even though a class is declared as public within a package, if that package is not explicitly exported by an exports statement, then that class is not accessible to other modules. It is important to understand that the public and protected types of a package, whether exported or not, are always accessible within that packages module. The exports statement simply makes them accessible to outside modules. Hence, any nonexported package is only for the internal use of its module."," Here is the general form of the exports statement used in the example: exports packageName; Here, packageName specifies the name of the package that is exported by the module in which this statement occurs. A module can export as many packages as needed, with each one specified in a separate exports statement. Thus, a module may have several exports statements. When a module exports a package, it makes all of the public and protected types in the package accessible to other modules. Furthermore, the public and protected members of those types are also accessible. However, if a package within a module is not exported, then it is private to that module, including all of its public types. For example, even though a class is declared as public within a package, if that package is not explicitly exported by an exports statement, then that class is not accessible to other modules. It is important to understand that the public and protected types of a package, whether exported or not, are always accessible within that packages module. The exports statement simply makes them accessible to outside modules. Thus, any nonexported package is only for the internal use of its module. The key to understanding requires and exports is that they work together. If one module depends on another, then it must specify that dependence with requires. The module on which another depends must explicitly export (i.e., make accessible) the packages that the dependent module needs. If either side of this dependence relationship is missing, the dependent module will not compile. As it relates to the foregoing example, MyModAppDemo uses the functions in SimpleMathFuncs. As a result, the appstart module declaration contains a requires statement that names the appfuncs module. The appfuncs module declaration exports the appfuncs.simplefuncs package, thus making the public types in the SimpleMathFuncs class available. Since both sides of the dependence relationship have been fulfilled, the application can compile and run. If either is missing, the compilation will fail"
"Explain how when a program does not use modules, all API modules in the Java platform are automatically accessible through the unnamed module","When you use code that is not part of a named module, it automatically becomes part of the unnamed module. The unnamed module has two important attributes. First, all of the packages in the unnamed module are automatically exported. Second, the unnamed module can access any and all other modules. Hence, when a program does not use modules, all API modules in the Java platform are automatically accessible through the unnamed module."," Support for legacy code is provided by two key features. The first is the unnamed module. When you use code that is not part of a named module, it automatically becomes part of the unnamed module. The unnamed module has two important attributes. First, all of the packages in the unnamed module are automatically exported. Second, the unnamed module can access any and all other modules. Thus, when a program does not use modules, all API modules in the Java platform are automatically accessible through the unnamed module. The second key feature that supports legacy code is the automatic use of the class path, rather than the module path. When you compile a program that does not use modules, the class path mechanism is employed, just as it has been since Javas original release. As a result, the program is compiled and run in the same way it was prior to the advent of modules"
Explain why the open modifier affects only run-time accessibility,"In an open module, types in all its packages are accessible at run time. Understand, however, that only those packages that are explicitly exported are available at compile time. Hence, the open modifier affects only run-time accessibility."," Three Specialized Module Features The preceding discussions have described the key features of modules supported by the Java language, and they are the features on which you will typically rely when creating your own modules. However, there are three additional module-related features that can be quite useful in certain circumstances. These are the open module, the opens statement, and the use of requires static. Each of these features is designed to handle a specialized situation, and each constitutes a fairly advanced aspect of the module system. That said, it is important for all Java programmers to have a general understanding of their purpose. Open Modules As you learned earlier in this chapter, by default, the types in a modules packages are accessible only if they are explicitly exported via an exports statement. While this is usually what you will want, there can be circumstances in which it is useful to enable run-time access to all packages in the module, whether a package is exported or not. To allow this, you can create an open module. An open module is declared by preceding the module keyword with the open modifier, as shown here: open module moduleName { // module definition } In an open module, types in all its packages are accessible at run time. Understand, however, that only those packages that are explicitly exported are available at compile time. Thus, the open modifier affects only run-time accessibility. The primary reason for an open module is to enable the packages in the module to be accessed through reflection. As explained in The opens Statement It is possible for a module to open a specific package for run-time access by other modules and for reflective access rather than opening an entire module. To do so, use the opens statement, shown here: opens packageName; Here, packageName specifies the package to open. It is also possible to include a to clause, which names those modules for which the package is opened. It is important to understand opens does not grant compile-time access. It is used only to open a package for run-time and reflective access. However, you can both export and open a module. One other point: an opens statement cannot be used in an open module. Remember, all packages in an open module are already open"
Explain how sealing a class or interface gives you fine-grained control over its inheritance and implementation,"It is now possible to specify a sealed class or interface. A sealed class can be inherited by only explicitly specified subclasses. A sealed interface can be implemented by only explicitly specified classes or extended by only explicitly specified interfaces. Hence, sealing a class or interface gives you fine-grained control over its inheritance and implementation."," 22/09/21 6:37 PM CHAPTER Switch Expressions, Records, and Other Recently Added Features A key attribute of Java has been its ability to adapt to the increasing demands of the modern computing environment. Over the years, Java has incorporated many new features, each responding to changes in the computing environment or to innovations in computer language design. This ongoing process has enabled Java to remain one of the worlds most important and popular computer languages. As explained earlier, this book has been updated for JDK 17, which is a long-term support (LTS) version of Java. JDK 17 incorporates a number of new language features that have been added to Java since the previous LTS version, which was JDK 11. A few of the smaller additions have been described in the preceding chapters. In this chapter, several major additions are examined. They are Enhancements to switch Text blocks Records Patterns in instanceof Sealed classes and interfaces Here is a brief description of each. The switch has been enhanced in a number of ways, the most impacting of which is the switch expression. A switch expression enables a switch to produce a value. Text blocks allow a string literal to occupy more than a single line. Supported by the new keyword record, records enable you to create a class that is specifically designed to hold a group of values. A second form of instanceof has been added that uses a type pattern. With this form, you can specify a variable that receives an instance of the type being tested if instanceof succeeds. It is now possible to specify a sealed class or interface. A sealed class can be inherited by only explicitly specified subclasses. A sealed interface can be implemented by only explicitly specified classes or extended by only explicitly specified interfaces. Thus, sealing a class or interface gives you fine-grained control over its inheritance and implementation. 17-ch17.indd 449 CompRef_2010 / Java: The Complete Reference, Twelfth Edition / Schildt / 126046-341-9 The Java Language Enhancements to switch The switch statement has been part of Java since the start. It is a crucial element of Javas program control statements and provides for a multiway branch. Moreover, switch is so fundamental to programming that it is found in one form or another in other popular programming languages. The traditional form of switch was described in Chapter 5. This is the form of switch that has always been part of Java. Beginning with JDK 14, switch has been substantially enhanced with the addition of four new features, shown here: The switch expression The yield statement Support for a list of case constants The case with an arrow Each new feature is examined in detail in the discussions that follow, but here is a brief description: The switch expression is, essentially, a switch that produces a value. Thus, a switch expression can be used on the right side of an assignment, for example. The yield statement specifies a value that is produced by a switch expression. It is now possible to have more than one case constant in a case statement through the use of a list of constants"
Explain why a record offers a convenient way for a switch to yield more than a single value,"Java provides an especially streamlined and efficient way to accomplish this: the record. Described later in this chapter, a record aggregates two or more values into a single logical unit. As it relates to this example, a record could hold both the priorityLevel and the stopNow values, and this record could be yielded by the switch as a unit. Hence, a record offers a convenient way for a switch to yield more than a single value."," As this example shows, when using a block, you must use yield to supply a value to a switch expression. Furthermore, even though block targets are used, each path through the switch expression must still provide a value. Although the preceding program provides a simple illustration of a block target of an arrow case, it also raises an interesting question. Notice that each case in the switch sets the value of two variables. The first is priorityLevel, which is the value yielded. The second is stopNow. Is there a way for a switch expression to yield more than one value? In a direct sense, the answer is no because only one value can be produced by the switch. However, it is possible to encapsulate two or more values within a class and yield an object of that class. Beginning with JDK 16, Java provides an especially streamlined and efficient way to accomplish this: the record. Described later in this chapter, a record aggregates two or more values into a single logical unit. As it relates to this example, a record could hold both the priorityLevel and the stopNow values, and this record could be yielded by the switch as a unit. Thus, a record offers a convenient way for a switch to yield more than a single value. Although the arrow case is very helpful in a switch expression, it is important to emphasize that it is not limited to that use. The arrow case can also be used in a switch statement, which enables you to write switches in which no case fall-through can occur. In this situation, no yield statement is required (or allowed), and no value is produced by the switch. In essence, it works much like a traditional switch but without the fall-through"
Explain why the data that a record holds is immutable," A record is instantiated by use of new, just the way you create an instance of a class. A key point about a record is that its data is held in private final fields and only getter methods are provided. Hence, the data that a record holds is immutable."," Another element created automatically by the compiler will be the records canonical constructor. This constructor has a parameter list that contains the same elements, in the same order, as the component list in the record declaration. The values passed to the constructor are automatically assigned to the corresponding fields in the record. In a record, the canonical constructor takes the place of the default constructor used by a class. A record is instantiated by use of new, just the way you create an instance of a class. For example, this creates a new Employee object, with the name ""Doe, John"" and ID number 1047: After this declaration executes, the private fields name and idNum for emp will contain the values ""Doe, John"" and 1047, respectively. Therefore, you can use the following statement to display the information associated with emp: System.out.println(""The employee ID for "" + emp.name() + "" is "" + emp.idNum()); The resulting output is shown here: The employee ID for Doe, John is 1047 A key point about a record is that its data is held in private final fields and only getter methods are provided. Thus, the data that a record holds is immutable. In other words, once you construct a record, its contents cannot be changed. However, if a record holds a reference to some object, you can make a change to that object, but you cannot change to what object the reference in the record refers. Thus, in Java terms, records are said to be shallowly immutable. The following program puts the preceding discussion into action. Records are often used as elements in a list. For example, a business might maintain a list of Employee records to store an employees name along with his or her corresponding ID number. The following program shows a simple example of such usage. It creates a small array of Employee records"
"Explain why in Java terms, records are said to be shallowly immutable","A key point about a record is that its data is held in private final fields and only getter methods are provided. Thus, the data that a record holds is immutable. In other words, once you construct a record, its contents cannot be changed. However, if a record holds a reference to some object, you can make a change to that object, but you cannot change to what object the reference in the record refers. Hence, in Java terms, records are said to be shallowly immutable."," Another element created automatically by the compiler will be the records canonical constructor. This constructor has a parameter list that contains the same elements, in the same order, as the component list in the record declaration. The values passed to the constructor are automatically assigned to the corresponding fields in the record. In a record, the canonical constructor takes the place of the default constructor used by a class. A record is instantiated by use of new, just the way you create an instance of a class. For example, this creates a new Employee object, with the name ""Doe, John"" and ID number 1047: After this declaration executes, the private fields name and idNum for emp will contain the values ""Doe, John"" and 1047, respectively. Therefore, you can use the following statement to display the information associated with emp: System.out.println(""The employee ID for "" + emp.name() + "" is "" + emp.idNum()); The resulting output is shown here: The employee ID for Doe, John is 1047 A key point about a record is that its data is held in private final fields and only getter methods are provided. Thus, the data that a record holds is immutable. In other words, once you construct a record, its contents cannot be changed. However, if a record holds a reference to some object, you can make a change to that object, but you cannot change to what object the reference in the record refers. Thus, in Java terms, records are said to be shallowly immutable. The following program puts the preceding discussion into action. Records are often used as elements in a list. For example, a business might maintain a list of Employee records to store an employees name along with his or her corresponding ID number. The following program shows a simple example of such usage. It creates a small array of Employee records"
Explain why all record declarations are considered final,"First, a record cannot inherit another class. However, all records implicitly inherit java.lang .Record, which specifies abstract overrides of the equals( ), hashCode( ), and toString( ) methods declared by Object. Implicit implementations of these methods are automatically created, based on the record declaration. A record type cannot be extended. Hence, all record declarations are considered final."," for(Employee e: empList) System.out.println(""The employee ID for "" + e.name() + "" is "" + e.idNum()); } } The output is shown here: The The The The employee employee employee employee ID ID ID ID for for for for Doe, John is 1047 Jones, Robert is 1048 Smith, Rachel is 1049 Martin, Dave is 1050 Before continuing, it is important to mention some key points related to records. First, a record cannot inherit another class. However, all records implicitly inherit java.lang .Record, which specifies abstract overrides of the equals( ), hashCode( ), and toString( ) methods declared by Object. Implicit implementations of these methods are automatically created, based on the record declaration. A record type cannot be extended. Thus, all record declarations are considered final. Although a record cannot extend another class, it can implement one or more interfaces. With the exception of equals, you cannot use the names of methods defined by Object as names for a records components. Aside from the fields associated with a records components, any other fields must be static. Finally, a record can be generic. Create Record Constructors Although you will often find that the automatically supplied canonical constructor is precisely what you want, you can also declare one or more of your own constructors. You can also define your own implementation of the canonical constructor. You might want to declare a record constructor for a number of reasons. For example, the constructor could check that a value is within a required range, verify that a value is in the proper format, ensure that an object implements optional functionality, or confirm that an argument is not null. For a record, there are two general types of constructors that you can explicitly create: canonical and non-canonical, and there are some differences between the two. The creation of each type is examined here, beginning with defining your own implementation of the canonical constructor"
"Explain why if the access modifier for the record is public, the constructor must also be specified public"," To define your own implementation of a canonical constructor, simply do so as you would with any other constructor, specifying the records name and its parameter list. It is important to emphasize that for the canonical constructor, the types and parameter names must be the same as those specified by the record declaration. This is because the parameter names are linked to the automatically created fields and accessor methods defined by the record declaration. Thus, they must agree in both type and name. Furthermore, each component must be fully initialized upon completion of the constructor. The following restrictions also apply: the constructor must be at least as accessible as its record declaration Hence, if the access modifier for the record is public, the constructor must also be specified public."," 21/09/21 5:48 PM Although the canonical constructor has a specific, predefined form, there are two ways that you can code your own implementation. First, you can explicitly declare the full form of the canonical constructor. Second, you can use what is called a compact record constructor. Each approach is examined here, beginning with the full form. To define your own implementation of a canonical constructor, simply do so as you would with any other constructor, specifying the records name and its parameter list. It is important to emphasize that for the canonical constructor, the types and parameter names must be the same as those specified by the record declaration. This is because the parameter names are linked to the automatically created fields and accessor methods defined by the record declaration. Thus, they must agree in both type and name. Furthermore, each component must be fully initialized upon completion of the constructor. The following restrictions also apply: the constructor must be at least as accessible as its record declaration Thus, if the access modifier for the record is public, the constructor must also be specified public. A constructor cannot be generic, and it cannot include a throws clause. It also cannot invoke another constructor defined for the record. Here is an example of the Employee record that explicitly defines the canonical constructor"
Explain why a getter that returns an altered value is semantically questionable (and should be avoided) even though such code would be syntactically correct,"Although it is seldom necessary, it is possible to create your own implementation of a getter method. When you declare the getter, the implicit version is no longer supplied. One possible reason you might want to declare your own getter is to throw an exception if some condition is not met. For example, if a record holds a filename and a URL, the getter for the filename might throw a FileNotFoundException if the file is not present at the URL. There is a very important requirement, however, that applies to creating your getters: they must adhere to the principle that a record is immutable. Hence, a getter that returns an altered value is semantically questionable (and should be avoided) even though such code would be syntactically correct."," empList[1] = new Employee(""Jones, ,Robert"", 1048); // Missing last name. empList[1] = new Employee("", Robert"", 1048); As an aside, you might find it interesting to think of ways that you can improve the ability of the constructor to verify that the name uses the proper format. For example, you might want to explore an approach that uses a regular expression. (See Chapter 31.) 21/09/21 5:48 PM 472PART I Create Record Getter Methods Although it is seldom necessary, it is possible to create your own implementation of a getter method. When you declare the getter, the implicit version is no longer supplied. One possible reason you might want to declare your own getter is to throw an exception if some condition is not met. For example, if a record holds a filename and a URL, the getter for the filename might throw a FileNotFoundException if the file is not present at the URL. There is a very important requirement, however, that applies to creating your getters: they must adhere to the principle that a record is immutable. Thus, a getter that returns an altered value is semantically questionable (and should be avoided) even though such code would be syntactically correct. If you do declare a getter implementation, there are a number of rules that apply. A getter must have the same return type and name as the component that it obtains. It must also be explicitly declared public. (Thus, default accessibility is not sufficient for a getter declaration in a record.) No throws clause is allowed in a getter declaration. Finally, a getter must be non-generic and non-static"
Explain why default accessibility is not sufficient for a getter declaration in a record,"If you do declare a getter implementation, there are a number of rules that apply. A getter must have the same return type and name as the component that it obtains. It must also be explicitly declared public. (Hence, default accessibility is not sufficient for a getter declaration in a record."," empList[1] = new Employee("", Robert"", 1048); As an aside, you might find it interesting to think of ways that you can improve the ability of the constructor to verify that the name uses the proper format. For example, you might want to explore an approach that uses a regular expression. (See Chapter 31.) 21/09/21 5:48 PM 472PART I Create Record Getter Methods Although it is seldom necessary, it is possible to create your own implementation of a getter method. When you declare the getter, the implicit version is no longer supplied. One possible reason you might want to declare your own getter is to throw an exception if some condition is not met. For example, if a record holds a filename and a URL, the getter for the filename might throw a FileNotFoundException if the file is not present at the URL. There is a very important requirement, however, that applies to creating your getters: they must adhere to the principle that a record is immutable. Thus, a getter that returns an altered value is semantically questionable (and should be avoided) even though such code would be syntactically correct. If you do declare a getter implementation, there are a number of rules that apply. A getter must have the same return type and name as the component that it obtains. It must also be explicitly declared public. (Thus, default accessibility is not sufficient for a getter declaration in a record.) No throws clause is allowed in a getter declaration. Finally, a getter must be non-generic and non-static. A better alternative to overriding a getter in cases in which you want to obtain a modified value of a component is to declare a separate method with its own name. For example, assuming the Employee record, you might want to obtain only the last name from the name component. Using the standard getter to do this would entail modifying the value obtained from the component. Doing this is a bad idea because it would violate the immutability aspect of the record. However, you could declare another method, called lastName( ), that returns only the last name. The following program demonstrates this approach. It also uses the format-checking constructor from the previous section to ensure that names are stored as lastname, firstname"
"Explain why if the right-side operand is evaluated, iObj will be in scope"," } The iObj pattern variable is created only if the left side of the && (the part that contains the instanceof operator) is true. Notice that iObj is also used by the right side. This is possible 21/09/21 5:48 PM Switch Expressions, Records, and Other Recently Added Features because the short-circuit form of the AND logical operator is used, and the right side is evaluated only if the left succeeds. Hence, if the right-side operand is evaluated, iObj will be in scope."," // ... } The iObj pattern variable is created only if the left side of the && (the part that contains the instanceof operator) is true. Notice that iObj is also used by the right side. This is possible 21/09/21 5:48 PM Switch Expressions, Records, and Other Recently Added Features because the short-circuit form of the AND logical operator is used, and the right side is evaluated only if the left succeeds. Thus, if the right-side operand is evaluated, iObj will be in scope. However, if you tried to write the preceding statement using the & operator like this: if((myOb instanceof Integer iObj) & (iObj >= 0)) { // Error! // myOb is both an Integer and nonnegative. // .."
explain homeschooling,"homeschooling is legal in many countries.in some other countries, while not restricted by law, homeschooling is not socially acceptable, or is considered undesirable, and is virtually non-existent.","homeschooling is legal in many countries. countries with the most prevalent homeschooling movements include australia, canada, new zealand, the united kingdom, and the united states. some countries have highly regulated homeschooling programs as an extension of the compulsory school system; few others, such as germany, have outlawed it entirely. in some other countries, while not restricted by law, homeschooling is not socially acceptable, or is considered undesirable, and is virtually non-existent."